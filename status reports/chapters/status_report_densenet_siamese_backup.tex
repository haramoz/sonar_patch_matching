%!TEX root = ../report.tex

\chapter{Introduction}

\section{Densenet}

\section{Status}
\begin{itemize}
 \item Coarse grid search with flatten -- Done
 \item Avg vs flatten pooling -- to analyze
 \item Growth rate analysis -- to analyze
 \item Std per dense block -- to analyze
 \item total number of parameters -- to analyze
 \item Finer grid search -- Next
 \item Compression and reduction -- to analyze
 \item Dropout -- to analyze
 \item Monte Carlo dropout
 \item Nb\_filter analysis --running
 \item Batch size -- to analyze
 \item optimizer and learning rate -- running only adadelta with various learning rates, takes too long otherwise. Even for that evalating for lower learning rates does not make sense so they are being run highest to lowest, 
 can be stopped from evaluating if the auc dips as expected.
 \item FINAL grid search -- TBD
\end{itemize}


In Densenet each layer connects to every layer in a feed-forward fashion. 
With the basic idea to enhance the feature propagation, each layer of Densenet blocks takes the feature-maps of the previous stages as input.  
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/densenet/densenet.png}
\caption{\label{fig:densenet}Densenet structure.}
\end{figure}
\flushbottom
\newpage

\section{Hyper-parameters}
Hyper-parameters are those parameters whose values are set before the training, unlike other parameters whose values are learned during the training. 
For example learning rate, batch size. \cite{wikihyper}
\subsection{Parameters of Densenet}
\begin{itemize}
  \item \textbf{Growth rate}: Number of filters to add per dense block. Growth rate regulates how much information is contributed by each layers to the global state. 
  Global state is the collective knowledge of the network, that is the state of the previous layers are flown into each layer in form of feature-maps, 
  which is considered as global state. Each layer adds k more feature-maps to the current state, when growth rate is k.

  \item \textbf{Nb\_filter}: initial number of filters. -1 indicates initial number of filters will default to 2 * growth\_rate.

  \item \textbf{Nb\_layers\_per\_block}: number of layers in each dense block. Can be a -1, positive integer or a list. If -1, calculates nb\_layer\_per\_block from the network depth. 
  If positive integer, a set number of layers per dense block. If list, nb\_layer is used as provided. Note that list size must be nb\_dense\_block.

  \item \textbf{Depth}: number or layers in the DenseNet.

  \item \textbf{Nb\_dense\_block}: number of dense blocks to add to end.

  \item \textbf{Bottleneck Layers}: To improve computation efficiency a bottleneck layer with 1x1 convolution is introduced before each 3x3 convolution layers. 
  \item \textbf{Compression}: Reduces the feature maps in transition layers and makes the model more compact and computationally efficient.
\end{itemize}



\subsection{Other hyper-parameters}
Apart from the densenet there are other general hyper-parameters such as learning rate, batch size for training, optimizer. 


\subsection{Grid search strategy}
Since there are lot of parameters, practically, infinite test cases might be designed. To keep the grid search focused and less computationally expensive it makes sense to 
first search for coarser grid of parameters rather than very fine ones. When and if a bracket of parameters are shortlisted which works better than others, the finer parameter
search will be performed only specific to those range of parameters and not the whole grid.

\section{Results} 

\subsection{Coarse grid search on Densenet hyper-parameters}
For the estimation of the best performing parameters of Densenet for the branches of the Siamese, first a coarse grid search is been performed with 
\begin{itemize}
 \item Layers per block are chosen among 2,3,4,5,6. For single dense block evaluation goes up-to 12 layers. 
 \item Each network has been evaluated for growth rates of 6,12,18,24,30,36. 
 \item Different dense block sizes of 1,2,3,4.  
 \item Not all the possible combinations has been evaluated as quite extensive as possible
 \item The basic idea here is to narrow down the possible network sizes from the Densenet parameters perspective. There are other parameters but number of dense block, 
 growth rate and layers per block are three main parameters which controls the architecture/size of the network the most.
 \item The parameters compression/reduction and bottleneck are set 0.5 and True respectively because both this parameters controls the compactness of the model and help 
 reducing the parameter required, hence in theory enabling us to evaluate much bigger networks. So the network that is being evaluated here are named Densenet-BC by authors, 
 I.e Densenet with bottleneck and compression.
 \item For more fine grained analysis the compression and bottleneck parameters will also be evaluated. 
 \item nb\_filter values are fixed at 16 for this test
 \item The parameter classes are set to 2, which represents 1 for matching and 0 for not-matching pairs
 \item 96,96 is the input image dimensions. And it’s two channel. So depending on local setting of the keras, “channel-first” or “channel-last” suitable input\_shape is 
 chosen automatically as  2,96,96  or 96,96,2 respectively.
 \item The learning rate used for the test was 2E-4
 \item Dropout for Densenet used as 0.2 to handle over-fitting.  
 \item Epochs are different for different architectures to ensure that the networks are able to train decently %95\% training accuracy. 
 \item From the keras side of the parameters, after concatenating the Densenet branch output feature maps, the “combined features” then passed through a fully-connected  
 layer of 512 filter size, which is followed by Relu activation and Batchnormalization(BN)  and then a dropout(0.5) has been added to ensure better generalization. 
 \item Some of this values needs to be further evaluated as well, how ever current values were obtained using some of manual tuning and assumed to be a decent starting point.
 \item Flatten is used as pooling at the end of the Densenet, it is introduced by this work which replaces the Globalaveragepooling step with a flatten. This causes increase 
 in parameters overall though.
 \item Binary\_crossentropy loss function with sigmoid activation function used for the binary classification, this final layer acts as the binary classifier.
 \item In all the cases the networks are being trained from scratch. The weights = None ensures that no previously trained weights are used.
\end{itemize}

\subsection{Coarse grid search parameters summary}
\subsubsection{Fixed hyper-parameters}
\begin{itemize}
 \item Nb\_filter: 16
 \item Subsample initial block: True
 \item Weights : None
 \item Dropout rate : 0.2
 \item Include top : True
 \item Compression : 0.5
 \item Bottleneck : False
 \item Transition pooling : max
\end{itemize}

\subsubsection{Varying Hyper-parameters}
\begin{itemize}
 \item \textbf{Nb\_layers\_per\_block}:
 \begin{itemize}
  \item \textbf{One dense block architecture (nb\_dense\_block=1):}\\
  '2', '4', '6', '8', '10', '12'
  \item \textbf{Two dense block architecture (nb\_dense\_block=2):}\\
  '2-2', '4-4', '6-6'
  \item \textbf{Three dense block architecture (nb\_dense\_block=3):}\\
  '2-2-2', '4-4-4', '6-6-6'
  \item \textbf{Four dense block architecture (nb\_dense\_block=4):}\\
  '2-2-2-2', '4-4-4-4', '6-6-6-6'
 \end{itemize}
 \item \textbf{Growth rate}:
  \begin{itemize}
  \item Thin layers:
    \begin{itemize} 
    \item 12
    \item 18
  \end{itemize}
  \item Thick layers:
    \begin{itemize} 
    \item 24
    \item 30
  \end{itemize}
 \end{itemize}
 \item \textbf{Pooling}:
 \begin{itemize}
  \item Flatten
  \item Average (avg)
 \end{itemize}  
 
\end{itemize}
\flushbottom
\newpage

\subsection{AUC analysis}
  

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/densenet/auc_analysis_sortedby_growth.png}
\caption{\label{fig:meanAUC_growth}AUC analysis sorted by number of growth rate.}
\end{figure}

\subsubsection{Conclusion}
\begin{itemize}
 \item It is clear that 4 blocks and 3 blocks network is performing better than the 1 and 2 blocks network
 \item Original paper also used 3 and 4 blocks densenet, so shall we
\end{itemize}

\subsection{Optimal growth rate analysis}
It is also important to find out the best growth rate for each of the architectures(layers per block). For this purpose, the growth rate with mean AUC of test data in 10 trials has been selected,
the Best growth rates across architectures are displayed in the graph below: 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{images/densenet/best_growthrate_analysis.png}
\caption{\label{fig:best_growthrate}Architecture vs growth rate based on mean AUC.}
\end{figure}

%But it is observed that the architectures with max AUC in 5 evaluations on test data. might consist of different growth rates than the one that displays best mean AUC. For this purpose, histogram of 
%best performing growth rates obtained from mean and max AUC analysis is displayed in the figure below:

%\begin{figure}
%    \centering
%    \begin{subfigure}[b]{0.4\textwidth}
%        \includegraphics[width=\textwidth]{images/densenet/performances_of_growth_rates_mean_AUC}
%        \caption{Histogram of growth rates depending on mean AUC}
%        \label{fig:mean_auc_histogram}
%    \end{subfigure}
%    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%      %(or a blank line to force the subfigure onto a new line)
%    \begin{subfigure}[b]{0.4\textwidth}
%        \includegraphics[width=\textwidth]{images/densenet/performances_of_growth_rates_max_AUC}
%        \caption{Histogram of growth rates depending on max AUC}
%       \label{fig:max_auc_histogram}
%    \end{subfigure}    
%    \caption{Cumulative growth rate analysis (histograms)}\label{fig:growthrate_histogram}
%\end{figure}

\subsubsection{Conclusion}
\begin{itemize}
 \item From the previous study it was concluded that 6,36 growth rates are either too thin or too high so for this test we are not evaluating them.
\end{itemize}

\subsection{Total parameters analysis}
\begin{itemize}
 \item It might be surprising but the single dense block networks have most parameters. This is also because of the flatten pooling that is used here in this work instead of global average pooling 2D. 
 \item And four block networks have the least total and trainable parameters. This is also supported by the theory in the paper
 \item How ever as the number of dense blocks keeps getting higher the non-trainable parameters also gets higher. So 4 blocks dense net has most number of non-trainable parameters.
 \item For the comparison 2, 2-2, 2-2-2, 2-2-2-2 blocks parameter sizes are compared below, all recorded for growth rate 18.
 \item FLATTEN VS AVG POOLING and effect
\end{itemize}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[height=5cm]{images/densenet/Denseblocks_vs_parameters}
        \caption{Parameters vs architectures}
        \label{fig:Denseblocks_vs_parameters}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[height=5cm]{images/densenet/growthrate_vs_parameters}
        \caption{Parameters vs growth rates}
        \label{fig:growthrate_vs_parameters}
    \end{subfigure}    
    \caption{Total number of parameters analysis}\label{fig:total_parameters_densenet}
\end{figure}

\subsection{Pooling}
\begin{itemize}
 \item 
\end{itemize}


\subsection{Standard deviation across blocks}
Another trend was observed that the standard deviation varies more as the number of dense blocks increases. 
So the std values for all the readings are collected for 1,2,3,4 number of dense blocks (nb\_dense\_blocks) separately and their mean values are presented in the table below.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{images/densenet/std_vs_denseblocks.png}
\caption{\label{fig:std_vs_denseblocks} Mean Standard deviation vs number of dense blocks.}
\end{figure}

\begin{itemize}
 \item 
\end{itemize}

\subsection{Finer grid search analysis}
\begin{itemize}
 \item More detailed analysis for the architectures with different layers per block combination. Specially closer to the highest performing networks.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[height=5cm,width=0.7\textwidth]{images/densenet/arch_vs_auc_finer}
\caption{Finer search with top 10 architectures}
\label{fig:arch_vs_auc_finer}
\end{figure}

\subsubsection{Conclusion}
\begin{itemize}
 \item 
\end{itemize}

\subsection{Nb\_filter analysis}
Initial number of filters. 8,16,32,64 are being evaluated here. Also a comparison between mean AUCs obtained from growth rate 18 and 30 is done under this analysis. Ten evaluations of each test cases has been done.
\begin{figure}[h]
\centering
\includegraphics[height=4cm,width=0.5\textwidth]{images/densenet/nb_filter_vs_auc}
\caption{Nb\_filter vs mean AUC for growth rate 30}
\label{fig:nb_filter_vs_auc}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[height=5cm,width=0.5\textwidth]{images/densenet/nb_filter_vs_auc_growthrate_compare}
\caption{Nb\_filter (different growth rates)}
\label{fig:nb_filter_vs_auc_growthrate_compare}
\end{figure}

\subsubsection{Conclusion}

\subsection{Dropout analysis}
Densenet dropout: 10 evaluations each \\


\begin{figure}[h]
\centering
\includegraphics[height=5cm,width=0.5\textwidth]{images/densenet/auc_vs_densenet_dropout}
\caption{Densenet dropout analysis}
\label{fig:auc_vs_densenet_dropout}
\end{figure}
Choosing 0.4 as the dropout rate value for future evaluations.

\subsection{Bottleneck and Compression(also called reduction) analysis}

\begin{figure}[h]
\centering
\includegraphics[height=5cm,width=0.6\textwidth]{images/densenet/compression_and_bottleneck_vs_auc}
\caption{Evaluation of compression and bottleneck and mean AUC(across 10 trials). The max AUC obtained by one of the trial is also displayed separately for experiment with bottleneck and without bottleneck.}
\label{fig:compression_and_bottleneck_vs_auc}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[height=5cm,width=0.5\textwidth]{images/densenet/compression_and_bn_vs_parameters}
\caption{The total parameters are varying with reduction. It also gets affected by use of bottleneck or not, how ever that is very minimal.}
\label{fig:compression_and_bn_vs_parameters}
\end{figure}

\subsubsection{Conclusion}


%\flushbottom
%\newpage

\subsection{Batch size analysis}
If the batch size is too low then it takes more time and after a certain size it does not train well too.\\
If the batch size is very big then it may train faster but they generalize lesser as they tend to converge to sharp minimizers of the training function.
TODO add source (https://arxiv.org/abs/1609.04836) 
\begin{figure}[h]
\centering
\includegraphics[height=5cm,width=0.5\textwidth]{images/densenet/batch_size}
\caption{Batch size analysis}
\label{fig:batch_size}
\end{figure}

\subsubsection{Conclusion}
From the figure \ref{fig:batch_size} it is evident that the larger the batch size gets the generalization on the test data gets worse. Batch size 32 or 64 looks very close so may be both can be top picks.
The max AUC score for batch size 64 is much higher than others so it is slight favorite though. 


\subsection{Learning rate and optimizer analysis}
\begin{figure}[htp]
\centering
\includegraphics[width=.4\textwidth]{images/densenet/adam_lr}\quad
\includegraphics[width=.4\textwidth]{images/densenet/nadam_lr}

\medskip
\includegraphics[width=.4\textwidth]{images/densenet/adamax_lr}\quad
\includegraphics[width=.4\textwidth]{images/densenet/rmsprop_lr}

\medskip
\includegraphics[width=.4\textwidth]{images/densenet/adadelta_lr}

\caption{In order, (a)Adam, (b)Nadam, (c)Adamax, (d)RMSProp, (e)Adadelta learning rate analysis}
\label{pics:lr_optimizers}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=.4\textwidth]{images/densenet/first_compare_lr_opt}\quad
\includegraphics[width=.4\textwidth]{images/densenet/second_compare_lr_opt}

\medskip
\includegraphics[width=.4\textwidth]{images/densenet/third_compare_lr_opt}\quad
\includegraphics[width=.4\textwidth]{images/densenet/fourth_compare_lr_opt}

\caption{comparison of different optimizers across different learning rates}
\label{pics:lr_optimizers_compare}
\end{figure}
Optimal learning rate selection is very important for effective learning. However, optimal learning rate varies optimizer to optimizer, hence learning rate and optimizer a very fine grained search is performed here
The search space looks as below:
\begin{itemize}
 \item Learning rate:
  \begin{itemize}
    \item 0.1 0.5 1.0 %adadelta
    \item 0.01, 0.02, 0.03, 0.05, 0.07
    \item 0.001, 0.002, 0.003, 0.005, 0.007
    \item 0.0001, 0.0002, 0.0003, 0.0005, 0.0007
    %\item 0.00001, 0.00002, 0.00003, 0.00005, 0.00007
  \end{itemize}
 \item Optimizers:
 \begin{itemize}
    \item Adam
    \item Nadam
    \item Adamax
    %\item RMSProp
    \item Adadelta
 \end{itemize}
\end{itemize}
TODO: Justify choice of the particular optimizers



\subsubsection{Conclusion} 
\begin{itemize}
    \item 10 trials for each test condition
    \item Not evaluating RMSProp, because others are performing better as seen in the last analysis. 
    \item Adam with decay rate analysis ?
    \item Also not evalating lower than 0.0001 learning rate because only RMSProp showed best performance in that region
\end{itemize}

%\begin{center}
% \begin{tabular}{||c c c c c||} 
% \hline\hline
% Learning rate & 0.07 & 0.1 & 0.5 & 1.0 \\ [0.5ex] 
% \hline
% Mean AUC & 0.906 & 0.906 & 0.891 & 0.887 \\ 
% \hline
%\end{tabular}
%end{center}

\subsection{Hyper-parameters for final grid search}
\begin{itemize}
 %\item Layers per block 2-2, 3-4
 %\item nb\_dense\_block 2
 %\item Growth rate 12, 18, 30
 %\item Nb\_filter 16
 %\item Densenet dropout 0.2, 0.4
 %\item Bottleneck False
 %\item Compression 0.3, 0.7
 %\item Learning rate 0.0002 and optimizer Adamax, Learning rate 0.07 optimizer Adadelta, Learning rate 0001 optimizer Nadam
 %item Batch size: 64
 \item 3 layer network.
\end{itemize}

\subsection{Best results in final grid search(as of now)}


%Divide this into three tables as well for densenet for fc + normal
%\begin{center}
% \begin{tabular}{||c c c c c c c c c c c c c c c c c||} 
% \hline\hline
% Layers & Growth\_rate & dense\_block & nb\_filter & dropout & reduction\_ & bottleneck & fc\_dropout & fc\_filter & Epochs & batch\_size & lr & optimizer & es\_patience & lr\_patience & batch\_size \\ [0.5ex] 
% \hline
% 0.608 & 0.854 & 0.881 & 0.902 & 0.934 & 3 & 0.608 & 0.854 & 0.881 & 0.902 & 0.934 & 3 & 0.608 & 0.854 & 0.881 & 0.902 & 0.934 \\ 
% \hline
% 0.608 & 0.854 & 0.881 & 0.902 & 0.934 & 3 & 0.608 & 0.854 & 0.881 & 0.902 & 0.934 & 3 & 0.608 & 0.854 & 0.881 & 0.902 & 0.934 \\
% \hline
%\end{tabular}
%\end{center}

\begin{figure}[h]
\centering
\includegraphics[height= 5cm]{images/densenet/fine_grid_search_top_configs}
\caption{Fine grid search top results}
\label{fig:fine_grid_search_top_configs}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[height= 4cm]{images/densenet/top5_analysis_bubble}
\caption{Fine grid search top results statistical analysis}
\label{fig:top5_analysis_bubble}
\end{figure}

%\flushbottom
%\newpage

\subsubsection{Conclusion}
\begin{itemize}
 \item State of the art highest AUC \textbf{0.91}
 \item Top result is that of 0.95 AUC
 \item Top mean AUC for 10 trials is 0.921.
\end{itemize}
