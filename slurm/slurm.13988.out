Epoch 1/150

 10/768 [..............................] - ETA: 2:57 - loss: 4.4516 - acc: 0.5000
260/768 [=========>....................] - ETA: 4s - loss: 2.9155 - acc: 0.5077  
530/768 [===================>..........] - ETA: 1s - loss: 2.4458 - acc: 0.5208
768/768 [==============================] - 2s 3ms/step - loss: 2.1981 - acc: 0.5208

Epoch 00001: acc improved from -inf to 0.52083, saving model to test-weights.h5
Epoch 2/150

 10/768 [..............................] - ETA: 0s - loss: 0.9567 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 1.4072 - acc: 0.4897
570/768 [=====================>........] - ETA: 0s - loss: 1.2690 - acc: 0.4842
768/768 [==============================] - 0s 182us/step - loss: 1.1846 - acc: 0.4766

Epoch 00002: acc did not improve from 0.52083
Epoch 3/150

 10/768 [..............................] - ETA: 0s - loss: 0.5701 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.9141 - acc: 0.5207
570/768 [=====================>........] - ETA: 0s - loss: 0.9075 - acc: 0.4947
768/768 [==============================] - 0s 184us/step - loss: 0.9011 - acc: 0.4870

Epoch 00003: acc did not improve from 0.52083
Epoch 4/150

 10/768 [..............................] - ETA: 0s - loss: 0.8229 - acc: 0.3000
310/768 [===========>..................] - ETA: 0s - loss: 0.7915 - acc: 0.4581
600/768 [======================>.......] - ETA: 0s - loss: 0.7905 - acc: 0.4517
768/768 [==============================] - 0s 177us/step - loss: 0.7869 - acc: 0.4792

Epoch 00004: acc did not improve from 0.52083
Epoch 5/150

 10/768 [..............................] - ETA: 0s - loss: 0.6411 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.7040 - acc: 0.5379
570/768 [=====================>........] - ETA: 0s - loss: 0.7490 - acc: 0.5526
768/768 [==============================] - 0s 183us/step - loss: 0.7298 - acc: 0.5742

Epoch 00005: acc improved from 0.52083 to 0.57422, saving model to test-weights.h5
Epoch 6/150

 10/768 [..............................] - ETA: 0s - loss: 0.6869 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.6763 - acc: 0.5517
580/768 [=====================>........] - ETA: 0s - loss: 0.6832 - acc: 0.5966
768/768 [==============================] - 0s 182us/step - loss: 0.6965 - acc: 0.6159

Epoch 00006: acc improved from 0.57422 to 0.61589, saving model to test-weights.h5
Epoch 7/150

 10/768 [..............................] - ETA: 0s - loss: 0.5991 - acc: 0.5000
280/768 [=========>....................] - ETA: 0s - loss: 0.6669 - acc: 0.6893
550/768 [====================>.........] - ETA: 0s - loss: 0.6685 - acc: 0.6509
768/768 [==============================] - 0s 187us/step - loss: 0.6844 - acc: 0.6523

Epoch 00007: acc improved from 0.61589 to 0.65234, saving model to test-weights.h5
Epoch 8/150

 10/768 [..............................] - ETA: 0s - loss: 0.6362 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.7573 - acc: 0.6552
570/768 [=====================>........] - ETA: 0s - loss: 0.7058 - acc: 0.6526
768/768 [==============================] - 0s 183us/step - loss: 0.6774 - acc: 0.6654

Epoch 00008: acc improved from 0.65234 to 0.66536, saving model to test-weights.h5
Epoch 9/150

 10/768 [..............................] - ETA: 0s - loss: 0.6348 - acc: 0.6000
280/768 [=========>....................] - ETA: 0s - loss: 0.6351 - acc: 0.6964
550/768 [====================>.........] - ETA: 0s - loss: 0.6399 - acc: 0.6691
768/768 [==============================] - 0s 187us/step - loss: 0.6722 - acc: 0.6641

Epoch 00009: acc did not improve from 0.66536
Epoch 10/150

 10/768 [..............................] - ETA: 0s - loss: 0.6560 - acc: 0.7000
270/768 [=========>....................] - ETA: 0s - loss: 0.6945 - acc: 0.6519
530/768 [===================>..........] - ETA: 0s - loss: 0.6917 - acc: 0.6604
768/768 [==============================] - 0s 191us/step - loss: 0.6592 - acc: 0.6758

Epoch 00010: acc improved from 0.66536 to 0.67578, saving model to test-weights.h5
Epoch 11/150

 10/768 [..............................] - ETA: 0s - loss: 0.7473 - acc: 0.4000
280/768 [=========>....................] - ETA: 0s - loss: 0.6652 - acc: 0.6714
560/768 [====================>.........] - ETA: 0s - loss: 0.6603 - acc: 0.6857
768/768 [==============================] - 0s 186us/step - loss: 0.6491 - acc: 0.6771

Epoch 00011: acc improved from 0.67578 to 0.67708, saving model to test-weights.h5
Epoch 12/150

 10/768 [..............................] - ETA: 0s - loss: 0.6023 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.6183 - acc: 0.6690
570/768 [=====================>........] - ETA: 0s - loss: 0.6274 - acc: 0.6825
768/768 [==============================] - 0s 183us/step - loss: 0.6460 - acc: 0.6875

Epoch 00012: acc improved from 0.67708 to 0.68750, saving model to test-weights.h5
Epoch 13/150

 10/768 [..............................] - ETA: 0s - loss: 0.5249 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.6557 - acc: 0.6897
570/768 [=====================>........] - ETA: 0s - loss: 0.6647 - acc: 0.6632
768/768 [==============================] - 0s 184us/step - loss: 0.6506 - acc: 0.6680

Epoch 00013: acc did not improve from 0.68750
Epoch 14/150

 10/768 [..............................] - ETA: 0s - loss: 0.6861 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.5987 - acc: 0.6655
570/768 [=====================>........] - ETA: 0s - loss: 0.6485 - acc: 0.6807
768/768 [==============================] - 0s 182us/step - loss: 0.6404 - acc: 0.6823

Epoch 00014: acc did not improve from 0.68750
Epoch 15/150

 10/768 [..............................] - ETA: 0s - loss: 0.5614 - acc: 0.7000
300/768 [==========>...................] - ETA: 0s - loss: 0.5985 - acc: 0.6733
580/768 [=====================>........] - ETA: 0s - loss: 0.6273 - acc: 0.6776
768/768 [==============================] - 0s 179us/step - loss: 0.6401 - acc: 0.6836

Epoch 00015: acc did not improve from 0.68750
Epoch 16/150

 10/768 [..............................] - ETA: 0s - loss: 0.6393 - acc: 0.5000
290/768 [==========>...................] - ETA: 0s - loss: 0.6508 - acc: 0.7034
580/768 [=====================>........] - ETA: 0s - loss: 0.6524 - acc: 0.7000
768/768 [==============================] - 0s 178us/step - loss: 0.6367 - acc: 0.6953

Epoch 00016: acc improved from 0.68750 to 0.69531, saving model to test-weights.h5
Epoch 17/150

 10/768 [..............................] - ETA: 0s - loss: 0.5917 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.6534 - acc: 0.6690
560/768 [====================>.........] - ETA: 0s - loss: 0.6381 - acc: 0.6893
768/768 [==============================] - 0s 187us/step - loss: 0.6256 - acc: 0.6979

Epoch 00017: acc improved from 0.69531 to 0.69792, saving model to test-weights.h5
Epoch 18/150

 10/768 [..............................] - ETA: 0s - loss: 0.5141 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.6597 - acc: 0.6655
570/768 [=====================>........] - ETA: 0s - loss: 0.6530 - acc: 0.6825
768/768 [==============================] - 0s 183us/step - loss: 0.6394 - acc: 0.6823

Epoch 00018: acc did not improve from 0.69792
Epoch 19/150

 10/768 [..............................] - ETA: 0s - loss: 0.6695 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.6091 - acc: 0.6621
560/768 [====================>.........] - ETA: 0s - loss: 0.6227 - acc: 0.6679
768/768 [==============================] - 0s 184us/step - loss: 0.6291 - acc: 0.6797

Epoch 00019: acc did not improve from 0.69792
Epoch 20/150

 10/768 [..............................] - ETA: 0s - loss: 0.6385 - acc: 0.4000
290/768 [==========>...................] - ETA: 0s - loss: 0.6218 - acc: 0.7103
570/768 [=====================>........] - ETA: 0s - loss: 0.6096 - acc: 0.6982
768/768 [==============================] - 0s 184us/step - loss: 0.6229 - acc: 0.7031

Epoch 00020: acc improved from 0.69792 to 0.70313, saving model to test-weights.h5
Epoch 21/150

 10/768 [..............................] - ETA: 0s - loss: 0.5121 - acc: 0.9000
270/768 [=========>....................] - ETA: 0s - loss: 0.5802 - acc: 0.6852
530/768 [===================>..........] - ETA: 0s - loss: 0.6125 - acc: 0.6887
768/768 [==============================] - 0s 196us/step - loss: 0.6225 - acc: 0.6875

Epoch 00021: acc did not improve from 0.70313
Epoch 22/150

 10/768 [..............................] - ETA: 0s - loss: 0.5209 - acc: 0.7000
270/768 [=========>....................] - ETA: 0s - loss: 0.5625 - acc: 0.7148
540/768 [====================>.........] - ETA: 0s - loss: 0.5772 - acc: 0.6981
768/768 [==============================] - 0s 189us/step - loss: 0.6233 - acc: 0.6849

Epoch 00022: acc did not improve from 0.70313
Epoch 23/150

 10/768 [..............................] - ETA: 0s - loss: 0.5326 - acc: 0.7000
300/768 [==========>...................] - ETA: 0s - loss: 0.6600 - acc: 0.6533
580/768 [=====================>........] - ETA: 0s - loss: 0.6376 - acc: 0.6879
768/768 [==============================] - 0s 180us/step - loss: 0.6274 - acc: 0.6914

Epoch 00023: acc did not improve from 0.70313
Epoch 24/150

 10/768 [..............................] - ETA: 0s - loss: 0.6431 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.5765 - acc: 0.6759
570/768 [=====================>........] - ETA: 0s - loss: 0.6168 - acc: 0.6737
768/768 [==============================] - 0s 183us/step - loss: 0.6272 - acc: 0.6836

Epoch 00024: acc did not improve from 0.70313
Epoch 25/150

 10/768 [..............................] - ETA: 0s - loss: 0.5011 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.6043 - acc: 0.7138
570/768 [=====================>........] - ETA: 0s - loss: 0.6352 - acc: 0.6965
768/768 [==============================] - 0s 182us/step - loss: 0.6152 - acc: 0.7135

Epoch 00025: acc improved from 0.70313 to 0.71354, saving model to test-weights.h5
Epoch 26/150

 10/768 [..............................] - ETA: 0s - loss: 0.4819 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5840 - acc: 0.7069
570/768 [=====================>........] - ETA: 0s - loss: 0.6318 - acc: 0.7035
768/768 [==============================] - 0s 182us/step - loss: 0.6145 - acc: 0.7109

Epoch 00026: acc did not improve from 0.71354
Epoch 27/150

 10/768 [..............................] - ETA: 0s - loss: 0.4972 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5771 - acc: 0.6931
580/768 [=====================>........] - ETA: 0s - loss: 0.6216 - acc: 0.7138
768/768 [==============================] - 0s 181us/step - loss: 0.6263 - acc: 0.7031

Epoch 00027: acc did not improve from 0.71354
Epoch 28/150

 10/768 [..............................] - ETA: 0s - loss: 0.3417 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.5734 - acc: 0.7069
580/768 [=====================>........] - ETA: 0s - loss: 0.5990 - acc: 0.7172
768/768 [==============================] - 0s 179us/step - loss: 0.6208 - acc: 0.6979

Epoch 00028: acc did not improve from 0.71354
Epoch 29/150

 10/768 [..............................] - ETA: 0s - loss: 0.3426 - acc: 0.9000
300/768 [==========>...................] - ETA: 0s - loss: 0.6185 - acc: 0.7533
580/768 [=====================>........] - ETA: 0s - loss: 0.6008 - acc: 0.7224
768/768 [==============================] - 0s 180us/step - loss: 0.6095 - acc: 0.7305

Epoch 00029: acc improved from 0.71354 to 0.73047, saving model to test-weights.h5
Epoch 30/150

 10/768 [..............................] - ETA: 0s - loss: 0.6366 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5574 - acc: 0.7483
570/768 [=====================>........] - ETA: 0s - loss: 0.5911 - acc: 0.7316
768/768 [==============================] - 0s 179us/step - loss: 0.6091 - acc: 0.7240

Epoch 00030: acc did not improve from 0.73047
Epoch 31/150

 10/768 [..............................] - ETA: 0s - loss: 0.6340 - acc: 0.7000
300/768 [==========>...................] - ETA: 0s - loss: 0.6180 - acc: 0.7167
580/768 [=====================>........] - ETA: 0s - loss: 0.5872 - acc: 0.7224
768/768 [==============================] - 0s 178us/step - loss: 0.5995 - acc: 0.7240

Epoch 00031: acc did not improve from 0.73047
Epoch 32/150

 10/768 [..............................] - ETA: 0s - loss: 0.4782 - acc: 1.0000
300/768 [==========>...................] - ETA: 0s - loss: 0.5514 - acc: 0.7300
590/768 [======================>.......] - ETA: 0s - loss: 0.5868 - acc: 0.7186
768/768 [==============================] - 0s 177us/step - loss: 0.6072 - acc: 0.7240

Epoch 00032: acc did not improve from 0.73047
Epoch 33/150

 10/768 [..............................] - ETA: 0s - loss: 0.5684 - acc: 0.7000
310/768 [===========>..................] - ETA: 0s - loss: 0.5787 - acc: 0.7032
600/768 [======================>.......] - ETA: 0s - loss: 0.6285 - acc: 0.6950
768/768 [==============================] - 0s 176us/step - loss: 0.6128 - acc: 0.7018

Epoch 00033: acc did not improve from 0.73047
Epoch 34/150

 10/768 [..............................] - ETA: 0s - loss: 0.5168 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.6276 - acc: 0.7000
570/768 [=====================>........] - ETA: 0s - loss: 0.6009 - acc: 0.7018
768/768 [==============================] - 0s 179us/step - loss: 0.6088 - acc: 0.7214

Epoch 00034: acc did not improve from 0.73047
Epoch 35/150

 10/768 [..............................] - ETA: 0s - loss: 0.3812 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.5625 - acc: 0.7207
580/768 [=====================>........] - ETA: 0s - loss: 0.6289 - acc: 0.7069
768/768 [==============================] - 0s 178us/step - loss: 0.6110 - acc: 0.7083

Epoch 00035: acc did not improve from 0.73047
Epoch 36/150

 10/768 [..............................] - ETA: 0s - loss: 0.5156 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.6227 - acc: 0.7379
570/768 [=====================>........] - ETA: 0s - loss: 0.6174 - acc: 0.7281
768/768 [==============================] - 0s 182us/step - loss: 0.6014 - acc: 0.7266

Epoch 00036: acc did not improve from 0.73047
Epoch 37/150

 10/768 [..............................] - ETA: 0s - loss: 0.5901 - acc: 0.6000
300/768 [==========>...................] - ETA: 0s - loss: 0.5631 - acc: 0.7067
590/768 [======================>.......] - ETA: 0s - loss: 0.6121 - acc: 0.7051
768/768 [==============================] - 0s 176us/step - loss: 0.6008 - acc: 0.7044

Epoch 00037: acc did not improve from 0.73047
Epoch 38/150

 10/768 [..............................] - ETA: 0s - loss: 0.4685 - acc: 1.0000
300/768 [==========>...................] - ETA: 0s - loss: 0.6187 - acc: 0.7167
580/768 [=====================>........] - ETA: 0s - loss: 0.6195 - acc: 0.7172
768/768 [==============================] - 0s 181us/step - loss: 0.6060 - acc: 0.7174

Epoch 00038: acc did not improve from 0.73047
Epoch 39/150

 10/768 [..............................] - ETA: 0s - loss: 0.4360 - acc: 0.9000
300/768 [==========>...................] - ETA: 0s - loss: 0.6019 - acc: 0.7367
590/768 [======================>.......] - ETA: 0s - loss: 0.6133 - acc: 0.7136
768/768 [==============================] - 0s 178us/step - loss: 0.6095 - acc: 0.7122

Epoch 00039: acc did not improve from 0.73047
Epoch 40/150

 10/768 [..............................] - ETA: 0s - loss: 0.4857 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5864 - acc: 0.6793
560/768 [====================>.........] - ETA: 0s - loss: 0.5842 - acc: 0.7161
768/768 [==============================] - 0s 185us/step - loss: 0.5992 - acc: 0.7161

Epoch 00040: acc did not improve from 0.73047
Epoch 41/150

 10/768 [..............................] - ETA: 0s - loss: 0.5002 - acc: 0.9000
280/768 [=========>....................] - ETA: 0s - loss: 0.6297 - acc: 0.7107
540/768 [====================>.........] - ETA: 0s - loss: 0.6243 - acc: 0.7167
768/768 [==============================] - 0s 191us/step - loss: 0.5953 - acc: 0.7292

Epoch 00041: acc did not improve from 0.73047
Epoch 42/150

 10/768 [..............................] - ETA: 0s - loss: 0.3343 - acc: 0.9000
280/768 [=========>....................] - ETA: 0s - loss: 0.6219 - acc: 0.7107
550/768 [====================>.........] - ETA: 0s - loss: 0.6142 - acc: 0.7145
768/768 [==============================] - 0s 189us/step - loss: 0.6007 - acc: 0.7201

Epoch 00042: acc did not improve from 0.73047
Epoch 43/150

 10/768 [..............................] - ETA: 0s - loss: 0.5130 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5599 - acc: 0.7250
540/768 [====================>.........] - ETA: 0s - loss: 0.6194 - acc: 0.7333
768/768 [==============================] - 0s 190us/step - loss: 0.6029 - acc: 0.7240

Epoch 00043: acc did not improve from 0.73047
Epoch 44/150

 10/768 [..............................] - ETA: 0s - loss: 2.2169 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.6056 - acc: 0.7552
550/768 [====================>.........] - ETA: 0s - loss: 0.6044 - acc: 0.7382
768/768 [==============================] - 0s 192us/step - loss: 0.5963 - acc: 0.7174

Epoch 00044: acc did not improve from 0.73047
Epoch 45/150

 10/768 [..............................] - ETA: 0s - loss: 0.5462 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5394 - acc: 0.7679
540/768 [====================>.........] - ETA: 0s - loss: 0.5645 - acc: 0.7426
768/768 [==============================] - 0s 190us/step - loss: 0.6020 - acc: 0.7357

Epoch 00045: acc improved from 0.73047 to 0.73568, saving model to test-weights.h5
Epoch 46/150

 10/768 [..............................] - ETA: 0s - loss: 0.8334 - acc: 0.5000
290/768 [==========>...................] - ETA: 0s - loss: 0.6574 - acc: 0.7138
560/768 [====================>.........] - ETA: 0s - loss: 0.6138 - acc: 0.7071
768/768 [==============================] - 0s 189us/step - loss: 0.6033 - acc: 0.7135

Epoch 00046: acc did not improve from 0.73568
Epoch 47/150

 10/768 [..............................] - ETA: 0s - loss: 0.7575 - acc: 0.7000
270/768 [=========>....................] - ETA: 0s - loss: 0.6017 - acc: 0.7222
540/768 [====================>.........] - ETA: 0s - loss: 0.5848 - acc: 0.7056
768/768 [==============================] - 0s 196us/step - loss: 0.5956 - acc: 0.7044

Epoch 00047: acc did not improve from 0.73568
Epoch 48/150

 10/768 [..............................] - ETA: 0s - loss: 0.7010 - acc: 0.6000
260/768 [=========>....................] - ETA: 0s - loss: 0.5482 - acc: 0.7231
510/768 [==================>...........] - ETA: 0s - loss: 0.5721 - acc: 0.7333
768/768 [==============================] - 0s 201us/step - loss: 0.5868 - acc: 0.7318

Epoch 00048: acc did not improve from 0.73568
Epoch 49/150

 10/768 [..............................] - ETA: 0s - loss: 0.5036 - acc: 0.7000
270/768 [=========>....................] - ETA: 0s - loss: 0.6172 - acc: 0.7111
530/768 [===================>..........] - ETA: 0s - loss: 0.5653 - acc: 0.7415
768/768 [==============================] - 0s 198us/step - loss: 0.5841 - acc: 0.7409

Epoch 00049: acc improved from 0.73568 to 0.74089, saving model to test-weights.h5
Epoch 50/150

 10/768 [..............................] - ETA: 0s - loss: 0.5054 - acc: 0.8000
270/768 [=========>....................] - ETA: 0s - loss: 0.5176 - acc: 0.7407
530/768 [===================>..........] - ETA: 0s - loss: 0.5814 - acc: 0.7434
768/768 [==============================] - 0s 193us/step - loss: 0.5900 - acc: 0.7214

Epoch 00050: acc did not improve from 0.74089
Epoch 51/150

 10/768 [..............................] - ETA: 0s - loss: 0.6112 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.6080 - acc: 0.6897
560/768 [====================>.........] - ETA: 0s - loss: 0.6201 - acc: 0.7089
768/768 [==============================] - 0s 186us/step - loss: 0.5919 - acc: 0.7174

Epoch 00051: acc did not improve from 0.74089
Epoch 52/150

 10/768 [..............................] - ETA: 0s - loss: 0.7407 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.5446 - acc: 0.7414
570/768 [=====================>........] - ETA: 0s - loss: 0.5842 - acc: 0.7316
768/768 [==============================] - 0s 183us/step - loss: 0.5896 - acc: 0.7318

Epoch 00052: acc did not improve from 0.74089
Epoch 53/150

 10/768 [..............................] - ETA: 0s - loss: 0.4507 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.6667 - acc: 0.6893
540/768 [====================>.........] - ETA: 0s - loss: 0.5955 - acc: 0.7278
768/768 [==============================] - 0s 190us/step - loss: 0.5983 - acc: 0.7253

Epoch 00053: acc did not improve from 0.74089
Epoch 54/150

 10/768 [..............................] - ETA: 0s - loss: 0.7211 - acc: 0.5000
290/768 [==========>...................] - ETA: 0s - loss: 0.5736 - acc: 0.6655
570/768 [=====================>........] - ETA: 0s - loss: 0.5669 - acc: 0.7228
768/768 [==============================] - 0s 183us/step - loss: 0.5840 - acc: 0.7240

Epoch 00054: acc did not improve from 0.74089
Epoch 55/150

 10/768 [..............................] - ETA: 0s - loss: 0.6709 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.6614 - acc: 0.7034
560/768 [====================>.........] - ETA: 0s - loss: 0.6102 - acc: 0.7196
768/768 [==============================] - 0s 186us/step - loss: 0.5902 - acc: 0.7305

Epoch 00055: acc did not improve from 0.74089
Epoch 56/150

 10/768 [..............................] - ETA: 0s - loss: 0.3242 - acc: 1.0000
290/768 [==========>...................] - ETA: 0s - loss: 0.5989 - acc: 0.7310
570/768 [=====================>........] - ETA: 0s - loss: 0.6077 - acc: 0.7193
768/768 [==============================] - 0s 184us/step - loss: 0.5853 - acc: 0.7305

Epoch 00056: acc did not improve from 0.74089
Epoch 57/150

 10/768 [..............................] - ETA: 0s - loss: 0.5047 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.6033 - acc: 0.7207
560/768 [====================>.........] - ETA: 0s - loss: 0.5664 - acc: 0.7357
768/768 [==============================] - 0s 190us/step - loss: 0.5823 - acc: 0.7266

Epoch 00057: acc did not improve from 0.74089
Epoch 58/150

 10/768 [..............................] - ETA: 0s - loss: 0.4153 - acc: 0.9000
270/768 [=========>....................] - ETA: 0s - loss: 0.6845 - acc: 0.7037
540/768 [====================>.........] - ETA: 0s - loss: 0.6269 - acc: 0.7185
768/768 [==============================] - 0s 196us/step - loss: 0.5987 - acc: 0.7188

Epoch 00058: acc did not improve from 0.74089
Epoch 59/150

 10/768 [..............................] - ETA: 0s - loss: 0.3637 - acc: 0.9000
280/768 [=========>....................] - ETA: 0s - loss: 0.5211 - acc: 0.7571
540/768 [====================>.........] - ETA: 0s - loss: 0.5549 - acc: 0.7259
768/768 [==============================] - 0s 194us/step - loss: 0.5877 - acc: 0.7318

Epoch 00059: acc did not improve from 0.74089
Epoch 60/150

 10/768 [..............................] - ETA: 0s - loss: 0.6963 - acc: 0.6000
280/768 [=========>....................] - ETA: 0s - loss: 0.5305 - acc: 0.7250
560/768 [====================>.........] - ETA: 0s - loss: 0.5909 - acc: 0.7125
768/768 [==============================] - 0s 184us/step - loss: 0.5969 - acc: 0.7174

Epoch 00060: acc did not improve from 0.74089
Epoch 61/150

 10/768 [..............................] - ETA: 0s - loss: 0.4752 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.6137 - acc: 0.7071
560/768 [====================>.........] - ETA: 0s - loss: 0.5753 - acc: 0.7268
768/768 [==============================] - 0s 184us/step - loss: 0.5922 - acc: 0.7161

Epoch 00061: acc did not improve from 0.74089
Epoch 62/150

 10/768 [..............................] - ETA: 0s - loss: 2.1323 - acc: 0.6000
280/768 [=========>....................] - ETA: 0s - loss: 0.5903 - acc: 0.7536
550/768 [====================>.........] - ETA: 0s - loss: 0.6082 - acc: 0.7400
768/768 [==============================] - 0s 187us/step - loss: 0.5863 - acc: 0.7318

Epoch 00062: acc did not improve from 0.74089
Epoch 63/150

 10/768 [..............................] - ETA: 0s - loss: 0.5263 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5643 - acc: 0.7379
560/768 [====================>.........] - ETA: 0s - loss: 0.5776 - acc: 0.7214
768/768 [==============================] - 0s 191us/step - loss: 0.5848 - acc: 0.7292

Epoch 00063: acc did not improve from 0.74089
Epoch 64/150

 10/768 [..............................] - ETA: 0s - loss: 0.6107 - acc: 0.6000
270/768 [=========>....................] - ETA: 0s - loss: 0.5721 - acc: 0.7519
530/768 [===================>..........] - ETA: 0s - loss: 0.5908 - acc: 0.7208
768/768 [==============================] - 0s 195us/step - loss: 0.5867 - acc: 0.7201

Epoch 00064: acc did not improve from 0.74089
Epoch 65/150

 10/768 [..............................] - ETA: 0s - loss: 1.0638 - acc: 0.3000
280/768 [=========>....................] - ETA: 0s - loss: 0.5662 - acc: 0.7179
540/768 [====================>.........] - ETA: 0s - loss: 0.5433 - acc: 0.7370
768/768 [==============================] - 0s 198us/step - loss: 0.5860 - acc: 0.7318

Epoch 00065: acc did not improve from 0.74089
Epoch 66/150

 10/768 [..............................] - ETA: 0s - loss: 0.6427 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.5539 - acc: 0.7786
560/768 [====================>.........] - ETA: 0s - loss: 0.5742 - acc: 0.7482
768/768 [==============================] - 0s 185us/step - loss: 0.5804 - acc: 0.7279

Epoch 00066: acc did not improve from 0.74089
Epoch 67/150

 10/768 [..............................] - ETA: 0s - loss: 0.6193 - acc: 0.7000
260/768 [=========>....................] - ETA: 0s - loss: 0.5002 - acc: 0.7692
520/768 [===================>..........] - ETA: 0s - loss: 0.5534 - acc: 0.7442
768/768 [==============================] - 0s 195us/step - loss: 0.5809 - acc: 0.7318

Epoch 00067: acc did not improve from 0.74089
Epoch 68/150

 10/768 [..............................] - ETA: 0s - loss: 0.4893 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.5851 - acc: 0.7207
570/768 [=====================>........] - ETA: 0s - loss: 0.6033 - acc: 0.7158
768/768 [==============================] - 0s 184us/step - loss: 0.5812 - acc: 0.7279

Epoch 00068: acc did not improve from 0.74089
Epoch 69/150

 10/768 [..............................] - ETA: 0s - loss: 0.4298 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5914 - acc: 0.7379
570/768 [=====================>........] - ETA: 0s - loss: 0.5836 - acc: 0.7404
768/768 [==============================] - 0s 182us/step - loss: 0.5783 - acc: 0.7331

Epoch 00069: acc did not improve from 0.74089
Epoch 70/150

 10/768 [..............................] - ETA: 0s - loss: 0.3927 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.5224 - acc: 0.7379
570/768 [=====================>........] - ETA: 0s - loss: 0.5839 - acc: 0.7298
768/768 [==============================] - 0s 183us/step - loss: 0.5797 - acc: 0.7305

Epoch 00070: acc did not improve from 0.74089
Epoch 71/150

 10/768 [..............................] - ETA: 0s - loss: 0.6412 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.5974 - acc: 0.7862
570/768 [=====================>........] - ETA: 0s - loss: 0.5776 - acc: 0.7544
768/768 [==============================] - 0s 183us/step - loss: 0.5832 - acc: 0.7409

Epoch 00071: acc improved from 0.74089 to 0.74089, saving model to test-weights.h5
Epoch 72/150

 10/768 [..............................] - ETA: 0s - loss: 0.4591 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.5033 - acc: 0.7429
530/768 [===================>..........] - ETA: 0s - loss: 0.5574 - acc: 0.7245
768/768 [==============================] - 0s 194us/step - loss: 0.5745 - acc: 0.7318

Epoch 00072: acc did not improve from 0.74089
Epoch 73/150

 10/768 [..............................] - ETA: 0s - loss: 0.4376 - acc: 0.8000
270/768 [=========>....................] - ETA: 0s - loss: 0.4880 - acc: 0.7778
520/768 [===================>..........] - ETA: 0s - loss: 0.5424 - acc: 0.7558
768/768 [==============================] - 0s 196us/step - loss: 0.5807 - acc: 0.7357

Epoch 00073: acc did not improve from 0.74089
Epoch 74/150

 10/768 [..............................] - ETA: 0s - loss: 0.9567 - acc: 0.3000
270/768 [=========>....................] - ETA: 0s - loss: 0.5806 - acc: 0.7259
530/768 [===================>..........] - ETA: 0s - loss: 0.6065 - acc: 0.7057
768/768 [==============================] - 0s 199us/step - loss: 0.5725 - acc: 0.7292

Epoch 00074: acc did not improve from 0.74089
Epoch 75/150

 10/768 [..............................] - ETA: 0s - loss: 0.5878 - acc: 0.7000
270/768 [=========>....................] - ETA: 0s - loss: 0.5286 - acc: 0.7296
520/768 [===================>..........] - ETA: 0s - loss: 0.5582 - acc: 0.7212
768/768 [==============================] - 0s 202us/step - loss: 0.5677 - acc: 0.7292

Epoch 00075: acc did not improve from 0.74089
Epoch 76/150

 10/768 [..............................] - ETA: 0s - loss: 0.6586 - acc: 0.7000
260/768 [=========>....................] - ETA: 0s - loss: 0.5279 - acc: 0.7385
510/768 [==================>...........] - ETA: 0s - loss: 0.5551 - acc: 0.7431
760/768 [============================>.] - ETA: 0s - loss: 0.5752 - acc: 0.7342
768/768 [==============================] - 0s 201us/step - loss: 0.5746 - acc: 0.7357

Epoch 00076: acc did not improve from 0.74089
Epoch 77/150

 10/768 [..............................] - ETA: 0s - loss: 0.4379 - acc: 0.8000
270/768 [=========>....................] - ETA: 0s - loss: 0.5181 - acc: 0.7481
530/768 [===================>..........] - ETA: 0s - loss: 0.5657 - acc: 0.7396
768/768 [==============================] - 0s 193us/step - loss: 0.5739 - acc: 0.7396

Epoch 00077: acc did not improve from 0.74089
Epoch 78/150

 10/768 [..............................] - ETA: 0s - loss: 0.5090 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5675 - acc: 0.7517
570/768 [=====================>........] - ETA: 0s - loss: 0.5697 - acc: 0.7526
768/768 [==============================] - 0s 181us/step - loss: 0.5690 - acc: 0.7318

Epoch 00078: acc did not improve from 0.74089
Epoch 79/150

 10/768 [..............................] - ETA: 0s - loss: 0.7053 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.5031 - acc: 0.7828
570/768 [=====================>........] - ETA: 0s - loss: 0.5465 - acc: 0.7544
768/768 [==============================] - 0s 181us/step - loss: 0.5509 - acc: 0.7487

Epoch 00079: acc improved from 0.74089 to 0.74870, saving model to test-weights.h5
Epoch 80/150

 10/768 [..............................] - ETA: 0s - loss: 0.6840 - acc: 0.5000
300/768 [==========>...................] - ETA: 0s - loss: 0.5340 - acc: 0.7467
590/768 [======================>.......] - ETA: 0s - loss: 0.5423 - acc: 0.7186
768/768 [==============================] - 0s 177us/step - loss: 0.5422 - acc: 0.7214

Epoch 00080: acc did not improve from 0.74870
Epoch 81/150

 10/768 [..............................] - ETA: 0s - loss: 0.5235 - acc: 0.7000
310/768 [===========>..................] - ETA: 0s - loss: 0.5470 - acc: 0.7452
610/768 [======================>.......] - ETA: 0s - loss: 0.5387 - acc: 0.7311
768/768 [==============================] - 0s 171us/step - loss: 0.5365 - acc: 0.7383

Epoch 00081: acc did not improve from 0.74870
Epoch 82/150

 10/768 [..............................] - ETA: 0s - loss: 0.4888 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5628 - acc: 0.7000
580/768 [=====================>........] - ETA: 0s - loss: 0.5246 - acc: 0.7414
768/768 [==============================] - 0s 175us/step - loss: 0.5335 - acc: 0.7331

Epoch 00082: acc did not improve from 0.74870
Epoch 83/150

 10/768 [..............................] - ETA: 0s - loss: 0.6656 - acc: 0.7000
300/768 [==========>...................] - ETA: 0s - loss: 0.5165 - acc: 0.7533
590/768 [======================>.......] - ETA: 0s - loss: 0.5276 - acc: 0.7424
768/768 [==============================] - 0s 173us/step - loss: 0.5312 - acc: 0.7448

Epoch 00083: acc did not improve from 0.74870
Epoch 84/150

 10/768 [..............................] - ETA: 0s - loss: 0.6846 - acc: 0.7000
300/768 [==========>...................] - ETA: 0s - loss: 0.4943 - acc: 0.7700
580/768 [=====================>........] - ETA: 0s - loss: 0.5182 - acc: 0.7431
768/768 [==============================] - 0s 179us/step - loss: 0.5296 - acc: 0.7292

Epoch 00084: acc did not improve from 0.74870
Epoch 85/150

 10/768 [..............................] - ETA: 0s - loss: 0.4359 - acc: 0.7000
310/768 [===========>..................] - ETA: 0s - loss: 0.5308 - acc: 0.7419
600/768 [======================>.......] - ETA: 0s - loss: 0.5316 - acc: 0.7400
768/768 [==============================] - 0s 172us/step - loss: 0.5316 - acc: 0.7422

Epoch 00085: acc did not improve from 0.74870
Epoch 86/150

 10/768 [..............................] - ETA: 0s - loss: 0.7528 - acc: 0.5000
300/768 [==========>...................] - ETA: 0s - loss: 0.5437 - acc: 0.7100
590/768 [======================>.......] - ETA: 0s - loss: 0.5277 - acc: 0.7390
768/768 [==============================] - 0s 175us/step - loss: 0.5275 - acc: 0.7279

Epoch 00086: acc did not improve from 0.74870
Epoch 87/150

 10/768 [..............................] - ETA: 0s - loss: 0.5839 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.4808 - acc: 0.8069
590/768 [======================>.......] - ETA: 0s - loss: 0.5010 - acc: 0.7695
768/768 [==============================] - 0s 175us/step - loss: 0.5212 - acc: 0.7487

Epoch 00087: acc did not improve from 0.74870
Epoch 88/150

 10/768 [..............................] - ETA: 0s - loss: 0.5995 - acc: 0.8000
310/768 [===========>..................] - ETA: 0s - loss: 0.5476 - acc: 0.7194
600/768 [======================>.......] - ETA: 0s - loss: 0.5372 - acc: 0.7250
768/768 [==============================] - 0s 172us/step - loss: 0.5322 - acc: 0.7357

Epoch 00088: acc did not improve from 0.74870
Epoch 89/150

 10/768 [..............................] - ETA: 0s - loss: 0.4339 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5514 - acc: 0.7133
600/768 [======================>.......] - ETA: 0s - loss: 0.5447 - acc: 0.7100
768/768 [==============================] - 0s 175us/step - loss: 0.5307 - acc: 0.7305

Epoch 00089: acc did not improve from 0.74870
Epoch 90/150

 10/768 [..............................] - ETA: 0s - loss: 0.4277 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5138 - acc: 0.7333
600/768 [======================>.......] - ETA: 0s - loss: 0.5325 - acc: 0.7283
768/768 [==============================] - 0s 173us/step - loss: 0.5263 - acc: 0.7357

Epoch 00090: acc did not improve from 0.74870
Epoch 91/150

 10/768 [..............................] - ETA: 0s - loss: 0.3151 - acc: 0.9000
310/768 [===========>..................] - ETA: 0s - loss: 0.5417 - acc: 0.7258
590/768 [======================>.......] - ETA: 0s - loss: 0.5248 - acc: 0.7373
768/768 [==============================] - 0s 177us/step - loss: 0.5248 - acc: 0.7383

Epoch 00091: acc did not improve from 0.74870
Epoch 92/150

 10/768 [..............................] - ETA: 0s - loss: 0.3904 - acc: 0.9000
270/768 [=========>....................] - ETA: 0s - loss: 0.4706 - acc: 0.7963
570/768 [=====================>........] - ETA: 0s - loss: 0.5184 - acc: 0.7509
768/768 [==============================] - 0s 179us/step - loss: 0.5280 - acc: 0.7357

Epoch 00092: acc did not improve from 0.74870
Epoch 93/150

 10/768 [..............................] - ETA: 0s - loss: 0.4943 - acc: 0.9000
310/768 [===========>..................] - ETA: 0s - loss: 0.4931 - acc: 0.7903
610/768 [======================>.......] - ETA: 0s - loss: 0.5182 - acc: 0.7443
768/768 [==============================] - 0s 174us/step - loss: 0.5208 - acc: 0.7396

Epoch 00093: acc did not improve from 0.74870
Epoch 94/150

 10/768 [..............................] - ETA: 0s - loss: 0.4283 - acc: 0.7000
310/768 [===========>..................] - ETA: 0s - loss: 0.5277 - acc: 0.7226
600/768 [======================>.......] - ETA: 0s - loss: 0.5320 - acc: 0.7250
768/768 [==============================] - 0s 173us/step - loss: 0.5335 - acc: 0.7253

Epoch 00094: acc did not improve from 0.74870
Epoch 95/150

 10/768 [..............................] - ETA: 0s - loss: 0.8656 - acc: 0.5000
300/768 [==========>...................] - ETA: 0s - loss: 0.5069 - acc: 0.7300
580/768 [=====================>........] - ETA: 0s - loss: 0.5144 - acc: 0.7431
768/768 [==============================] - 0s 178us/step - loss: 0.5233 - acc: 0.7383

Epoch 00095: acc did not improve from 0.74870
Epoch 96/150

 10/768 [..............................] - ETA: 0s - loss: 0.4643 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5293 - acc: 0.7300
590/768 [======================>.......] - ETA: 0s - loss: 0.5295 - acc: 0.7339
768/768 [==============================] - 0s 176us/step - loss: 0.5298 - acc: 0.7292

Epoch 00096: acc did not improve from 0.74870
Epoch 97/150

 10/768 [..............................] - ETA: 0s - loss: 0.4237 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5293 - acc: 0.7567
590/768 [======================>.......] - ETA: 0s - loss: 0.5298 - acc: 0.7492
768/768 [==============================] - 0s 176us/step - loss: 0.5267 - acc: 0.7500

Epoch 00097: acc improved from 0.74870 to 0.75000, saving model to test-weights.h5
Epoch 98/150

 10/768 [..............................] - ETA: 0s - loss: 0.4100 - acc: 0.9000
300/768 [==========>...................] - ETA: 0s - loss: 0.5223 - acc: 0.7567
590/768 [======================>.......] - ETA: 0s - loss: 0.5138 - acc: 0.7441
768/768 [==============================] - 0s 176us/step - loss: 0.5235 - acc: 0.7396

Epoch 00098: acc did not improve from 0.75000
Epoch 99/150

 10/768 [..............................] - ETA: 0s - loss: 0.7017 - acc: 0.6000
310/768 [===========>..................] - ETA: 0s - loss: 0.5112 - acc: 0.7419
600/768 [======================>.......] - ETA: 0s - loss: 0.5315 - acc: 0.7267
768/768 [==============================] - 0s 172us/step - loss: 0.5221 - acc: 0.7292

Epoch 00099: acc did not improve from 0.75000
Epoch 100/150

 10/768 [..............................] - ETA: 0s - loss: 0.5165 - acc: 0.7000
310/768 [===========>..................] - ETA: 0s - loss: 0.4924 - acc: 0.7871
610/768 [======================>.......] - ETA: 0s - loss: 0.4996 - acc: 0.7705
768/768 [==============================] - 0s 172us/step - loss: 0.5100 - acc: 0.7552

Epoch 00100: acc improved from 0.75000 to 0.75521, saving model to test-weights.h5
Epoch 101/150

 10/768 [..............................] - ETA: 0s - loss: 0.3004 - acc: 0.9000
300/768 [==========>...................] - ETA: 0s - loss: 0.5268 - acc: 0.7267
600/768 [======================>.......] - ETA: 0s - loss: 0.5186 - acc: 0.7367
768/768 [==============================] - 0s 174us/step - loss: 0.5256 - acc: 0.7305

Epoch 00101: acc did not improve from 0.75521
Epoch 102/150

 10/768 [..............................] - ETA: 0s - loss: 0.4424 - acc: 0.9000
300/768 [==========>...................] - ETA: 0s - loss: 0.5437 - acc: 0.7067
590/768 [======================>.......] - ETA: 0s - loss: 0.5251 - acc: 0.7305
768/768 [==============================] - 0s 175us/step - loss: 0.5222 - acc: 0.7357

Epoch 00102: acc did not improve from 0.75521
Epoch 103/150

 10/768 [..............................] - ETA: 0s - loss: 0.4513 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.5175 - acc: 0.7714
570/768 [=====================>........] - ETA: 0s - loss: 0.5290 - acc: 0.7474
768/768 [==============================] - 0s 182us/step - loss: 0.5193 - acc: 0.7474

Epoch 00103: acc did not improve from 0.75521
Epoch 104/150

 10/768 [..............................] - ETA: 0s - loss: 0.3628 - acc: 0.9000
310/768 [===========>..................] - ETA: 0s - loss: 0.5264 - acc: 0.7387
590/768 [======================>.......] - ETA: 0s - loss: 0.5275 - acc: 0.7356
768/768 [==============================] - 0s 178us/step - loss: 0.5129 - acc: 0.7474

Epoch 00104: acc did not improve from 0.75521
Epoch 105/150

 10/768 [..............................] - ETA: 0s - loss: 0.7946 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.4879 - acc: 0.7414
570/768 [=====================>........] - ETA: 0s - loss: 0.5015 - acc: 0.7491
768/768 [==============================] - 0s 178us/step - loss: 0.5159 - acc: 0.7396

Epoch 00105: acc did not improve from 0.75521
Epoch 106/150

 10/768 [..............................] - ETA: 0s - loss: 0.5784 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5476 - acc: 0.6897
550/768 [====================>.........] - ETA: 0s - loss: 0.5147 - acc: 0.7400
768/768 [==============================] - 0s 189us/step - loss: 0.5143 - acc: 0.7435

Epoch 00106: acc did not improve from 0.75521
Epoch 107/150

 10/768 [..............................] - ETA: 0s - loss: 0.4394 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5114 - acc: 0.7414
570/768 [=====================>........] - ETA: 0s - loss: 0.5112 - acc: 0.7509
768/768 [==============================] - 0s 182us/step - loss: 0.5111 - acc: 0.7396

Epoch 00107: acc did not improve from 0.75521
Epoch 108/150

 10/768 [..............................] - ETA: 0s - loss: 0.6097 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.5045 - acc: 0.7655
570/768 [=====================>........] - ETA: 0s - loss: 0.5115 - acc: 0.7474
768/768 [==============================] - 0s 181us/step - loss: 0.5176 - acc: 0.7396

Epoch 00108: acc did not improve from 0.75521
Epoch 109/150

 10/768 [..............................] - ETA: 0s - loss: 0.4717 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5155 - acc: 0.7467
580/768 [=====================>........] - ETA: 0s - loss: 0.5176 - acc: 0.7431
768/768 [==============================] - 0s 180us/step - loss: 0.5148 - acc: 0.7461

Epoch 00109: acc did not improve from 0.75521
Epoch 110/150

 10/768 [..............................] - ETA: 0s - loss: 0.4290 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5328 - acc: 0.7345
580/768 [=====================>........] - ETA: 0s - loss: 0.5184 - acc: 0.7483
768/768 [==============================] - 0s 181us/step - loss: 0.5192 - acc: 0.7422

Epoch 00110: acc did not improve from 0.75521
Epoch 111/150

 10/768 [..............................] - ETA: 0s - loss: 0.6282 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.5229 - acc: 0.7207
570/768 [=====================>........] - ETA: 0s - loss: 0.5231 - acc: 0.7404
768/768 [==============================] - 0s 181us/step - loss: 0.5147 - acc: 0.7435

Epoch 00111: acc did not improve from 0.75521
Epoch 112/150

 10/768 [..............................] - ETA: 0s - loss: 0.4831 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5007 - acc: 0.7733
590/768 [======================>.......] - ETA: 0s - loss: 0.5157 - acc: 0.7508
768/768 [==============================] - 0s 178us/step - loss: 0.5153 - acc: 0.7422

Epoch 00112: acc did not improve from 0.75521
Epoch 113/150

 10/768 [..............................] - ETA: 0s - loss: 0.4655 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5230 - acc: 0.7286
560/768 [====================>.........] - ETA: 0s - loss: 0.5157 - acc: 0.7357
768/768 [==============================] - 0s 182us/step - loss: 0.5153 - acc: 0.7409

Epoch 00113: acc did not improve from 0.75521
Epoch 114/150

 10/768 [..............................] - ETA: 0s - loss: 0.5066 - acc: 0.6000
300/768 [==========>...................] - ETA: 0s - loss: 0.5145 - acc: 0.7133
580/768 [=====================>........] - ETA: 0s - loss: 0.5034 - acc: 0.7448
768/768 [==============================] - 0s 179us/step - loss: 0.5138 - acc: 0.7474

Epoch 00114: acc did not improve from 0.75521
Epoch 115/150

 10/768 [..............................] - ETA: 0s - loss: 0.4642 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5158 - acc: 0.7400
570/768 [=====================>........] - ETA: 0s - loss: 0.5144 - acc: 0.7421
768/768 [==============================] - 0s 186us/step - loss: 0.5206 - acc: 0.7344

Epoch 00115: acc did not improve from 0.75521
Epoch 116/150

 10/768 [..............................] - ETA: 0s - loss: 0.5901 - acc: 0.6000
280/768 [=========>....................] - ETA: 0s - loss: 0.5002 - acc: 0.7643
550/768 [====================>.........] - ETA: 0s - loss: 0.5024 - acc: 0.7564
768/768 [==============================] - 0s 187us/step - loss: 0.5140 - acc: 0.7461

Epoch 00116: acc did not improve from 0.75521
Epoch 117/150

 10/768 [..............................] - ETA: 0s - loss: 0.4342 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.4775 - acc: 0.7750
550/768 [====================>.........] - ETA: 0s - loss: 0.4991 - acc: 0.7691
768/768 [==============================] - 0s 188us/step - loss: 0.5141 - acc: 0.7435

Epoch 00117: acc did not improve from 0.75521
Epoch 118/150

 10/768 [..............................] - ETA: 0s - loss: 0.8042 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.5339 - acc: 0.7345
580/768 [=====================>........] - ETA: 0s - loss: 0.5301 - acc: 0.7379
768/768 [==============================] - 0s 178us/step - loss: 0.5196 - acc: 0.7396

Epoch 00118: acc did not improve from 0.75521
Epoch 119/150

 10/768 [..............................] - ETA: 0s - loss: 0.2432 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.5109 - acc: 0.7655
570/768 [=====================>........] - ETA: 0s - loss: 0.4924 - acc: 0.7614
768/768 [==============================] - 0s 180us/step - loss: 0.5143 - acc: 0.7552

Epoch 00119: acc improved from 0.75521 to 0.75521, saving model to test-weights.h5
Epoch 120/150

 10/768 [..............................] - ETA: 0s - loss: 0.3638 - acc: 0.9000
290/768 [==========>...................] - ETA: 0s - loss: 0.4788 - acc: 0.7621
580/768 [=====================>........] - ETA: 0s - loss: 0.5012 - acc: 0.7431
768/768 [==============================] - 0s 179us/step - loss: 0.5095 - acc: 0.7409

Epoch 00120: acc did not improve from 0.75521
Epoch 121/150

 10/768 [..............................] - ETA: 0s - loss: 0.5799 - acc: 0.7000
300/768 [==========>...................] - ETA: 0s - loss: 0.4699 - acc: 0.7867
580/768 [=====================>........] - ETA: 0s - loss: 0.5032 - acc: 0.7638
768/768 [==============================] - 0s 180us/step - loss: 0.5079 - acc: 0.7526

Epoch 00121: acc did not improve from 0.75521
Epoch 122/150

 10/768 [..............................] - ETA: 0s - loss: 0.3279 - acc: 1.0000
290/768 [==========>...................] - ETA: 0s - loss: 0.5272 - acc: 0.7483
570/768 [=====================>........] - ETA: 0s - loss: 0.5284 - acc: 0.7333
768/768 [==============================] - 0s 180us/step - loss: 0.5083 - acc: 0.7474

Epoch 00122: acc did not improve from 0.75521
Epoch 123/150

 10/768 [..............................] - ETA: 0s - loss: 0.4132 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5222 - acc: 0.7345
570/768 [=====================>........] - ETA: 0s - loss: 0.5204 - acc: 0.7509
768/768 [==============================] - 0s 185us/step - loss: 0.5141 - acc: 0.7526

Epoch 00123: acc did not improve from 0.75521
Epoch 124/150

 10/768 [..............................] - ETA: 0s - loss: 0.8817 - acc: 0.4000
300/768 [==========>...................] - ETA: 0s - loss: 0.5320 - acc: 0.7500
590/768 [======================>.......] - ETA: 0s - loss: 0.4943 - acc: 0.7644
768/768 [==============================] - 0s 178us/step - loss: 0.5111 - acc: 0.7526

Epoch 00124: acc did not improve from 0.75521
Epoch 125/150

 10/768 [..............................] - ETA: 0s - loss: 0.4387 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.5103 - acc: 0.7552
570/768 [=====================>........] - ETA: 0s - loss: 0.5115 - acc: 0.7614
768/768 [==============================] - 0s 180us/step - loss: 0.5095 - acc: 0.7513

Epoch 00125: acc did not improve from 0.75521
Epoch 126/150

 10/768 [..............................] - ETA: 0s - loss: 0.6075 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.5163 - acc: 0.7586
580/768 [=====================>........] - ETA: 0s - loss: 0.5201 - acc: 0.7500
768/768 [==============================] - 0s 179us/step - loss: 0.5119 - acc: 0.7526

Epoch 00126: acc did not improve from 0.75521
Epoch 127/150

 10/768 [..............................] - ETA: 0s - loss: 0.5759 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5202 - acc: 0.7500
580/768 [=====================>........] - ETA: 0s - loss: 0.5056 - acc: 0.7466
768/768 [==============================] - 0s 179us/step - loss: 0.5047 - acc: 0.7461

Epoch 00127: acc did not improve from 0.75521
Epoch 128/150

 10/768 [..............................] - ETA: 0s - loss: 0.4012 - acc: 0.8000
300/768 [==========>...................] - ETA: 0s - loss: 0.5122 - acc: 0.7300
580/768 [=====================>........] - ETA: 0s - loss: 0.5097 - acc: 0.7517
768/768 [==============================] - 0s 183us/step - loss: 0.5064 - acc: 0.7565

Epoch 00128: acc improved from 0.75521 to 0.75651, saving model to test-weights.h5
Epoch 129/150

 10/768 [..............................] - ETA: 0s - loss: 0.6558 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5042 - acc: 0.7536
570/768 [=====================>........] - ETA: 0s - loss: 0.5017 - acc: 0.7632
768/768 [==============================] - 0s 180us/step - loss: 0.5053 - acc: 0.7552

Epoch 00129: acc did not improve from 0.75651
Epoch 130/150

 10/768 [..............................] - ETA: 0s - loss: 0.4594 - acc: 0.7000
290/768 [==========>...................] - ETA: 0s - loss: 0.4778 - acc: 0.7759
570/768 [=====================>........] - ETA: 0s - loss: 0.4950 - acc: 0.7649
768/768 [==============================] - 0s 181us/step - loss: 0.5042 - acc: 0.7604

Epoch 00130: acc improved from 0.75651 to 0.76042, saving model to test-weights.h5
Epoch 131/150

 10/768 [..............................] - ETA: 0s - loss: 0.7999 - acc: 0.6000
290/768 [==========>...................] - ETA: 0s - loss: 0.5506 - acc: 0.7241
560/768 [====================>.........] - ETA: 0s - loss: 0.5190 - acc: 0.7500
768/768 [==============================] - 0s 184us/step - loss: 0.5048 - acc: 0.7513

Epoch 00131: acc did not improve from 0.76042
Epoch 132/150

 10/768 [..............................] - ETA: 0s - loss: 0.4045 - acc: 1.0000
290/768 [==========>...................] - ETA: 0s - loss: 0.4863 - acc: 0.7552
560/768 [====================>.........] - ETA: 0s - loss: 0.5075 - acc: 0.7411
768/768 [==============================] - 0s 188us/step - loss: 0.5096 - acc: 0.7513

Epoch 00132: acc did not improve from 0.76042
Epoch 133/150

 10/768 [..............................] - ETA: 0s - loss: 0.4238 - acc: 0.8000
270/768 [=========>....................] - ETA: 0s - loss: 0.5278 - acc: 0.7481
520/768 [===================>..........] - ETA: 0s - loss: 0.4970 - acc: 0.7692
768/768 [==============================] - 0s 198us/step - loss: 0.5013 - acc: 0.7591

Epoch 00133: acc did not improve from 0.76042
Epoch 134/150

 10/768 [..............................] - ETA: 0s - loss: 0.3465 - acc: 0.9000
280/768 [=========>....................] - ETA: 0s - loss: 0.5077 - acc: 0.7250
540/768 [====================>.........] - ETA: 0s - loss: 0.5044 - acc: 0.7426
768/768 [==============================] - 0s 196us/step - loss: 0.5016 - acc: 0.7474

Epoch 00134: acc did not improve from 0.76042
Epoch 135/150

 10/768 [..............................] - ETA: 0s - loss: 0.4405 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.4755 - acc: 0.7607
550/768 [====================>.........] - ETA: 0s - loss: 0.4946 - acc: 0.7527
768/768 [==============================] - 0s 186us/step - loss: 0.5017 - acc: 0.7461

Epoch 00135: acc did not improve from 0.76042
Epoch 136/150

 10/768 [..............................] - ETA: 0s - loss: 0.4251 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.4789 - acc: 0.7862
570/768 [=====================>........] - ETA: 0s - loss: 0.5024 - acc: 0.7509
768/768 [==============================] - 0s 184us/step - loss: 0.5037 - acc: 0.7448

Epoch 00136: acc did not improve from 0.76042
Epoch 137/150

 10/768 [..............................] - ETA: 0s - loss: 0.2816 - acc: 1.0000
280/768 [=========>....................] - ETA: 0s - loss: 0.5119 - acc: 0.7286
540/768 [====================>.........] - ETA: 0s - loss: 0.5082 - acc: 0.7241
768/768 [==============================] - 0s 195us/step - loss: 0.5064 - acc: 0.7435

Epoch 00137: acc did not improve from 0.76042
Epoch 138/150

 10/768 [..............................] - ETA: 0s - loss: 0.5590 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5171 - acc: 0.7286
540/768 [====================>.........] - ETA: 0s - loss: 0.4931 - acc: 0.7463
768/768 [==============================] - 0s 190us/step - loss: 0.5010 - acc: 0.7409

Epoch 00138: acc did not improve from 0.76042
Epoch 139/150

 10/768 [..............................] - ETA: 0s - loss: 0.4602 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5574 - acc: 0.7107
550/768 [====================>.........] - ETA: 0s - loss: 0.5167 - acc: 0.7418
768/768 [==============================] - 0s 188us/step - loss: 0.5119 - acc: 0.7422

Epoch 00139: acc did not improve from 0.76042
Epoch 140/150

 10/768 [..............................] - ETA: 0s - loss: 0.4893 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.4981 - acc: 0.7679
550/768 [====================>.........] - ETA: 0s - loss: 0.4989 - acc: 0.7655
768/768 [==============================] - 0s 191us/step - loss: 0.5039 - acc: 0.7578

Epoch 00140: acc did not improve from 0.76042
Epoch 141/150

 10/768 [..............................] - ETA: 0s - loss: 0.4287 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.4812 - acc: 0.7679
550/768 [====================>.........] - ETA: 0s - loss: 0.5206 - acc: 0.7345
768/768 [==============================] - 0s 189us/step - loss: 0.5055 - acc: 0.7487

Epoch 00141: acc did not improve from 0.76042
Epoch 142/150

 10/768 [..............................] - ETA: 0s - loss: 0.5102 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.4959 - acc: 0.7821
550/768 [====================>.........] - ETA: 0s - loss: 0.5053 - acc: 0.7745
768/768 [==============================] - 0s 188us/step - loss: 0.4959 - acc: 0.7773

Epoch 00142: acc improved from 0.76042 to 0.77734, saving model to test-weights.h5
Epoch 143/150

 10/768 [..............................] - ETA: 0s - loss: 0.3366 - acc: 0.9000
270/768 [=========>....................] - ETA: 0s - loss: 0.5098 - acc: 0.7556
530/768 [===================>..........] - ETA: 0s - loss: 0.5308 - acc: 0.7434
768/768 [==============================] - 0s 193us/step - loss: 0.5204 - acc: 0.7474

Epoch 00143: acc did not improve from 0.77734
Epoch 144/150

 10/768 [..............................] - ETA: 0s - loss: 0.4503 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.4995 - acc: 0.7607
550/768 [====================>.........] - ETA: 0s - loss: 0.5085 - acc: 0.7491
768/768 [==============================] - 0s 191us/step - loss: 0.5090 - acc: 0.7487

Epoch 00144: acc did not improve from 0.77734
Epoch 145/150

 10/768 [..............................] - ETA: 0s - loss: 0.3869 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5608 - acc: 0.6786
560/768 [====================>.........] - ETA: 0s - loss: 0.5097 - acc: 0.7357
768/768 [==============================] - 0s 186us/step - loss: 0.5006 - acc: 0.7500

Epoch 00145: acc did not improve from 0.77734
Epoch 146/150

 10/768 [..............................] - ETA: 0s - loss: 0.4786 - acc: 0.8000
290/768 [==========>...................] - ETA: 0s - loss: 0.4968 - acc: 0.7690
560/768 [====================>.........] - ETA: 0s - loss: 0.4892 - acc: 0.7625
768/768 [==============================] - 0s 188us/step - loss: 0.5044 - acc: 0.7448

Epoch 00146: acc did not improve from 0.77734
Epoch 147/150

 10/768 [..............................] - ETA: 0s - loss: 0.7859 - acc: 0.3000
270/768 [=========>....................] - ETA: 0s - loss: 0.5218 - acc: 0.7519
540/768 [====================>.........] - ETA: 0s - loss: 0.4983 - acc: 0.7630
768/768 [==============================] - 0s 195us/step - loss: 0.5012 - acc: 0.7578

Epoch 00147: acc did not improve from 0.77734
Epoch 148/150

 10/768 [..............................] - ETA: 0s - loss: 0.8431 - acc: 0.6000
270/768 [=========>....................] - ETA: 0s - loss: 0.5055 - acc: 0.7481
540/768 [====================>.........] - ETA: 0s - loss: 0.4999 - acc: 0.7519
768/768 [==============================] - 0s 192us/step - loss: 0.5012 - acc: 0.7500

Epoch 00148: acc did not improve from 0.77734
Epoch 149/150

 10/768 [..............................] - ETA: 0s - loss: 0.4931 - acc: 0.8000
280/768 [=========>....................] - ETA: 0s - loss: 0.5130 - acc: 0.7464
540/768 [====================>.........] - ETA: 0s - loss: 0.5064 - acc: 0.7500
768/768 [==============================] - 0s 195us/step - loss: 0.5064 - acc: 0.7474

Epoch 00149: acc did not improve from 0.77734
Epoch 150/150

 10/768 [..............................] - ETA: 0s - loss: 0.4361 - acc: 0.7000
280/768 [=========>....................] - ETA: 0s - loss: 0.5357 - acc: 0.7214
540/768 [====================>.........] - ETA: 0s - loss: 0.5015 - acc: 0.7444
768/768 [==============================] - 0s 195us/step - loss: 0.5175 - acc: 0.7474

Epoch 00150: acc did not improve from 0.77734

 32/768 [>.............................] - ETA: 0s
768/768 [==============================] - 0s 48us/step

acc: 75.65%
5 arka
>>> Imports:
#coding=utf-8

from __future__ import print_function

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras.models import Sequential
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.optimizers import RMSprop
except:
    pass

try:
    from keras.datasets import mnist
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'Dropout': hp.uniform('Dropout', 0, 1),
        'Dense': hp.choice('Dense', [256, 512, 1024]),
        'Dropout_1': hp.uniform('Dropout_1', 0, 1),
        'batch_size': hp.choice('batch_size', [64, 128]),
    }

>>> Functions
  1: def gen_data(y,z):
  2:     print(y,z)
  3:     (X_train, y_train), (X_test, y_test) = mnist.load_data()
  4:     X_train = X_train.reshape(60000, 784)
  5:     X_test = X_test.reshape(10000, 784)
  6:     X_train = X_train.astype('float32')
  7:     X_test = X_test.astype('float32')
  8:     X_train /= 255
  9:     X_test /= 255
 10:     nb_classes = 10
 11:     Y_train = np_utils.to_categorical(y_train, nb_classes)
 12:     Y_test = np_utils.to_categorical(y_test, nb_classes)
 13:     return X_train, Y_train, X_test, Y_test
 14: 
 15: 
>>> Data
  1: 
  2: '''
  3: Data providing function:
  4: This function is separated from model() so that hyperopt
  5: won't reload data for each evaluation run.
  6: '''
  7: X_train, Y_train, X_test, Y_test = gen_data(5,'arka')
  8: 
  9: 
 10: 
 11: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     '''
   4:     Model providing function:
   5:     Create Keras model with double curly brackets dropped-in as needed.
   6:     Return value has to be a valid python dictionary with two customary keys:
   7:         - loss: Specify a numeric evaluation metric to be minimized
   8:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible
   9:     The last one is optional, though recommended, namely:
  10:         - model: specify the model just created so that we can later use it again.
  11:     '''
  12:     model = Sequential()
  13:     model.add(Dense(512, input_shape=(784,)))
  14:     model.add(Activation('relu'))
  15:     model.add(Dropout(space['Dropout']))
  16:     model.add(Dense(space['Dense']))
  17:     model.add(Activation('relu'))
  18:     model.add(Dropout(space['Dropout_1']))
  19:     model.add(Dense(10))
  20:     model.add(Activation('softmax'))
  21: 
  22:     rms = RMSprop()
  23:     model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
  24: 
  25:     model.fit(X_train, Y_train,
  26:               batch_size=space['batch_size'],
  27:               nb_epoch=1,
  28:               verbose=2,
  29:               validation_data=(X_test, Y_test))
  30:     score, acc = model.evaluate(X_test, Y_test, verbose=0)
  31:     print('Test accuracy:', acc)
  32:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}
  33: 
5 arka
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
 - 4s - loss: 0.3764 - acc: 0.8854 - val_loss: 0.1575 - val_acc: 0.9536
Test accuracy: 0.9536
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
 - 2s - loss: 0.2613 - acc: 0.9220 - val_loss: 0.1115 - val_acc: 0.9654
Test accuracy: 0.9654
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
 - 1s - loss: 0.5811 - acc: 0.8182 - val_loss: 0.1940 - val_acc: 0.9447
Test accuracy: 0.9447
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
 - 1s - loss: 0.5683 - acc: 0.8241 - val_loss: 0.1782 - val_acc: 0.9491
Test accuracy: 0.9491
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
 - 1s - loss: 0.4172 - acc: 0.8736 - val_loss: 0.1499 - val_acc: 0.9564
Test accuracy: 0.9564
Evalutation of best performing model:

   32/10000 [..............................] - ETA: 0s
 2720/10000 [=======>......................] - ETA: 0s
 5440/10000 [===============>..............] - ETA: 0s
 8128/10000 [=======================>......] - ETA: 0s
10000/10000 [==============================] - 0s 19us/step
[0.11151329515604302, 0.9654]
best model <keras.engine.sequential.Sequential object at 0x7f6bfb4e6588>
best run {'Dense': 1, 'Dropout': 0.42522861686845626, 'Dropout_1': 0.23316134447477344, 'batch_size': 0}
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    import pickle
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'dense_filter': hp.choice('dense_filter', [2048,1024,512,256]),
        'dense_filter1': hp.choice('dense_filter1', [1024,512,256]),
        'dense_filter2': hp.choice('dense_filter2', [1024,512,256,96]),
        'dropout1': hp.uniform('dropout1', 0,1),
        'dropout1_1': hp.uniform('dropout1_1', 0,1),
        'layers': hp.choice('layers', ["one","two"]),
    }

>>> Functions
   1: def process_data():
   2:     filename = 'sonar_data'
   3:     infile = open(filename,'rb')
   4:     sonar_data_dict = pickle.load(infile)
   5: 
   6:     infile.close()
   7:     x_train1 = sonar_data_dict.get("x_train1")
   8:     x_train2 = sonar_data_dict.get("x_train2")
   9:     y_train = sonar_data_dict.get("y_train")
  10: 
  11:     x_val1 = sonar_data_dict.get("x_val1")
  12:     x_val2 = sonar_data_dict.get("x_val2")
  13:     y_val = sonar_data_dict.get("y_val")
  14: 
  15:     x_test1 = sonar_data_dict.get("x_test1")
  16:     x_test2 = sonar_data_dict.get("x_test2")
  17:     y_test = sonar_data_dict.get("y_test")
  18: 
  19:     y_test_inverted = 1 - y_test
  20:     y_train_inverted = 1 - y_train
  21:     y_val_inverted = 1 - y_val
  22: 
  23:     return x_train1, x_train2, y_train_inverted, x_val1, x_val2, y_val_inverted, x_test1, x_test2, y_test_inverted
  24: 
  25: def create_base_network(input_shape,dense_filter,dense_filter1,dense_filter2,dropout1,dropout2,layers):
  26:     random_seed = 7
  27: 
  28:     model = Input(shape=input_shape)
  29:     x = Conv2D(16, (3, 3), padding="same",activation='relu', data_format='channels_first',
  30:                kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed),
  31:               bias_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed))(model)
  32: 
  33:     x = Conv2D(16, (3, 3), padding="same", activation='relu',
  34:                kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed),
  35:               bias_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed))(x)
  36:     x = MaxPooling2D(pool_size=(2, 2),strides=(2, 2))(x)
  37: 
  38:     x = Conv2D(32, (3, 3), padding="same", activation='relu',
  39:                kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed),
  40:               bias_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed))(x)
  41: 
  42:     x = Conv2D(32, (3, 3), padding="same", activation='relu',
  43:                kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed),
  44:               bias_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=random_seed))(x)
  45: 
  46:     x = MaxPooling2D(pool_size=(2, 2),strides=(2, 2))(x)
  47:     x = Flatten()(x)
  48:     x = Dense(dense_filter, kernel_initializer=keras.initializers.he_normal(seed=random_seed),activation='relu')(x)
  49:     if layers == "two":
  50:         x = Dense(dense_filter1, kernel_initializer=keras.initializers.he_normal(seed=random_seed),activation='relu')(x)
  51:         x = Dropout(dropout1)(x)
  52:     return Model(model,x)
  53: 
  54: def euclidean_distance(vects):
  55:     x, y = vects
  56:     return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))
  57: 
  58: def eucl_dist_output_shape(shapes):
  59:     shape1, shape2 = shapes
  60:     return (shape1[0], 1)
  61: 
  62: def contrastive_loss_altered(y_true, y_pred):
  63:     margin = 1.0
  64:     return K.mean((1-y_true) * K.square(y_pred) +
  65:                   y_true * K.square(K.maximum(margin - y_pred, 0)))
  66: 
  67: 
>>> Data
 1: 
 2: x_train1, x_train2, y_train, x_val1, x_val2, y_val, x_test1, x_test2, y_test = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 30
   4:     input_shape = (1,96,96)
   5:     patience_ = 1
   6:     dense_filter = space['dense_filter']
   7:     dense_filter1 = space['dense_filter1']
   8:     dense_filter2 = space['dense_filter2']
   9:     dropout1 = space['dropout1']
  10:     dropout2 = space['dropout1_1']
  11:     layers = space['layers']
  12:     base_network = create_base_network(input_shape,dense_filter,dense_filter1,dense_filter2,dropout1,dropout2,layers)
  13: 
  14:     input_a = Input(shape=input_shape)
  15:     input_b = Input(shape=input_shape)
  16: 
  17:     processed_a = base_network(input_a)
  18:     processed_b = base_network(input_b)
  19: 
  20:     opt = Adam(lr=1e-5,decay=1e-3)
  21: 
  22:     distance = Lambda(euclidean_distance,
  23:                       output_shape=eucl_dist_output_shape)([processed_a, processed_b])
  24:     model = Model([input_a, input_b], distance)
  25:     model.compile(loss=contrastive_loss_altered, optimizer=opt, metrics=["accuracy"])
  26: 
  27: 
  28:     es = EarlyStopping(monitor='val_acc', patience=patience_,verbose=1)
  29:     checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',
  30:                                    verbose=1,
  31:                                    save_best_only=True)
  32:     model.fit([x_train1, x_train2],y_train,
  33:           batch_size=64,
  34:           epochs=epochs,
  35:           validation_data=([x_val1,x_val2], y_val),
  36:           callbacks=[es],
  37:           verbose=1)
  38: 
  39:     score, acc = model.evaluate([x_test1, x_test2], y_test,verbose=0)
  40:     print('Test accuracy:', acc)
  41:     pred = model.predict([x_test1, x_test2],verbose=0)
  42:     auc_score = roc_auc_score(y_test,pred)
  43:     
  44:     threshold = .4
  45:     pred_scores2 = (pred>threshold).astype(int) 
  46: 
  47:     test_acc2 = accuracy_score(y_test,pred_scores2)
  48:     print("score new .7 ------------- >",np.round(test_acc2,3))
  49:     threshold = .6                 
  50:     pred_scores2 = (pred>threshold).astype(int)
  51: 
  52:     test_acc2 = accuracy_score(y_test,pred_scores2)
  53:     print("score new .6 ----------- >",np.round(test_acc2,3))
  54: 
  55:     print("auc_score ------------------ ",np.round(auc_score,4))
  56:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}
  57: 
Train on 31872 samples, validate on 7968 samples
Epoch 1/30

   64/31872 [..............................] - ETA: 32:28 - loss: 0.4396 - acc: 0.4688
  128/31872 [..............................] - ETA: 16:24 - loss: 0.4301 - acc: 0.4766
  320/31872 [..............................] - ETA: 6:37 - loss: 0.4279 - acc: 0.4719 
  384/31872 [..............................] - ETA: 5:35 - loss: 0.4207 - acc: 0.4792
  576/31872 [..............................] - ETA: 3:45 - loss: 0.3996 - acc: 0.5017
  704/31872 [..............................] - ETA: 3:06 - loss: 0.4032 - acc: 0.4943
  896/31872 [..............................] - ETA: 2:28 - loss: 0.3899 - acc: 0.5078
 1024/31872 [..............................] - ETA: 2:11 - loss: 0.3825 - acc: 0.5146
 1152/31872 [>.............................] - ETA: 1:57 - loss: 0.3798 - acc: 0.5156
 1280/31872 [>.............................] - ETA: 1:46 - loss: 0.3773 - acc: 0.5156
 1408/31872 [>.............................] - ETA: 1:37 - loss: 0.3761 - acc: 0.5149
 1536/31872 [>.............................] - ETA: 1:30 - loss: 0.3742 - acc: 0.5143
 1728/31872 [>.............................] - ETA: 1:21 - loss: 0.3763 - acc: 0.5069
 1920/31872 [>.............................] - ETA: 1:13 - loss: 0.3753 - acc: 0.5042
 2112/31872 [>.............................] - ETA: 1:07 - loss: 0.3733 - acc: 0.5024
 2304/31872 [=>............................] - ETA: 1:02 - loss: 0.3681 - acc: 0.5052
 2496/31872 [=>............................] - ETA: 57s - loss: 0.3677 - acc: 0.4980 
 2688/31872 [=>............................] - ETA: 54s - loss: 0.3645 - acc: 0.4989
 2880/31872 [=>............................] - ETA: 50s - loss: 0.3628 - acc: 0.4962
 3072/31872 [=>............................] - ETA: 47s - loss: 0.3595 - acc: 0.4958
 3264/31872 [==>...........................] - ETA: 45s - loss: 0.3563 - acc: 0.4954
 3456/31872 [==>...........................] - ETA: 42s - loss: 0.3523 - acc: 0.4988
 3648/31872 [==>...........................] - ETA: 40s - loss: 0.3481 - acc: 0.5008
 3840/31872 [==>...........................] - ETA: 39s - loss: 0.3445 - acc: 0.5031
 4032/31872 [==>...........................] - ETA: 37s - loss: 0.3406 - acc: 0.5060
 4224/31872 [==>...........................] - ETA: 35s - loss: 0.3373 - acc: 0.5092
 4416/31872 [===>..........................] - ETA: 34s - loss: 0.3332 - acc: 0.5125
 4608/31872 [===>..........................] - ETA: 33s - loss: 0.3305 - acc: 0.5135
 4800/31872 [===>..........................] - ETA: 31s - loss: 0.3274 - acc: 0.5146
 4992/31872 [===>..........................] - ETA: 30s - loss: 0.3241 - acc: 0.5168
 5184/31872 [===>..........................] - ETA: 29s - loss: 0.3209 - acc: 0.5201
 5376/31872 [====>.........................] - ETA: 28s - loss: 0.3177 - acc: 0.5236
 5568/31872 [====>.........................] - ETA: 27s - loss: 0.3146 - acc: 0.5262
 5760/31872 [====>.........................] - ETA: 27s - loss: 0.3123 - acc: 0.5274
 5952/31872 [====>.........................] - ETA: 26s - loss: 0.3095 - acc: 0.5306
 6144/31872 [====>.........................] - ETA: 25s - loss: 0.3069 - acc: 0.5330
 6336/31872 [====>.........................] - ETA: 24s - loss: 0.3044 - acc: 0.5363
 6528/31872 [=====>........................] - ETA: 24s - loss: 0.3020 - acc: 0.5389
 6720/31872 [=====>........................] - ETA: 23s - loss: 0.3003 - acc: 0.5403
 6912/31872 [=====>........................] - ETA: 22s - loss: 0.2986 - acc: 0.5421
 7104/31872 [=====>........................] - ETA: 22s - loss: 0.2971 - acc: 0.5438
 7296/31872 [=====>........................] - ETA: 21s - loss: 0.2945 - acc: 0.5471
 7488/31872 [======>.......................] - ETA: 21s - loss: 0.2927 - acc: 0.5494
 7680/31872 [======>.......................] - ETA: 20s - loss: 0.2908 - acc: 0.5517
 7872/31872 [======>.......................] - ETA: 20s - loss: 0.2891 - acc: 0.5532
 8064/31872 [======>.......................] - ETA: 19s - loss: 0.2874 - acc: 0.5557
 8256/31872 [======>.......................] - ETA: 19s - loss: 0.2867 - acc: 0.5555
 8448/31872 [======>.......................] - ETA: 19s - loss: 0.2857 - acc: 0.5566
 8640/31872 [=======>......................] - ETA: 18s - loss: 0.2840 - acc: 0.5589
 8832/31872 [=======>......................] - ETA: 18s - loss: 0.2833 - acc: 0.5589
 9024/31872 [=======>......................] - ETA: 17s - loss: 0.2812 - acc: 0.5626
 9216/31872 [=======>......................] - ETA: 17s - loss: 0.2794 - acc: 0.5660
 9408/31872 [=======>......................] - ETA: 17s - loss: 0.2778 - acc: 0.5680
 9600/31872 [========>.....................] - ETA: 16s - loss: 0.2764 - acc: 0.5702
 9792/31872 [========>.....................] - ETA: 16s - loss: 0.2750 - acc: 0.5720
 9984/31872 [========>.....................] - ETA: 16s - loss: 0.2742 - acc: 0.5730
10176/31872 [========>.....................] - ETA: 15s - loss: 0.2728 - acc: 0.5752
10368/31872 [========>.....................] - ETA: 15s - loss: 0.2716 - acc: 0.5772
10560/31872 [========>.....................] - ETA: 15s - loss: 0.2708 - acc: 0.5782
10752/31872 [=========>....................] - ETA: 14s - loss: 0.2695 - acc: 0.5800
10944/31872 [=========>....................] - ETA: 14s - loss: 0.2678 - acc: 0.5827
11136/31872 [=========>....................] - ETA: 14s - loss: 0.2668 - acc: 0.5841
11328/31872 [=========>....................] - ETA: 14s - loss: 0.2656 - acc: 0.5859
11520/31872 [=========>....................] - ETA: 13s - loss: 0.2646 - acc: 0.5873
11712/31872 [==========>...................] - ETA: 13s - loss: 0.2635 - acc: 0.5891
11904/31872 [==========>...................] - ETA: 13s - loss: 0.2628 - acc: 0.5901
12096/31872 [==========>...................] - ETA: 13s - loss: 0.2617 - acc: 0.5914
12288/31872 [==========>...................] - ETA: 12s - loss: 0.2605 - acc: 0.5935
12480/31872 [==========>...................] - ETA: 12s - loss: 0.2596 - acc: 0.5951
12672/31872 [==========>...................] - ETA: 12s - loss: 0.2587 - acc: 0.5965
12864/31872 [===========>..................] - ETA: 12s - loss: 0.2579 - acc: 0.5978
13056/31872 [===========>..................] - ETA: 12s - loss: 0.2567 - acc: 0.5999
13248/31872 [===========>..................] - ETA: 11s - loss: 0.2559 - acc: 0.6011
13440/31872 [===========>..................] - ETA: 11s - loss: 0.2552 - acc: 0.6019
13632/31872 [===========>..................] - ETA: 11s - loss: 0.2543 - acc: 0.6033
13824/31872 [============>.................] - ETA: 11s - loss: 0.2533 - acc: 0.6048
14016/31872 [============>.................] - ETA: 11s - loss: 0.2526 - acc: 0.6063
14208/31872 [============>.................] - ETA: 10s - loss: 0.2517 - acc: 0.6078
14400/31872 [============>.................] - ETA: 10s - loss: 0.2508 - acc: 0.6090
14592/31872 [============>.................] - ETA: 10s - loss: 0.2498 - acc: 0.6108
14784/31872 [============>.................] - ETA: 10s - loss: 0.2488 - acc: 0.6128
14976/31872 [=============>................] - ETA: 10s - loss: 0.2479 - acc: 0.6142
15168/31872 [=============>................] - ETA: 9s - loss: 0.2472 - acc: 0.6158 
15360/31872 [=============>................] - ETA: 9s - loss: 0.2467 - acc: 0.6165
15552/31872 [=============>................] - ETA: 9s - loss: 0.2457 - acc: 0.6183
15744/31872 [=============>................] - ETA: 9s - loss: 0.2448 - acc: 0.6194
15936/31872 [==============>...............] - ETA: 9s - loss: 0.2440 - acc: 0.6210
16128/31872 [==============>...............] - ETA: 9s - loss: 0.2432 - acc: 0.6225
16320/31872 [==============>...............] - ETA: 8s - loss: 0.2423 - acc: 0.6236
16512/31872 [==============>...............] - ETA: 8s - loss: 0.2412 - acc: 0.6253
16704/31872 [==============>...............] - ETA: 8s - loss: 0.2404 - acc: 0.6269
16896/31872 [==============>...............] - ETA: 8s - loss: 0.2394 - acc: 0.6286
17088/31872 [===============>..............] - ETA: 8s - loss: 0.2385 - acc: 0.6303
17280/31872 [===============>..............] - ETA: 8s - loss: 0.2378 - acc: 0.6311
17472/31872 [===============>..............] - ETA: 8s - loss: 0.2369 - acc: 0.6328
17664/31872 [===============>..............] - ETA: 7s - loss: 0.2363 - acc: 0.6340
17856/31872 [===============>..............] - ETA: 7s - loss: 0.2356 - acc: 0.6354
18048/31872 [===============>..............] - ETA: 7s - loss: 0.2350 - acc: 0.6365
18240/31872 [================>.............] - ETA: 7s - loss: 0.2344 - acc: 0.6375
18432/31872 [================>.............] - ETA: 7s - loss: 0.2337 - acc: 0.6385
18624/31872 [================>.............] - ETA: 7s - loss: 0.2330 - acc: 0.6397
18816/31872 [================>.............] - ETA: 7s - loss: 0.2324 - acc: 0.6404
19008/31872 [================>.............] - ETA: 6s - loss: 0.2317 - acc: 0.6415
19200/31872 [=================>............] - ETA: 6s - loss: 0.2312 - acc: 0.6424
19392/31872 [=================>............] - ETA: 6s - loss: 0.2305 - acc: 0.6436
19584/31872 [=================>............] - ETA: 6s - loss: 0.2300 - acc: 0.6445
19776/31872 [=================>............] - ETA: 6s - loss: 0.2293 - acc: 0.6458
19968/31872 [=================>............] - ETA: 6s - loss: 0.2284 - acc: 0.6474
20160/31872 [=================>............] - ETA: 6s - loss: 0.2277 - acc: 0.6484
20352/31872 [==================>...........] - ETA: 6s - loss: 0.2272 - acc: 0.6492
20544/31872 [==================>...........] - ETA: 5s - loss: 0.2265 - acc: 0.6501
20736/31872 [==================>...........] - ETA: 5s - loss: 0.2260 - acc: 0.6511
20928/31872 [==================>...........] - ETA: 5s - loss: 0.2256 - acc: 0.6513
21120/31872 [==================>...........] - ETA: 5s - loss: 0.2249 - acc: 0.6525
21312/31872 [===================>..........] - ETA: 5s - loss: 0.2244 - acc: 0.6535
21504/31872 [===================>..........] - ETA: 5s - loss: 0.2238 - acc: 0.6547
21696/31872 [===================>..........] - ETA: 5s - loss: 0.2233 - acc: 0.6555
21888/31872 [===================>..........] - ETA: 5s - loss: 0.2228 - acc: 0.6563
22080/31872 [===================>..........] - ETA: 5s - loss: 0.2224 - acc: 0.6572
22272/31872 [===================>..........] - ETA: 4s - loss: 0.2219 - acc: 0.6576
22464/31872 [====================>.........] - ETA: 4s - loss: 0.2213 - acc: 0.6587
22656/31872 [====================>.........] - ETA: 4s - loss: 0.2206 - acc: 0.6601
22848/31872 [====================>.........] - ETA: 4s - loss: 0.2201 - acc: 0.6610
23040/31872 [====================>.........] - ETA: 4s - loss: 0.2196 - acc: 0.6620
23232/31872 [====================>.........] - ETA: 4s - loss: 0.2192 - acc: 0.6628
23424/31872 [=====================>........] - ETA: 4s - loss: 0.2187 - acc: 0.6639
23616/31872 [=====================>........] - ETA: 4s - loss: 0.2182 - acc: 0.6648
23808/31872 [=====================>........] - ETA: 4s - loss: 0.2177 - acc: 0.6659
24000/31872 [=====================>........] - ETA: 3s - loss: 0.2172 - acc: 0.6670
24192/31872 [=====================>........] - ETA: 3s - loss: 0.2168 - acc: 0.6679
24384/31872 [=====================>........] - ETA: 3s - loss: 0.2163 - acc: 0.6688
24576/31872 [======================>.......] - ETA: 3s - loss: 0.2158 - acc: 0.6697
24768/31872 [======================>.......] - ETA: 3s - loss: 0.2153 - acc: 0.6704
24960/31872 [======================>.......] - ETA: 3s - loss: 0.2149 - acc: 0.6713
25152/31872 [======================>.......] - ETA: 3s - loss: 0.2145 - acc: 0.6722
25344/31872 [======================>.......] - ETA: 3s - loss: 0.2140 - acc: 0.6730
25536/31872 [=======================>......] - ETA: 3s - loss: 0.2135 - acc: 0.6740
25728/31872 [=======================>......] - ETA: 2s - loss: 0.2132 - acc: 0.6746
25920/31872 [=======================>......] - ETA: 2s - loss: 0.2125 - acc: 0.6760
26112/31872 [=======================>......] - ETA: 2s - loss: 0.2121 - acc: 0.6768
26304/31872 [=======================>......] - ETA: 2s - loss: 0.2115 - acc: 0.6777
26496/31872 [=======================>......] - ETA: 2s - loss: 0.2110 - acc: 0.6786
26688/31872 [========================>.....] - ETA: 2s - loss: 0.2105 - acc: 0.6794
26880/31872 [========================>.....] - ETA: 2s - loss: 0.2102 - acc: 0.6797
27072/31872 [========================>.....] - ETA: 2s - loss: 0.2098 - acc: 0.6804
27264/31872 [========================>.....] - ETA: 2s - loss: 0.2091 - acc: 0.6817
27456/31872 [========================>.....] - ETA: 2s - loss: 0.2087 - acc: 0.6825
27648/31872 [=========================>....] - ETA: 2s - loss: 0.2083 - acc: 0.6833
27840/31872 [=========================>....] - ETA: 1s - loss: 0.2080 - acc: 0.6839
28032/31872 [=========================>....] - ETA: 1s - loss: 0.2076 - acc: 0.6845
28224/31872 [=========================>....] - ETA: 1s - loss: 0.2071 - acc: 0.6854
28416/31872 [=========================>....] - ETA: 1s - loss: 0.2066 - acc: 0.6861
28608/31872 [=========================>....] - ETA: 1s - loss: 0.2063 - acc: 0.6869
28800/31872 [==========================>...] - ETA: 1s - loss: 0.2059 - acc: 0.6877
28992/31872 [==========================>...] - ETA: 1s - loss: 0.2055 - acc: 0.6884
29184/31872 [==========================>...] - ETA: 1s - loss: 0.2051 - acc: 0.6890
29376/31872 [==========================>...] - ETA: 1s - loss: 0.2047 - acc: 0.6899
29568/31872 [==========================>...] - ETA: 1s - loss: 0.2043 - acc: 0.6907
29760/31872 [===========================>..] - ETA: 0s - loss: 0.2039 - acc: 0.6917
29952/31872 [===========================>..] - ETA: 0s - loss: 0.2035 - acc: 0.6925
30144/31872 [===========================>..] - ETA: 0s - loss: 0.2030 - acc: 0.6936
30336/31872 [===========================>..] - ETA: 0s - loss: 0.2026 - acc: 0.6944
30528/31872 [===========================>..] - ETA: 0s - loss: 0.2022 - acc: 0.6950
30720/31872 [===========================>..] - ETA: 0s - loss: 0.2018 - acc: 0.6959
30912/31872 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.6968
31104/31872 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.6978
31296/31872 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.6984
31488/31872 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.6992
31680/31872 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.7000
31872/31872 [==============================] - 16s 487us/step - loss: 0.1995 - acc: 0.7005 - val_loss: 0.1366 - val_acc: 0.8251
Epoch 2/30

   64/31872 [..............................] - ETA: 10s - loss: 0.1631 - acc: 0.7500
  256/31872 [..............................] - ETA: 10s - loss: 0.1566 - acc: 0.7773
  448/31872 [..............................] - ETA: 10s - loss: 0.1482 - acc: 0.8080
  640/31872 [..............................] - ETA: 10s - loss: 0.1457 - acc: 0.7984
  832/31872 [..............................] - ETA: 10s - loss: 0.1435 - acc: 0.8041
 1024/31872 [..............................] - ETA: 10s - loss: 0.1429 - acc: 0.8066
 1216/31872 [>.............................] - ETA: 9s - loss: 0.1428 - acc: 0.8076 
 1408/31872 [>.............................] - ETA: 9s - loss: 0.1425 - acc: 0.8082
 1600/31872 [>.............................] - ETA: 9s - loss: 0.1431 - acc: 0.8087
 1792/31872 [>.............................] - ETA: 9s - loss: 0.1410 - acc: 0.8142
 1984/31872 [>.............................] - ETA: 9s - loss: 0.1405 - acc: 0.8125
 2176/31872 [=>............................] - ETA: 9s - loss: 0.1396 - acc: 0.8134
 2368/31872 [=>............................] - ETA: 9s - loss: 0.1377 - acc: 0.8171
 2560/31872 [=>............................] - ETA: 9s - loss: 0.1378 - acc: 0.8164
 2752/31872 [=>............................] - ETA: 9s - loss: 0.1361 - acc: 0.8201
 2944/31872 [=>............................] - ETA: 9s - loss: 0.1348 - acc: 0.8220
 3136/31872 [=>............................] - ETA: 9s - loss: 0.1339 - acc: 0.8256
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.1349 - acc: 0.8239
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.1332 - acc: 0.8276
 3712/31872 [==>...........................] - ETA: 9s - loss: 0.1331 - acc: 0.8287
 3904/31872 [==>...........................] - ETA: 9s - loss: 0.1333 - acc: 0.8284
 4096/31872 [==>...........................] - ETA: 9s - loss: 0.1341 - acc: 0.8276
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.1338 - acc: 0.8286
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.1340 - acc: 0.8275
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.1334 - acc: 0.8286
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.1332 - acc: 0.8292
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.1333 - acc: 0.8289
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.1332 - acc: 0.8295
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.1327 - acc: 0.8311
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.1324 - acc: 0.8322
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.1324 - acc: 0.8328
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.1320 - acc: 0.8338
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.1315 - acc: 0.8344
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.1312 - acc: 0.8356
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.1312 - acc: 0.8356
 6784/31872 [=====>........................] - ETA: 8s - loss: 0.1311 - acc: 0.8351
 6976/31872 [=====>........................] - ETA: 8s - loss: 0.1309 - acc: 0.8362
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.1306 - acc: 0.8376
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.1303 - acc: 0.8386
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.1302 - acc: 0.8387
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.1301 - acc: 0.8391
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.1304 - acc: 0.8383
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.1301 - acc: 0.8390
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.1296 - acc: 0.8393
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.1296 - acc: 0.8393
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.1290 - acc: 0.8401
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.1289 - acc: 0.8406
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.1286 - acc: 0.8414
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.1280 - acc: 0.8427
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.1279 - acc: 0.8421
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.1280 - acc: 0.8413
 9856/31872 [========>.....................] - ETA: 7s - loss: 0.1281 - acc: 0.8413
10048/31872 [========>.....................] - ETA: 7s - loss: 0.1277 - acc: 0.8421
10240/31872 [========>.....................] - ETA: 6s - loss: 0.1278 - acc: 0.8423
10432/31872 [========>.....................] - ETA: 6s - loss: 0.1279 - acc: 0.8423
10624/31872 [=========>....................] - ETA: 6s - loss: 0.1280 - acc: 0.8424
10816/31872 [=========>....................] - ETA: 6s - loss: 0.1279 - acc: 0.8429
11008/31872 [=========>....................] - ETA: 6s - loss: 0.1281 - acc: 0.8423
11200/31872 [=========>....................] - ETA: 6s - loss: 0.1278 - acc: 0.8426
11392/31872 [=========>....................] - ETA: 6s - loss: 0.1277 - acc: 0.8427
11584/31872 [=========>....................] - ETA: 6s - loss: 0.1279 - acc: 0.8424
11776/31872 [==========>...................] - ETA: 6s - loss: 0.1276 - acc: 0.8435
11968/31872 [==========>...................] - ETA: 6s - loss: 0.1276 - acc: 0.8439
12160/31872 [==========>...................] - ETA: 6s - loss: 0.1274 - acc: 0.8441
12352/31872 [==========>...................] - ETA: 6s - loss: 0.1272 - acc: 0.8440
12544/31872 [==========>...................] - ETA: 6s - loss: 0.1271 - acc: 0.8445
12736/31872 [==========>...................] - ETA: 6s - loss: 0.1268 - acc: 0.8448
12928/31872 [===========>..................] - ETA: 6s - loss: 0.1268 - acc: 0.8449
13120/31872 [===========>..................] - ETA: 6s - loss: 0.1270 - acc: 0.8450
13312/31872 [===========>..................] - ETA: 5s - loss: 0.1269 - acc: 0.8450
13504/31872 [===========>..................] - ETA: 5s - loss: 0.1269 - acc: 0.8450
13696/31872 [===========>..................] - ETA: 5s - loss: 0.1266 - acc: 0.8456
13888/31872 [============>.................] - ETA: 5s - loss: 0.1266 - acc: 0.8450
14080/31872 [============>.................] - ETA: 5s - loss: 0.1266 - acc: 0.8452
14272/31872 [============>.................] - ETA: 5s - loss: 0.1264 - acc: 0.8454
14464/31872 [============>.................] - ETA: 5s - loss: 0.1263 - acc: 0.8456
14656/31872 [============>.................] - ETA: 5s - loss: 0.1261 - acc: 0.8459
14848/31872 [============>.................] - ETA: 5s - loss: 0.1260 - acc: 0.8462
15040/31872 [=============>................] - ETA: 5s - loss: 0.1259 - acc: 0.8463
15232/31872 [=============>................] - ETA: 5s - loss: 0.1259 - acc: 0.8464
15424/31872 [=============>................] - ETA: 5s - loss: 0.1257 - acc: 0.8471
15616/31872 [=============>................] - ETA: 5s - loss: 0.1256 - acc: 0.8470
15808/31872 [=============>................] - ETA: 5s - loss: 0.1254 - acc: 0.8475
16000/31872 [==============>...............] - ETA: 5s - loss: 0.1252 - acc: 0.8479
16192/31872 [==============>...............] - ETA: 5s - loss: 0.1251 - acc: 0.8481
16384/31872 [==============>...............] - ETA: 4s - loss: 0.1247 - acc: 0.8486
16576/31872 [==============>...............] - ETA: 4s - loss: 0.1244 - acc: 0.8492
16768/31872 [==============>...............] - ETA: 4s - loss: 0.1245 - acc: 0.8492
16960/31872 [==============>...............] - ETA: 4s - loss: 0.1245 - acc: 0.8494
17152/31872 [===============>..............] - ETA: 4s - loss: 0.1243 - acc: 0.8496
17344/31872 [===============>..............] - ETA: 4s - loss: 0.1241 - acc: 0.8496
17536/31872 [===============>..............] - ETA: 4s - loss: 0.1240 - acc: 0.8501
17728/31872 [===============>..............] - ETA: 4s - loss: 0.1239 - acc: 0.8506
17920/31872 [===============>..............] - ETA: 4s - loss: 0.1239 - acc: 0.8507
18112/31872 [================>.............] - ETA: 4s - loss: 0.1238 - acc: 0.8510
18304/31872 [================>.............] - ETA: 4s - loss: 0.1236 - acc: 0.8511
18496/31872 [================>.............] - ETA: 4s - loss: 0.1235 - acc: 0.8514
18688/31872 [================>.............] - ETA: 4s - loss: 0.1234 - acc: 0.8519
18880/31872 [================>.............] - ETA: 4s - loss: 0.1234 - acc: 0.8514
19072/31872 [================>.............] - ETA: 4s - loss: 0.1234 - acc: 0.8517
19264/31872 [=================>............] - ETA: 4s - loss: 0.1233 - acc: 0.8518
19456/31872 [=================>............] - ETA: 4s - loss: 0.1233 - acc: 0.8517
19648/31872 [=================>............] - ETA: 3s - loss: 0.1232 - acc: 0.8519
19840/31872 [=================>............] - ETA: 3s - loss: 0.1232 - acc: 0.8518
20032/31872 [=================>............] - ETA: 3s - loss: 0.1229 - acc: 0.8523
20224/31872 [==================>...........] - ETA: 3s - loss: 0.1230 - acc: 0.8523
20416/31872 [==================>...........] - ETA: 3s - loss: 0.1229 - acc: 0.8526
20608/31872 [==================>...........] - ETA: 3s - loss: 0.1228 - acc: 0.8529
20800/31872 [==================>...........] - ETA: 3s - loss: 0.1226 - acc: 0.8530
20992/31872 [==================>...........] - ETA: 3s - loss: 0.1227 - acc: 0.8529
21184/31872 [==================>...........] - ETA: 3s - loss: 0.1225 - acc: 0.8531
21376/31872 [===================>..........] - ETA: 3s - loss: 0.1224 - acc: 0.8535
21568/31872 [===================>..........] - ETA: 3s - loss: 0.1223 - acc: 0.8540
21760/31872 [===================>..........] - ETA: 3s - loss: 0.1222 - acc: 0.8541
21952/31872 [===================>..........] - ETA: 3s - loss: 0.1221 - acc: 0.8543
22144/31872 [===================>..........] - ETA: 3s - loss: 0.1220 - acc: 0.8545
22336/31872 [====================>.........] - ETA: 3s - loss: 0.1217 - acc: 0.8548
22528/31872 [====================>.........] - ETA: 3s - loss: 0.1217 - acc: 0.8548
22720/31872 [====================>.........] - ETA: 2s - loss: 0.1216 - acc: 0.8550
22912/31872 [====================>.........] - ETA: 2s - loss: 0.1216 - acc: 0.8551
23104/31872 [====================>.........] - ETA: 2s - loss: 0.1214 - acc: 0.8552
23296/31872 [====================>.........] - ETA: 2s - loss: 0.1214 - acc: 0.8553
23488/31872 [=====================>........] - ETA: 2s - loss: 0.1212 - acc: 0.8554
23680/31872 [=====================>........] - ETA: 2s - loss: 0.1211 - acc: 0.8556
23872/31872 [=====================>........] - ETA: 2s - loss: 0.1209 - acc: 0.8559
24064/31872 [=====================>........] - ETA: 2s - loss: 0.1210 - acc: 0.8556
24256/31872 [=====================>........] - ETA: 2s - loss: 0.1210 - acc: 0.8555
24448/31872 [======================>.......] - ETA: 2s - loss: 0.1209 - acc: 0.8559
24640/31872 [======================>.......] - ETA: 2s - loss: 0.1208 - acc: 0.8560
24832/31872 [======================>.......] - ETA: 2s - loss: 0.1206 - acc: 0.8564
25024/31872 [======================>.......] - ETA: 2s - loss: 0.1205 - acc: 0.8565
25216/31872 [======================>.......] - ETA: 2s - loss: 0.1203 - acc: 0.8566
25408/31872 [======================>.......] - ETA: 2s - loss: 0.1203 - acc: 0.8565
25600/31872 [=======================>......] - ETA: 2s - loss: 0.1203 - acc: 0.8564
25792/31872 [=======================>......] - ETA: 1s - loss: 0.1202 - acc: 0.8566
25984/31872 [=======================>......] - ETA: 1s - loss: 0.1201 - acc: 0.8568
26176/31872 [=======================>......] - ETA: 1s - loss: 0.1200 - acc: 0.8571
26368/31872 [=======================>......] - ETA: 1s - loss: 0.1199 - acc: 0.8576
26560/31872 [========================>.....] - ETA: 1s - loss: 0.1198 - acc: 0.8577
26752/31872 [========================>.....] - ETA: 1s - loss: 0.1196 - acc: 0.8579
26944/31872 [========================>.....] - ETA: 1s - loss: 0.1196 - acc: 0.8582
27136/31872 [========================>.....] - ETA: 1s - loss: 0.1195 - acc: 0.8584
27328/31872 [========================>.....] - ETA: 1s - loss: 0.1194 - acc: 0.8586
27520/31872 [========================>.....] - ETA: 1s - loss: 0.1193 - acc: 0.8587
27712/31872 [=========================>....] - ETA: 1s - loss: 0.1193 - acc: 0.8587
27904/31872 [=========================>....] - ETA: 1s - loss: 0.1191 - acc: 0.8591
28096/31872 [=========================>....] - ETA: 1s - loss: 0.1190 - acc: 0.8591
28288/31872 [=========================>....] - ETA: 1s - loss: 0.1190 - acc: 0.8592
28480/31872 [=========================>....] - ETA: 1s - loss: 0.1189 - acc: 0.8595
28672/31872 [=========================>....] - ETA: 1s - loss: 0.1188 - acc: 0.8598
28864/31872 [==========================>...] - ETA: 0s - loss: 0.1187 - acc: 0.8598
29056/31872 [==========================>...] - ETA: 0s - loss: 0.1185 - acc: 0.8600
29248/31872 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.8601
29440/31872 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.8604
29632/31872 [==========================>...] - ETA: 0s - loss: 0.1183 - acc: 0.8606
29824/31872 [===========================>..] - ETA: 0s - loss: 0.1182 - acc: 0.8607
30016/31872 [===========================>..] - ETA: 0s - loss: 0.1183 - acc: 0.8607
30208/31872 [===========================>..] - ETA: 0s - loss: 0.1181 - acc: 0.8610
30400/31872 [===========================>..] - ETA: 0s - loss: 0.1181 - acc: 0.8609
30592/31872 [===========================>..] - ETA: 0s - loss: 0.1180 - acc: 0.8612
30784/31872 [===========================>..] - ETA: 0s - loss: 0.1180 - acc: 0.8614
30976/31872 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.8613
31168/31872 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.8617
31360/31872 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.8618
31552/31872 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.8621
31744/31872 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.8620
31872/31872 [==============================] - 11s 351us/step - loss: 0.1175 - acc: 0.8620 - val_loss: 0.1017 - val_acc: 0.8923
Epoch 3/30

   64/31872 [..............................] - ETA: 10s - loss: 0.1136 - acc: 0.9062
  256/31872 [..............................] - ETA: 10s - loss: 0.1023 - acc: 0.9141
  448/31872 [..............................] - ETA: 10s - loss: 0.0973 - acc: 0.9219
  640/31872 [..............................] - ETA: 10s - loss: 0.0944 - acc: 0.9219
  832/31872 [..............................] - ETA: 10s - loss: 0.0966 - acc: 0.9171
 1024/31872 [..............................] - ETA: 10s - loss: 0.0953 - acc: 0.9150
 1216/31872 [>.............................] - ETA: 10s - loss: 0.0970 - acc: 0.9087
 1408/31872 [>.............................] - ETA: 10s - loss: 0.0952 - acc: 0.9105
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0944 - acc: 0.9087 
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0935 - acc: 0.9107
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0940 - acc: 0.9078
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0944 - acc: 0.9072
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0942 - acc: 0.9071
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0945 - acc: 0.9055
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0950 - acc: 0.9052
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0954 - acc: 0.9056
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0961 - acc: 0.9047
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0964 - acc: 0.9029
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0967 - acc: 0.9023
 3712/31872 [==>...........................] - ETA: 9s - loss: 0.0965 - acc: 0.9022
 3904/31872 [==>...........................] - ETA: 9s - loss: 0.0963 - acc: 0.9024
 4096/31872 [==>...........................] - ETA: 9s - loss: 0.0967 - acc: 0.9016
 4288/31872 [===>..........................] - ETA: 9s - loss: 0.0963 - acc: 0.9021
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0968 - acc: 0.9016
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0967 - acc: 0.9011
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0969 - acc: 0.9005
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0976 - acc: 0.8989
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0975 - acc: 0.8992
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0972 - acc: 0.9000
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0974 - acc: 0.9002
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0973 - acc: 0.9001
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0974 - acc: 0.9001
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0973 - acc: 0.9014
 6336/31872 [====>.........................] - ETA: 8s - loss: 0.0974 - acc: 0.9015
 6528/31872 [=====>........................] - ETA: 8s - loss: 0.0978 - acc: 0.9007
 6656/31872 [=====>........................] - ETA: 8s - loss: 0.0976 - acc: 0.9008
 6784/31872 [=====>........................] - ETA: 8s - loss: 0.0974 - acc: 0.9017
 6912/31872 [=====>........................] - ETA: 8s - loss: 0.0972 - acc: 0.9021
 7104/31872 [=====>........................] - ETA: 8s - loss: 0.0969 - acc: 0.9024
 7296/31872 [=====>........................] - ETA: 8s - loss: 0.0970 - acc: 0.9020
 7424/31872 [=====>........................] - ETA: 8s - loss: 0.0967 - acc: 0.9026
 7552/31872 [======>.......................] - ETA: 8s - loss: 0.0966 - acc: 0.9024
 7744/31872 [======>.......................] - ETA: 8s - loss: 0.0970 - acc: 0.9021
 7936/31872 [======>.......................] - ETA: 8s - loss: 0.0969 - acc: 0.9027
 8128/31872 [======>.......................] - ETA: 8s - loss: 0.0969 - acc: 0.9026
 8320/31872 [======>.......................] - ETA: 8s - loss: 0.0970 - acc: 0.9022
 8448/31872 [======>.......................] - ETA: 8s - loss: 0.0968 - acc: 0.9026
 8640/31872 [=======>......................] - ETA: 8s - loss: 0.0968 - acc: 0.9031
 8832/31872 [=======>......................] - ETA: 8s - loss: 0.0966 - acc: 0.9035
 9024/31872 [=======>......................] - ETA: 7s - loss: 0.0967 - acc: 0.9036
 9216/31872 [=======>......................] - ETA: 7s - loss: 0.0967 - acc: 0.9033
 9408/31872 [=======>......................] - ETA: 7s - loss: 0.0967 - acc: 0.9036
 9600/31872 [========>.....................] - ETA: 7s - loss: 0.0966 - acc: 0.9036
 9792/31872 [========>.....................] - ETA: 7s - loss: 0.0967 - acc: 0.9035
 9984/31872 [========>.....................] - ETA: 7s - loss: 0.0964 - acc: 0.9042
10176/31872 [========>.....................] - ETA: 7s - loss: 0.0965 - acc: 0.9033
10368/31872 [========>.....................] - ETA: 7s - loss: 0.0965 - acc: 0.9035
10560/31872 [========>.....................] - ETA: 7s - loss: 0.0964 - acc: 0.9034
10752/31872 [=========>....................] - ETA: 7s - loss: 0.0964 - acc: 0.9036
10880/31872 [=========>....................] - ETA: 7s - loss: 0.0966 - acc: 0.9029
11072/31872 [=========>....................] - ETA: 7s - loss: 0.0967 - acc: 0.9030
11264/31872 [=========>....................] - ETA: 7s - loss: 0.0968 - acc: 0.9029
11456/31872 [=========>....................] - ETA: 7s - loss: 0.0971 - acc: 0.9022
11648/31872 [=========>....................] - ETA: 7s - loss: 0.0971 - acc: 0.9021
11840/31872 [==========>...................] - ETA: 7s - loss: 0.0971 - acc: 0.9022
12032/31872 [==========>...................] - ETA: 6s - loss: 0.0970 - acc: 0.9024
12224/31872 [==========>...................] - ETA: 6s - loss: 0.0970 - acc: 0.9024
12416/31872 [==========>...................] - ETA: 6s - loss: 0.0967 - acc: 0.9030
12608/31872 [==========>...................] - ETA: 6s - loss: 0.0968 - acc: 0.9028
12800/31872 [===========>..................] - ETA: 6s - loss: 0.0969 - acc: 0.9026
12992/31872 [===========>..................] - ETA: 6s - loss: 0.0968 - acc: 0.9027
13184/31872 [===========>..................] - ETA: 6s - loss: 0.0969 - acc: 0.9028
13376/31872 [===========>..................] - ETA: 6s - loss: 0.0969 - acc: 0.9030
13568/31872 [===========>..................] - ETA: 6s - loss: 0.0969 - acc: 0.9027
13760/31872 [===========>..................] - ETA: 6s - loss: 0.0968 - acc: 0.9028
13952/31872 [============>.................] - ETA: 6s - loss: 0.0968 - acc: 0.9027
14144/31872 [============>.................] - ETA: 6s - loss: 0.0970 - acc: 0.9023
14336/31872 [============>.................] - ETA: 6s - loss: 0.0967 - acc: 0.9024
14528/31872 [============>.................] - ETA: 6s - loss: 0.0969 - acc: 0.9022
14720/31872 [============>.................] - ETA: 5s - loss: 0.0969 - acc: 0.9020
14912/31872 [=============>................] - ETA: 5s - loss: 0.0970 - acc: 0.9018
15104/31872 [=============>................] - ETA: 5s - loss: 0.0970 - acc: 0.9017
15296/31872 [=============>................] - ETA: 5s - loss: 0.0969 - acc: 0.9017
15488/31872 [=============>................] - ETA: 5s - loss: 0.0967 - acc: 0.9019
15680/31872 [=============>................] - ETA: 5s - loss: 0.0965 - acc: 0.9021
15872/31872 [=============>................] - ETA: 5s - loss: 0.0964 - acc: 0.9017
16064/31872 [==============>...............] - ETA: 5s - loss: 0.0964 - acc: 0.9018
16256/31872 [==============>...............] - ETA: 5s - loss: 0.0965 - acc: 0.9016
16448/31872 [==============>...............] - ETA: 5s - loss: 0.0966 - acc: 0.9014
16640/31872 [==============>...............] - ETA: 5s - loss: 0.0966 - acc: 0.9011
16832/31872 [==============>...............] - ETA: 5s - loss: 0.0965 - acc: 0.9012
17024/31872 [===============>..............] - ETA: 5s - loss: 0.0965 - acc: 0.9009
17216/31872 [===============>..............] - ETA: 5s - loss: 0.0964 - acc: 0.9012
17408/31872 [===============>..............] - ETA: 4s - loss: 0.0965 - acc: 0.9009
17600/31872 [===============>..............] - ETA: 4s - loss: 0.0965 - acc: 0.9009
17792/31872 [===============>..............] - ETA: 4s - loss: 0.0963 - acc: 0.9012
17984/31872 [===============>..............] - ETA: 4s - loss: 0.0964 - acc: 0.9011
18176/31872 [================>.............] - ETA: 4s - loss: 0.0965 - acc: 0.9009
18368/31872 [================>.............] - ETA: 4s - loss: 0.0964 - acc: 0.9011
18560/31872 [================>.............] - ETA: 4s - loss: 0.0962 - acc: 0.9011
18752/31872 [================>.............] - ETA: 4s - loss: 0.0962 - acc: 0.9011
18944/31872 [================>.............] - ETA: 4s - loss: 0.0962 - acc: 0.9005
19136/31872 [=================>............] - ETA: 4s - loss: 0.0962 - acc: 0.9004
19328/31872 [=================>............] - ETA: 4s - loss: 0.0963 - acc: 0.9006
19520/31872 [=================>............] - ETA: 4s - loss: 0.0963 - acc: 0.9006
19712/31872 [=================>............] - ETA: 4s - loss: 0.0963 - acc: 0.9004
19904/31872 [=================>............] - ETA: 4s - loss: 0.0963 - acc: 0.9003
20096/31872 [=================>............] - ETA: 4s - loss: 0.0963 - acc: 0.9005
20288/31872 [==================>...........] - ETA: 3s - loss: 0.0962 - acc: 0.9007
20480/31872 [==================>...........] - ETA: 3s - loss: 0.0961 - acc: 0.9007
20672/31872 [==================>...........] - ETA: 3s - loss: 0.0958 - acc: 0.9012
20864/31872 [==================>...........] - ETA: 3s - loss: 0.0958 - acc: 0.9011
21056/31872 [==================>...........] - ETA: 3s - loss: 0.0958 - acc: 0.9012
21248/31872 [===================>..........] - ETA: 3s - loss: 0.0957 - acc: 0.9015
21440/31872 [===================>..........] - ETA: 3s - loss: 0.0956 - acc: 0.9020
21632/31872 [===================>..........] - ETA: 3s - loss: 0.0955 - acc: 0.9020
21824/31872 [===================>..........] - ETA: 3s - loss: 0.0956 - acc: 0.9015
22016/31872 [===================>..........] - ETA: 3s - loss: 0.0956 - acc: 0.9016
22208/31872 [===================>..........] - ETA: 3s - loss: 0.0955 - acc: 0.9017
22400/31872 [====================>.........] - ETA: 3s - loss: 0.0954 - acc: 0.9019
22592/31872 [====================>.........] - ETA: 3s - loss: 0.0953 - acc: 0.9020
22784/31872 [====================>.........] - ETA: 3s - loss: 0.0952 - acc: 0.9021
22976/31872 [====================>.........] - ETA: 3s - loss: 0.0953 - acc: 0.9021
23168/31872 [====================>.........] - ETA: 2s - loss: 0.0953 - acc: 0.9022
23360/31872 [====================>.........] - ETA: 2s - loss: 0.0953 - acc: 0.9023
23552/31872 [=====================>........] - ETA: 2s - loss: 0.0953 - acc: 0.9023
23744/31872 [=====================>........] - ETA: 2s - loss: 0.0952 - acc: 0.9024
23936/31872 [=====================>........] - ETA: 2s - loss: 0.0952 - acc: 0.9024
24128/31872 [=====================>........] - ETA: 2s - loss: 0.0952 - acc: 0.9024
24320/31872 [=====================>........] - ETA: 2s - loss: 0.0951 - acc: 0.9027
24512/31872 [======================>.......] - ETA: 2s - loss: 0.0949 - acc: 0.9031
24704/31872 [======================>.......] - ETA: 2s - loss: 0.0949 - acc: 0.9032
24896/31872 [======================>.......] - ETA: 2s - loss: 0.0950 - acc: 0.9032
25088/31872 [======================>.......] - ETA: 2s - loss: 0.0949 - acc: 0.9033
25280/31872 [======================>.......] - ETA: 2s - loss: 0.0948 - acc: 0.9036
25472/31872 [======================>.......] - ETA: 2s - loss: 0.0947 - acc: 0.9038
25664/31872 [=======================>......] - ETA: 2s - loss: 0.0947 - acc: 0.9036
25856/31872 [=======================>......] - ETA: 2s - loss: 0.0946 - acc: 0.9037
26048/31872 [=======================>......] - ETA: 1s - loss: 0.0946 - acc: 0.9036
26240/31872 [=======================>......] - ETA: 1s - loss: 0.0945 - acc: 0.9037
26432/31872 [=======================>......] - ETA: 1s - loss: 0.0944 - acc: 0.9038
26624/31872 [========================>.....] - ETA: 1s - loss: 0.0944 - acc: 0.9037
26816/31872 [========================>.....] - ETA: 1s - loss: 0.0944 - acc: 0.9039
27008/31872 [========================>.....] - ETA: 1s - loss: 0.0943 - acc: 0.9039
27200/31872 [========================>.....] - ETA: 1s - loss: 0.0942 - acc: 0.9040
27392/31872 [========================>.....] - ETA: 1s - loss: 0.0942 - acc: 0.9038
27584/31872 [========================>.....] - ETA: 1s - loss: 0.0942 - acc: 0.9036
27776/31872 [=========================>....] - ETA: 1s - loss: 0.0941 - acc: 0.9037
27968/31872 [=========================>....] - ETA: 1s - loss: 0.0941 - acc: 0.9039
28160/31872 [=========================>....] - ETA: 1s - loss: 0.0940 - acc: 0.9040
28352/31872 [=========================>....] - ETA: 1s - loss: 0.0941 - acc: 0.9038
28544/31872 [=========================>....] - ETA: 1s - loss: 0.0940 - acc: 0.9039
28736/31872 [==========================>...] - ETA: 1s - loss: 0.0939 - acc: 0.9039
28928/31872 [==========================>...] - ETA: 0s - loss: 0.0939 - acc: 0.9039
29120/31872 [==========================>...] - ETA: 0s - loss: 0.0939 - acc: 0.9039
29312/31872 [==========================>...] - ETA: 0s - loss: 0.0938 - acc: 0.9041
29504/31872 [==========================>...] - ETA: 0s - loss: 0.0938 - acc: 0.9041
29696/31872 [==========================>...] - ETA: 0s - loss: 0.0937 - acc: 0.9041
29888/31872 [===========================>..] - ETA: 0s - loss: 0.0937 - acc: 0.9041
30080/31872 [===========================>..] - ETA: 0s - loss: 0.0937 - acc: 0.9043
30272/31872 [===========================>..] - ETA: 0s - loss: 0.0935 - acc: 0.9044
30464/31872 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9047
30656/31872 [===========================>..] - ETA: 0s - loss: 0.0934 - acc: 0.9047
30848/31872 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9048
31040/31872 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9050
31232/31872 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9049
31424/31872 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9051
31616/31872 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9052
31808/31872 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9053
31872/31872 [==============================] - 12s 362us/step - loss: 0.0931 - acc: 0.9053 - val_loss: 0.0850 - val_acc: 0.9198
Epoch 4/30

   64/31872 [..............................] - ETA: 10s - loss: 0.0901 - acc: 0.8906
  256/31872 [..............................] - ETA: 10s - loss: 0.0940 - acc: 0.9180
  448/31872 [..............................] - ETA: 10s - loss: 0.0901 - acc: 0.9241
  640/31872 [..............................] - ETA: 10s - loss: 0.0939 - acc: 0.9156
  832/31872 [..............................] - ETA: 9s - loss: 0.0922 - acc: 0.9135 
 1024/31872 [..............................] - ETA: 9s - loss: 0.0928 - acc: 0.9092
 1216/31872 [>.............................] - ETA: 9s - loss: 0.0919 - acc: 0.9112
 1408/31872 [>.............................] - ETA: 9s - loss: 0.0921 - acc: 0.9098
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0908 - acc: 0.9106
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0890 - acc: 0.9129
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0904 - acc: 0.9108
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0904 - acc: 0.9113
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0908 - acc: 0.9105
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0895 - acc: 0.9129
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0891 - acc: 0.9142
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0884 - acc: 0.9147
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0879 - acc: 0.9165
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0882 - acc: 0.9153
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0879 - acc: 0.9170
 3712/31872 [==>...........................] - ETA: 8s - loss: 0.0880 - acc: 0.9157
 3904/31872 [==>...........................] - ETA: 8s - loss: 0.0878 - acc: 0.9150
 4096/31872 [==>...........................] - ETA: 8s - loss: 0.0879 - acc: 0.9146
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.0874 - acc: 0.9149
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0874 - acc: 0.9150
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0871 - acc: 0.9161
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0872 - acc: 0.9163
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0872 - acc: 0.9163
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0868 - acc: 0.9171
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0863 - acc: 0.9186
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0862 - acc: 0.9189
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0862 - acc: 0.9184
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0861 - acc: 0.9182
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0858 - acc: 0.9187
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.0857 - acc: 0.9178
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.0856 - acc: 0.9176
 6784/31872 [=====>........................] - ETA: 7s - loss: 0.0853 - acc: 0.9177
 6976/31872 [=====>........................] - ETA: 7s - loss: 0.0850 - acc: 0.9181
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.0848 - acc: 0.9182
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.0847 - acc: 0.9183
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.0845 - acc: 0.9187
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.0843 - acc: 0.9189
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.0843 - acc: 0.9190
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.0843 - acc: 0.9189
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.0846 - acc: 0.9183
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.0847 - acc: 0.9185
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.0845 - acc: 0.9185
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.0845 - acc: 0.9190
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.0846 - acc: 0.9192
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.0846 - acc: 0.9189
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.0845 - acc: 0.9187
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.0845 - acc: 0.9191
 9856/31872 [========>.....................] - ETA: 7s - loss: 0.0845 - acc: 0.9186
10048/31872 [========>.....................] - ETA: 6s - loss: 0.0843 - acc: 0.9188
10240/31872 [========>.....................] - ETA: 6s - loss: 0.0840 - acc: 0.9193
10432/31872 [========>.....................] - ETA: 6s - loss: 0.0840 - acc: 0.9191
10624/31872 [=========>....................] - ETA: 6s - loss: 0.0838 - acc: 0.9194
10816/31872 [=========>....................] - ETA: 6s - loss: 0.0839 - acc: 0.9195
11008/31872 [=========>....................] - ETA: 6s - loss: 0.0837 - acc: 0.9198
11200/31872 [=========>....................] - ETA: 6s - loss: 0.0836 - acc: 0.9200
11392/31872 [=========>....................] - ETA: 6s - loss: 0.0837 - acc: 0.9201
11584/31872 [=========>....................] - ETA: 6s - loss: 0.0837 - acc: 0.9202
11776/31872 [==========>...................] - ETA: 6s - loss: 0.0837 - acc: 0.9200
11968/31872 [==========>...................] - ETA: 6s - loss: 0.0835 - acc: 0.9205
12160/31872 [==========>...................] - ETA: 6s - loss: 0.0834 - acc: 0.9210
12352/31872 [==========>...................] - ETA: 6s - loss: 0.0834 - acc: 0.9206
12544/31872 [==========>...................] - ETA: 6s - loss: 0.0833 - acc: 0.9208
12736/31872 [==========>...................] - ETA: 6s - loss: 0.0832 - acc: 0.9209
12928/31872 [===========>..................] - ETA: 6s - loss: 0.0833 - acc: 0.9209
13120/31872 [===========>..................] - ETA: 5s - loss: 0.0832 - acc: 0.9213
13312/31872 [===========>..................] - ETA: 5s - loss: 0.0831 - acc: 0.9216
13504/31872 [===========>..................] - ETA: 5s - loss: 0.0831 - acc: 0.9219
13696/31872 [===========>..................] - ETA: 5s - loss: 0.0830 - acc: 0.9222
13888/31872 [============>.................] - ETA: 5s - loss: 0.0829 - acc: 0.9221
14080/31872 [============>.................] - ETA: 5s - loss: 0.0828 - acc: 0.9224
14272/31872 [============>.................] - ETA: 5s - loss: 0.0828 - acc: 0.9224
14464/31872 [============>.................] - ETA: 5s - loss: 0.0828 - acc: 0.9225
14656/31872 [============>.................] - ETA: 5s - loss: 0.0830 - acc: 0.9216
14848/31872 [============>.................] - ETA: 5s - loss: 0.0830 - acc: 0.9215
15040/31872 [=============>................] - ETA: 5s - loss: 0.0830 - acc: 0.9217
15232/31872 [=============>................] - ETA: 5s - loss: 0.0828 - acc: 0.9221
15424/31872 [=============>................] - ETA: 5s - loss: 0.0829 - acc: 0.9221
15616/31872 [=============>................] - ETA: 5s - loss: 0.0828 - acc: 0.9223
15808/31872 [=============>................] - ETA: 5s - loss: 0.0827 - acc: 0.9224
16000/31872 [==============>...............] - ETA: 5s - loss: 0.0827 - acc: 0.9223
16192/31872 [==============>...............] - ETA: 4s - loss: 0.0826 - acc: 0.9222
16384/31872 [==============>...............] - ETA: 4s - loss: 0.0825 - acc: 0.9224
16576/31872 [==============>...............] - ETA: 4s - loss: 0.0824 - acc: 0.9226
16768/31872 [==============>...............] - ETA: 4s - loss: 0.0824 - acc: 0.9225
16960/31872 [==============>...............] - ETA: 4s - loss: 0.0823 - acc: 0.9228
17152/31872 [===============>..............] - ETA: 4s - loss: 0.0824 - acc: 0.9225
17344/31872 [===============>..............] - ETA: 4s - loss: 0.0822 - acc: 0.9229
17536/31872 [===============>..............] - ETA: 4s - loss: 0.0820 - acc: 0.9232
17728/31872 [===============>..............] - ETA: 4s - loss: 0.0819 - acc: 0.9234
17920/31872 [===============>..............] - ETA: 4s - loss: 0.0818 - acc: 0.9233
18112/31872 [================>.............] - ETA: 4s - loss: 0.0818 - acc: 0.9235
18304/31872 [================>.............] - ETA: 4s - loss: 0.0817 - acc: 0.9237
18496/31872 [================>.............] - ETA: 4s - loss: 0.0817 - acc: 0.9234
18688/31872 [================>.............] - ETA: 4s - loss: 0.0817 - acc: 0.9232
18880/31872 [================>.............] - ETA: 4s - loss: 0.0817 - acc: 0.9235
19072/31872 [================>.............] - ETA: 4s - loss: 0.0815 - acc: 0.9236
19264/31872 [=================>............] - ETA: 4s - loss: 0.0815 - acc: 0.9237
19456/31872 [=================>............] - ETA: 3s - loss: 0.0815 - acc: 0.9235
19648/31872 [=================>............] - ETA: 3s - loss: 0.0813 - acc: 0.9234
19840/31872 [=================>............] - ETA: 3s - loss: 0.0812 - acc: 0.9237
20032/31872 [=================>............] - ETA: 3s - loss: 0.0813 - acc: 0.9237
20224/31872 [==================>...........] - ETA: 3s - loss: 0.0812 - acc: 0.9236
20416/31872 [==================>...........] - ETA: 3s - loss: 0.0813 - acc: 0.9232
20608/31872 [==================>...........] - ETA: 3s - loss: 0.0813 - acc: 0.9233
20800/31872 [==================>...........] - ETA: 3s - loss: 0.0812 - acc: 0.9236
20992/31872 [==================>...........] - ETA: 3s - loss: 0.0810 - acc: 0.9238
21184/31872 [==================>...........] - ETA: 3s - loss: 0.0810 - acc: 0.9240
21376/31872 [===================>..........] - ETA: 3s - loss: 0.0808 - acc: 0.9241
21568/31872 [===================>..........] - ETA: 3s - loss: 0.0809 - acc: 0.9239
21760/31872 [===================>..........] - ETA: 3s - loss: 0.0809 - acc: 0.9235
21952/31872 [===================>..........] - ETA: 3s - loss: 0.0808 - acc: 0.9236
22144/31872 [===================>..........] - ETA: 3s - loss: 0.0807 - acc: 0.9235
22336/31872 [====================>.........] - ETA: 3s - loss: 0.0807 - acc: 0.9235
22528/31872 [====================>.........] - ETA: 2s - loss: 0.0807 - acc: 0.9234
22720/31872 [====================>.........] - ETA: 2s - loss: 0.0807 - acc: 0.9235
22912/31872 [====================>.........] - ETA: 2s - loss: 0.0807 - acc: 0.9236
23104/31872 [====================>.........] - ETA: 2s - loss: 0.0806 - acc: 0.9236
23296/31872 [====================>.........] - ETA: 2s - loss: 0.0806 - acc: 0.9235
23488/31872 [=====================>........] - ETA: 2s - loss: 0.0806 - acc: 0.9235
23680/31872 [=====================>........] - ETA: 2s - loss: 0.0806 - acc: 0.9236
23872/31872 [=====================>........] - ETA: 2s - loss: 0.0806 - acc: 0.9237
24064/31872 [=====================>........] - ETA: 2s - loss: 0.0806 - acc: 0.9237
24256/31872 [=====================>........] - ETA: 2s - loss: 0.0805 - acc: 0.9239
24448/31872 [======================>.......] - ETA: 2s - loss: 0.0805 - acc: 0.9239
24640/31872 [======================>.......] - ETA: 2s - loss: 0.0804 - acc: 0.9241
24832/31872 [======================>.......] - ETA: 2s - loss: 0.0804 - acc: 0.9240
25024/31872 [======================>.......] - ETA: 2s - loss: 0.0804 - acc: 0.9242
25216/31872 [======================>.......] - ETA: 2s - loss: 0.0803 - acc: 0.9245
25408/31872 [======================>.......] - ETA: 2s - loss: 0.0802 - acc: 0.9245
25600/31872 [=======================>......] - ETA: 1s - loss: 0.0803 - acc: 0.9245
25792/31872 [=======================>......] - ETA: 1s - loss: 0.0802 - acc: 0.9246
25984/31872 [=======================>......] - ETA: 1s - loss: 0.0801 - acc: 0.9244
26176/31872 [=======================>......] - ETA: 1s - loss: 0.0802 - acc: 0.9242
26368/31872 [=======================>......] - ETA: 1s - loss: 0.0802 - acc: 0.9240
26560/31872 [========================>.....] - ETA: 1s - loss: 0.0802 - acc: 0.9239
26752/31872 [========================>.....] - ETA: 1s - loss: 0.0802 - acc: 0.9241
26944/31872 [========================>.....] - ETA: 1s - loss: 0.0802 - acc: 0.9241
27136/31872 [========================>.....] - ETA: 1s - loss: 0.0802 - acc: 0.9243
27328/31872 [========================>.....] - ETA: 1s - loss: 0.0802 - acc: 0.9244
27520/31872 [========================>.....] - ETA: 1s - loss: 0.0802 - acc: 0.9243
27712/31872 [=========================>....] - ETA: 1s - loss: 0.0801 - acc: 0.9245
27904/31872 [=========================>....] - ETA: 1s - loss: 0.0801 - acc: 0.9246
28096/31872 [=========================>....] - ETA: 1s - loss: 0.0801 - acc: 0.9246
28288/31872 [=========================>....] - ETA: 1s - loss: 0.0800 - acc: 0.9248
28480/31872 [=========================>....] - ETA: 1s - loss: 0.0800 - acc: 0.9245
28672/31872 [=========================>....] - ETA: 1s - loss: 0.0800 - acc: 0.9245
28864/31872 [==========================>...] - ETA: 0s - loss: 0.0800 - acc: 0.9245
29056/31872 [==========================>...] - ETA: 0s - loss: 0.0799 - acc: 0.9246
29248/31872 [==========================>...] - ETA: 0s - loss: 0.0799 - acc: 0.9246
29440/31872 [==========================>...] - ETA: 0s - loss: 0.0798 - acc: 0.9245
29632/31872 [==========================>...] - ETA: 0s - loss: 0.0798 - acc: 0.9246
29824/31872 [===========================>..] - ETA: 0s - loss: 0.0798 - acc: 0.9244
30016/31872 [===========================>..] - ETA: 0s - loss: 0.0797 - acc: 0.9244
30208/31872 [===========================>..] - ETA: 0s - loss: 0.0797 - acc: 0.9243
30400/31872 [===========================>..] - ETA: 0s - loss: 0.0797 - acc: 0.9244
30592/31872 [===========================>..] - ETA: 0s - loss: 0.0796 - acc: 0.9246
30784/31872 [===========================>..] - ETA: 0s - loss: 0.0796 - acc: 0.9245
30976/31872 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9246
31168/31872 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9248
31360/31872 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9249
31552/31872 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9250
31744/31872 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9251
31872/31872 [==============================] - 11s 346us/step - loss: 0.0793 - acc: 0.9251 - val_loss: 0.0746 - val_acc: 0.9312
Epoch 5/30

   64/31872 [..............................] - ETA: 10s - loss: 0.0678 - acc: 0.9531
  256/31872 [..............................] - ETA: 10s - loss: 0.0720 - acc: 0.9492
  448/31872 [..............................] - ETA: 10s - loss: 0.0743 - acc: 0.9531
  640/31872 [..............................] - ETA: 9s - loss: 0.0713 - acc: 0.9547 
  832/31872 [..............................] - ETA: 9s - loss: 0.0722 - acc: 0.9459
 1024/31872 [..............................] - ETA: 9s - loss: 0.0724 - acc: 0.9473
 1216/31872 [>.............................] - ETA: 9s - loss: 0.0725 - acc: 0.9441
 1408/31872 [>.............................] - ETA: 9s - loss: 0.0726 - acc: 0.9425
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0717 - acc: 0.9425
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0718 - acc: 0.9431
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0716 - acc: 0.9405
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0721 - acc: 0.9412
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0727 - acc: 0.9383
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0717 - acc: 0.9398
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0720 - acc: 0.9397
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0719 - acc: 0.9382
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0719 - acc: 0.9381
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0725 - acc: 0.9369
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0726 - acc: 0.9369
 3712/31872 [==>...........................] - ETA: 8s - loss: 0.0718 - acc: 0.9372
 3904/31872 [==>...........................] - ETA: 8s - loss: 0.0713 - acc: 0.9375
 4096/31872 [==>...........................] - ETA: 8s - loss: 0.0716 - acc: 0.9373
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.0719 - acc: 0.9363
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0717 - acc: 0.9366
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0717 - acc: 0.9364
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0714 - acc: 0.9375
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0713 - acc: 0.9375
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0714 - acc: 0.9373
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0713 - acc: 0.9379
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0713 - acc: 0.9373
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0716 - acc: 0.9370
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0712 - acc: 0.9378
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0710 - acc: 0.9375
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.0713 - acc: 0.9364
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.0713 - acc: 0.9357
 6784/31872 [=====>........................] - ETA: 7s - loss: 0.0713 - acc: 0.9354
 6976/31872 [=====>........................] - ETA: 7s - loss: 0.0712 - acc: 0.9353
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.0715 - acc: 0.9351
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.0714 - acc: 0.9353
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.0714 - acc: 0.9354
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.0711 - acc: 0.9361
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.0712 - acc: 0.9357
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.0714 - acc: 0.9361
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.0713 - acc: 0.9365
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.0711 - acc: 0.9364
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.0709 - acc: 0.9366
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.0710 - acc: 0.9362
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.0712 - acc: 0.9357
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.0711 - acc: 0.9359
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.0710 - acc: 0.9361
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.0713 - acc: 0.9355
 9856/31872 [========>.....................] - ETA: 7s - loss: 0.0715 - acc: 0.9352
10048/31872 [========>.....................] - ETA: 6s - loss: 0.0715 - acc: 0.9357
10240/31872 [========>.....................] - ETA: 6s - loss: 0.0716 - acc: 0.9354
10432/31872 [========>.....................] - ETA: 6s - loss: 0.0715 - acc: 0.9355
10624/31872 [=========>....................] - ETA: 6s - loss: 0.0715 - acc: 0.9354
10816/31872 [=========>....................] - ETA: 6s - loss: 0.0713 - acc: 0.9359
11008/31872 [=========>....................] - ETA: 6s - loss: 0.0713 - acc: 0.9360
11200/31872 [=========>....................] - ETA: 6s - loss: 0.0714 - acc: 0.9363
11392/31872 [=========>....................] - ETA: 6s - loss: 0.0714 - acc: 0.9364
11584/31872 [=========>....................] - ETA: 6s - loss: 0.0712 - acc: 0.9363
11776/31872 [==========>...................] - ETA: 6s - loss: 0.0710 - acc: 0.9367
11968/31872 [==========>...................] - ETA: 6s - loss: 0.0709 - acc: 0.9369
12160/31872 [==========>...................] - ETA: 6s - loss: 0.0708 - acc: 0.9368
12352/31872 [==========>...................] - ETA: 6s - loss: 0.0708 - acc: 0.9367
12544/31872 [==========>...................] - ETA: 6s - loss: 0.0708 - acc: 0.9368
12736/31872 [==========>...................] - ETA: 6s - loss: 0.0709 - acc: 0.9363
12928/31872 [===========>..................] - ETA: 6s - loss: 0.0709 - acc: 0.9365
13120/31872 [===========>..................] - ETA: 5s - loss: 0.0708 - acc: 0.9367
13312/31872 [===========>..................] - ETA: 5s - loss: 0.0707 - acc: 0.9371
13504/31872 [===========>..................] - ETA: 5s - loss: 0.0705 - acc: 0.9370
13696/31872 [===========>..................] - ETA: 5s - loss: 0.0706 - acc: 0.9371
13888/31872 [============>.................] - ETA: 5s - loss: 0.0706 - acc: 0.9372
14080/31872 [============>.................] - ETA: 5s - loss: 0.0708 - acc: 0.9367
14272/31872 [============>.................] - ETA: 5s - loss: 0.0709 - acc: 0.9367
14464/31872 [============>.................] - ETA: 5s - loss: 0.0711 - acc: 0.9363
14656/31872 [============>.................] - ETA: 5s - loss: 0.0711 - acc: 0.9364
14848/31872 [============>.................] - ETA: 5s - loss: 0.0712 - acc: 0.9364
15040/31872 [=============>................] - ETA: 5s - loss: 0.0713 - acc: 0.9363
15232/31872 [=============>................] - ETA: 5s - loss: 0.0713 - acc: 0.9363
15424/31872 [=============>................] - ETA: 5s - loss: 0.0714 - acc: 0.9361
15616/31872 [=============>................] - ETA: 5s - loss: 0.0714 - acc: 0.9360
15808/31872 [=============>................] - ETA: 5s - loss: 0.0716 - acc: 0.9357
16000/31872 [==============>...............] - ETA: 5s - loss: 0.0714 - acc: 0.9357
16192/31872 [==============>...............] - ETA: 4s - loss: 0.0715 - acc: 0.9355
16384/31872 [==============>...............] - ETA: 4s - loss: 0.0715 - acc: 0.9352
16576/31872 [==============>...............] - ETA: 4s - loss: 0.0715 - acc: 0.9352
16768/31872 [==============>...............] - ETA: 4s - loss: 0.0715 - acc: 0.9354
16960/31872 [==============>...............] - ETA: 4s - loss: 0.0715 - acc: 0.9354
17152/31872 [===============>..............] - ETA: 4s - loss: 0.0715 - acc: 0.9356
17344/31872 [===============>..............] - ETA: 4s - loss: 0.0715 - acc: 0.9354
17536/31872 [===============>..............] - ETA: 4s - loss: 0.0714 - acc: 0.9353
17728/31872 [===============>..............] - ETA: 4s - loss: 0.0716 - acc: 0.9350
17920/31872 [===============>..............] - ETA: 4s - loss: 0.0716 - acc: 0.9349
18112/31872 [================>.............] - ETA: 4s - loss: 0.0716 - acc: 0.9351
18304/31872 [================>.............] - ETA: 4s - loss: 0.0716 - acc: 0.9351
18496/31872 [================>.............] - ETA: 4s - loss: 0.0716 - acc: 0.9351
18688/31872 [================>.............] - ETA: 4s - loss: 0.0716 - acc: 0.9353
18880/31872 [================>.............] - ETA: 4s - loss: 0.0716 - acc: 0.9353
19072/31872 [================>.............] - ETA: 4s - loss: 0.0716 - acc: 0.9354
19264/31872 [=================>............] - ETA: 4s - loss: 0.0716 - acc: 0.9353
19456/31872 [=================>............] - ETA: 3s - loss: 0.0715 - acc: 0.9354
19648/31872 [=================>............] - ETA: 3s - loss: 0.0716 - acc: 0.9352
19840/31872 [=================>............] - ETA: 3s - loss: 0.0717 - acc: 0.9351
20032/31872 [=================>............] - ETA: 3s - loss: 0.0717 - acc: 0.9350
20224/31872 [==================>...........] - ETA: 3s - loss: 0.0716 - acc: 0.9351
20416/31872 [==================>...........] - ETA: 3s - loss: 0.0716 - acc: 0.9351
20608/31872 [==================>...........] - ETA: 3s - loss: 0.0716 - acc: 0.9350
20800/31872 [==================>...........] - ETA: 3s - loss: 0.0716 - acc: 0.9351
20992/31872 [==================>...........] - ETA: 3s - loss: 0.0717 - acc: 0.9349
21184/31872 [==================>...........] - ETA: 3s - loss: 0.0717 - acc: 0.9348
21376/31872 [===================>..........] - ETA: 3s - loss: 0.0717 - acc: 0.9348
21568/31872 [===================>..........] - ETA: 3s - loss: 0.0717 - acc: 0.9348
21760/31872 [===================>..........] - ETA: 3s - loss: 0.0717 - acc: 0.9349
21952/31872 [===================>..........] - ETA: 3s - loss: 0.0717 - acc: 0.9351
22144/31872 [===================>..........] - ETA: 3s - loss: 0.0716 - acc: 0.9352
22336/31872 [====================>.........] - ETA: 3s - loss: 0.0717 - acc: 0.9351
22528/31872 [====================>.........] - ETA: 2s - loss: 0.0718 - acc: 0.9351
22720/31872 [====================>.........] - ETA: 2s - loss: 0.0718 - acc: 0.9352
22912/31872 [====================>.........] - ETA: 2s - loss: 0.0717 - acc: 0.9352
23104/31872 [====================>.........] - ETA: 2s - loss: 0.0717 - acc: 0.9352
23296/31872 [====================>.........] - ETA: 2s - loss: 0.0717 - acc: 0.9353
23488/31872 [=====================>........] - ETA: 2s - loss: 0.0717 - acc: 0.9353
23680/31872 [=====================>........] - ETA: 2s - loss: 0.0716 - acc: 0.9351
23872/31872 [=====================>........] - ETA: 2s - loss: 0.0717 - acc: 0.9350
24064/31872 [=====================>........] - ETA: 2s - loss: 0.0717 - acc: 0.9352
24256/31872 [=====================>........] - ETA: 2s - loss: 0.0716 - acc: 0.9352
24448/31872 [======================>.......] - ETA: 2s - loss: 0.0715 - acc: 0.9353
24640/31872 [======================>.......] - ETA: 2s - loss: 0.0715 - acc: 0.9353
24832/31872 [======================>.......] - ETA: 2s - loss: 0.0715 - acc: 0.9354
25024/31872 [======================>.......] - ETA: 2s - loss: 0.0716 - acc: 0.9354
25216/31872 [======================>.......] - ETA: 2s - loss: 0.0715 - acc: 0.9355
25408/31872 [======================>.......] - ETA: 2s - loss: 0.0714 - acc: 0.9357
25600/31872 [=======================>......] - ETA: 1s - loss: 0.0714 - acc: 0.9357
25792/31872 [=======================>......] - ETA: 1s - loss: 0.0714 - acc: 0.9358
25984/31872 [=======================>......] - ETA: 1s - loss: 0.0714 - acc: 0.9358
26176/31872 [=======================>......] - ETA: 1s - loss: 0.0714 - acc: 0.9359
26368/31872 [=======================>......] - ETA: 1s - loss: 0.0713 - acc: 0.9359
26560/31872 [========================>.....] - ETA: 1s - loss: 0.0712 - acc: 0.9360
26752/31872 [========================>.....] - ETA: 1s - loss: 0.0711 - acc: 0.9362
26944/31872 [========================>.....] - ETA: 1s - loss: 0.0711 - acc: 0.9362
27136/31872 [========================>.....] - ETA: 1s - loss: 0.0711 - acc: 0.9364
27328/31872 [========================>.....] - ETA: 1s - loss: 0.0710 - acc: 0.9362
27520/31872 [========================>.....] - ETA: 1s - loss: 0.0710 - acc: 0.9363
27712/31872 [=========================>....] - ETA: 1s - loss: 0.0709 - acc: 0.9366
27904/31872 [=========================>....] - ETA: 1s - loss: 0.0709 - acc: 0.9364
28096/31872 [=========================>....] - ETA: 1s - loss: 0.0709 - acc: 0.9363
28288/31872 [=========================>....] - ETA: 1s - loss: 0.0709 - acc: 0.9366
28480/31872 [=========================>....] - ETA: 1s - loss: 0.0708 - acc: 0.9367
28672/31872 [=========================>....] - ETA: 1s - loss: 0.0708 - acc: 0.9366
28864/31872 [==========================>...] - ETA: 0s - loss: 0.0708 - acc: 0.9366
29056/31872 [==========================>...] - ETA: 0s - loss: 0.0708 - acc: 0.9364
29248/31872 [==========================>...] - ETA: 0s - loss: 0.0708 - acc: 0.9364
29440/31872 [==========================>...] - ETA: 0s - loss: 0.0709 - acc: 0.9364
29632/31872 [==========================>...] - ETA: 0s - loss: 0.0709 - acc: 0.9363
29824/31872 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9362
30016/31872 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9364
30208/31872 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9362
30400/31872 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9362
30592/31872 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9362
30784/31872 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9361
30976/31872 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9359
31168/31872 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9357
31360/31872 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9357
31552/31872 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9358
31744/31872 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9360
31872/31872 [==============================] - 11s 345us/step - loss: 0.0708 - acc: 0.9360 - val_loss: 0.0681 - val_acc: 0.9379
Epoch 6/30

   64/31872 [..............................] - ETA: 10s - loss: 0.0766 - acc: 0.8906
  256/31872 [..............................] - ETA: 10s - loss: 0.0679 - acc: 0.9219
  448/31872 [..............................] - ETA: 9s - loss: 0.0694 - acc: 0.9219 
  640/31872 [..............................] - ETA: 9s - loss: 0.0657 - acc: 0.9328
  832/31872 [..............................] - ETA: 9s - loss: 0.0659 - acc: 0.9315
 1024/31872 [..............................] - ETA: 9s - loss: 0.0653 - acc: 0.9365
 1216/31872 [>.............................] - ETA: 9s - loss: 0.0642 - acc: 0.9342
 1408/31872 [>.............................] - ETA: 9s - loss: 0.0649 - acc: 0.9325
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0640 - acc: 0.9363
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0663 - acc: 0.9330
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0656 - acc: 0.9345
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0653 - acc: 0.9338
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0670 - acc: 0.9316
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0666 - acc: 0.9320
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0668 - acc: 0.9339
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0671 - acc: 0.9324
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0675 - acc: 0.9327
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0675 - acc: 0.9327
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0676 - acc: 0.9330
 3712/31872 [==>...........................] - ETA: 8s - loss: 0.0677 - acc: 0.9340
 3904/31872 [==>...........................] - ETA: 8s - loss: 0.0683 - acc: 0.9342
 4096/31872 [==>...........................] - ETA: 8s - loss: 0.0680 - acc: 0.9336
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.0678 - acc: 0.9331
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0678 - acc: 0.9339
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0683 - acc: 0.9334
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0680 - acc: 0.9342
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0682 - acc: 0.9345
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0677 - acc: 0.9356
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0678 - acc: 0.9349
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0680 - acc: 0.9341
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0676 - acc: 0.9349
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0676 - acc: 0.9362
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0680 - acc: 0.9359
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.0680 - acc: 0.9361
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.0679 - acc: 0.9361
 6784/31872 [=====>........................] - ETA: 7s - loss: 0.0680 - acc: 0.9362
 6976/31872 [=====>........................] - ETA: 7s - loss: 0.0680 - acc: 0.9365
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.0678 - acc: 0.9368
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.0679 - acc: 0.9374
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.0677 - acc: 0.9383
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.0679 - acc: 0.9383
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.0678 - acc: 0.9384
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.0678 - acc: 0.9380
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.0678 - acc: 0.9377
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.0677 - acc: 0.9375
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.0677 - acc: 0.9374
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.0674 - acc: 0.9378
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.0672 - acc: 0.9385
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.0671 - acc: 0.9389
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.0670 - acc: 0.9391
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.0670 - acc: 0.9388
 9856/31872 [========>.....................] - ETA: 7s - loss: 0.0671 - acc: 0.9382
10048/31872 [========>.....................] - ETA: 6s - loss: 0.0670 - acc: 0.9385
10240/31872 [========>.....................] - ETA: 6s - loss: 0.0671 - acc: 0.9379
10432/31872 [========>.....................] - ETA: 6s - loss: 0.0670 - acc: 0.9383
10624/31872 [=========>....................] - ETA: 6s - loss: 0.0670 - acc: 0.9383
10816/31872 [=========>....................] - ETA: 6s - loss: 0.0671 - acc: 0.9381
11008/31872 [=========>....................] - ETA: 6s - loss: 0.0670 - acc: 0.9384
11200/31872 [=========>....................] - ETA: 6s - loss: 0.0668 - acc: 0.9386
11392/31872 [=========>....................] - ETA: 6s - loss: 0.0667 - acc: 0.9385
11584/31872 [=========>....................] - ETA: 6s - loss: 0.0667 - acc: 0.9384
11776/31872 [==========>...................] - ETA: 6s - loss: 0.0667 - acc: 0.9383
11968/31872 [==========>...................] - ETA: 6s - loss: 0.0666 - acc: 0.9387
12160/31872 [==========>...................] - ETA: 6s - loss: 0.0666 - acc: 0.9389
12352/31872 [==========>...................] - ETA: 6s - loss: 0.0666 - acc: 0.9392
12544/31872 [==========>...................] - ETA: 6s - loss: 0.0665 - acc: 0.9393
12736/31872 [==========>...................] - ETA: 6s - loss: 0.0666 - acc: 0.9391
12928/31872 [===========>..................] - ETA: 6s - loss: 0.0665 - acc: 0.9394
13120/31872 [===========>..................] - ETA: 5s - loss: 0.0663 - acc: 0.9399
13312/31872 [===========>..................] - ETA: 5s - loss: 0.0663 - acc: 0.9402
13504/31872 [===========>..................] - ETA: 5s - loss: 0.0662 - acc: 0.9397
13696/31872 [===========>..................] - ETA: 5s - loss: 0.0663 - acc: 0.9398
13888/31872 [============>.................] - ETA: 5s - loss: 0.0661 - acc: 0.9403
14080/31872 [============>.................] - ETA: 5s - loss: 0.0661 - acc: 0.9401
14272/31872 [============>.................] - ETA: 5s - loss: 0.0661 - acc: 0.9401
14464/31872 [============>.................] - ETA: 5s - loss: 0.0660 - acc: 0.9403
14656/31872 [============>.................] - ETA: 5s - loss: 0.0660 - acc: 0.9399
14848/31872 [============>.................] - ETA: 5s - loss: 0.0659 - acc: 0.9402
15040/31872 [=============>................] - ETA: 5s - loss: 0.0659 - acc: 0.9402
15232/31872 [=============>................] - ETA: 5s - loss: 0.0659 - acc: 0.9403
15424/31872 [=============>................] - ETA: 5s - loss: 0.0659 - acc: 0.9403
15616/31872 [=============>................] - ETA: 5s - loss: 0.0660 - acc: 0.9403
15808/31872 [=============>................] - ETA: 5s - loss: 0.0659 - acc: 0.9402
16000/31872 [==============>...............] - ETA: 5s - loss: 0.0659 - acc: 0.9402
16192/31872 [==============>...............] - ETA: 4s - loss: 0.0661 - acc: 0.9400
16384/31872 [==============>...............] - ETA: 4s - loss: 0.0661 - acc: 0.9400
16576/31872 [==============>...............] - ETA: 4s - loss: 0.0661 - acc: 0.9400
16768/31872 [==============>...............] - ETA: 4s - loss: 0.0660 - acc: 0.9403
16960/31872 [==============>...............] - ETA: 4s - loss: 0.0661 - acc: 0.9402
17152/31872 [===============>..............] - ETA: 4s - loss: 0.0661 - acc: 0.9402
17344/31872 [===============>..............] - ETA: 4s - loss: 0.0660 - acc: 0.9403
17536/31872 [===============>..............] - ETA: 4s - loss: 0.0660 - acc: 0.9401
17728/31872 [===============>..............] - ETA: 4s - loss: 0.0660 - acc: 0.9401
17920/31872 [===============>..............] - ETA: 4s - loss: 0.0660 - acc: 0.9401
18112/31872 [================>.............] - ETA: 4s - loss: 0.0660 - acc: 0.9401
18304/31872 [================>.............] - ETA: 4s - loss: 0.0659 - acc: 0.9402
18496/31872 [================>.............] - ETA: 4s - loss: 0.0658 - acc: 0.9402
18688/31872 [================>.............] - ETA: 4s - loss: 0.0658 - acc: 0.9399
18880/31872 [================>.............] - ETA: 4s - loss: 0.0658 - acc: 0.9399
19072/31872 [================>.............] - ETA: 4s - loss: 0.0659 - acc: 0.9399
19264/31872 [=================>............] - ETA: 4s - loss: 0.0658 - acc: 0.9397
19456/31872 [=================>............] - ETA: 3s - loss: 0.0658 - acc: 0.9396
19648/31872 [=================>............] - ETA: 3s - loss: 0.0658 - acc: 0.9394
19840/31872 [=================>............] - ETA: 3s - loss: 0.0658 - acc: 0.9392
20032/31872 [=================>............] - ETA: 3s - loss: 0.0658 - acc: 0.9391
20224/31872 [==================>...........] - ETA: 3s - loss: 0.0658 - acc: 0.9389
20416/31872 [==================>...........] - ETA: 3s - loss: 0.0658 - acc: 0.9387
20608/31872 [==================>...........] - ETA: 3s - loss: 0.0657 - acc: 0.9390
20800/31872 [==================>...........] - ETA: 3s - loss: 0.0657 - acc: 0.9392
20992/31872 [==================>...........] - ETA: 3s - loss: 0.0657 - acc: 0.9393
21184/31872 [==================>...........] - ETA: 3s - loss: 0.0657 - acc: 0.9393
21376/31872 [===================>..........] - ETA: 3s - loss: 0.0658 - acc: 0.9390
21568/31872 [===================>..........] - ETA: 3s - loss: 0.0657 - acc: 0.9393
21760/31872 [===================>..........] - ETA: 3s - loss: 0.0657 - acc: 0.9393
21952/31872 [===================>..........] - ETA: 3s - loss: 0.0658 - acc: 0.9392
22144/31872 [===================>..........] - ETA: 3s - loss: 0.0658 - acc: 0.9392
22336/31872 [====================>.........] - ETA: 3s - loss: 0.0658 - acc: 0.9392
22528/31872 [====================>.........] - ETA: 2s - loss: 0.0658 - acc: 0.9392
22720/31872 [====================>.........] - ETA: 2s - loss: 0.0658 - acc: 0.9393
22912/31872 [====================>.........] - ETA: 2s - loss: 0.0658 - acc: 0.9392
23104/31872 [====================>.........] - ETA: 2s - loss: 0.0658 - acc: 0.9394
23296/31872 [====================>.........] - ETA: 2s - loss: 0.0657 - acc: 0.9395
23488/31872 [=====================>........] - ETA: 2s - loss: 0.0656 - acc: 0.9394
23680/31872 [=====================>........] - ETA: 2s - loss: 0.0656 - acc: 0.9394
23872/31872 [=====================>........] - ETA: 2s - loss: 0.0656 - acc: 0.9396
24064/31872 [=====================>........] - ETA: 2s - loss: 0.0655 - acc: 0.9396
24256/31872 [=====================>........] - ETA: 2s - loss: 0.0656 - acc: 0.9397
24448/31872 [======================>.......] - ETA: 2s - loss: 0.0655 - acc: 0.9400
24640/31872 [======================>.......] - ETA: 2s - loss: 0.0655 - acc: 0.9402
24832/31872 [======================>.......] - ETA: 2s - loss: 0.0654 - acc: 0.9402
25024/31872 [======================>.......] - ETA: 2s - loss: 0.0654 - acc: 0.9398
25216/31872 [======================>.......] - ETA: 2s - loss: 0.0655 - acc: 0.9398
25408/31872 [======================>.......] - ETA: 2s - loss: 0.0656 - acc: 0.9398
25600/31872 [=======================>......] - ETA: 1s - loss: 0.0655 - acc: 0.9399
25792/31872 [=======================>......] - ETA: 1s - loss: 0.0656 - acc: 0.9398
25984/31872 [=======================>......] - ETA: 1s - loss: 0.0657 - acc: 0.9399
26176/31872 [=======================>......] - ETA: 1s - loss: 0.0656 - acc: 0.9398
26368/31872 [=======================>......] - ETA: 1s - loss: 0.0655 - acc: 0.9400
26560/31872 [========================>.....] - ETA: 1s - loss: 0.0655 - acc: 0.9401
26752/31872 [========================>.....] - ETA: 1s - loss: 0.0655 - acc: 0.9400
26944/31872 [========================>.....] - ETA: 1s - loss: 0.0655 - acc: 0.9398
27136/31872 [========================>.....] - ETA: 1s - loss: 0.0656 - acc: 0.9397
27328/31872 [========================>.....] - ETA: 1s - loss: 0.0656 - acc: 0.9397
27520/31872 [========================>.....] - ETA: 1s - loss: 0.0656 - acc: 0.9397
27712/31872 [=========================>....] - ETA: 1s - loss: 0.0655 - acc: 0.9397
27904/31872 [=========================>....] - ETA: 1s - loss: 0.0655 - acc: 0.9398
28096/31872 [=========================>....] - ETA: 1s - loss: 0.0654 - acc: 0.9398
28288/31872 [=========================>....] - ETA: 1s - loss: 0.0654 - acc: 0.9399
28480/31872 [=========================>....] - ETA: 1s - loss: 0.0653 - acc: 0.9399
28672/31872 [=========================>....] - ETA: 1s - loss: 0.0654 - acc: 0.9400
28864/31872 [==========================>...] - ETA: 0s - loss: 0.0654 - acc: 0.9400
29056/31872 [==========================>...] - ETA: 0s - loss: 0.0653 - acc: 0.9400
29248/31872 [==========================>...] - ETA: 0s - loss: 0.0652 - acc: 0.9401
29440/31872 [==========================>...] - ETA: 0s - loss: 0.0652 - acc: 0.9401
29632/31872 [==========================>...] - ETA: 0s - loss: 0.0653 - acc: 0.9401
29824/31872 [===========================>..] - ETA: 0s - loss: 0.0652 - acc: 0.9403
30016/31872 [===========================>..] - ETA: 0s - loss: 0.0652 - acc: 0.9404
30208/31872 [===========================>..] - ETA: 0s - loss: 0.0652 - acc: 0.9403
30400/31872 [===========================>..] - ETA: 0s - loss: 0.0652 - acc: 0.9403
30592/31872 [===========================>..] - ETA: 0s - loss: 0.0652 - acc: 0.9404
30784/31872 [===========================>..] - ETA: 0s - loss: 0.0651 - acc: 0.9406
30976/31872 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9406
31168/31872 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9406
31360/31872 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9407
31552/31872 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9409
31744/31872 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9410
31872/31872 [==============================] - 11s 347us/step - loss: 0.0650 - acc: 0.9411 - val_loss: 0.0630 - val_acc: 0.9421
Epoch 7/30

   64/31872 [..............................] - ETA: 10s - loss: 0.0700 - acc: 0.9375
  256/31872 [..............................] - ETA: 10s - loss: 0.0611 - acc: 0.9414
  448/31872 [..............................] - ETA: 10s - loss: 0.0594 - acc: 0.9286
  640/31872 [..............................] - ETA: 10s - loss: 0.0577 - acc: 0.9375
  832/31872 [..............................] - ETA: 10s - loss: 0.0601 - acc: 0.9339
 1024/31872 [..............................] - ETA: 9s - loss: 0.0601 - acc: 0.9297 
 1216/31872 [>.............................] - ETA: 9s - loss: 0.0598 - acc: 0.9334
 1408/31872 [>.............................] - ETA: 9s - loss: 0.0606 - acc: 0.9339
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0595 - acc: 0.9387
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0606 - acc: 0.9364
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0609 - acc: 0.9380
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0613 - acc: 0.9380
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0610 - acc: 0.9405
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0607 - acc: 0.9410
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0599 - acc: 0.9419
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0601 - acc: 0.9419
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0603 - acc: 0.9420
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0601 - acc: 0.9423
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0601 - acc: 0.9429
 3712/31872 [==>...........................] - ETA: 8s - loss: 0.0606 - acc: 0.9429
 3904/31872 [==>...........................] - ETA: 8s - loss: 0.0602 - acc: 0.9439
 4096/31872 [==>...........................] - ETA: 8s - loss: 0.0604 - acc: 0.9453
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.0602 - acc: 0.9464
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0603 - acc: 0.9471
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0606 - acc: 0.9465
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0610 - acc: 0.9470
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0609 - acc: 0.9472
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0610 - acc: 0.9457
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0612 - acc: 0.9450
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0610 - acc: 0.9450
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0611 - acc: 0.9449
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0610 - acc: 0.9450
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0607 - acc: 0.9459
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.0607 - acc: 0.9459
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.0607 - acc: 0.9461
 6784/31872 [=====>........................] - ETA: 7s - loss: 0.0605 - acc: 0.9460
 6976/31872 [=====>........................] - ETA: 7s - loss: 0.0606 - acc: 0.9462
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.0604 - acc: 0.9466
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.0607 - acc: 0.9459
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.0609 - acc: 0.9453
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.0609 - acc: 0.9454
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.0608 - acc: 0.9452
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.0612 - acc: 0.9445
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.0613 - acc: 0.9442
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.0613 - acc: 0.9444
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.0611 - acc: 0.9452
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.0609 - acc: 0.9456
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.0610 - acc: 0.9458
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.0610 - acc: 0.9455
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.0612 - acc: 0.9452
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.0612 - acc: 0.9455
 9856/31872 [========>.....................] - ETA: 7s - loss: 0.0612 - acc: 0.9456
10048/31872 [========>.....................] - ETA: 6s - loss: 0.0612 - acc: 0.9457
10240/31872 [========>.....................] - ETA: 6s - loss: 0.0613 - acc: 0.9458
10432/31872 [========>.....................] - ETA: 6s - loss: 0.0613 - acc: 0.9459
10624/31872 [=========>....................] - ETA: 6s - loss: 0.0614 - acc: 0.9460
10816/31872 [=========>....................] - ETA: 6s - loss: 0.0615 - acc: 0.9459
11008/31872 [=========>....................] - ETA: 6s - loss: 0.0615 - acc: 0.9457
11200/31872 [=========>....................] - ETA: 6s - loss: 0.0615 - acc: 0.9457
11392/31872 [=========>....................] - ETA: 6s - loss: 0.0613 - acc: 0.9460
11584/31872 [=========>....................] - ETA: 6s - loss: 0.0612 - acc: 0.9460
11776/31872 [==========>...................] - ETA: 6s - loss: 0.0611 - acc: 0.9457
11968/31872 [==========>...................] - ETA: 6s - loss: 0.0613 - acc: 0.9453
12160/31872 [==========>...................] - ETA: 6s - loss: 0.0614 - acc: 0.9453
12352/31872 [==========>...................] - ETA: 6s - loss: 0.0613 - acc: 0.9456
12544/31872 [==========>...................] - ETA: 6s - loss: 0.0614 - acc: 0.9454
12736/31872 [==========>...................] - ETA: 6s - loss: 0.0614 - acc: 0.9458
12928/31872 [===========>..................] - ETA: 6s - loss: 0.0613 - acc: 0.9455
13120/31872 [===========>..................] - ETA: 5s - loss: 0.0613 - acc: 0.9457
13312/31872 [===========>..................] - ETA: 5s - loss: 0.0613 - acc: 0.9461
13504/31872 [===========>..................] - ETA: 5s - loss: 0.0613 - acc: 0.9457
13696/31872 [===========>..................] - ETA: 5s - loss: 0.0614 - acc: 0.9460
13888/31872 [============>.................] - ETA: 5s - loss: 0.0614 - acc: 0.9458
14080/31872 [============>.................] - ETA: 5s - loss: 0.0614 - acc: 0.9460
14272/31872 [============>.................] - ETA: 5s - loss: 0.0614 - acc: 0.9458
14464/31872 [============>.................] - ETA: 5s - loss: 0.0615 - acc: 0.9458
14656/31872 [============>.................] - ETA: 5s - loss: 0.0614 - acc: 0.9460
14848/31872 [============>.................] - ETA: 5s - loss: 0.0613 - acc: 0.9459
15040/31872 [=============>................] - ETA: 5s - loss: 0.0613 - acc: 0.9458
15232/31872 [=============>................] - ETA: 5s - loss: 0.0612 - acc: 0.9460
15424/31872 [=============>................] - ETA: 5s - loss: 0.0612 - acc: 0.9462
15616/31872 [=============>................] - ETA: 5s - loss: 0.0611 - acc: 0.9461
15808/31872 [=============>................] - ETA: 5s - loss: 0.0613 - acc: 0.9457
16000/31872 [==============>...............] - ETA: 5s - loss: 0.0613 - acc: 0.9459
16192/31872 [==============>...............] - ETA: 4s - loss: 0.0613 - acc: 0.9460
16384/31872 [==============>...............] - ETA: 4s - loss: 0.0613 - acc: 0.9460
16576/31872 [==============>...............] - ETA: 4s - loss: 0.0613 - acc: 0.9456
16768/31872 [==============>...............] - ETA: 4s - loss: 0.0614 - acc: 0.9451
16960/31872 [==============>...............] - ETA: 4s - loss: 0.0614 - acc: 0.9451
17152/31872 [===============>..............] - ETA: 4s - loss: 0.0614 - acc: 0.9452
17344/31872 [===============>..............] - ETA: 4s - loss: 0.0613 - acc: 0.9453
17536/31872 [===============>..............] - ETA: 4s - loss: 0.0613 - acc: 0.9453
17728/31872 [===============>..............] - ETA: 4s - loss: 0.0611 - acc: 0.9454
17920/31872 [===============>..............] - ETA: 4s - loss: 0.0612 - acc: 0.9451
18112/31872 [================>.............] - ETA: 4s - loss: 0.0613 - acc: 0.9450
18304/31872 [================>.............] - ETA: 4s - loss: 0.0612 - acc: 0.9450
18496/31872 [================>.............] - ETA: 4s - loss: 0.0613 - acc: 0.9451
18688/31872 [================>.............] - ETA: 4s - loss: 0.0612 - acc: 0.9449
18880/31872 [================>.............] - ETA: 4s - loss: 0.0613 - acc: 0.9448
19072/31872 [================>.............] - ETA: 4s - loss: 0.0612 - acc: 0.9447
19264/31872 [=================>............] - ETA: 4s - loss: 0.0612 - acc: 0.9447
19456/31872 [=================>............] - ETA: 3s - loss: 0.0613 - acc: 0.9444
19648/31872 [=================>............] - ETA: 3s - loss: 0.0614 - acc: 0.9444
19840/31872 [=================>............] - ETA: 3s - loss: 0.0613 - acc: 0.9446
20032/31872 [=================>............] - ETA: 3s - loss: 0.0613 - acc: 0.9446
20224/31872 [==================>...........] - ETA: 3s - loss: 0.0613 - acc: 0.9447
20416/31872 [==================>...........] - ETA: 3s - loss: 0.0613 - acc: 0.9446
20608/31872 [==================>...........] - ETA: 3s - loss: 0.0612 - acc: 0.9446
20800/31872 [==================>...........] - ETA: 3s - loss: 0.0612 - acc: 0.9449
20992/31872 [==================>...........] - ETA: 3s - loss: 0.0612 - acc: 0.9449
21184/31872 [==================>...........] - ETA: 3s - loss: 0.0612 - acc: 0.9450
21376/31872 [===================>..........] - ETA: 3s - loss: 0.0612 - acc: 0.9453
21568/31872 [===================>..........] - ETA: 3s - loss: 0.0612 - acc: 0.9453
21760/31872 [===================>..........] - ETA: 3s - loss: 0.0612 - acc: 0.9455
21952/31872 [===================>..........] - ETA: 3s - loss: 0.0612 - acc: 0.9454
22144/31872 [===================>..........] - ETA: 3s - loss: 0.0612 - acc: 0.9454
22336/31872 [====================>.........] - ETA: 3s - loss: 0.0612 - acc: 0.9454
22528/31872 [====================>.........] - ETA: 2s - loss: 0.0612 - acc: 0.9454
22720/31872 [====================>.........] - ETA: 2s - loss: 0.0611 - acc: 0.9454
22912/31872 [====================>.........] - ETA: 2s - loss: 0.0611 - acc: 0.9455
23104/31872 [====================>.........] - ETA: 2s - loss: 0.0611 - acc: 0.9455
23296/31872 [====================>.........] - ETA: 2s - loss: 0.0611 - acc: 0.9455
23488/31872 [=====================>........] - ETA: 2s - loss: 0.0611 - acc: 0.9455
23680/31872 [=====================>........] - ETA: 2s - loss: 0.0610 - acc: 0.9457
23872/31872 [=====================>........] - ETA: 2s - loss: 0.0611 - acc: 0.9456
24064/31872 [=====================>........] - ETA: 2s - loss: 0.0611 - acc: 0.9456
24256/31872 [=====================>........] - ETA: 2s - loss: 0.0611 - acc: 0.9457
24448/31872 [======================>.......] - ETA: 2s - loss: 0.0611 - acc: 0.9458
24640/31872 [======================>.......] - ETA: 2s - loss: 0.0611 - acc: 0.9458
24832/31872 [======================>.......] - ETA: 2s - loss: 0.0610 - acc: 0.9458
25024/31872 [======================>.......] - ETA: 2s - loss: 0.0611 - acc: 0.9459
25216/31872 [======================>.......] - ETA: 2s - loss: 0.0611 - acc: 0.9459
25408/31872 [======================>.......] - ETA: 2s - loss: 0.0611 - acc: 0.9458
25600/31872 [=======================>......] - ETA: 1s - loss: 0.0611 - acc: 0.9459
25792/31872 [=======================>......] - ETA: 1s - loss: 0.0610 - acc: 0.9460
25984/31872 [=======================>......] - ETA: 1s - loss: 0.0610 - acc: 0.9460
26176/31872 [=======================>......] - ETA: 1s - loss: 0.0610 - acc: 0.9462
26368/31872 [=======================>......] - ETA: 1s - loss: 0.0609 - acc: 0.9463
26560/31872 [========================>.....] - ETA: 1s - loss: 0.0610 - acc: 0.9463
26752/31872 [========================>.....] - ETA: 1s - loss: 0.0610 - acc: 0.9464
26944/31872 [========================>.....] - ETA: 1s - loss: 0.0609 - acc: 0.9466
27136/31872 [========================>.....] - ETA: 1s - loss: 0.0608 - acc: 0.9467
27328/31872 [========================>.....] - ETA: 1s - loss: 0.0608 - acc: 0.9468
27520/31872 [========================>.....] - ETA: 1s - loss: 0.0608 - acc: 0.9469
27712/31872 [=========================>....] - ETA: 1s - loss: 0.0608 - acc: 0.9470
27904/31872 [=========================>....] - ETA: 1s - loss: 0.0608 - acc: 0.9470
28096/31872 [=========================>....] - ETA: 1s - loss: 0.0607 - acc: 0.9471
28288/31872 [=========================>....] - ETA: 1s - loss: 0.0607 - acc: 0.9469
28480/31872 [=========================>....] - ETA: 1s - loss: 0.0607 - acc: 0.9470
28672/31872 [=========================>....] - ETA: 1s - loss: 0.0607 - acc: 0.9471
28864/31872 [==========================>...] - ETA: 0s - loss: 0.0606 - acc: 0.9472
29056/31872 [==========================>...] - ETA: 0s - loss: 0.0605 - acc: 0.9473
29248/31872 [==========================>...] - ETA: 0s - loss: 0.0605 - acc: 0.9474
29440/31872 [==========================>...] - ETA: 0s - loss: 0.0605 - acc: 0.9474
29632/31872 [==========================>...] - ETA: 0s - loss: 0.0604 - acc: 0.9476
29824/31872 [===========================>..] - ETA: 0s - loss: 0.0605 - acc: 0.9476
30016/31872 [===========================>..] - ETA: 0s - loss: 0.0604 - acc: 0.9476
30208/31872 [===========================>..] - ETA: 0s - loss: 0.0603 - acc: 0.9476
30400/31872 [===========================>..] - ETA: 0s - loss: 0.0604 - acc: 0.9474
30592/31872 [===========================>..] - ETA: 0s - loss: 0.0604 - acc: 0.9475
30784/31872 [===========================>..] - ETA: 0s - loss: 0.0604 - acc: 0.9473
30976/31872 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9473
31168/31872 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9474
31360/31872 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9474
31552/31872 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9473
31744/31872 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9475
31872/31872 [==============================] - 11s 347us/step - loss: 0.0604 - acc: 0.9475 - val_loss: 0.0591 - val_acc: 0.9484
Epoch 8/30

   64/31872 [..............................] - ETA: 10s - loss: 0.0591 - acc: 0.9688
  256/31872 [..............................] - ETA: 10s - loss: 0.0551 - acc: 0.9570
  448/31872 [..............................] - ETA: 10s - loss: 0.0587 - acc: 0.9598
  640/31872 [..............................] - ETA: 10s - loss: 0.0568 - acc: 0.9609
  832/31872 [..............................] - ETA: 9s - loss: 0.0553 - acc: 0.9639 
 1024/31872 [..............................] - ETA: 9s - loss: 0.0548 - acc: 0.9629
 1216/31872 [>.............................] - ETA: 9s - loss: 0.0554 - acc: 0.9622
 1408/31872 [>.............................] - ETA: 9s - loss: 0.0548 - acc: 0.9616
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0560 - acc: 0.9587
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0553 - acc: 0.9587
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0545 - acc: 0.9577
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0548 - acc: 0.9554
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0555 - acc: 0.9527
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0562 - acc: 0.9527
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0564 - acc: 0.9517
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0567 - acc: 0.9514
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0571 - acc: 0.9509
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0574 - acc: 0.9510
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0579 - acc: 0.9500
 3712/31872 [==>...........................] - ETA: 9s - loss: 0.0579 - acc: 0.9502
 3904/31872 [==>...........................] - ETA: 8s - loss: 0.0576 - acc: 0.9498
 4096/31872 [==>...........................] - ETA: 8s - loss: 0.0580 - acc: 0.9478
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.0583 - acc: 0.9466
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0582 - acc: 0.9471
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0579 - acc: 0.9482
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0578 - acc: 0.9486
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0577 - acc: 0.9484
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0576 - acc: 0.9480
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0576 - acc: 0.9483
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0578 - acc: 0.9482
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0577 - acc: 0.9481
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0580 - acc: 0.9476
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0581 - acc: 0.9473
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.0578 - acc: 0.9480
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.0575 - acc: 0.9478
 6784/31872 [=====>........................] - ETA: 8s - loss: 0.0576 - acc: 0.9475
 6976/31872 [=====>........................] - ETA: 7s - loss: 0.0575 - acc: 0.9474
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.0579 - acc: 0.9473
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.0580 - acc: 0.9473
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.0580 - acc: 0.9472
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.0581 - acc: 0.9472
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.0579 - acc: 0.9473
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.0578 - acc: 0.9475
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.0578 - acc: 0.9477
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.0579 - acc: 0.9476
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.0577 - acc: 0.9482
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.0576 - acc: 0.9482
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.0575 - acc: 0.9483
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.0575 - acc: 0.9487
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.0575 - acc: 0.9488
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.0574 - acc: 0.9485
 9856/31872 [========>.....................] - ETA: 7s - loss: 0.0576 - acc: 0.9482
10048/31872 [========>.....................] - ETA: 6s - loss: 0.0575 - acc: 0.9486
10240/31872 [========>.....................] - ETA: 6s - loss: 0.0575 - acc: 0.9488
10432/31872 [========>.....................] - ETA: 6s - loss: 0.0574 - acc: 0.9488
10624/31872 [=========>....................] - ETA: 6s - loss: 0.0574 - acc: 0.9491
10816/31872 [=========>....................] - ETA: 6s - loss: 0.0575 - acc: 0.9492
11008/31872 [=========>....................] - ETA: 6s - loss: 0.0577 - acc: 0.9487
11200/31872 [=========>....................] - ETA: 6s - loss: 0.0578 - acc: 0.9485
11392/31872 [=========>....................] - ETA: 6s - loss: 0.0577 - acc: 0.9488
11584/31872 [=========>....................] - ETA: 6s - loss: 0.0577 - acc: 0.9487
11776/31872 [==========>...................] - ETA: 6s - loss: 0.0577 - acc: 0.9488
11968/31872 [==========>...................] - ETA: 6s - loss: 0.0576 - acc: 0.9486
12160/31872 [==========>...................] - ETA: 6s - loss: 0.0576 - acc: 0.9488
12352/31872 [==========>...................] - ETA: 6s - loss: 0.0577 - acc: 0.9486
12544/31872 [==========>...................] - ETA: 6s - loss: 0.0579 - acc: 0.9482
12736/31872 [==========>...................] - ETA: 6s - loss: 0.0579 - acc: 0.9482
12928/31872 [===========>..................] - ETA: 6s - loss: 0.0578 - acc: 0.9482
13120/31872 [===========>..................] - ETA: 5s - loss: 0.0578 - acc: 0.9483
13312/31872 [===========>..................] - ETA: 5s - loss: 0.0579 - acc: 0.9480
13504/31872 [===========>..................] - ETA: 5s - loss: 0.0578 - acc: 0.9481
13696/31872 [===========>..................] - ETA: 5s - loss: 0.0578 - acc: 0.9482
13888/31872 [============>.................] - ETA: 5s - loss: 0.0578 - acc: 0.9485
14080/31872 [============>.................] - ETA: 5s - loss: 0.0579 - acc: 0.9484
14272/31872 [============>.................] - ETA: 5s - loss: 0.0579 - acc: 0.9483
14464/31872 [============>.................] - ETA: 5s - loss: 0.0581 - acc: 0.9479
14656/31872 [============>.................] - ETA: 5s - loss: 0.0581 - acc: 0.9482
14848/31872 [============>.................] - ETA: 5s - loss: 0.0580 - acc: 0.9486
15040/31872 [=============>................] - ETA: 5s - loss: 0.0580 - acc: 0.9485
15232/31872 [=============>................] - ETA: 5s - loss: 0.0578 - acc: 0.9489
15424/31872 [=============>................] - ETA: 5s - loss: 0.0579 - acc: 0.9489
15616/31872 [=============>................] - ETA: 5s - loss: 0.0579 - acc: 0.9485
15808/31872 [=============>................] - ETA: 5s - loss: 0.0578 - acc: 0.9486
16000/31872 [==============>...............] - ETA: 5s - loss: 0.0579 - acc: 0.9487
16192/31872 [==============>...............] - ETA: 4s - loss: 0.0579 - acc: 0.9486
16384/31872 [==============>...............] - ETA: 4s - loss: 0.0578 - acc: 0.9486
16576/31872 [==============>...............] - ETA: 4s - loss: 0.0579 - acc: 0.9485
16768/31872 [==============>...............] - ETA: 4s - loss: 0.0578 - acc: 0.9485
16960/31872 [==============>...............] - ETA: 4s - loss: 0.0578 - acc: 0.9486
17152/31872 [===============>..............] - ETA: 4s - loss: 0.0579 - acc: 0.9487
17344/31872 [===============>..............] - ETA: 4s - loss: 0.0578 - acc: 0.9487
17536/31872 [===============>..............] - ETA: 4s - loss: 0.0579 - acc: 0.9484
17728/31872 [===============>..............] - ETA: 4s - loss: 0.0579 - acc: 0.9484
17920/31872 [===============>..............] - ETA: 4s - loss: 0.0579 - acc: 0.9484
18112/31872 [================>.............] - ETA: 4s - loss: 0.0579 - acc: 0.9485
18304/31872 [================>.............] - ETA: 4s - loss: 0.0579 - acc: 0.9485
18496/31872 [================>.............] - ETA: 4s - loss: 0.0579 - acc: 0.9486
18688/31872 [================>.............] - ETA: 4s - loss: 0.0579 - acc: 0.9487
18880/31872 [================>.............] - ETA: 4s - loss: 0.0579 - acc: 0.9484
19072/31872 [================>.............] - ETA: 4s - loss: 0.0580 - acc: 0.9483
19264/31872 [=================>............] - ETA: 4s - loss: 0.0579 - acc: 0.9484
19456/31872 [=================>............] - ETA: 3s - loss: 0.0578 - acc: 0.9485
19648/31872 [=================>............] - ETA: 3s - loss: 0.0578 - acc: 0.9485
19840/31872 [=================>............] - ETA: 3s - loss: 0.0578 - acc: 0.9485
20032/31872 [=================>............] - ETA: 3s - loss: 0.0578 - acc: 0.9488
20224/31872 [==================>...........] - ETA: 3s - loss: 0.0577 - acc: 0.9489
20416/31872 [==================>...........] - ETA: 3s - loss: 0.0578 - acc: 0.9489
20608/31872 [==================>...........] - ETA: 3s - loss: 0.0577 - acc: 0.9491
20800/31872 [==================>...........] - ETA: 3s - loss: 0.0577 - acc: 0.9491
20992/31872 [==================>...........] - ETA: 3s - loss: 0.0578 - acc: 0.9492
21184/31872 [==================>...........] - ETA: 3s - loss: 0.0577 - acc: 0.9494
21376/31872 [===================>..........] - ETA: 3s - loss: 0.0578 - acc: 0.9490
21568/31872 [===================>..........] - ETA: 3s - loss: 0.0577 - acc: 0.9490
21760/31872 [===================>..........] - ETA: 3s - loss: 0.0576 - acc: 0.9492
21952/31872 [===================>..........] - ETA: 3s - loss: 0.0576 - acc: 0.9492
22144/31872 [===================>..........] - ETA: 3s - loss: 0.0576 - acc: 0.9490
22336/31872 [====================>.........] - ETA: 3s - loss: 0.0575 - acc: 0.9492
22528/31872 [====================>.........] - ETA: 2s - loss: 0.0575 - acc: 0.9493
22720/31872 [====================>.........] - ETA: 2s - loss: 0.0574 - acc: 0.9494
22912/31872 [====================>.........] - ETA: 2s - loss: 0.0574 - acc: 0.9493
23104/31872 [====================>.........] - ETA: 2s - loss: 0.0574 - acc: 0.9494
23296/31872 [====================>.........] - ETA: 2s - loss: 0.0573 - acc: 0.9495
23488/31872 [=====================>........] - ETA: 2s - loss: 0.0572 - acc: 0.9497
23680/31872 [=====================>........] - ETA: 2s - loss: 0.0572 - acc: 0.9499
23872/31872 [=====================>........] - ETA: 2s - loss: 0.0572 - acc: 0.9498
24064/31872 [=====================>........] - ETA: 2s - loss: 0.0572 - acc: 0.9496
24256/31872 [=====================>........] - ETA: 2s - loss: 0.0572 - acc: 0.9496
24448/31872 [======================>.......] - ETA: 2s - loss: 0.0571 - acc: 0.9497
24640/31872 [======================>.......] - ETA: 2s - loss: 0.0571 - acc: 0.9498
24832/31872 [======================>.......] - ETA: 2s - loss: 0.0572 - acc: 0.9498
25024/31872 [======================>.......] - ETA: 2s - loss: 0.0572 - acc: 0.9498
25216/31872 [======================>.......] - ETA: 2s - loss: 0.0572 - acc: 0.9498
25408/31872 [======================>.......] - ETA: 2s - loss: 0.0571 - acc: 0.9497
25600/31872 [=======================>......] - ETA: 1s - loss: 0.0571 - acc: 0.9498
25792/31872 [=======================>......] - ETA: 1s - loss: 0.0572 - acc: 0.9498
25984/31872 [=======================>......] - ETA: 1s - loss: 0.0571 - acc: 0.9497
26176/31872 [=======================>......] - ETA: 1s - loss: 0.0571 - acc: 0.9496
26368/31872 [=======================>......] - ETA: 1s - loss: 0.0571 - acc: 0.9496
26560/31872 [========================>.....] - ETA: 1s - loss: 0.0571 - acc: 0.9496
26752/31872 [========================>.....] - ETA: 1s - loss: 0.0571 - acc: 0.9496
26944/31872 [========================>.....] - ETA: 1s - loss: 0.0571 - acc: 0.9497
27136/31872 [========================>.....] - ETA: 1s - loss: 0.0571 - acc: 0.9498
27328/31872 [========================>.....] - ETA: 1s - loss: 0.0571 - acc: 0.9498
27520/31872 [========================>.....] - ETA: 1s - loss: 0.0571 - acc: 0.9499
27712/31872 [=========================>....] - ETA: 1s - loss: 0.0570 - acc: 0.9500
27904/31872 [=========================>....] - ETA: 1s - loss: 0.0570 - acc: 0.9499
28096/31872 [=========================>....] - ETA: 1s - loss: 0.0570 - acc: 0.9497
28288/31872 [=========================>....] - ETA: 1s - loss: 0.0570 - acc: 0.9499
28480/31872 [=========================>....] - ETA: 1s - loss: 0.0570 - acc: 0.9499
28672/31872 [=========================>....] - ETA: 1s - loss: 0.0570 - acc: 0.9500
28864/31872 [==========================>...] - ETA: 0s - loss: 0.0570 - acc: 0.9501
29056/31872 [==========================>...] - ETA: 0s - loss: 0.0569 - acc: 0.9502
29248/31872 [==========================>...] - ETA: 0s - loss: 0.0569 - acc: 0.9503
29440/31872 [==========================>...] - ETA: 0s - loss: 0.0569 - acc: 0.9504
29632/31872 [==========================>...] - ETA: 0s - loss: 0.0569 - acc: 0.9504
29824/31872 [===========================>..] - ETA: 0s - loss: 0.0569 - acc: 0.9503
30016/31872 [===========================>..] - ETA: 0s - loss: 0.0569 - acc: 0.9502
30208/31872 [===========================>..] - ETA: 0s - loss: 0.0569 - acc: 0.9502
30400/31872 [===========================>..] - ETA: 0s - loss: 0.0569 - acc: 0.9501
30592/31872 [===========================>..] - ETA: 0s - loss: 0.0569 - acc: 0.9503
30784/31872 [===========================>..] - ETA: 0s - loss: 0.0569 - acc: 0.9505
30976/31872 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9504
31168/31872 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9505
31360/31872 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9506
31552/31872 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9507
31744/31872 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9507
31872/31872 [==============================] - 11s 346us/step - loss: 0.0568 - acc: 0.9505 - val_loss: 0.0559 - val_acc: 0.9497
Epoch 9/30

   64/31872 [..............................] - ETA: 10s - loss: 0.0499 - acc: 0.9844
  256/31872 [..............................] - ETA: 10s - loss: 0.0554 - acc: 0.9727
  448/31872 [..............................] - ETA: 10s - loss: 0.0527 - acc: 0.9665
  640/31872 [..............................] - ETA: 9s - loss: 0.0565 - acc: 0.9578 
  832/31872 [..............................] - ETA: 9s - loss: 0.0576 - acc: 0.9519
 1024/31872 [..............................] - ETA: 9s - loss: 0.0580 - acc: 0.9492
 1216/31872 [>.............................] - ETA: 9s - loss: 0.0596 - acc: 0.9474
 1408/31872 [>.............................] - ETA: 9s - loss: 0.0591 - acc: 0.9453
 1600/31872 [>.............................] - ETA: 9s - loss: 0.0591 - acc: 0.9450
 1792/31872 [>.............................] - ETA: 9s - loss: 0.0582 - acc: 0.9453
 1984/31872 [>.............................] - ETA: 9s - loss: 0.0579 - acc: 0.9481
 2176/31872 [=>............................] - ETA: 9s - loss: 0.0581 - acc: 0.9494
 2368/31872 [=>............................] - ETA: 9s - loss: 0.0568 - acc: 0.9502
 2560/31872 [=>............................] - ETA: 9s - loss: 0.0561 - acc: 0.9520
 2752/31872 [=>............................] - ETA: 9s - loss: 0.0553 - acc: 0.9524
 2944/31872 [=>............................] - ETA: 9s - loss: 0.0555 - acc: 0.9528
 3136/31872 [=>............................] - ETA: 9s - loss: 0.0555 - acc: 0.9534
 3328/31872 [==>...........................] - ETA: 9s - loss: 0.0555 - acc: 0.9537
 3520/31872 [==>...........................] - ETA: 9s - loss: 0.0562 - acc: 0.9537
 3712/31872 [==>...........................] - ETA: 8s - loss: 0.0562 - acc: 0.9539
 3904/31872 [==>...........................] - ETA: 8s - loss: 0.0563 - acc: 0.9539
 4096/31872 [==>...........................] - ETA: 8s - loss: 0.0566 - acc: 0.9531
 4288/31872 [===>..........................] - ETA: 8s - loss: 0.0568 - acc: 0.9522
 4480/31872 [===>..........................] - ETA: 8s - loss: 0.0571 - acc: 0.9511
 4672/31872 [===>..........................] - ETA: 8s - loss: 0.0568 - acc: 0.9514
 4864/31872 [===>..........................] - ETA: 8s - loss: 0.0567 - acc: 0.9513
 5056/31872 [===>..........................] - ETA: 8s - loss: 0.0567 - acc: 0.9509
 5248/31872 [===>..........................] - ETA: 8s - loss: 0.0565 - acc: 0.9510
 5440/31872 [====>.........................] - ETA: 8s - loss: 0.0568 - acc: 0.9513
 5632/31872 [====>.........................] - ETA: 8s - loss: 0.0566 - acc: 0.9517
 5824/31872 [====>.........................] - ETA: 8s - loss: 0.0567 - acc: 0.9519
 6016/31872 [====>.........................] - ETA: 8s - loss: 0.0565 - acc: 0.9518
 6208/31872 [====>.........................] - ETA: 8s - loss: 0.0565 - acc: 0.9514
 6400/31872 [=====>........................] - ETA: 8s - loss: 0.0565 - acc: 0.9509
 6592/31872 [=====>........................] - ETA: 8s - loss: 0.0566 - acc: 0.9510
 6784/31872 [=====>........................] - ETA: 7s - loss: 0.0565 - acc: 0.9508
 6976/31872 [=====>........................] - ETA: 7s - loss: 0.0565 - acc: 0.9517
 7168/31872 [=====>........................] - ETA: 7s - loss: 0.0563 - acc: 0.9520
 7360/31872 [=====>........................] - ETA: 7s - loss: 0.0562 - acc: 0.9526
 7552/31872 [======>.......................] - ETA: 7s - loss: 0.0561 - acc: 0.9530
 7744/31872 [======>.......................] - ETA: 7s - loss: 0.0563 - acc: 0.9520
 7936/31872 [======>.......................] - ETA: 7s - loss: 0.0561 - acc: 0.9525
 8128/31872 [======>.......................] - ETA: 7s - loss: 0.0558 - acc: 0.9531
 8320/31872 [======>.......................] - ETA: 7s - loss: 0.0558 - acc: 0.9531
 8512/31872 [=======>......................] - ETA: 7s - loss: 0.0557 - acc: 0.9530
 8704/31872 [=======>......................] - ETA: 7s - loss: 0.0559 - acc: 0.9528
 8896/31872 [=======>......................] - ETA: 7s - loss: 0.0560 - acc: 0.9527
 9088/31872 [=======>......................] - ETA: 7s - loss: 0.0558 - acc: 0.9524
 9280/31872 [=======>......................] - ETA: 7s - loss: 0.0559 - acc: 0.9524
 9472/31872 [=======>......................] - ETA: 7s - loss: 0.0557 - acc: 0.9528
 9664/31872 [========>.....................] - ETA: 7s - loss: 0.0557 - acc: 0.9530
 9856/31872 [========>.....................] - ETA: 6s - loss: 0.0556 - acc: 0.9532
10048/31872 [========>.....................] - ETA: 6s - loss: 0.0555 - acc: 0.9532
10240/31872 [========>.....................] - ETA: 6s - loss: 0.0553 - acc: 0.9527
10432/31872 [========>.....................] - ETA: 6s - loss: 0.0552 - acc: 0.9528
10624/31872 [=========>....................] - ETA: 6s - loss: 0.0552 - acc: 0.9528
10816/31872 [=========>....................] - ETA: 6s - loss: 0.0553 - acc: 0.9528
11008/31872 [=========>....................] - ETA: 6s - loss: 0.0553 - acc: 0.9525
11200/31872 [=========>....................] - ETA: 6s - loss: 0.0554 - acc: 0.9521
11392/31872 [=========>....................] - ETA: 6s - loss: 0.0554 - acc: 0.9520
11584/31872 [=========>....................] - ETA: 6s - loss: 0.0555 - acc: 0.9522
11776/31872 [==========>...................] - ETA: 6s - loss: 0.0556 - acc: 0.9517
11968/31872 [==========>...................] - ETA: 6s - loss: 0.0555 - acc: 0.9520
12160/31872 [==========>...................] - ETA: 6s - loss: 0.0555 - acc: 0.9521
12352/31872 [==========>...................] - ETA: 6s - loss: 0.0554 - acc: 0.9526
12544/31872 [==========>...................] - ETA: 6s - loss: 0.0552 - acc: 0.9531
12736/31872 [==========>...................] - ETA: 6s - loss: 0.0553 - acc: 0.9527
12928/31872 [===========>..................] - ETA: 6s - loss: 0.0552 - acc: 0.9527
13120/31872 [===========>..................] - ETA: 5s - loss: 0.0551 - acc: 0.9525
13312/31872 [===========>..................] - ETA: 5s - loss: 0.0551 - acc: 0.9524
13504/31872 [===========>..................] - ETA: 5s - loss: 0.0551 - acc: 0.9525
13696/31872 [===========>..................] - ETA: 5s - loss: 0.0550 - acc: 0.9527
13888/31872 [============>.................] - ETA: 5s - loss: 0.0549 - acc: 0.9525
14080/31872 [============>.................] - ETA: 5s - loss: 0.0550 - acc: 0.9523
14272/31872 [============>.................] - ETA: 5s - loss: 0.0549 - acc: 0.9524
14464/31872 [============>.................] - ETA: 5s - loss: 0.0550 - acc: 0.9521
14656/31872 [============>.................] - ETA: 5s - loss: 0.0550 - acc: 0.9518
14848/31872 [============>.................] - ETA: 5s - loss: 0.0548 - acc: 0.9524
15040/31872 [=============>................] - ETA: 5s - loss: 0.0547 - acc: 0.9526
15232/31872 [=============>................] - ETA: 5s - loss: 0.0547 - acc: 0.9529
15424/31872 [=============>................] - ETA: 5s - loss: 0.0547 - acc: 0.9528
15616/31872 [=============>................] - ETA: 5s - loss: 0.0547 - acc: 0.9528
15808/31872 [=============>................] - ETA: 5s - loss: 0.0547 - acc: 0.9528
16000/31872 [==============>...............] - ETA: 5s - loss: 0.0548 - acc: 0.9525
16192/31872 [==============>...............] - ETA: 4s - loss: 0.0547 - acc: 0.9526
16384/31872 [==============>...............] - ETA: 4s - loss: 0.0546 - acc: 0.9528
16576/31872 [==============>...............] - ETA: 4s - loss: 0.0547 - acc: 0.9528
16768/31872 [==============>...............] - ETA: 4s - loss: 0.0547 - acc: 0.9526
16960/31872 [==============>...............] - ETA: 4s - loss: 0.0548 - acc: 0.9528
17152/31872 [===============>..............] - ETA: 4s - loss: 0.0548 - acc: 0.9525
17344/31872 [===============>..............] - ETA: 4s - loss: 0.0548 - acc: 0.9525
17536/31872 [===============>..............] - ETA: 4s - loss: 0.0548 - acc: 0.9526
17728/31872 [===============>..............] - ETA: 4s - loss: 0.0548 - acc: 0.9522
17920/31872 [===============>..............] - ETA: 4s - loss: 0.0548 - acc: 0.9523
18112/31872 [================>.............] - ETA: 4s - loss: 0.0548 - acc: 0.9521
18304/31872 [================>.............] - ETA: 4s - loss: 0.0548 - acc: 0.9520
18496/31872 [================>.............] - ETA: 4s - loss: 0.0548 - acc: 0.9520
18688/31872 [================>.............] - ETA: 4s - loss: 0.0547 - acc: 0.9520
18880/31872 [================>.............] - ETA: 4s - loss: 0.0547 - acc: 0.9519
19072/31872 [================>.............] - ETA: 4s - loss: 0.0546 - acc: 0.9520
19264/31872 [=================>............] - ETA: 4s - loss: 0.0546 - acc: 0.9520
19456/31872 [=================>............] - ETA: 3s - loss: 0.0546 - acc: 0.9519
19648/31872 [=================>............] - ETA: 3s - loss: 0.0547 - acc: 0.9520
19840/31872 [=================>............] - ETA: 3s - loss: 0.0547 - acc: 0.9521
20032/31872 [=================>............] - ETA: 3s - loss: 0.0546 - acc: 0.9523
20224/31872 [==================>...........] - ETA: 3s - loss: 0.0546 - acc: 0.9524
20416/31872 [==================>...........] - ETA: 3s - loss: 0.0547 - acc: 0.9524
20608/31872 [==================>...........] - ETA: 3s - loss: 0.0547 - acc: 0.9524
20800/31872 [==================>...........] - ETA: 3s - loss: 0.0547 - acc: 0.9525
20992/31872 [==================>...........] - ETA: 3s - loss: 0.0547 - acc: 0.9526
21184/31872 [==================>...........] - ETA: 3s - loss: 0.0547 - acc: 0.9527
21376/31872 [===================>..........] - ETA: 3s - loss: 0.0546 - acc: 0.9530
21568/31872 [===================>..........] - ETA: 3s - loss: 0.0547 - acc: 0.9529
21760/31872 [===================>..........] - ETA: 3s - loss: 0.0546 - acc: 0.9531
21952/31872 [===================>..........] - ETA: 3s - loss: 0.0546 - acc: 0.9533
22144/31872 [===================>..........] - ETA: 3s - loss: 0.0547 - acc: 0.9533
22336/31872 [====================>.........] - ETA: 3s - loss: 0.0546 - acc: 0.9533
22528/31872 [====================>.........] - ETA: 2s - loss: 0.0546 - acc: 0.9533
22720/31872 [====================>.........] - ETA: 2s - loss: 0.0545 - acc: 0.9534
22912/31872 [====================>.........] - ETA: 2s - loss: 0.0545 - acc: 0.9533
23104/31872 [====================>.........] - ETA: 2s - loss: 0.0545 - acc: 0.9533
23296/31872 [====================>.........] - ETA: 2s - loss: 0.0545 - acc: 0.9532
23488/31872 [=====================>........] - ETA: 2s - loss: 0.0545 - acc: 0.9533
23680/31872 [=====================>........] - ETA: 2s - loss: 0.0545 - acc: 0.9530
23872/31872 [=====================>........] - ETA: 2s - loss: 0.0545 - acc: 0.9530
24064/31872 [=====================>........] - ETA: 2s - loss: 0.0545 - acc: 0.9532
24256/31872 [=====================>........] - ETA: 2s - loss: 0.0545 - acc: 0.9532
24448/31872 [======================>.......] - ETA: 2s - loss: 0.0544 - acc: 0.9533
24640/31872 [======================>.......] - ETA: 2s - loss: 0.0545 - acc: 0.9532
24832/31872 [======================>.......] - ETA: 2s - loss: 0.0546 - acc: 0.9532
25024/31872 [======================>.......] - ETA: 2s - loss: 0.0546 - acc: 0.9530
25216/31872 [======================>.......] - ETA: 2s - loss: 0.0547 - acc: 0.9532
25408/31872 [======================>.......] - ETA: 2s - loss: 0.0547 - acc: 0.9532
25600/31872 [=======================>......] - ETA: 1s - loss: 0.0546 - acc: 0.9532
25792/31872 [=======================>......] - ETA: 1s - loss: 0.0546 - acc: 0.9533
25984/31872 [=======================>......] - ETA: 1s - loss: 0.0546 - acc: 0.9533
26176/31872 [=======================>......] - ETA: 1s - loss: 0.0545 - acc: 0.9535
26368/31872 [=======================>......] - ETA: 1s - loss: 0.0545 - acc: 0.9534
26560/31872 [========================>.....] - ETA: 1s - loss: 0.0546 - acc: 0.9534
26752/31872 [========================>.....] - ETA: 1s - loss: 0.0546 - acc: 0.9534
26944/31872 [========================>.....] - ETA: 1s - loss: 0.0545 - acc: 0.9535
27136/31872 [========================>.....] - ETA: 1s - loss: 0.0545 - acc: 0.9536
27328/31872 [========================>.....] - ETA: 1s - loss: 0.0544 - acc: 0.9537
27520/31872 [========================>.....] - ETA: 1s - loss: 0.0544 - acc: 0.9538
27712/31872 [=========================>....] - ETA: 1s - loss: 0.0543 - acc: 0.9538
27904/31872 [=========================>....] - ETA: 1s - loss: 0.0543 - acc: 0.9537
28096/31872 [=========================>....] - ETA: 1s - loss: 0.0543 - acc: 0.9538
28288/31872 [=========================>....] - ETA: 1s - loss: 0.0543 - acc: 0.9538
28480/31872 [=========================>....] - ETA: 1s - loss: 0.0543 - acc: 0.9539
28672/31872 [=========================>....] - ETA: 1s - loss: 0.0543 - acc: 0.9538
28864/31872 [==========================>...] - ETA: 0s - loss: 0.0542 - acc: 0.9538
29056/31872 [==========================>...] - ETA: 0s - loss: 0.0542 - acc: 0.9538
29248/31872 [==========================>...] - ETA: 0s - loss: 0.0542 - acc: 0.9538
29440/31872 [==========================>...] - ETA: 0s - loss: 0.0542 - acc: 0.9537
29632/31872 [==========================>...] - ETA: 0s - loss: 0.0542 - acc: 0.9537
29824/31872 [===========================>..] - ETA: 0s - loss: 0.0542 - acc: 0.9536
30016/31872 [===========================>..] - ETA: 0s - loss: 0.0542 - acc: 0.9535
30208/31872 [===========================>..] - ETA: 0s - loss: 0.0542 - acc: 0.9536
30400/31872 [===========================>..] - ETA: 0s - loss: 0.0542 - acc: 0.9536
30592/31872 [===========================>..] - ETA: 0s - loss: 0.0542 - acc: 0.9536
30784/31872 [===========================>..] - ETA: 0s - loss: 0.0541 - acc: 0.9536
30976/31872 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9535
31168/31872 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9537
31360/31872 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9537
31552/31872 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9537
31744/31872 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9537
31872/31872 [==============================] - 11s 346us/step - loss: 0.0539 - acc: 0.9538 - val_loss: 0.0535 - val_acc: 0.9493
Epoch 00009: early stopping
Test accuracy: 0.7055107526881721
score new .7 ------------- > 0.68
score new .6 ----------- > 0.793
auc_score ------------------  0.8825
best run {'dense_filter': 1, 'dense_filter1': 1, 'dense_filter2': 1, 'dropout1': 0.692539034315719, 'dropout1_1': 0.21280043312755825, 'layers': 0}
Evalutation of best performing model:

  32/7440 [..............................] - ETA: 1s
 384/7440 [>.............................] - ETA: 1s
 736/7440 [=>............................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 0s
1440/7440 [====>.........................] - ETA: 0s
1792/7440 [======>.......................] - ETA: 0s
2144/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2848/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 146us/step
[0.2000599504158061, 0.7055107526881721]
val roc_auc_score 0.883
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_1[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_1[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 30)           0           activation_3[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            31          global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 15s - loss: 0.5685 - acc: 0.7280 - val_loss: 0.5495 - val_acc: 0.8078
Epoch 2/20
 - 13s - loss: 0.5246 - acc: 0.7513 - val_loss: 1.3659 - val_acc: 0.4999
Epoch 3/20
 - 13s - loss: 0.5076 - acc: 0.7598 - val_loss: 0.7154 - val_acc: 0.7731
Epoch 4/20
 - 12s - loss: 0.4978 - acc: 0.7635 - val_loss: 0.4932 - val_acc: 0.7878
Epoch 5/20
 - 12s - loss: 0.4906 - acc: 0.7673 - val_loss: 0.5005 - val_acc: 0.7448
Epoch 6/20
 - 12s - loss: 0.4840 - acc: 0.7726 - val_loss: 2.1211 - val_acc: 0.6183
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 133us/step
Test accuracy: 0.6182795698924731
Test accuracy 0.6: 0.6384408602150538
auc_score ------------------>  0.7852198592322812
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_4[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 30)           0           activation_6[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            31          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5488 - acc: 0.7382 - val_loss: 0.6174 - val_acc: 0.8120
Epoch 2/20
 - 12s - loss: 0.5178 - acc: 0.7564 - val_loss: 1.3454 - val_acc: 0.7577
Epoch 3/20
 - 12s - loss: 0.5027 - acc: 0.7635 - val_loss: 0.5312 - val_acc: 0.7341
Epoch 4/20
 - 12s - loss: 0.4908 - acc: 0.7710 - val_loss: 5.0669 - val_acc: 0.5934
Epoch 5/20
 - 12s - loss: 0.4858 - acc: 0.7734 - val_loss: 0.6447 - val_acc: 0.7249
Epoch 6/20
 - 12s - loss: 0.4805 - acc: 0.7761 - val_loss: 0.6250 - val_acc: 0.7487

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 1s
 384/7440 [>.............................] - ETA: 1s
 768/7440 [==>...........................] - ETA: 0s
1152/7440 [===>..........................] - ETA: 0s
1536/7440 [=====>........................] - ETA: 0s
1920/7440 [======>.......................] - ETA: 0s
2304/7440 [========>.....................] - ETA: 0s
2688/7440 [=========>....................] - ETA: 0s
3072/7440 [===========>..................] - ETA: 0s
3456/7440 [============>.................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4224/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 136us/step
Test accuracy: 0.7486559139784946
Test accuracy 0.6: 0.7801075268817205
auc_score ------------------>  0.8712408226384553
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_7[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_8[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 30)           0           activation_9[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            31          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5557 - acc: 0.7345 - val_loss: 0.6925 - val_acc: 0.7901
Epoch 2/20
 - 12s - loss: 0.5243 - acc: 0.7512 - val_loss: 0.5410 - val_acc: 0.7964
Epoch 3/20
 - 12s - loss: 0.5105 - acc: 0.7546 - val_loss: 0.7460 - val_acc: 0.7621
Epoch 4/20
 - 12s - loss: 0.5022 - acc: 0.7592 - val_loss: 2.2565 - val_acc: 0.6202
Epoch 5/20
 - 12s - loss: 0.4993 - acc: 0.7631 - val_loss: 1.1531 - val_acc: 0.7505

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 6/20
 - 12s - loss: 0.4744 - acc: 0.7750 - val_loss: 0.4443 - val_acc: 0.8122
Epoch 7/20
 - 12s - loss: 0.4647 - acc: 0.7761 - val_loss: 0.5751 - val_acc: 0.7925
Epoch 8/20
 - 12s - loss: 0.4572 - acc: 0.7834 - val_loss: 0.5028 - val_acc: 0.7368
Epoch 9/20
 - 12s - loss: 0.4520 - acc: 0.7863 - val_loss: 0.4507 - val_acc: 0.8091

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0029999998973718025.
Epoch 10/20
 - 12s - loss: 0.4374 - acc: 0.7941 - val_loss: 0.4525 - val_acc: 0.8344
Epoch 11/20
 - 12s - loss: 0.4314 - acc: 0.7982 - val_loss: 0.4384 - val_acc: 0.8397
Epoch 12/20
 - 12s - loss: 0.4300 - acc: 0.7973 - val_loss: 0.4672 - val_acc: 0.7965
Epoch 13/20
 - 12s - loss: 0.4269 - acc: 0.7988 - val_loss: 0.4748 - val_acc: 0.8323
Epoch 14/20
 - 12s - loss: 0.4264 - acc: 0.8017 - val_loss: 0.4705 - val_acc: 0.8082

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009486832326692812.
Epoch 15/20
 - 12s - loss: 0.4174 - acc: 0.8044 - val_loss: 0.4527 - val_acc: 0.8109
Epoch 16/20
 - 12s - loss: 0.4156 - acc: 0.8049 - val_loss: 0.3956 - val_acc: 0.8597
Epoch 17/20
 - 12s - loss: 0.4151 - acc: 0.8059 - val_loss: 0.4413 - val_acc: 0.8149
Epoch 18/20
 - 12s - loss: 0.4132 - acc: 0.8066 - val_loss: 0.4282 - val_acc: 0.8603
Epoch 19/20
 - 12s - loss: 0.4126 - acc: 0.8065 - val_loss: 0.4027 - val_acc: 0.8621

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00029999998237442883.
Epoch 20/20
 - 12s - loss: 0.4113 - acc: 0.8070 - val_loss: 0.3937 - val_acc: 0.8489

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 832/7440 [==>...........................] - ETA: 0s
1248/7440 [====>.........................] - ETA: 0s
1664/7440 [=====>........................] - ETA: 0s
2080/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3328/7440 [============>.................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 126us/step
Test accuracy: 0.8489247311827957
Test accuracy 0.6: 0.8254032258064516
auc_score ------------------>  0.9194850199445023
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_10[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 30)           0           activation_12[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            31          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5540 - acc: 0.7357 - val_loss: 0.4851 - val_acc: 0.7917
Epoch 2/20
 - 12s - loss: 0.5274 - acc: 0.7472 - val_loss: 1.1152 - val_acc: 0.5016
Epoch 3/20
 - 12s - loss: 0.5123 - acc: 0.7550 - val_loss: 0.8192 - val_acc: 0.6140
Epoch 4/20
 - 12s - loss: 0.5033 - acc: 0.7608 - val_loss: 0.9964 - val_acc: 0.5294

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 5/20
 - 12s - loss: 0.4774 - acc: 0.7715 - val_loss: 0.5400 - val_acc: 0.7761
Epoch 6/20
 - 12s - loss: 0.4673 - acc: 0.7789 - val_loss: 0.9209 - val_acc: 0.7098
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 138us/step
Test accuracy: 0.7098118279569893
Test accuracy 0.6: 0.734005376344086
auc_score ------------------>  0.8495779858943231
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_13[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 30)           0           activation_15[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            31          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5538 - acc: 0.7379 - val_loss: 0.6819 - val_acc: 0.7621
Epoch 2/20
 - 12s - loss: 0.5213 - acc: 0.7510 - val_loss: 2.9857 - val_acc: 0.5888
Epoch 3/20
 - 12s - loss: 0.5075 - acc: 0.7595 - val_loss: 0.6889 - val_acc: 0.7218
Epoch 4/20
 - 12s - loss: 0.5010 - acc: 0.7612 - val_loss: 0.6019 - val_acc: 0.7399
Epoch 5/20
 - 12s - loss: 0.4959 - acc: 0.7649 - val_loss: 0.6737 - val_acc: 0.7156
Epoch 6/20
 - 12s - loss: 0.4923 - acc: 0.7670 - val_loss: 3.3853 - val_acc: 0.6363
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 0s
 448/7440 [>.............................] - ETA: 0s
 896/7440 [==>...........................] - ETA: 0s
1312/7440 [====>.........................] - ETA: 0s
1760/7440 [======>.......................] - ETA: 0s
2208/7440 [=======>......................] - ETA: 0s
2656/7440 [=========>....................] - ETA: 0s
3072/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 121us/step
Test accuracy: 0.6362903225806451
Test accuracy 0.6: 0.6442204301075268
auc_score ------------------>  0.7257774742744826
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_16[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 30)           0           activation_18[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            31          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 12s - loss: 0.5629 - acc: 0.7326 - val_loss: 1.1042 - val_acc: 0.7027
Epoch 2/20
 - 12s - loss: 0.5307 - acc: 0.7485 - val_loss: 0.7023 - val_acc: 0.8431
Epoch 3/20
 - 12s - loss: 0.5188 - acc: 0.7529 - val_loss: 0.7107 - val_acc: 0.6918
Epoch 4/20
 - 12s - loss: 0.5057 - acc: 0.7588 - val_loss: 3.2487 - val_acc: 0.6315
Epoch 5/20
 - 12s - loss: 0.5040 - acc: 0.7611 - val_loss: 0.6507 - val_acc: 0.7833
Epoch 6/20
 - 12s - loss: 0.5020 - acc: 0.7607 - val_loss: 2.0619 - val_acc: 0.6659
Epoch 7/20
 - 12s - loss: 0.4959 - acc: 0.7659 - val_loss: 4.0177 - val_acc: 0.5417
Epoch 00007: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 133us/step
Test accuracy: 0.5416666666666666
Test accuracy 0.6: 0.5655913978494623
auc_score ------------------>  0.6757851702508961
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_19[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 30)           0           activation_21[0][0]              
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            31          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5565 - acc: 0.7362 - val_loss: 0.8250 - val_acc: 0.7844
Epoch 2/20
 - 12s - loss: 0.5224 - acc: 0.7520 - val_loss: 0.8685 - val_acc: 0.6890
Epoch 3/20
 - 12s - loss: 0.5113 - acc: 0.7576 - val_loss: 2.6404 - val_acc: 0.6210
Epoch 4/20
 - 12s - loss: 0.4982 - acc: 0.7624 - val_loss: 0.7718 - val_acc: 0.6147
Epoch 5/20
 - 12s - loss: 0.4918 - acc: 0.7663 - val_loss: 4.3550 - val_acc: 0.5077
Epoch 6/20
 - 12s - loss: 0.4882 - acc: 0.7679 - val_loss: 0.6405 - val_acc: 0.6597
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 134us/step
Test accuracy: 0.6596774193548387
Test accuracy 0.6: 0.6709677419354839
auc_score ------------------>  0.7364558041392069
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_22[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 30)           0           activation_24[0][0]              
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            31          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5480 - acc: 0.7391 - val_loss: 1.5842 - val_acc: 0.7234
Epoch 2/20
 - 12s - loss: 0.5233 - acc: 0.7502 - val_loss: 1.3825 - val_acc: 0.5304
Epoch 3/20
 - 12s - loss: 0.5164 - acc: 0.7540 - val_loss: 0.7966 - val_acc: 0.7950
Epoch 4/20
 - 12s - loss: 0.5037 - acc: 0.7620 - val_loss: 1.0533 - val_acc: 0.7836
Epoch 5/20
 - 12s - loss: 0.4926 - acc: 0.7706 - val_loss: 0.5897 - val_acc: 0.7805
Epoch 6/20
 - 12s - loss: 0.4882 - acc: 0.7716 - val_loss: 0.6638 - val_acc: 0.8241
Epoch 7/20
 - 12s - loss: 0.4767 - acc: 0.7777 - val_loss: 0.5757 - val_acc: 0.7239
Epoch 8/20
 - 12s - loss: 0.4770 - acc: 0.7786 - val_loss: 0.4209 - val_acc: 0.8289
Epoch 9/20
 - 12s - loss: 0.4730 - acc: 0.7795 - val_loss: 0.6721 - val_acc: 0.6527
Epoch 10/20
 - 12s - loss: 0.4701 - acc: 0.7835 - val_loss: 0.8632 - val_acc: 0.6489
Epoch 11/20
 - 12s - loss: 0.4678 - acc: 0.7856 - val_loss: 0.7597 - val_acc: 0.6509

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 12/20
 - 12s - loss: 0.4480 - acc: 0.7931 - val_loss: 0.5009 - val_acc: 0.7844
Epoch 13/20
 - 12s - loss: 0.4430 - acc: 0.7961 - val_loss: 0.7187 - val_acc: 0.7414
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 133us/step
Test accuracy: 0.7413978494623656
Test accuracy 0.6: 0.7275537634408602
auc_score ------------------>  0.8364981139438086
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_25[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 30)           0           activation_27[0][0]              
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            31          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5635 - acc: 0.7291 - val_loss: 0.8155 - val_acc: 0.7386
Epoch 2/20
 - 12s - loss: 0.5319 - acc: 0.7474 - val_loss: 0.8713 - val_acc: 0.7605
Epoch 3/20
 - 12s - loss: 0.5178 - acc: 0.7508 - val_loss: 0.6484 - val_acc: 0.7281
Epoch 4/20
 - 12s - loss: 0.5110 - acc: 0.7545 - val_loss: 1.0837 - val_acc: 0.5871
Epoch 5/20
 - 12s - loss: 0.5054 - acc: 0.7561 - val_loss: 1.1691 - val_acc: 0.6160
Epoch 6/20
 - 12s - loss: 0.5037 - acc: 0.7576 - val_loss: 0.7408 - val_acc: 0.6349

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 7/20
 - 12s - loss: 0.4808 - acc: 0.7681 - val_loss: 0.7451 - val_acc: 0.6751
Epoch 00007: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 136us/step
Test accuracy: 0.6751344086021506
Test accuracy 0.6: 0.6348118279569892
auc_score ------------------>  0.7920206093189964
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   864         activation_28[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 30)           0           activation_30[0][0]              
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            31          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 6,787
Trainable params: 6,595
Non-trainable params: 192
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 13s - loss: 0.5560 - acc: 0.7382 - val_loss: 0.5737 - val_acc: 0.7608
Epoch 2/20
 - 12s - loss: 0.5159 - acc: 0.7571 - val_loss: 0.7646 - val_acc: 0.6040
Epoch 3/20
 - 12s - loss: 0.5001 - acc: 0.7631 - val_loss: 1.3200 - val_acc: 0.5038
Epoch 4/20
 - 12s - loss: 0.4877 - acc: 0.7711 - val_loss: 1.2173 - val_acc: 0.5784

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 5/20
 - 12s - loss: 0.4672 - acc: 0.7807 - val_loss: 0.5479 - val_acc: 0.7616
Epoch 6/20
 - 12s - loss: 0.4586 - acc: 0.7844 - val_loss: 0.4452 - val_acc: 0.7867
Epoch 7/20
 - 12s - loss: 0.4577 - acc: 0.7867 - val_loss: 0.6269 - val_acc: 0.7508
Epoch 8/20
 - 12s - loss: 0.4534 - acc: 0.7878 - val_loss: 0.6642 - val_acc: 0.6827
Epoch 9/20
 - 12s - loss: 0.4500 - acc: 0.7894 - val_loss: 0.6381 - val_acc: 0.7974

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0029999998973718025.
Epoch 10/20
 - 12s - loss: 0.4387 - acc: 0.7958 - val_loss: 0.5287 - val_acc: 0.7722
Epoch 11/20
 - 12s - loss: 0.4352 - acc: 0.7987 - val_loss: 0.5372 - val_acc: 0.7755
Epoch 12/20
 - 12s - loss: 0.4319 - acc: 0.7984 - val_loss: 0.6751 - val_acc: 0.7035

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009486832326692812.
Epoch 13/20
 - 12s - loss: 0.4280 - acc: 0.8019 - val_loss: 0.4375 - val_acc: 0.8220
Epoch 14/20
 - 12s - loss: 0.4271 - acc: 0.8009 - val_loss: 0.4620 - val_acc: 0.8079
Epoch 15/20
 - 12s - loss: 0.4263 - acc: 0.8011 - val_loss: 0.4382 - val_acc: 0.8250
Epoch 16/20
 - 12s - loss: 0.4247 - acc: 0.8038 - val_loss: 0.4476 - val_acc: 0.8147

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00029999998237442883.
Epoch 17/20
 - 12s - loss: 0.4240 - acc: 0.8040 - val_loss: 0.4262 - val_acc: 0.8289
Epoch 18/20
 - 12s - loss: 0.4231 - acc: 0.8039 - val_loss: 0.4281 - val_acc: 0.8304
Epoch 19/20
 - 12s - loss: 0.4230 - acc: 0.8041 - val_loss: 0.4399 - val_acc: 0.8198
Epoch 20/20
 - 12s - loss: 0.4225 - acc: 0.8047 - val_loss: 0.4229 - val_acc: 0.8294

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 134us/step
Test accuracy: 0.8294354838709678
Test accuracy 0.6: 0.821505376344086
auc_score ------------------>  0.9038045221990981
[0.785, 0.871, 0.919, 0.85, 0.726, 0.676, 0.736, 0.836, 0.792, 0.904]
0.81 ± 0.076
