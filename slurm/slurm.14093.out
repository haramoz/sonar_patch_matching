python hello-world.py
python hyperas_simple.py
python hyperas_contrastive_loss.py
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
python keras_densenet_simple.py
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import pickle
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'depth': hp.choice('depth', [7,10,13,16,19,22,25]),
        'growth_rate': hp.choice('growth_rate', [6,8,10,12,14,16]),
    }

>>> Functions
  1: def process_data():
  2:     f = h5py.File('matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  3:     X_train = f['X_train'].value
  4:     y_train = f['y_train'].value
  5:     X_val = f['X_val'].value
  6:     y_val = f['y_val'].value
  7: 
  8:     return X_train,y_train,X_val,y_val
  9: 
 10: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 40
   4:     #input_shape = (1,96,96)
   5:     es_patience = 7
   6:     lr_patience = 7
   7:     dropout = None
   8:     depth = space['depth']
   9:     nb_dense_block = 2
  10:     nb_filter = 16
  11:     growth_rate = space['growth_rate']
  12:     weight_decay = 1E-4
  13:     lr = 3E-5 #########################################################CHange file name##########################################
  14:     weight_file = 'keras_densenet_simple_wt_29Sept_1404.h5'
  15:     
  16:     nb_classes = 1
  17:     img_dim = (2,96,96) 
  18:     n_channels = 2 
  19: 
  20:     
  21:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  22:                  growth_rate=growth_rate, nb_filter=nb_filter,
  23:                  dropout_rate=dropout,activation='sigmoid',
  24:                  input_shape=img_dim,include_top=True,
  25:                  bottleneck=True,reduction=0.5,
  26:                  classes=nb_classes,pooling='avg',
  27:                  weights=None)
  28:     
  29: 
  30:     model.summary()
  31:     opt = Adam(lr=lr)
  32:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  33: 
  34:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  35:     #es = EarlyStopping(monitor='val_acc', patience=es_patience,verbose=1,restore_best_weights=True)
  36:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  37: 
  38:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  39: 
  40:     model.fit(X_train,y_train,
  41:           batch_size=32,
  42:           epochs=epochs,
  43:           callbacks=[es,lr_reducer,checkpointer],
  44:           validation_data=(X_val,y_val),
  45:           verbose=2)
  46:     
  47:     score, acc = model.evaluate(X_val, y_val)
  48:     print('current Test accuracy:', acc)
  49:     pred = model.predict(X_val)
  50:     auc_score = roc_auc_score(y_val,pred)
  51:     print("current auc_score ------------------> ",auc_score)
  52: 
  53:     model = load_model(weight_file) #This is the best model
  54:     score, acc = model.evaluate(X_val, y_val)
  55:     print('Best saved model Test accuracy:', acc)
  56:     pred = model.predict(X_val)
  57:     auc_score = roc_auc_score(y_val,pred)
  58:     print("best saved model auc_score ------------------> ",auc_score)
  59: 
  60:     
  61:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  62: 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_1 (Activation)    (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_1 (Average (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_2 (Activation)    (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_1 ( (None, 8)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 11s - loss: 0.6973 - acc: 0.5255 - val_loss: 0.6923 - val_acc: 0.5579

Epoch 00001: val_loss improved from inf to 0.69235, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 7s - loss: 0.6795 - acc: 0.5707 - val_loss: 0.6675 - val_acc: 0.5766

Epoch 00002: val_loss improved from 0.69235 to 0.66753, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 8s - loss: 0.6693 - acc: 0.6060 - val_loss: 0.6524 - val_acc: 0.6116

Epoch 00003: val_loss improved from 0.66753 to 0.65242, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 7s - loss: 0.6624 - acc: 0.6269 - val_loss: 0.6399 - val_acc: 0.6470

Epoch 00004: val_loss improved from 0.65242 to 0.63993, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 7s - loss: 0.6558 - acc: 0.6457 - val_loss: 0.6268 - val_acc: 0.6922

Epoch 00005: val_loss improved from 0.63993 to 0.62678, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 7s - loss: 0.6486 - acc: 0.6692 - val_loss: 0.6103 - val_acc: 0.7995

Epoch 00006: val_loss improved from 0.62678 to 0.61027, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 7s - loss: 0.6406 - acc: 0.6953 - val_loss: 0.5925 - val_acc: 0.8402

Epoch 00007: val_loss improved from 0.61027 to 0.59253, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 7s - loss: 0.6342 - acc: 0.7033 - val_loss: 0.5778 - val_acc: 0.8401

Epoch 00008: val_loss improved from 0.59253 to 0.57781, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 7s - loss: 0.6273 - acc: 0.7067 - val_loss: 0.5664 - val_acc: 0.8390

Epoch 00009: val_loss improved from 0.57781 to 0.56639, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 7s - loss: 0.6235 - acc: 0.7055 - val_loss: 0.5608 - val_acc: 0.8387

Epoch 00010: val_loss improved from 0.56639 to 0.56077, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 7s - loss: 0.6202 - acc: 0.7080 - val_loss: 0.5584 - val_acc: 0.8383

Epoch 00011: val_loss improved from 0.56077 to 0.55835, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 7s - loss: 0.6172 - acc: 0.7100 - val_loss: 0.5530 - val_acc: 0.8383

Epoch 00012: val_loss improved from 0.55835 to 0.55300, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 7s - loss: 0.6148 - acc: 0.7097 - val_loss: 0.5490 - val_acc: 0.8378

Epoch 00013: val_loss improved from 0.55300 to 0.54904, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 7s - loss: 0.6117 - acc: 0.7109 - val_loss: 0.5469 - val_acc: 0.8341

Epoch 00014: val_loss improved from 0.54904 to 0.54692, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 7s - loss: 0.6102 - acc: 0.7115 - val_loss: 0.5422 - val_acc: 0.8344

Epoch 00015: val_loss improved from 0.54692 to 0.54221, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 7s - loss: 0.6070 - acc: 0.7119 - val_loss: 0.5399 - val_acc: 0.8272

Epoch 00016: val_loss improved from 0.54221 to 0.53995, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 7s - loss: 0.6049 - acc: 0.7169 - val_loss: 0.5343 - val_acc: 0.8281

Epoch 00017: val_loss improved from 0.53995 to 0.53427, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 7s - loss: 0.6032 - acc: 0.7121 - val_loss: 0.5338 - val_acc: 0.8309

Epoch 00018: val_loss improved from 0.53427 to 0.53381, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 7s - loss: 0.6011 - acc: 0.7146 - val_loss: 0.5307 - val_acc: 0.8281

Epoch 00019: val_loss improved from 0.53381 to 0.53072, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 7s - loss: 0.5996 - acc: 0.7145 - val_loss: 0.5259 - val_acc: 0.8308

Epoch 00020: val_loss improved from 0.53072 to 0.52592, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 7s - loss: 0.5979 - acc: 0.7177 - val_loss: 0.5241 - val_acc: 0.8280

Epoch 00021: val_loss improved from 0.52592 to 0.52407, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 7s - loss: 0.5967 - acc: 0.7175 - val_loss: 0.5219 - val_acc: 0.8329

Epoch 00022: val_loss improved from 0.52407 to 0.52186, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 7s - loss: 0.5941 - acc: 0.7175 - val_loss: 0.5196 - val_acc: 0.8263

Epoch 00023: val_loss improved from 0.52186 to 0.51961, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 7s - loss: 0.5933 - acc: 0.7177 - val_loss: 0.5160 - val_acc: 0.8198

Epoch 00024: val_loss improved from 0.51961 to 0.51599, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 7s - loss: 0.5920 - acc: 0.7189 - val_loss: 0.5176 - val_acc: 0.8192

Epoch 00025: val_loss did not improve from 0.51599
Epoch 26/40
 - 7s - loss: 0.5905 - acc: 0.7188 - val_loss: 0.5132 - val_acc: 0.8344

Epoch 00026: val_loss improved from 0.51599 to 0.51320, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 7s - loss: 0.5896 - acc: 0.7197 - val_loss: 0.5125 - val_acc: 0.8319

Epoch 00027: val_loss improved from 0.51320 to 0.51249, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 28/40
 - 7s - loss: 0.5883 - acc: 0.7185 - val_loss: 0.5129 - val_acc: 0.8313

Epoch 00028: val_loss did not improve from 0.51249
Epoch 29/40
 - 7s - loss: 0.5873 - acc: 0.7183 - val_loss: 0.5076 - val_acc: 0.8300

Epoch 00029: val_loss improved from 0.51249 to 0.50764, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 7s - loss: 0.5863 - acc: 0.7183 - val_loss: 0.5095 - val_acc: 0.8292

Epoch 00030: val_loss did not improve from 0.50764
Epoch 31/40
 - 7s - loss: 0.5848 - acc: 0.7208 - val_loss: 0.5143 - val_acc: 0.8177

Epoch 00031: val_loss did not improve from 0.50764
Epoch 32/40
 - 7s - loss: 0.5838 - acc: 0.7220 - val_loss: 0.5053 - val_acc: 0.8315

Epoch 00032: val_loss improved from 0.50764 to 0.50531, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 33/40
 - 7s - loss: 0.5825 - acc: 0.7228 - val_loss: 0.5060 - val_acc: 0.8278

Epoch 00033: val_loss did not improve from 0.50531
Epoch 34/40
 - 7s - loss: 0.5816 - acc: 0.7237 - val_loss: 0.5095 - val_acc: 0.8147

Epoch 00034: val_loss did not improve from 0.50531
Epoch 35/40
 - 7s - loss: 0.5795 - acc: 0.7222 - val_loss: 0.5042 - val_acc: 0.8300

Epoch 00035: val_loss improved from 0.50531 to 0.50425, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 36/40
 - 7s - loss: 0.5793 - acc: 0.7224 - val_loss: 0.5058 - val_acc: 0.8290

Epoch 00036: val_loss did not improve from 0.50425
Epoch 37/40
 - 7s - loss: 0.5789 - acc: 0.7223 - val_loss: 0.5015 - val_acc: 0.8276

Epoch 00037: val_loss improved from 0.50425 to 0.50149, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 38/40
 - 7s - loss: 0.5780 - acc: 0.7234 - val_loss: 0.5043 - val_acc: 0.8149

Epoch 00038: val_loss did not improve from 0.50149
Epoch 39/40
 - 7s - loss: 0.5762 - acc: 0.7252 - val_loss: 0.4987 - val_acc: 0.8294

Epoch 00039: val_loss improved from 0.50149 to 0.49873, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 40/40
 - 7s - loss: 0.5761 - acc: 0.7224 - val_loss: 0.5005 - val_acc: 0.8219

Epoch 00040: val_loss did not improve from 0.49873

  32/7440 [..............................] - ETA: 0s
 768/7440 [==>...........................] - ETA: 0s
1472/7440 [====>.........................] - ETA: 0s
2208/7440 [=======>......................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
4320/7440 [================>.............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 73us/step
current Test accuracy: 0.8219086021505376
current auc_score ------------------>  0.8756271317493353

  32/7440 [..............................] - ETA: 17s
 640/7440 [=>............................] - ETA: 1s 
1280/7440 [====>.........................] - ETA: 0s
1920/7440 [======>.......................] - ETA: 0s
2560/7440 [=========>....................] - ETA: 0s
3168/7440 [===========>..................] - ETA: 0s
3808/7440 [==============>...............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 91us/step
Best saved model Test accuracy: 0.8294354838709678
best saved model auc_score ------------------>  0.8749584489536363
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_3[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 56, 96, 96)   1680        activation_5[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 56, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 44, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 44, 96, 96)   176         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 44, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 56, 96, 96)   2464        activation_7[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 56, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_8[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 58, 96, 96)   0           concatenate_2[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 58, 96, 96)   232         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 58, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 29, 96, 96)   1682        activation_9[0][0]               
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 29, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 29, 48, 48)   116         average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 29, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   1624        activation_10[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 43, 48, 48)   0           average_pooling2d_2[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 43, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 56, 48, 48)   2408        activation_12[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 56, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_13[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 57, 48, 48)   0           concatenate_4[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 57, 48, 48)   228         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 57, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 56, 48, 48)   3192        activation_14[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 56, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_15[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 71, 48, 48)   0           concatenate_5[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 71, 48, 48)   284         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 71, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 71)           0           activation_16[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            72          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 59,378
Trainable params: 58,010
Non-trainable params: 1,368
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 45s - loss: 0.6560 - acc: 0.7055 - val_loss: 0.5535 - val_acc: 0.8344

Epoch 00001: val_loss improved from inf to 0.55351, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 43s - loss: 0.5747 - acc: 0.7629 - val_loss: 0.5535 - val_acc: 0.8067

Epoch 00002: val_loss did not improve from 0.55351
Epoch 3/40
 - 43s - loss: 0.5485 - acc: 0.7736 - val_loss: 0.5516 - val_acc: 0.8015

Epoch 00003: val_loss improved from 0.55351 to 0.55160, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 43s - loss: 0.5333 - acc: 0.7798 - val_loss: 0.5377 - val_acc: 0.8136

Epoch 00004: val_loss improved from 0.55160 to 0.53774, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 43s - loss: 0.5208 - acc: 0.7864 - val_loss: 0.5082 - val_acc: 0.8321

Epoch 00005: val_loss improved from 0.53774 to 0.50817, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 43s - loss: 0.5084 - acc: 0.7920 - val_loss: 0.5900 - val_acc: 0.7637

Epoch 00006: val_loss did not improve from 0.50817
Epoch 7/40
 - 43s - loss: 0.5011 - acc: 0.7966 - val_loss: 0.5466 - val_acc: 0.7880

Epoch 00007: val_loss did not improve from 0.50817
Epoch 8/40
 - 43s - loss: 0.4931 - acc: 0.7998 - val_loss: 0.5630 - val_acc: 0.8069

Epoch 00008: val_loss did not improve from 0.50817
Epoch 9/40
 - 43s - loss: 0.4865 - acc: 0.8011 - val_loss: 0.5264 - val_acc: 0.7851

Epoch 00009: val_loss did not improve from 0.50817
Epoch 10/40
 - 43s - loss: 0.4794 - acc: 0.8056 - val_loss: 0.4981 - val_acc: 0.8160

Epoch 00010: val_loss improved from 0.50817 to 0.49812, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 43s - loss: 0.4723 - acc: 0.8103 - val_loss: 0.6048 - val_acc: 0.7824

Epoch 00011: val_loss did not improve from 0.49812
Epoch 12/40
 - 43s - loss: 0.4671 - acc: 0.8121 - val_loss: 0.4961 - val_acc: 0.8017

Epoch 00012: val_loss improved from 0.49812 to 0.49607, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 42s - loss: 0.4623 - acc: 0.8159 - val_loss: 0.5317 - val_acc: 0.7694

Epoch 00013: val_loss did not improve from 0.49607
Epoch 14/40
 - 43s - loss: 0.4555 - acc: 0.8207 - val_loss: 0.5259 - val_acc: 0.7809

Epoch 00014: val_loss did not improve from 0.49607
Epoch 15/40
 - 43s - loss: 0.4493 - acc: 0.8241 - val_loss: 0.5331 - val_acc: 0.7940

Epoch 00015: val_loss did not improve from 0.49607
Epoch 16/40
 - 43s - loss: 0.4467 - acc: 0.8247 - val_loss: 0.5440 - val_acc: 0.7716

Epoch 00016: val_loss did not improve from 0.49607
Epoch 17/40
 - 43s - loss: 0.4414 - acc: 0.8298 - val_loss: 0.5470 - val_acc: 0.7649

Epoch 00017: val_loss did not improve from 0.49607
Epoch 18/40
 - 43s - loss: 0.4371 - acc: 0.8300 - val_loss: 0.5319 - val_acc: 0.7859

Epoch 00018: val_loss did not improve from 0.49607
Epoch 19/40
 - 43s - loss: 0.4324 - acc: 0.8344 - val_loss: 0.5199 - val_acc: 0.7772

Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00019: val_loss did not improve from 0.49607
Epoch 00019: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 360us/step
current Test accuracy: 0.7771505376344086
current auc_score ------------------>  0.8697685426060816

  32/7440 [..............................] - ETA: 1:30
 192/7440 [..............................] - ETA: 17s 
 352/7440 [>.............................] - ETA: 10s
 512/7440 [=>............................] - ETA: 7s 
 672/7440 [=>............................] - ETA: 6s
 832/7440 [==>...........................] - ETA: 5s
 992/7440 [===>..........................] - ETA: 4s
1152/7440 [===>..........................] - ETA: 4s
1312/7440 [====>.........................] - ETA: 4s
1472/7440 [====>.........................] - ETA: 3s
1632/7440 [=====>........................] - ETA: 3s
1792/7440 [======>.......................] - ETA: 3s
1952/7440 [======>.......................] - ETA: 3s
2112/7440 [=======>......................] - ETA: 2s
2272/7440 [========>.....................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2752/7440 [==========>...................] - ETA: 2s
2912/7440 [==========>...................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 416us/step
Best saved model Test accuracy: 0.801747311827957
best saved model auc_score ------------------>  0.8853878699849693
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_17[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 32, 96, 96)   128         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 16, 96, 96)   512         activation_19[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 16, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 16, 48, 48)   64          average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 16, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1024        activation_20[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 32, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 48, 48)   128         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 32, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 32)           0           activation_22[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            33          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 22,209
Trainable params: 21,761
Non-trainable params: 448
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 21s - loss: 0.6325 - acc: 0.6939 - val_loss: 0.5441 - val_acc: 0.8367

Epoch 00001: val_loss improved from inf to 0.54413, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 19s - loss: 0.5864 - acc: 0.7421 - val_loss: 0.5207 - val_acc: 0.8593

Epoch 00002: val_loss improved from 0.54413 to 0.52069, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 19s - loss: 0.5650 - acc: 0.7545 - val_loss: 0.5070 - val_acc: 0.8487

Epoch 00003: val_loss improved from 0.52069 to 0.50699, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 19s - loss: 0.5518 - acc: 0.7604 - val_loss: 0.5146 - val_acc: 0.8386

Epoch 00004: val_loss did not improve from 0.50699
Epoch 5/40
 - 19s - loss: 0.5394 - acc: 0.7646 - val_loss: 0.5203 - val_acc: 0.8199

Epoch 00005: val_loss did not improve from 0.50699
Epoch 6/40
 - 19s - loss: 0.5315 - acc: 0.7667 - val_loss: 0.4978 - val_acc: 0.8161

Epoch 00006: val_loss improved from 0.50699 to 0.49780, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 19s - loss: 0.5234 - acc: 0.7678 - val_loss: 0.5125 - val_acc: 0.8180

Epoch 00007: val_loss did not improve from 0.49780
Epoch 8/40
 - 19s - loss: 0.5162 - acc: 0.7731 - val_loss: 0.4932 - val_acc: 0.8224

Epoch 00008: val_loss improved from 0.49780 to 0.49318, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 19s - loss: 0.5104 - acc: 0.7744 - val_loss: 0.4878 - val_acc: 0.8226

Epoch 00009: val_loss improved from 0.49318 to 0.48782, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 19s - loss: 0.5051 - acc: 0.7744 - val_loss: 0.4894 - val_acc: 0.8089

Epoch 00010: val_loss did not improve from 0.48782
Epoch 11/40
 - 19s - loss: 0.5005 - acc: 0.7769 - val_loss: 0.4820 - val_acc: 0.8233

Epoch 00011: val_loss improved from 0.48782 to 0.48202, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 19s - loss: 0.4952 - acc: 0.7802 - val_loss: 0.4752 - val_acc: 0.8277

Epoch 00012: val_loss improved from 0.48202 to 0.47517, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 19s - loss: 0.4916 - acc: 0.7805 - val_loss: 0.5056 - val_acc: 0.8026

Epoch 00013: val_loss did not improve from 0.47517
Epoch 14/40
 - 19s - loss: 0.4872 - acc: 0.7824 - val_loss: 0.4855 - val_acc: 0.8052

Epoch 00014: val_loss did not improve from 0.47517
Epoch 15/40
 - 19s - loss: 0.4836 - acc: 0.7851 - val_loss: 0.4809 - val_acc: 0.8190

Epoch 00015: val_loss did not improve from 0.47517
Epoch 16/40
 - 19s - loss: 0.4796 - acc: 0.7877 - val_loss: 0.4835 - val_acc: 0.8020

Epoch 00016: val_loss did not improve from 0.47517
Epoch 17/40
 - 19s - loss: 0.4787 - acc: 0.7878 - val_loss: 0.4709 - val_acc: 0.8151

Epoch 00017: val_loss improved from 0.47517 to 0.47090, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 19s - loss: 0.4736 - acc: 0.7908 - val_loss: 0.4784 - val_acc: 0.8020

Epoch 00018: val_loss did not improve from 0.47090
Epoch 19/40
 - 19s - loss: 0.4711 - acc: 0.7903 - val_loss: 0.5156 - val_acc: 0.7824

Epoch 00019: val_loss did not improve from 0.47090
Epoch 20/40
 - 19s - loss: 0.4689 - acc: 0.7911 - val_loss: 0.4715 - val_acc: 0.8097

Epoch 00020: val_loss did not improve from 0.47090
Epoch 21/40
 - 19s - loss: 0.4664 - acc: 0.7917 - val_loss: 0.4802 - val_acc: 0.8012

Epoch 00021: val_loss did not improve from 0.47090
Epoch 22/40
 - 19s - loss: 0.4631 - acc: 0.7941 - val_loss: 0.4663 - val_acc: 0.8204

Epoch 00022: val_loss improved from 0.47090 to 0.46626, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 19s - loss: 0.4607 - acc: 0.7955 - val_loss: 0.4925 - val_acc: 0.7864

Epoch 00023: val_loss did not improve from 0.46626
Epoch 24/40
 - 19s - loss: 0.4595 - acc: 0.7970 - val_loss: 0.4731 - val_acc: 0.8093

Epoch 00024: val_loss did not improve from 0.46626
Epoch 25/40
 - 19s - loss: 0.4565 - acc: 0.7986 - val_loss: 0.4609 - val_acc: 0.8130

Epoch 00025: val_loss improved from 0.46626 to 0.46087, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 26/40
 - 19s - loss: 0.4550 - acc: 0.7997 - val_loss: 0.4708 - val_acc: 0.8059

Epoch 00026: val_loss did not improve from 0.46087
Epoch 27/40
 - 19s - loss: 0.4531 - acc: 0.7996 - val_loss: 0.4707 - val_acc: 0.8276

Epoch 00027: val_loss did not improve from 0.46087
Epoch 28/40
 - 19s - loss: 0.4513 - acc: 0.8007 - val_loss: 0.4607 - val_acc: 0.8160

Epoch 00028: val_loss improved from 0.46087 to 0.46072, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 29/40
 - 19s - loss: 0.4477 - acc: 0.8026 - val_loss: 0.4485 - val_acc: 0.8304

Epoch 00029: val_loss improved from 0.46072 to 0.44853, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 19s - loss: 0.4462 - acc: 0.8046 - val_loss: 0.4549 - val_acc: 0.8140

Epoch 00030: val_loss did not improve from 0.44853
Epoch 31/40
 - 19s - loss: 0.4440 - acc: 0.8055 - val_loss: 0.4567 - val_acc: 0.8278

Epoch 00031: val_loss did not improve from 0.44853
Epoch 32/40
 - 19s - loss: 0.4425 - acc: 0.8069 - val_loss: 0.4474 - val_acc: 0.8300

Epoch 00032: val_loss improved from 0.44853 to 0.44744, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 33/40
 - 19s - loss: 0.4408 - acc: 0.8086 - val_loss: 0.4704 - val_acc: 0.8044

Epoch 00033: val_loss did not improve from 0.44744
Epoch 34/40
 - 19s - loss: 0.4396 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8179

Epoch 00034: val_loss did not improve from 0.44744
Epoch 35/40
 - 19s - loss: 0.4381 - acc: 0.8079 - val_loss: 0.4603 - val_acc: 0.8184

Epoch 00035: val_loss did not improve from 0.44744
Epoch 36/40
 - 19s - loss: 0.4376 - acc: 0.8086 - val_loss: 0.4673 - val_acc: 0.8003

Epoch 00036: val_loss did not improve from 0.44744
Epoch 37/40
 - 19s - loss: 0.4350 - acc: 0.8094 - val_loss: 0.5441 - val_acc: 0.7703

Epoch 00037: val_loss did not improve from 0.44744
Epoch 38/40
 - 19s - loss: 0.4359 - acc: 0.8097 - val_loss: 0.4587 - val_acc: 0.8263

Epoch 00038: val_loss did not improve from 0.44744
Epoch 39/40
 - 19s - loss: 0.4325 - acc: 0.8111 - val_loss: 0.4577 - val_acc: 0.8091

Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00039: val_loss did not improve from 0.44744
Epoch 00039: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2656/7440 [=========>....................] - ETA: 0s
2944/7440 [==========>...................] - ETA: 0s
3264/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 175us/step
current Test accuracy: 0.8091397849462365
current auc_score ------------------>  0.8842142155162447

  32/7440 [..............................] - ETA: 1:33
 320/7440 [>.............................] - ETA: 10s 
 608/7440 [=>............................] - ETA: 5s 
 896/7440 [==>...........................] - ETA: 4s
1216/7440 [===>..........................] - ETA: 3s
1536/7440 [=====>........................] - ETA: 2s
1856/7440 [======>.......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
3136/7440 [===========>..................] - ETA: 1s
3456/7440 [============>.................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 224us/step
Best saved model Test accuracy: 0.8299731182795699
best saved model auc_score ------------------>  0.8927244840443982
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_23[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_24[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_25[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 48, 96, 96)   0           concatenate_9[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_27[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 64, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_29[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_30[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 48, 48, 48)   0           average_pooling2d_4[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_32[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 64, 48, 48)   0           concatenate_12[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_34[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 80, 48, 48)   0           concatenate_13[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 80, 48, 48)   320         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 80, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 80)           0           activation_36[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            81          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 76,145
Trainable params: 74,609
Non-trainable params: 1,536
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 49s - loss: 0.6450 - acc: 0.7242 - val_loss: 0.5870 - val_acc: 0.8363

Epoch 00001: val_loss improved from inf to 0.58703, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 46s - loss: 0.5720 - acc: 0.7715 - val_loss: 0.5394 - val_acc: 0.8103

Epoch 00002: val_loss improved from 0.58703 to 0.53938, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 47s - loss: 0.5468 - acc: 0.7829 - val_loss: 0.5469 - val_acc: 0.8042

Epoch 00003: val_loss did not improve from 0.53938
Epoch 4/40
 - 46s - loss: 0.5292 - acc: 0.7892 - val_loss: 0.5449 - val_acc: 0.8034

Epoch 00004: val_loss did not improve from 0.53938
Epoch 5/40
 - 46s - loss: 0.5187 - acc: 0.7936 - val_loss: 0.5280 - val_acc: 0.8019

Epoch 00005: val_loss improved from 0.53938 to 0.52802, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 46s - loss: 0.5064 - acc: 0.7992 - val_loss: 0.5474 - val_acc: 0.7886

Epoch 00006: val_loss did not improve from 0.52802
Epoch 7/40
 - 46s - loss: 0.4976 - acc: 0.8042 - val_loss: 0.5319 - val_acc: 0.7944

Epoch 00007: val_loss did not improve from 0.52802
Epoch 8/40
 - 47s - loss: 0.4910 - acc: 0.8075 - val_loss: 0.5459 - val_acc: 0.7964

Epoch 00008: val_loss did not improve from 0.52802
Epoch 9/40
 - 47s - loss: 0.4827 - acc: 0.8089 - val_loss: 0.5186 - val_acc: 0.8058

Epoch 00009: val_loss improved from 0.52802 to 0.51861, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 46s - loss: 0.4773 - acc: 0.8144 - val_loss: 0.5587 - val_acc: 0.7789

Epoch 00010: val_loss did not improve from 0.51861
Epoch 11/40
 - 46s - loss: 0.4702 - acc: 0.8171 - val_loss: 0.5106 - val_acc: 0.7886

Epoch 00011: val_loss improved from 0.51861 to 0.51057, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 46s - loss: 0.4657 - acc: 0.8207 - val_loss: 0.5103 - val_acc: 0.8288

Epoch 00012: val_loss improved from 0.51057 to 0.51029, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 46s - loss: 0.4623 - acc: 0.8218 - val_loss: 0.5152 - val_acc: 0.8097

Epoch 00013: val_loss did not improve from 0.51029
Epoch 14/40
 - 46s - loss: 0.4549 - acc: 0.8273 - val_loss: 0.5626 - val_acc: 0.7858

Epoch 00014: val_loss did not improve from 0.51029
Epoch 15/40
 - 46s - loss: 0.4496 - acc: 0.8281 - val_loss: 0.5701 - val_acc: 0.7542

Epoch 00015: val_loss did not improve from 0.51029
Epoch 16/40
 - 46s - loss: 0.4462 - acc: 0.8282 - val_loss: 0.5765 - val_acc: 0.7841

Epoch 00016: val_loss did not improve from 0.51029
Epoch 17/40
 - 46s - loss: 0.4421 - acc: 0.8323 - val_loss: 0.5447 - val_acc: 0.8153

Epoch 00017: val_loss did not improve from 0.51029
Epoch 18/40
 - 46s - loss: 0.4360 - acc: 0.8347 - val_loss: 0.5291 - val_acc: 0.8090

Epoch 00018: val_loss did not improve from 0.51029
Epoch 19/40
 - 46s - loss: 0.4335 - acc: 0.8378 - val_loss: 0.5970 - val_acc: 0.8012

Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00019: val_loss did not improve from 0.51029
Epoch 00019: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 2s
 320/7440 [>.............................] - ETA: 2s
 480/7440 [>.............................] - ETA: 2s
 640/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 960/7440 [==>...........................] - ETA: 2s
1120/7440 [===>..........................] - ETA: 2s
1280/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1600/7440 [=====>........................] - ETA: 2s
1760/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2048/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2368/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2656/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 388us/step
current Test accuracy: 0.8012096774193549
current auc_score ------------------>  0.8732084634061743

  32/7440 [..............................] - ETA: 2:43
 160/7440 [..............................] - ETA: 34s 
 288/7440 [>.............................] - ETA: 20s
 416/7440 [>.............................] - ETA: 14s
 544/7440 [=>............................] - ETA: 11s
 672/7440 [=>............................] - ETA: 9s 
 800/7440 [==>...........................] - ETA: 8s
 928/7440 [==>...........................] - ETA: 7s
1056/7440 [===>..........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 6s
1344/7440 [====>.........................] - ETA: 5s
1472/7440 [====>.........................] - ETA: 5s
1600/7440 [=====>........................] - ETA: 4s
1728/7440 [=====>........................] - ETA: 4s
1856/7440 [======>.......................] - ETA: 4s
1984/7440 [=======>......................] - ETA: 4s
2112/7440 [=======>......................] - ETA: 3s
2240/7440 [========>.....................] - ETA: 3s
2368/7440 [========>.....................] - ETA: 3s
2496/7440 [=========>....................] - ETA: 3s
2656/7440 [=========>....................] - ETA: 3s
2816/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3264/7440 [============>.................] - ETA: 2s
3424/7440 [============>.................] - ETA: 2s
3584/7440 [=============>................] - ETA: 2s
3744/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 488us/step
Best saved model Test accuracy: 0.828763440860215
best saved model auc_score ------------------>  0.8938374378540872
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_37[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_39[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_40[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 40, 96, 96)   0           concatenate_15[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_41[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_42[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_43[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_44[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_45[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 44, 48, 48)   0           concatenate_17[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 44)           0           activation_46[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            45          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 31s - loss: 0.6375 - acc: 0.7050 - val_loss: 0.5358 - val_acc: 0.8548

Epoch 00001: val_loss improved from inf to 0.53575, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 28s - loss: 0.5828 - acc: 0.7476 - val_loss: 0.5139 - val_acc: 0.8516

Epoch 00002: val_loss improved from 0.53575 to 0.51390, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 28s - loss: 0.5566 - acc: 0.7587 - val_loss: 0.5225 - val_acc: 0.8353

Epoch 00003: val_loss did not improve from 0.51390
Epoch 4/40
 - 28s - loss: 0.5387 - acc: 0.7660 - val_loss: 0.5205 - val_acc: 0.8360

Epoch 00004: val_loss did not improve from 0.51390
Epoch 5/40
 - 28s - loss: 0.5277 - acc: 0.7707 - val_loss: 0.5227 - val_acc: 0.8227

Epoch 00005: val_loss did not improve from 0.51390
Epoch 6/40
 - 28s - loss: 0.5174 - acc: 0.7727 - val_loss: 0.5300 - val_acc: 0.8148

Epoch 00006: val_loss did not improve from 0.51390
Epoch 7/40
 - 28s - loss: 0.5111 - acc: 0.7758 - val_loss: 0.5145 - val_acc: 0.8185

Epoch 00007: val_loss did not improve from 0.51390
Epoch 8/40
 - 28s - loss: 0.5039 - acc: 0.7770 - val_loss: 0.5138 - val_acc: 0.8160

Epoch 00008: val_loss improved from 0.51390 to 0.51376, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 28s - loss: 0.4984 - acc: 0.7811 - val_loss: 0.5160 - val_acc: 0.8202

Epoch 00009: val_loss did not improve from 0.51376
Epoch 10/40
 - 28s - loss: 0.4947 - acc: 0.7832 - val_loss: 0.5244 - val_acc: 0.8125

Epoch 00010: val_loss did not improve from 0.51376
Epoch 11/40
 - 28s - loss: 0.4889 - acc: 0.7852 - val_loss: 0.5259 - val_acc: 0.7926

Epoch 00011: val_loss did not improve from 0.51376
Epoch 12/40
 - 28s - loss: 0.4859 - acc: 0.7879 - val_loss: 0.5123 - val_acc: 0.8054

Epoch 00012: val_loss improved from 0.51376 to 0.51227, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 28s - loss: 0.4798 - acc: 0.7893 - val_loss: 0.5209 - val_acc: 0.7921

Epoch 00013: val_loss did not improve from 0.51227
Epoch 14/40
 - 28s - loss: 0.4764 - acc: 0.7930 - val_loss: 0.5003 - val_acc: 0.8136

Epoch 00014: val_loss improved from 0.51227 to 0.50027, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 29s - loss: 0.4736 - acc: 0.7938 - val_loss: 0.5273 - val_acc: 0.7812

Epoch 00015: val_loss did not improve from 0.50027
Epoch 16/40
 - 28s - loss: 0.4712 - acc: 0.7933 - val_loss: 0.4977 - val_acc: 0.8145

Epoch 00016: val_loss improved from 0.50027 to 0.49771, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 28s - loss: 0.4668 - acc: 0.7951 - val_loss: 0.5114 - val_acc: 0.7965

Epoch 00017: val_loss did not improve from 0.49771
Epoch 18/40
 - 28s - loss: 0.4647 - acc: 0.7973 - val_loss: 0.4800 - val_acc: 0.8097

Epoch 00018: val_loss improved from 0.49771 to 0.48004, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 28s - loss: 0.4605 - acc: 0.7998 - val_loss: 0.4932 - val_acc: 0.8160

Epoch 00019: val_loss did not improve from 0.48004
Epoch 20/40
 - 28s - loss: 0.4586 - acc: 0.7999 - val_loss: 0.5087 - val_acc: 0.7917

Epoch 00020: val_loss did not improve from 0.48004
Epoch 21/40
 - 28s - loss: 0.4555 - acc: 0.8037 - val_loss: 0.5108 - val_acc: 0.8016

Epoch 00021: val_loss did not improve from 0.48004
Epoch 22/40
 - 28s - loss: 0.4532 - acc: 0.8049 - val_loss: 0.5210 - val_acc: 0.7766

Epoch 00022: val_loss did not improve from 0.48004
Epoch 23/40
 - 28s - loss: 0.4489 - acc: 0.8070 - val_loss: 0.4881 - val_acc: 0.8226

Epoch 00023: val_loss did not improve from 0.48004
Epoch 24/40
 - 28s - loss: 0.4502 - acc: 0.8071 - val_loss: 0.4881 - val_acc: 0.7954

Epoch 00024: val_loss did not improve from 0.48004
Epoch 25/40
 - 28s - loss: 0.4460 - acc: 0.8100 - val_loss: 0.4796 - val_acc: 0.8062

Epoch 00025: val_loss improved from 0.48004 to 0.47960, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 26/40
 - 28s - loss: 0.4428 - acc: 0.8113 - val_loss: 0.4998 - val_acc: 0.8011

Epoch 00026: val_loss did not improve from 0.47960
Epoch 27/40
 - 28s - loss: 0.4391 - acc: 0.8111 - val_loss: 0.4921 - val_acc: 0.7984

Epoch 00027: val_loss did not improve from 0.47960
Epoch 28/40
 - 28s - loss: 0.4397 - acc: 0.8130 - val_loss: 0.4646 - val_acc: 0.8253

Epoch 00028: val_loss improved from 0.47960 to 0.46461, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 29/40
 - 29s - loss: 0.4366 - acc: 0.8150 - val_loss: 0.4593 - val_acc: 0.8266

Epoch 00029: val_loss improved from 0.46461 to 0.45932, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 28s - loss: 0.4345 - acc: 0.8134 - val_loss: 0.4775 - val_acc: 0.8149

Epoch 00030: val_loss did not improve from 0.45932
Epoch 31/40
 - 28s - loss: 0.4319 - acc: 0.8180 - val_loss: 0.4549 - val_acc: 0.8329

Epoch 00031: val_loss improved from 0.45932 to 0.45486, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 32/40
 - 29s - loss: 0.4308 - acc: 0.8178 - val_loss: 0.4815 - val_acc: 0.8075

Epoch 00032: val_loss did not improve from 0.45486
Epoch 33/40
 - 29s - loss: 0.4288 - acc: 0.8175 - val_loss: 0.4551 - val_acc: 0.8278

Epoch 00033: val_loss did not improve from 0.45486
Epoch 34/40
 - 28s - loss: 0.4252 - acc: 0.8201 - val_loss: 0.4819 - val_acc: 0.7960

Epoch 00034: val_loss did not improve from 0.45486
Epoch 35/40
 - 28s - loss: 0.4241 - acc: 0.8213 - val_loss: 0.4732 - val_acc: 0.8116

Epoch 00035: val_loss did not improve from 0.45486
Epoch 36/40
 - 28s - loss: 0.4225 - acc: 0.8216 - val_loss: 0.4671 - val_acc: 0.8285

Epoch 00036: val_loss did not improve from 0.45486
Epoch 37/40
 - 28s - loss: 0.4204 - acc: 0.8249 - val_loss: 0.5410 - val_acc: 0.7911

Epoch 00037: val_loss did not improve from 0.45486
Epoch 38/40
 - 28s - loss: 0.4185 - acc: 0.8255 - val_loss: 0.4616 - val_acc: 0.8399

Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00038: val_loss did not improve from 0.45486
Epoch 00038: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 252us/step
current Test accuracy: 0.8399193548387097
current auc_score ------------------>  0.8957695253786565

  32/7440 [..............................] - ETA: 3:10
 224/7440 [..............................] - ETA: 28s 
 416/7440 [>.............................] - ETA: 15s
 640/7440 [=>............................] - ETA: 10s
 864/7440 [==>...........................] - ETA: 7s 
1088/7440 [===>..........................] - ETA: 6s
1312/7440 [====>.........................] - ETA: 5s
1536/7440 [=====>........................] - ETA: 4s
1760/7440 [======>.......................] - ETA: 4s
1984/7440 [=======>......................] - ETA: 3s
2208/7440 [=======>......................] - ETA: 3s
2432/7440 [========>.....................] - ETA: 2s
2656/7440 [=========>....................] - ETA: 2s
2880/7440 [==========>...................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3552/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5568/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 358us/step
Best saved model Test accuracy: 0.8329301075268817
best saved model auc_score ------------------>  0.894537663313678
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_47[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_48[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 26, 96, 96)   104         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 26, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 13, 96, 96)   338         activation_49[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 13, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 13, 48, 48)   52          average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 13, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   520         activation_50[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_51[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_6[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 23, 48, 48)   92          concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 23, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 23)           0           activation_52[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            24          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 9,642
Trainable params: 9,326
Non-trainable params: 316
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 19s - loss: 0.6445 - acc: 0.6752 - val_loss: 0.5625 - val_acc: 0.8056

Epoch 00001: val_loss improved from inf to 0.56251, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 16s - loss: 0.5990 - acc: 0.7273 - val_loss: 0.5437 - val_acc: 0.8300

Epoch 00002: val_loss improved from 0.56251 to 0.54374, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 16s - loss: 0.5843 - acc: 0.7369 - val_loss: 0.5268 - val_acc: 0.8477

Epoch 00003: val_loss improved from 0.54374 to 0.52677, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 16s - loss: 0.5734 - acc: 0.7417 - val_loss: 0.5174 - val_acc: 0.8657

Epoch 00004: val_loss improved from 0.52677 to 0.51738, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 16s - loss: 0.5631 - acc: 0.7450 - val_loss: 0.5066 - val_acc: 0.8624

Epoch 00005: val_loss improved from 0.51738 to 0.50656, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 16s - loss: 0.5571 - acc: 0.7463 - val_loss: 0.5075 - val_acc: 0.8578

Epoch 00006: val_loss did not improve from 0.50656
Epoch 7/40
 - 16s - loss: 0.5494 - acc: 0.7511 - val_loss: 0.5071 - val_acc: 0.8543

Epoch 00007: val_loss did not improve from 0.50656
Epoch 8/40
 - 16s - loss: 0.5443 - acc: 0.7528 - val_loss: 0.4985 - val_acc: 0.8501

Epoch 00008: val_loss improved from 0.50656 to 0.49849, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 16s - loss: 0.5401 - acc: 0.7543 - val_loss: 0.4933 - val_acc: 0.8434

Epoch 00009: val_loss improved from 0.49849 to 0.49332, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 16s - loss: 0.5354 - acc: 0.7558 - val_loss: 0.4968 - val_acc: 0.8383

Epoch 00010: val_loss did not improve from 0.49332
Epoch 11/40
 - 16s - loss: 0.5309 - acc: 0.7568 - val_loss: 0.5026 - val_acc: 0.8321

Epoch 00011: val_loss did not improve from 0.49332
Epoch 12/40
 - 16s - loss: 0.5268 - acc: 0.7590 - val_loss: 0.4931 - val_acc: 0.8390

Epoch 00012: val_loss improved from 0.49332 to 0.49314, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 16s - loss: 0.5224 - acc: 0.7617 - val_loss: 0.4900 - val_acc: 0.8398

Epoch 00013: val_loss improved from 0.49314 to 0.48995, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 16s - loss: 0.5190 - acc: 0.7607 - val_loss: 0.4868 - val_acc: 0.8301

Epoch 00014: val_loss improved from 0.48995 to 0.48681, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 16s - loss: 0.5153 - acc: 0.7643 - val_loss: 0.4849 - val_acc: 0.8324

Epoch 00015: val_loss improved from 0.48681 to 0.48485, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 16s - loss: 0.5130 - acc: 0.7634 - val_loss: 0.4836 - val_acc: 0.8333

Epoch 00016: val_loss improved from 0.48485 to 0.48355, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 16s - loss: 0.5099 - acc: 0.7660 - val_loss: 0.4823 - val_acc: 0.8255

Epoch 00017: val_loss improved from 0.48355 to 0.48233, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 16s - loss: 0.5071 - acc: 0.7682 - val_loss: 0.4769 - val_acc: 0.8324

Epoch 00018: val_loss improved from 0.48233 to 0.47691, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 16s - loss: 0.5044 - acc: 0.7708 - val_loss: 0.4751 - val_acc: 0.8282

Epoch 00019: val_loss improved from 0.47691 to 0.47508, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 16s - loss: 0.5008 - acc: 0.7734 - val_loss: 0.4737 - val_acc: 0.8267

Epoch 00020: val_loss improved from 0.47508 to 0.47366, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 16s - loss: 0.4979 - acc: 0.7733 - val_loss: 0.4869 - val_acc: 0.8109

Epoch 00021: val_loss did not improve from 0.47366
Epoch 22/40
 - 16s - loss: 0.4963 - acc: 0.7735 - val_loss: 0.4900 - val_acc: 0.8007

Epoch 00022: val_loss did not improve from 0.47366
Epoch 23/40
 - 16s - loss: 0.4936 - acc: 0.7754 - val_loss: 0.4865 - val_acc: 0.8097

Epoch 00023: val_loss did not improve from 0.47366
Epoch 24/40
 - 16s - loss: 0.4906 - acc: 0.7767 - val_loss: 0.4953 - val_acc: 0.7898

Epoch 00024: val_loss did not improve from 0.47366
Epoch 25/40
 - 16s - loss: 0.4888 - acc: 0.7767 - val_loss: 0.4825 - val_acc: 0.8046

Epoch 00025: val_loss did not improve from 0.47366
Epoch 26/40
 - 16s - loss: 0.4860 - acc: 0.7793 - val_loss: 0.4764 - val_acc: 0.8173

Epoch 00026: val_loss did not improve from 0.47366
Epoch 27/40
 - 16s - loss: 0.4859 - acc: 0.7781 - val_loss: 0.4783 - val_acc: 0.8188

Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00027: val_loss did not improve from 0.47366
Epoch 00027: early stopping

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 162us/step
current Test accuracy: 0.8188172043010753
current auc_score ------------------>  0.8569482021042896

  32/7440 [..............................] - ETA: 3:27
 320/7440 [>.............................] - ETA: 21s 
 640/7440 [=>............................] - ETA: 10s
 960/7440 [==>...........................] - ETA: 7s 
1280/7440 [====>.........................] - ETA: 5s
1600/7440 [=====>........................] - ETA: 4s
1920/7440 [======>.......................] - ETA: 3s
2240/7440 [========>.....................] - ETA: 2s
2560/7440 [=========>....................] - ETA: 2s
2880/7440 [==========>...................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
4160/7440 [===============>..............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 284us/step
Best saved model Test accuracy: 0.826747311827957
best saved model auc_score ------------------>  0.8655757529772228
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_53[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_54[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_55[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_56[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 40, 96, 96)   0           concatenate_21[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_57[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_58[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_59[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_60[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_61[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 44, 48, 48)   0           concatenate_23[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 44)           0           activation_62[0][0]              
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            45          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 34s - loss: 0.6435 - acc: 0.7123 - val_loss: 0.5516 - val_acc: 0.8509

Epoch 00001: val_loss improved from inf to 0.55156, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 29s - loss: 0.5898 - acc: 0.7502 - val_loss: 0.5247 - val_acc: 0.8430

Epoch 00002: val_loss improved from 0.55156 to 0.52471, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 28s - loss: 0.5661 - acc: 0.7585 - val_loss: 0.5220 - val_acc: 0.8258

Epoch 00003: val_loss improved from 0.52471 to 0.52202, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 29s - loss: 0.5497 - acc: 0.7604 - val_loss: 0.5224 - val_acc: 0.8172

Epoch 00004: val_loss did not improve from 0.52202
Epoch 5/40
 - 29s - loss: 0.5362 - acc: 0.7662 - val_loss: 0.5158 - val_acc: 0.8090

Epoch 00005: val_loss improved from 0.52202 to 0.51582, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 28s - loss: 0.5250 - acc: 0.7697 - val_loss: 0.5173 - val_acc: 0.8004

Epoch 00006: val_loss did not improve from 0.51582
Epoch 7/40
 - 29s - loss: 0.5167 - acc: 0.7747 - val_loss: 0.5056 - val_acc: 0.8250

Epoch 00007: val_loss improved from 0.51582 to 0.50562, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 29s - loss: 0.5098 - acc: 0.7770 - val_loss: 0.5125 - val_acc: 0.7989

Epoch 00008: val_loss did not improve from 0.50562
Epoch 9/40
 - 28s - loss: 0.5030 - acc: 0.7793 - val_loss: 0.5118 - val_acc: 0.7966

Epoch 00009: val_loss did not improve from 0.50562
Epoch 10/40
 - 29s - loss: 0.4973 - acc: 0.7842 - val_loss: 0.5222 - val_acc: 0.8105

Epoch 00010: val_loss did not improve from 0.50562
Epoch 11/40
 - 29s - loss: 0.4914 - acc: 0.7862 - val_loss: 0.5029 - val_acc: 0.8063

Epoch 00011: val_loss improved from 0.50562 to 0.50291, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 29s - loss: 0.4858 - acc: 0.7899 - val_loss: 0.5389 - val_acc: 0.7917

Epoch 00012: val_loss did not improve from 0.50291
Epoch 13/40
 - 29s - loss: 0.4804 - acc: 0.7934 - val_loss: 0.5083 - val_acc: 0.8012

Epoch 00013: val_loss did not improve from 0.50291
Epoch 14/40
 - 29s - loss: 0.4758 - acc: 0.7953 - val_loss: 0.5067 - val_acc: 0.8125

Epoch 00014: val_loss did not improve from 0.50291
Epoch 15/40
 - 28s - loss: 0.4743 - acc: 0.7958 - val_loss: 0.5110 - val_acc: 0.7844

Epoch 00015: val_loss did not improve from 0.50291
Epoch 16/40
 - 29s - loss: 0.4689 - acc: 0.7974 - val_loss: 0.5066 - val_acc: 0.7953

Epoch 00016: val_loss did not improve from 0.50291
Epoch 17/40
 - 29s - loss: 0.4656 - acc: 0.7992 - val_loss: 0.5127 - val_acc: 0.7989

Epoch 00017: val_loss did not improve from 0.50291
Epoch 18/40
 - 29s - loss: 0.4622 - acc: 0.8022 - val_loss: 0.5134 - val_acc: 0.7821

Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00018: val_loss did not improve from 0.50291
Epoch 00018: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1216/7440 [===>..........................] - ETA: 1s
1440/7440 [====>.........................] - ETA: 1s
1664/7440 [=====>........................] - ETA: 1s
1888/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
3008/7440 [===========>..................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3616/7440 [=============>................] - ETA: 0s
3808/7440 [==============>...............] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 262us/step
current Test accuracy: 0.7821236559139785
current auc_score ------------------>  0.8536399800554978

  32/7440 [..............................] - ETA: 4:15
 224/7440 [..............................] - ETA: 37s 
 416/7440 [>.............................] - ETA: 20s
 608/7440 [=>............................] - ETA: 14s
 800/7440 [==>...........................] - ETA: 10s
 992/7440 [===>..........................] - ETA: 8s 
1184/7440 [===>..........................] - ETA: 7s
1376/7440 [====>.........................] - ETA: 6s
1568/7440 [=====>........................] - ETA: 5s
1760/7440 [======>.......................] - ETA: 5s
1952/7440 [======>.......................] - ETA: 4s
2144/7440 [=======>......................] - ETA: 4s
2336/7440 [========>.....................] - ETA: 3s
2528/7440 [=========>....................] - ETA: 3s
2720/7440 [=========>....................] - ETA: 3s
2912/7440 [==========>...................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3296/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3680/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 414us/step
Best saved model Test accuracy: 0.8063172043010752
best saved model auc_score ------------------>  0.8717293906810035
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_63[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_64[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_65[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_66[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 36, 96, 96)   0           concatenate_25[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_67[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_68[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 46, 96, 96)   0           concatenate_26[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_69[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_70[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_71[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 33, 48, 48)   0           average_pooling2d_8[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_72[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_73[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 43, 48, 48)   0           concatenate_28[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_74[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_75[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 53, 48, 48)   0           concatenate_29[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 53, 48, 48)   212         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 53, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 53)           0           activation_76[0][0]              
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            54          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 32,144
Trainable params: 31,112
Non-trainable params: 1,032
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 43s - loss: 0.6821 - acc: 0.6586 - val_loss: 0.5750 - val_acc: 0.8227

Epoch 00001: val_loss improved from inf to 0.57497, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 38s - loss: 0.5950 - acc: 0.7494 - val_loss: 0.5455 - val_acc: 0.8075

Epoch 00002: val_loss improved from 0.57497 to 0.54547, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 38s - loss: 0.5639 - acc: 0.7624 - val_loss: 0.5473 - val_acc: 0.8036

Epoch 00003: val_loss did not improve from 0.54547
Epoch 4/40
 - 38s - loss: 0.5482 - acc: 0.7682 - val_loss: 0.5353 - val_acc: 0.7973

Epoch 00004: val_loss improved from 0.54547 to 0.53528, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 38s - loss: 0.5344 - acc: 0.7751 - val_loss: 0.5300 - val_acc: 0.7925

Epoch 00005: val_loss improved from 0.53528 to 0.53004, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 38s - loss: 0.5232 - acc: 0.7795 - val_loss: 0.5285 - val_acc: 0.8051

Epoch 00006: val_loss improved from 0.53004 to 0.52846, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 38s - loss: 0.5144 - acc: 0.7842 - val_loss: 0.5251 - val_acc: 0.8011

Epoch 00007: val_loss improved from 0.52846 to 0.52507, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 38s - loss: 0.5059 - acc: 0.7889 - val_loss: 0.5083 - val_acc: 0.8171

Epoch 00008: val_loss improved from 0.52507 to 0.50831, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 38s - loss: 0.4996 - acc: 0.7878 - val_loss: 0.5044 - val_acc: 0.8234

Epoch 00009: val_loss improved from 0.50831 to 0.50438, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 38s - loss: 0.4927 - acc: 0.7910 - val_loss: 0.5216 - val_acc: 0.8054

Epoch 00010: val_loss did not improve from 0.50438
Epoch 11/40
 - 39s - loss: 0.4857 - acc: 0.7960 - val_loss: 0.5055 - val_acc: 0.8207

Epoch 00011: val_loss did not improve from 0.50438
Epoch 12/40
 - 38s - loss: 0.4827 - acc: 0.7962 - val_loss: 0.5129 - val_acc: 0.8133

Epoch 00012: val_loss did not improve from 0.50438
Epoch 13/40
 - 38s - loss: 0.4751 - acc: 0.7996 - val_loss: 0.5125 - val_acc: 0.8065

Epoch 00013: val_loss did not improve from 0.50438
Epoch 14/40
 - 38s - loss: 0.4704 - acc: 0.8041 - val_loss: 0.4992 - val_acc: 0.8103

Epoch 00014: val_loss improved from 0.50438 to 0.49922, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 38s - loss: 0.4648 - acc: 0.8048 - val_loss: 0.5549 - val_acc: 0.8106

Epoch 00015: val_loss did not improve from 0.49922
Epoch 16/40
 - 38s - loss: 0.4630 - acc: 0.8052 - val_loss: 0.5274 - val_acc: 0.8298

Epoch 00016: val_loss did not improve from 0.49922
Epoch 17/40
 - 38s - loss: 0.4587 - acc: 0.8084 - val_loss: 0.5481 - val_acc: 0.7906

Epoch 00017: val_loss did not improve from 0.49922
Epoch 18/40
 - 38s - loss: 0.4550 - acc: 0.8101 - val_loss: 0.5208 - val_acc: 0.8176

Epoch 00018: val_loss did not improve from 0.49922
Epoch 19/40
 - 38s - loss: 0.4521 - acc: 0.8108 - val_loss: 0.5101 - val_acc: 0.8292

Epoch 00019: val_loss did not improve from 0.49922
Epoch 20/40
 - 38s - loss: 0.4500 - acc: 0.8116 - val_loss: 0.4949 - val_acc: 0.8122

Epoch 00020: val_loss improved from 0.49922 to 0.49490, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 38s - loss: 0.4467 - acc: 0.8137 - val_loss: 0.5515 - val_acc: 0.8134

Epoch 00021: val_loss did not improve from 0.49490
Epoch 22/40
 - 38s - loss: 0.4430 - acc: 0.8155 - val_loss: 0.5334 - val_acc: 0.8023

Epoch 00022: val_loss did not improve from 0.49490
Epoch 23/40
 - 38s - loss: 0.4403 - acc: 0.8167 - val_loss: 0.5256 - val_acc: 0.7897

Epoch 00023: val_loss did not improve from 0.49490
Epoch 24/40
 - 38s - loss: 0.4382 - acc: 0.8192 - val_loss: 0.5610 - val_acc: 0.8243

Epoch 00024: val_loss did not improve from 0.49490
Epoch 25/40
 - 38s - loss: 0.4342 - acc: 0.8199 - val_loss: 0.5358 - val_acc: 0.8073

Epoch 00025: val_loss did not improve from 0.49490
Epoch 26/40
 - 37s - loss: 0.4334 - acc: 0.8197 - val_loss: 0.5259 - val_acc: 0.8109

Epoch 00026: val_loss did not improve from 0.49490
Epoch 27/40
 - 37s - loss: 0.4295 - acc: 0.8224 - val_loss: 0.5190 - val_acc: 0.8173

Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00027: val_loss did not improve from 0.49490
Epoch 00027: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 337us/step
current Test accuracy: 0.8173387096774194
current auc_score ------------------>  0.8773836209388368

  32/7440 [..............................] - ETA: 6:20
 192/7440 [..............................] - ETA: 1:04
 352/7440 [>.............................] - ETA: 35s 
 512/7440 [=>............................] - ETA: 24s
 672/7440 [=>............................] - ETA: 18s
 832/7440 [==>...........................] - ETA: 15s
 992/7440 [===>..........................] - ETA: 12s
1152/7440 [===>..........................] - ETA: 11s
1312/7440 [====>.........................] - ETA: 9s 
1472/7440 [====>.........................] - ETA: 8s
1632/7440 [=====>........................] - ETA: 7s
1792/7440 [======>.......................] - ETA: 7s
1952/7440 [======>.......................] - ETA: 6s
2112/7440 [=======>......................] - ETA: 5s
2272/7440 [========>.....................] - ETA: 5s
2432/7440 [========>.....................] - ETA: 5s
2592/7440 [=========>....................] - ETA: 4s
2752/7440 [==========>...................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3552/7440 [=============>................] - ETA: 3s
3712/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 2s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 559us/step
Best saved model Test accuracy: 0.812231182795699
best saved model auc_score ------------------>  0.8819493583073188
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_77[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_78[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 30, 96, 96)   120         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 30, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 15, 96, 96)   450         activation_79[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 15, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 15, 48, 48)   60          average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 15, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   840         activation_80[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_81[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 29, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 29, 48, 48)   116         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 29, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 29)           0           activation_82[0][0]              
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            30          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 17,424
Trainable params: 17,020
Non-trainable params: 404
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 24s - loss: 0.6421 - acc: 0.6929 - val_loss: 0.5447 - val_acc: 0.8411

Epoch 00001: val_loss improved from inf to 0.54468, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 19s - loss: 0.5910 - acc: 0.7431 - val_loss: 0.5237 - val_acc: 0.8485

Epoch 00002: val_loss improved from 0.54468 to 0.52375, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 19s - loss: 0.5714 - acc: 0.7531 - val_loss: 0.5009 - val_acc: 0.8454

Epoch 00003: val_loss improved from 0.52375 to 0.50090, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 19s - loss: 0.5578 - acc: 0.7601 - val_loss: 0.5042 - val_acc: 0.8332

Epoch 00004: val_loss did not improve from 0.50090
Epoch 5/40
 - 19s - loss: 0.5480 - acc: 0.7602 - val_loss: 0.4899 - val_acc: 0.8348

Epoch 00005: val_loss improved from 0.50090 to 0.48994, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 19s - loss: 0.5411 - acc: 0.7627 - val_loss: 0.4856 - val_acc: 0.8320

Epoch 00006: val_loss improved from 0.48994 to 0.48563, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 19s - loss: 0.5341 - acc: 0.7640 - val_loss: 0.4886 - val_acc: 0.8251

Epoch 00007: val_loss did not improve from 0.48563
Epoch 8/40
 - 19s - loss: 0.5276 - acc: 0.7681 - val_loss: 0.4913 - val_acc: 0.8172

Epoch 00008: val_loss did not improve from 0.48563
Epoch 9/40
 - 19s - loss: 0.5238 - acc: 0.7670 - val_loss: 0.4920 - val_acc: 0.8070

Epoch 00009: val_loss did not improve from 0.48563
Epoch 10/40
 - 19s - loss: 0.5193 - acc: 0.7699 - val_loss: 0.4920 - val_acc: 0.8147

Epoch 00010: val_loss did not improve from 0.48563
Epoch 11/40
 - 19s - loss: 0.5168 - acc: 0.7680 - val_loss: 0.4988 - val_acc: 0.8122

Epoch 00011: val_loss did not improve from 0.48563
Epoch 12/40
 - 19s - loss: 0.5115 - acc: 0.7718 - val_loss: 0.4818 - val_acc: 0.8099

Epoch 00012: val_loss improved from 0.48563 to 0.48183, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 19s - loss: 0.5088 - acc: 0.7738 - val_loss: 0.4856 - val_acc: 0.8069

Epoch 00013: val_loss did not improve from 0.48183
Epoch 14/40
 - 19s - loss: 0.5043 - acc: 0.7756 - val_loss: 0.4774 - val_acc: 0.8078

Epoch 00014: val_loss improved from 0.48183 to 0.47741, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 19s - loss: 0.5000 - acc: 0.7746 - val_loss: 0.4791 - val_acc: 0.8055

Epoch 00015: val_loss did not improve from 0.47741
Epoch 16/40
 - 19s - loss: 0.4979 - acc: 0.7772 - val_loss: 0.4885 - val_acc: 0.8015

Epoch 00016: val_loss did not improve from 0.47741
Epoch 17/40
 - 19s - loss: 0.4961 - acc: 0.7773 - val_loss: 0.4790 - val_acc: 0.8175

Epoch 00017: val_loss did not improve from 0.47741
Epoch 18/40
 - 19s - loss: 0.4929 - acc: 0.7808 - val_loss: 0.4790 - val_acc: 0.8047

Epoch 00018: val_loss did not improve from 0.47741
Epoch 19/40
 - 19s - loss: 0.4895 - acc: 0.7809 - val_loss: 0.4841 - val_acc: 0.8044

Epoch 00019: val_loss did not improve from 0.47741
Epoch 20/40
 - 19s - loss: 0.4876 - acc: 0.7812 - val_loss: 0.4839 - val_acc: 0.7976

Epoch 00020: val_loss did not improve from 0.47741
Epoch 21/40
 - 19s - loss: 0.4850 - acc: 0.7814 - val_loss: 0.4884 - val_acc: 0.7954

Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00021: val_loss did not improve from 0.47741
Epoch 00021: early stopping

  32/7440 [..............................] - ETA: 1s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1056/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2080/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 0s
2848/7440 [==========>...................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 202us/step
current Test accuracy: 0.7954301075268817
current auc_score ------------------>  0.8735166637761591

  32/7440 [..............................] - ETA: 6:52
 288/7440 [>.............................] - ETA: 45s 
 544/7440 [=>............................] - ETA: 23s
 800/7440 [==>...........................] - ETA: 16s
1056/7440 [===>..........................] - ETA: 12s
1312/7440 [====>.........................] - ETA: 9s 
1568/7440 [=====>........................] - ETA: 7s
1824/7440 [======>.......................] - ETA: 6s
2080/7440 [=======>......................] - ETA: 5s
2336/7440 [========>.....................] - ETA: 4s
2592/7440 [=========>....................] - ETA: 4s
2848/7440 [==========>...................] - ETA: 3s
3104/7440 [===========>..................] - ETA: 3s
3360/7440 [============>.................] - ETA: 3s
3616/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4128/7440 [===============>..............] - ETA: 2s
4384/7440 [================>.............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5696/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 445us/step
Best saved model Test accuracy: 0.8077956989247311
best saved model auc_score ------------------>  0.8797923892935599
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_10 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_83 (Activation)   (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_10 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_84 (Activation)   (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_10  (None, 8)                 0         
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 13s - loss: 0.7209 - acc: 0.5141 - val_loss: 0.6355 - val_acc: 0.5917

Epoch 00001: val_loss improved from inf to 0.63546, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 9s - loss: 0.6740 - acc: 0.5653 - val_loss: 0.6054 - val_acc: 0.6294

Epoch 00002: val_loss improved from 0.63546 to 0.60539, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 9s - loss: 0.6517 - acc: 0.6225 - val_loss: 0.5855 - val_acc: 0.7109

Epoch 00003: val_loss improved from 0.60539 to 0.58551, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 9s - loss: 0.6394 - acc: 0.6668 - val_loss: 0.5764 - val_acc: 0.7763

Epoch 00004: val_loss improved from 0.58551 to 0.57640, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 9s - loss: 0.6313 - acc: 0.6868 - val_loss: 0.5687 - val_acc: 0.8179

Epoch 00005: val_loss improved from 0.57640 to 0.56869, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 9s - loss: 0.6263 - acc: 0.6924 - val_loss: 0.5641 - val_acc: 0.8159

Epoch 00006: val_loss improved from 0.56869 to 0.56409, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 9s - loss: 0.6228 - acc: 0.6951 - val_loss: 0.5588 - val_acc: 0.8156

Epoch 00007: val_loss improved from 0.56409 to 0.55879, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 9s - loss: 0.6192 - acc: 0.6968 - val_loss: 0.5546 - val_acc: 0.8148

Epoch 00008: val_loss improved from 0.55879 to 0.55464, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 9s - loss: 0.6167 - acc: 0.6986 - val_loss: 0.5499 - val_acc: 0.8129

Epoch 00009: val_loss improved from 0.55464 to 0.54995, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 9s - loss: 0.6146 - acc: 0.6997 - val_loss: 0.5492 - val_acc: 0.8144

Epoch 00010: val_loss improved from 0.54995 to 0.54918, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 9s - loss: 0.6116 - acc: 0.6997 - val_loss: 0.5458 - val_acc: 0.8171

Epoch 00011: val_loss improved from 0.54918 to 0.54584, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 9s - loss: 0.6106 - acc: 0.6990 - val_loss: 0.5429 - val_acc: 0.8167

Epoch 00012: val_loss improved from 0.54584 to 0.54286, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 9s - loss: 0.6088 - acc: 0.6997 - val_loss: 0.5420 - val_acc: 0.8147

Epoch 00013: val_loss improved from 0.54286 to 0.54197, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 9s - loss: 0.6077 - acc: 0.7016 - val_loss: 0.5391 - val_acc: 0.8167

Epoch 00014: val_loss improved from 0.54197 to 0.53911, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 9s - loss: 0.6058 - acc: 0.7042 - val_loss: 0.5417 - val_acc: 0.8079

Epoch 00015: val_loss did not improve from 0.53911
Epoch 16/40
 - 9s - loss: 0.6049 - acc: 0.7059 - val_loss: 0.5381 - val_acc: 0.8156

Epoch 00016: val_loss improved from 0.53911 to 0.53812, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 9s - loss: 0.6026 - acc: 0.7065 - val_loss: 0.5354 - val_acc: 0.8172

Epoch 00017: val_loss improved from 0.53812 to 0.53536, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 9s - loss: 0.6012 - acc: 0.7053 - val_loss: 0.5356 - val_acc: 0.8145

Epoch 00018: val_loss did not improve from 0.53536
Epoch 19/40
 - 9s - loss: 0.6008 - acc: 0.7052 - val_loss: 0.5346 - val_acc: 0.8159

Epoch 00019: val_loss improved from 0.53536 to 0.53457, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 9s - loss: 0.5985 - acc: 0.7076 - val_loss: 0.5347 - val_acc: 0.8156

Epoch 00020: val_loss did not improve from 0.53457
Epoch 21/40
 - 9s - loss: 0.5978 - acc: 0.7087 - val_loss: 0.5326 - val_acc: 0.8105

Epoch 00021: val_loss improved from 0.53457 to 0.53265, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 9s - loss: 0.5962 - acc: 0.7096 - val_loss: 0.5334 - val_acc: 0.8124

Epoch 00022: val_loss did not improve from 0.53265
Epoch 23/40
 - 9s - loss: 0.5955 - acc: 0.7102 - val_loss: 0.5297 - val_acc: 0.8172

Epoch 00023: val_loss improved from 0.53265 to 0.52968, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 9s - loss: 0.5947 - acc: 0.7111 - val_loss: 0.5294 - val_acc: 0.8163

Epoch 00024: val_loss improved from 0.52968 to 0.52940, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 9s - loss: 0.5931 - acc: 0.7105 - val_loss: 0.5301 - val_acc: 0.8141

Epoch 00025: val_loss did not improve from 0.52940
Epoch 26/40
 - 9s - loss: 0.5927 - acc: 0.7117 - val_loss: 0.5339 - val_acc: 0.8066

Epoch 00026: val_loss did not improve from 0.52940
Epoch 27/40
 - 9s - loss: 0.5918 - acc: 0.7134 - val_loss: 0.5292 - val_acc: 0.8114

Epoch 00027: val_loss improved from 0.52940 to 0.52917, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 28/40
 - 9s - loss: 0.5900 - acc: 0.7135 - val_loss: 0.5306 - val_acc: 0.8077

Epoch 00028: val_loss did not improve from 0.52917
Epoch 29/40
 - 9s - loss: 0.5881 - acc: 0.7155 - val_loss: 0.5277 - val_acc: 0.8105

Epoch 00029: val_loss improved from 0.52917 to 0.52767, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 9s - loss: 0.5876 - acc: 0.7155 - val_loss: 0.5284 - val_acc: 0.8063

Epoch 00030: val_loss did not improve from 0.52767
Epoch 31/40
 - 9s - loss: 0.5869 - acc: 0.7154 - val_loss: 0.5293 - val_acc: 0.8108

Epoch 00031: val_loss did not improve from 0.52767
Epoch 32/40
 - 9s - loss: 0.5853 - acc: 0.7170 - val_loss: 0.5233 - val_acc: 0.8118

Epoch 00032: val_loss improved from 0.52767 to 0.52334, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 33/40
 - 9s - loss: 0.5853 - acc: 0.7155 - val_loss: 0.5264 - val_acc: 0.8121

Epoch 00033: val_loss did not improve from 0.52334
Epoch 34/40
 - 9s - loss: 0.5844 - acc: 0.7162 - val_loss: 0.5278 - val_acc: 0.8097

Epoch 00034: val_loss did not improve from 0.52334
Epoch 35/40
 - 9s - loss: 0.5840 - acc: 0.7163 - val_loss: 0.5216 - val_acc: 0.8046

Epoch 00035: val_loss improved from 0.52334 to 0.52156, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 36/40
 - 9s - loss: 0.5820 - acc: 0.7199 - val_loss: 0.5237 - val_acc: 0.8106

Epoch 00036: val_loss did not improve from 0.52156
Epoch 37/40
 - 9s - loss: 0.5818 - acc: 0.7168 - val_loss: 0.5312 - val_acc: 0.8070

Epoch 00037: val_loss did not improve from 0.52156
Epoch 38/40
 - 9s - loss: 0.5799 - acc: 0.7205 - val_loss: 0.5204 - val_acc: 0.8157

Epoch 00038: val_loss improved from 0.52156 to 0.52040, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 39/40
 - 9s - loss: 0.5785 - acc: 0.7201 - val_loss: 0.5205 - val_acc: 0.8102

Epoch 00039: val_loss did not improve from 0.52040
Epoch 40/40
 - 9s - loss: 0.5780 - acc: 0.7209 - val_loss: 0.5190 - val_acc: 0.8151

Epoch 00040: val_loss improved from 0.52040 to 0.51898, saving model to keras_densenet_simple_wt_29Sept_1404.h5

  32/7440 [..............................] - ETA: 0s
 512/7440 [=>............................] - ETA: 0s
 992/7440 [===>..........................] - ETA: 0s
1472/7440 [====>.........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2432/7440 [========>.....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3808/7440 [==============>...............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 112us/step
current Test accuracy: 0.8150537634408602
current auc_score ------------------>  0.8705430179789571

  32/7440 [..............................] - ETA: 5:53
 480/7440 [>.............................] - ETA: 22s 
 928/7440 [==>...........................] - ETA: 11s
1376/7440 [====>.........................] - ETA: 7s 
1856/7440 [======>.......................] - ETA: 5s
2336/7440 [========>.....................] - ETA: 3s
2816/7440 [==========>...................] - ETA: 3s
3328/7440 [============>.................] - ETA: 2s
3808/7440 [==============>...............] - ETA: 1s
4288/7440 [================>.............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 313us/step
Best saved model Test accuracy: 0.8150537634408602
best saved model auc_score ------------------>  0.8705430179789571
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_11[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_85[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_86[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_87[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 32, 96, 96)   0           concatenate_33[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 32, 96, 96)   128         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 32, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 16, 96, 96)   512         activation_89[0][0]              
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 16, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 16, 48, 48)   64          average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 16, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   512         activation_90[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_91[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 24, 48, 48)   0           average_pooling2d_11[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 24, 48, 48)   96          concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 24, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   768         activation_92[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_93[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 32, 48, 48)   0           concatenate_35[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 48, 48)   128         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 32, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_11 (Gl (None, 32)           0           activation_94[0][0]              
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            33          global_average_pooling2d_11[0][0]
==================================================================================================
Total params: 13,697
Trainable params: 13,153
Non-trainable params: 544
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 31s - loss: 0.6424 - acc: 0.6979 - val_loss: 0.5629 - val_acc: 0.8302

Epoch 00001: val_loss improved from inf to 0.56286, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 27s - loss: 0.5864 - acc: 0.7406 - val_loss: 0.5320 - val_acc: 0.8519

Epoch 00002: val_loss improved from 0.56286 to 0.53200, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 27s - loss: 0.5670 - acc: 0.7476 - val_loss: 0.5213 - val_acc: 0.8453

Epoch 00003: val_loss improved from 0.53200 to 0.52133, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 26s - loss: 0.5557 - acc: 0.7527 - val_loss: 0.5211 - val_acc: 0.8379

Epoch 00004: val_loss improved from 0.52133 to 0.52107, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 26s - loss: 0.5449 - acc: 0.7565 - val_loss: 0.5141 - val_acc: 0.8321

Epoch 00005: val_loss improved from 0.52107 to 0.51414, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 26s - loss: 0.5347 - acc: 0.7602 - val_loss: 0.5147 - val_acc: 0.8249

Epoch 00006: val_loss did not improve from 0.51414
Epoch 7/40
 - 26s - loss: 0.5263 - acc: 0.7640 - val_loss: 0.5061 - val_acc: 0.8234

Epoch 00007: val_loss improved from 0.51414 to 0.50606, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 26s - loss: 0.5168 - acc: 0.7700 - val_loss: 0.5062 - val_acc: 0.8220

Epoch 00008: val_loss did not improve from 0.50606
Epoch 9/40
 - 26s - loss: 0.5101 - acc: 0.7725 - val_loss: 0.4947 - val_acc: 0.8219

Epoch 00009: val_loss improved from 0.50606 to 0.49473, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 26s - loss: 0.5051 - acc: 0.7761 - val_loss: 0.4987 - val_acc: 0.8215

Epoch 00010: val_loss did not improve from 0.49473
Epoch 11/40
 - 26s - loss: 0.4979 - acc: 0.7780 - val_loss: 0.4884 - val_acc: 0.8161

Epoch 00011: val_loss improved from 0.49473 to 0.48841, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 26s - loss: 0.4936 - acc: 0.7797 - val_loss: 0.4852 - val_acc: 0.8270

Epoch 00012: val_loss improved from 0.48841 to 0.48519, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 26s - loss: 0.4901 - acc: 0.7830 - val_loss: 0.4771 - val_acc: 0.8181

Epoch 00013: val_loss improved from 0.48519 to 0.47714, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 26s - loss: 0.4840 - acc: 0.7853 - val_loss: 0.4814 - val_acc: 0.8145

Epoch 00014: val_loss did not improve from 0.47714
Epoch 15/40
 - 26s - loss: 0.4802 - acc: 0.7859 - val_loss: 0.4801 - val_acc: 0.8106

Epoch 00015: val_loss did not improve from 0.47714
Epoch 16/40
 - 26s - loss: 0.4769 - acc: 0.7896 - val_loss: 0.4867 - val_acc: 0.8190

Epoch 00016: val_loss did not improve from 0.47714
Epoch 17/40
 - 26s - loss: 0.4735 - acc: 0.7897 - val_loss: 0.4879 - val_acc: 0.8120

Epoch 00017: val_loss did not improve from 0.47714
Epoch 18/40
 - 26s - loss: 0.4705 - acc: 0.7919 - val_loss: 0.4772 - val_acc: 0.8047

Epoch 00018: val_loss did not improve from 0.47714
Epoch 19/40
 - 26s - loss: 0.4671 - acc: 0.7924 - val_loss: 0.4739 - val_acc: 0.8071

Epoch 00019: val_loss improved from 0.47714 to 0.47391, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 26s - loss: 0.4643 - acc: 0.7965 - val_loss: 0.4625 - val_acc: 0.8145

Epoch 00020: val_loss improved from 0.47391 to 0.46252, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 26s - loss: 0.4625 - acc: 0.7962 - val_loss: 0.4687 - val_acc: 0.8066

Epoch 00021: val_loss did not improve from 0.46252
Epoch 22/40
 - 26s - loss: 0.4604 - acc: 0.7963 - val_loss: 0.4751 - val_acc: 0.8017

Epoch 00022: val_loss did not improve from 0.46252
Epoch 23/40
 - 26s - loss: 0.4573 - acc: 0.7982 - val_loss: 0.4634 - val_acc: 0.8147

Epoch 00023: val_loss did not improve from 0.46252
Epoch 24/40
 - 26s - loss: 0.4551 - acc: 0.7982 - val_loss: 0.4674 - val_acc: 0.8075

Epoch 00024: val_loss did not improve from 0.46252
Epoch 25/40
 - 26s - loss: 0.4542 - acc: 0.8003 - val_loss: 0.4597 - val_acc: 0.8199

Epoch 00025: val_loss improved from 0.46252 to 0.45973, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 26/40
 - 26s - loss: 0.4504 - acc: 0.8010 - val_loss: 0.4698 - val_acc: 0.8136

Epoch 00026: val_loss did not improve from 0.45973
Epoch 27/40
 - 26s - loss: 0.4494 - acc: 0.8011 - val_loss: 0.4586 - val_acc: 0.8125

Epoch 00027: val_loss improved from 0.45973 to 0.45864, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 28/40
 - 26s - loss: 0.4491 - acc: 0.8024 - val_loss: 0.4506 - val_acc: 0.8124

Epoch 00028: val_loss improved from 0.45864 to 0.45061, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 29/40
 - 26s - loss: 0.4452 - acc: 0.8026 - val_loss: 0.4602 - val_acc: 0.8180

Epoch 00029: val_loss did not improve from 0.45061
Epoch 30/40
 - 26s - loss: 0.4437 - acc: 0.8050 - val_loss: 0.4518 - val_acc: 0.8208

Epoch 00030: val_loss did not improve from 0.45061
Epoch 31/40
 - 26s - loss: 0.4423 - acc: 0.8059 - val_loss: 0.4363 - val_acc: 0.8262

Epoch 00031: val_loss improved from 0.45061 to 0.43628, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 32/40
 - 26s - loss: 0.4402 - acc: 0.8082 - val_loss: 0.4854 - val_acc: 0.8113

Epoch 00032: val_loss did not improve from 0.43628
Epoch 33/40
 - 26s - loss: 0.4372 - acc: 0.8077 - val_loss: 0.4607 - val_acc: 0.8059

Epoch 00033: val_loss did not improve from 0.43628
Epoch 34/40
 - 26s - loss: 0.4366 - acc: 0.8065 - val_loss: 0.4547 - val_acc: 0.8169

Epoch 00034: val_loss did not improve from 0.43628
Epoch 35/40
 - 26s - loss: 0.4368 - acc: 0.8084 - val_loss: 0.4513 - val_acc: 0.8047

Epoch 00035: val_loss did not improve from 0.43628
Epoch 36/40
 - 26s - loss: 0.4339 - acc: 0.8072 - val_loss: 0.4649 - val_acc: 0.7992

Epoch 00036: val_loss did not improve from 0.43628
Epoch 37/40
 - 26s - loss: 0.4333 - acc: 0.8094 - val_loss: 0.4581 - val_acc: 0.7953

Epoch 00037: val_loss did not improve from 0.43628
Epoch 38/40
 - 26s - loss: 0.4320 - acc: 0.8099 - val_loss: 0.4648 - val_acc: 0.8148

Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00038: val_loss did not improve from 0.43628
Epoch 00038: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 0s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 234us/step
current Test accuracy: 0.8147849462365592
current auc_score ------------------>  0.8793239825413343

  32/7440 [..............................] - ETA: 7:01
 224/7440 [..............................] - ETA: 1:00
 448/7440 [>.............................] - ETA: 30s 
 672/7440 [=>............................] - ETA: 19s
 896/7440 [==>...........................] - ETA: 14s
1120/7440 [===>..........................] - ETA: 11s
1344/7440 [====>.........................] - ETA: 9s 
1568/7440 [=====>........................] - ETA: 8s
1792/7440 [======>.......................] - ETA: 7s
2016/7440 [=======>......................] - ETA: 6s
2240/7440 [========>.....................] - ETA: 5s
2464/7440 [========>.....................] - ETA: 4s
2688/7440 [=========>....................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 3s
3136/7440 [===========>..................] - ETA: 3s
3360/7440 [============>.................] - ETA: 3s
3584/7440 [=============>................] - ETA: 2s
3808/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4256/7440 [================>.............] - ETA: 2s
4480/7440 [=================>............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4928/7440 [==================>...........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 491us/step
Best saved model Test accuracy: 0.8262096774193548
best saved model auc_score ------------------>  0.8955411752803792
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_12[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_95[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_96[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_97[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_98[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 32, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 32, 96, 96)   128         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 32, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 16, 96, 96)   512         activation_99[0][0]              
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 16, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 16, 48, 48)   64          average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 16, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   512         activation_100[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_101[0][0]             
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 24, 48, 48)   0           average_pooling2d_12[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 24, 48, 48)   96          concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   768         activation_102[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_103[0][0]             
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 32, 48, 48)   0           concatenate_39[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 48, 48)   128         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 32, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_12 (Gl (None, 32)           0           activation_104[0][0]             
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 1)            33          global_average_pooling2d_12[0][0]
==================================================================================================
Total params: 13,697
Trainable params: 13,153
Non-trainable params: 544
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 32s - loss: 0.6833 - acc: 0.6196 - val_loss: 0.5501 - val_acc: 0.8560

Epoch 00001: val_loss improved from inf to 0.55014, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 26s - loss: 0.6107 - acc: 0.7307 - val_loss: 0.5225 - val_acc: 0.8520

Epoch 00002: val_loss improved from 0.55014 to 0.52247, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 26s - loss: 0.5862 - acc: 0.7446 - val_loss: 0.5153 - val_acc: 0.8527

Epoch 00003: val_loss improved from 0.52247 to 0.51535, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 26s - loss: 0.5659 - acc: 0.7536 - val_loss: 0.5006 - val_acc: 0.8441

Epoch 00004: val_loss improved from 0.51535 to 0.50063, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 26s - loss: 0.5504 - acc: 0.7596 - val_loss: 0.4979 - val_acc: 0.8304

Epoch 00005: val_loss improved from 0.50063 to 0.49792, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 26s - loss: 0.5381 - acc: 0.7635 - val_loss: 0.4909 - val_acc: 0.8372

Epoch 00006: val_loss improved from 0.49792 to 0.49086, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 26s - loss: 0.5281 - acc: 0.7666 - val_loss: 0.4924 - val_acc: 0.8237

Epoch 00007: val_loss did not improve from 0.49086
Epoch 8/40
 - 26s - loss: 0.5188 - acc: 0.7703 - val_loss: 0.4914 - val_acc: 0.8156

Epoch 00008: val_loss did not improve from 0.49086
Epoch 9/40
 - 26s - loss: 0.5118 - acc: 0.7722 - val_loss: 0.4903 - val_acc: 0.8136

Epoch 00009: val_loss improved from 0.49086 to 0.49025, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 26s - loss: 0.5042 - acc: 0.7771 - val_loss: 0.4840 - val_acc: 0.8272

Epoch 00010: val_loss improved from 0.49025 to 0.48399, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 26s - loss: 0.5009 - acc: 0.7754 - val_loss: 0.5195 - val_acc: 0.7977

Epoch 00011: val_loss did not improve from 0.48399
Epoch 12/40
 - 26s - loss: 0.4967 - acc: 0.7779 - val_loss: 0.4780 - val_acc: 0.8163

Epoch 00012: val_loss improved from 0.48399 to 0.47803, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 26s - loss: 0.4911 - acc: 0.7803 - val_loss: 0.5103 - val_acc: 0.8086

Epoch 00013: val_loss did not improve from 0.47803
Epoch 14/40
 - 26s - loss: 0.4880 - acc: 0.7801 - val_loss: 0.4937 - val_acc: 0.8215

Epoch 00014: val_loss did not improve from 0.47803
Epoch 15/40
 - 26s - loss: 0.4843 - acc: 0.7831 - val_loss: 0.4971 - val_acc: 0.8138

Epoch 00015: val_loss did not improve from 0.47803
Epoch 16/40
 - 26s - loss: 0.4796 - acc: 0.7856 - val_loss: 0.5502 - val_acc: 0.7968

Epoch 00016: val_loss did not improve from 0.47803
Epoch 17/40
 - 26s - loss: 0.4778 - acc: 0.7842 - val_loss: 0.5034 - val_acc: 0.8046

Epoch 00017: val_loss did not improve from 0.47803
Epoch 18/40
 - 26s - loss: 0.4738 - acc: 0.7879 - val_loss: 0.5013 - val_acc: 0.8165

Epoch 00018: val_loss did not improve from 0.47803
Epoch 19/40
 - 26s - loss: 0.4730 - acc: 0.7862 - val_loss: 0.5163 - val_acc: 0.7946

Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00019: val_loss did not improve from 0.47803
Epoch 00019: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 242us/step
current Test accuracy: 0.7946236559139785
current auc_score ------------------>  0.8629201352757545

  32/7440 [..............................] - ETA: 7:58
 224/7440 [..............................] - ETA: 1:08
 448/7440 [>.............................] - ETA: 33s 
 672/7440 [=>............................] - ETA: 22s
 896/7440 [==>...........................] - ETA: 16s
1120/7440 [===>..........................] - ETA: 13s
1344/7440 [====>.........................] - ETA: 10s
1568/7440 [=====>........................] - ETA: 9s 
1792/7440 [======>.......................] - ETA: 7s
2016/7440 [=======>......................] - ETA: 6s
2240/7440 [========>.....................] - ETA: 6s
2464/7440 [========>.....................] - ETA: 5s
2688/7440 [=========>....................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 4s
3136/7440 [===========>..................] - ETA: 3s
3360/7440 [============>.................] - ETA: 3s
3584/7440 [=============>................] - ETA: 3s
3808/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4256/7440 [================>.............] - ETA: 2s
4480/7440 [=================>............] - ETA: 2s
4704/7440 [=================>............] - ETA: 1s
4928/7440 [==================>...........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 519us/step
Best saved model Test accuracy: 0.8162634408602151
best saved model auc_score ------------------>  0.8799135738235634
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_13[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_105[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_106[0][0]             
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_107[0][0]             
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_108[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_109[0][0]             
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 26, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_13 (Gl (None, 26)           0           activation_110[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            27          global_average_pooling2d_13[0][0]
==================================================================================================
Total params: 13,235
Trainable params: 12,875
Non-trainable params: 360
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 25s - loss: 0.6568 - acc: 0.6569 - val_loss: 0.5590 - val_acc: 0.8094

Epoch 00001: val_loss improved from inf to 0.55901, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 19s - loss: 0.6165 - acc: 0.7144 - val_loss: 0.5480 - val_acc: 0.8231

Epoch 00002: val_loss improved from 0.55901 to 0.54799, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 19s - loss: 0.6019 - acc: 0.7224 - val_loss: 0.5403 - val_acc: 0.8371

Epoch 00003: val_loss improved from 0.54799 to 0.54025, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 19s - loss: 0.5887 - acc: 0.7322 - val_loss: 0.5362 - val_acc: 0.8520

Epoch 00004: val_loss improved from 0.54025 to 0.53625, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 19s - loss: 0.5756 - acc: 0.7430 - val_loss: 0.5311 - val_acc: 0.8516

Epoch 00005: val_loss improved from 0.53625 to 0.53113, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 19s - loss: 0.5652 - acc: 0.7482 - val_loss: 0.5441 - val_acc: 0.8423

Epoch 00006: val_loss did not improve from 0.53113
Epoch 7/40
 - 19s - loss: 0.5543 - acc: 0.7558 - val_loss: 0.5182 - val_acc: 0.8442

Epoch 00007: val_loss improved from 0.53113 to 0.51823, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 19s - loss: 0.5473 - acc: 0.7584 - val_loss: 0.5306 - val_acc: 0.8380

Epoch 00008: val_loss did not improve from 0.51823
Epoch 9/40
 - 19s - loss: 0.5401 - acc: 0.7605 - val_loss: 0.4996 - val_acc: 0.8363

Epoch 00009: val_loss improved from 0.51823 to 0.49960, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 19s - loss: 0.5334 - acc: 0.7624 - val_loss: 0.5092 - val_acc: 0.8375

Epoch 00010: val_loss did not improve from 0.49960
Epoch 11/40
 - 19s - loss: 0.5284 - acc: 0.7647 - val_loss: 0.4980 - val_acc: 0.8302

Epoch 00011: val_loss improved from 0.49960 to 0.49802, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 19s - loss: 0.5221 - acc: 0.7665 - val_loss: 0.5014 - val_acc: 0.8302

Epoch 00012: val_loss did not improve from 0.49802
Epoch 13/40
 - 19s - loss: 0.5189 - acc: 0.7671 - val_loss: 0.4881 - val_acc: 0.8253

Epoch 00013: val_loss improved from 0.49802 to 0.48809, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 19s - loss: 0.5152 - acc: 0.7690 - val_loss: 0.5006 - val_acc: 0.8304

Epoch 00014: val_loss did not improve from 0.48809
Epoch 15/40
 - 19s - loss: 0.5105 - acc: 0.7698 - val_loss: 0.4887 - val_acc: 0.8298

Epoch 00015: val_loss did not improve from 0.48809
Epoch 16/40
 - 19s - loss: 0.5076 - acc: 0.7717 - val_loss: 0.4925 - val_acc: 0.8301

Epoch 00016: val_loss did not improve from 0.48809
Epoch 17/40
 - 19s - loss: 0.5044 - acc: 0.7710 - val_loss: 0.4892 - val_acc: 0.8196

Epoch 00017: val_loss did not improve from 0.48809
Epoch 18/40
 - 19s - loss: 0.5014 - acc: 0.7723 - val_loss: 0.4929 - val_acc: 0.8169

Epoch 00018: val_loss did not improve from 0.48809
Epoch 19/40
 - 19s - loss: 0.4982 - acc: 0.7728 - val_loss: 0.4882 - val_acc: 0.8251

Epoch 00019: val_loss did not improve from 0.48809
Epoch 20/40
 - 19s - loss: 0.4955 - acc: 0.7758 - val_loss: 0.4894 - val_acc: 0.8188

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00020: val_loss did not improve from 0.48809
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 576/7440 [=>............................] - ETA: 1s
 832/7440 [==>...........................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1888/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2400/7440 [========>.....................] - ETA: 0s
2688/7440 [=========>....................] - ETA: 0s
2976/7440 [===========>..................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4320/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 196us/step
current Test accuracy: 0.8188172043010753
current auc_score ------------------>  0.8734420886807724

  32/7440 [..............................] - ETA: 8:25
 288/7440 [>.............................] - ETA: 55s 
 544/7440 [=>............................] - ETA: 29s
 800/7440 [==>...........................] - ETA: 19s
1056/7440 [===>..........................] - ETA: 14s
1312/7440 [====>.........................] - ETA: 11s
1568/7440 [=====>........................] - ETA: 9s 
1824/7440 [======>.......................] - ETA: 7s
2080/7440 [=======>......................] - ETA: 6s
2336/7440 [========>.....................] - ETA: 5s
2592/7440 [=========>....................] - ETA: 5s
2848/7440 [==========>...................] - ETA: 4s
3104/7440 [===========>..................] - ETA: 3s
3360/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 3s
3904/7440 [==============>...............] - ETA: 2s
4160/7440 [===============>..............] - ETA: 2s
4416/7440 [================>.............] - ETA: 2s
4704/7440 [=================>............] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5856/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 491us/step
Best saved model Test accuracy: 0.8252688172043011
best saved model auc_score ------------------>  0.8770868380737658
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_14[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_111[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_112[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 22, 96, 96)   88          concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 22, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 11, 96, 96)   242         activation_113[0][0]             
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 11, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 11, 48, 48)   44          average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 11, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   264         activation_114[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_115[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 17, 48, 48)   0           average_pooling2d_14[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 17, 48, 48)   68          concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 17, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_14 (Gl (None, 17)           0           activation_116[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            18          global_average_pooling2d_14[0][0]
==================================================================================================
Total params: 4,244
Trainable params: 4,016
Non-trainable params: 228
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 22s - loss: 0.6634 - acc: 0.6597 - val_loss: 0.5739 - val_acc: 0.8277

Epoch 00001: val_loss improved from inf to 0.57393, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 16s - loss: 0.6221 - acc: 0.7127 - val_loss: 0.5533 - val_acc: 0.8203

Epoch 00002: val_loss improved from 0.57393 to 0.55330, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 16s - loss: 0.6048 - acc: 0.7236 - val_loss: 0.5388 - val_acc: 0.8116

Epoch 00003: val_loss improved from 0.55330 to 0.53877, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 16s - loss: 0.5912 - acc: 0.7300 - val_loss: 0.5350 - val_acc: 0.8199

Epoch 00004: val_loss improved from 0.53877 to 0.53499, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 16s - loss: 0.5809 - acc: 0.7342 - val_loss: 0.5244 - val_acc: 0.8358

Epoch 00005: val_loss improved from 0.53499 to 0.52436, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 16s - loss: 0.5743 - acc: 0.7384 - val_loss: 0.5197 - val_acc: 0.8571

Epoch 00006: val_loss improved from 0.52436 to 0.51970, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 16s - loss: 0.5672 - acc: 0.7414 - val_loss: 0.5141 - val_acc: 0.8558

Epoch 00007: val_loss improved from 0.51970 to 0.51415, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 16s - loss: 0.5618 - acc: 0.7426 - val_loss: 0.5164 - val_acc: 0.8597

Epoch 00008: val_loss did not improve from 0.51415
Epoch 9/40
 - 16s - loss: 0.5570 - acc: 0.7444 - val_loss: 0.5131 - val_acc: 0.8562

Epoch 00009: val_loss improved from 0.51415 to 0.51309, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 16s - loss: 0.5534 - acc: 0.7455 - val_loss: 0.5198 - val_acc: 0.8530

Epoch 00010: val_loss did not improve from 0.51309
Epoch 11/40
 - 16s - loss: 0.5490 - acc: 0.7470 - val_loss: 0.5132 - val_acc: 0.8503

Epoch 00011: val_loss did not improve from 0.51309
Epoch 12/40
 - 16s - loss: 0.5460 - acc: 0.7471 - val_loss: 0.5084 - val_acc: 0.8489

Epoch 00012: val_loss improved from 0.51309 to 0.50836, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 16s - loss: 0.5426 - acc: 0.7480 - val_loss: 0.5077 - val_acc: 0.8496

Epoch 00013: val_loss improved from 0.50836 to 0.50773, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 16s - loss: 0.5396 - acc: 0.7499 - val_loss: 0.5097 - val_acc: 0.8450

Epoch 00014: val_loss did not improve from 0.50773
Epoch 15/40
 - 16s - loss: 0.5361 - acc: 0.7514 - val_loss: 0.5122 - val_acc: 0.8419

Epoch 00015: val_loss did not improve from 0.50773
Epoch 16/40
 - 16s - loss: 0.5350 - acc: 0.7499 - val_loss: 0.5082 - val_acc: 0.8392

Epoch 00016: val_loss did not improve from 0.50773
Epoch 17/40
 - 16s - loss: 0.5334 - acc: 0.7496 - val_loss: 0.5131 - val_acc: 0.8386

Epoch 00017: val_loss did not improve from 0.50773
Epoch 18/40
 - 16s - loss: 0.5295 - acc: 0.7527 - val_loss: 0.5000 - val_acc: 0.8349

Epoch 00018: val_loss improved from 0.50773 to 0.50000, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 16s - loss: 0.5282 - acc: 0.7528 - val_loss: 0.5045 - val_acc: 0.8335

Epoch 00019: val_loss did not improve from 0.50000
Epoch 20/40
 - 16s - loss: 0.5266 - acc: 0.7553 - val_loss: 0.4985 - val_acc: 0.8327

Epoch 00020: val_loss improved from 0.50000 to 0.49854, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 16s - loss: 0.5243 - acc: 0.7538 - val_loss: 0.5014 - val_acc: 0.8328

Epoch 00021: val_loss did not improve from 0.49854
Epoch 22/40
 - 16s - loss: 0.5227 - acc: 0.7538 - val_loss: 0.4999 - val_acc: 0.8327

Epoch 00022: val_loss did not improve from 0.49854
Epoch 23/40
 - 16s - loss: 0.5202 - acc: 0.7565 - val_loss: 0.5054 - val_acc: 0.8292

Epoch 00023: val_loss did not improve from 0.49854
Epoch 24/40
 - 16s - loss: 0.5192 - acc: 0.7554 - val_loss: 0.5010 - val_acc: 0.8305

Epoch 00024: val_loss did not improve from 0.49854
Epoch 25/40
 - 16s - loss: 0.5187 - acc: 0.7553 - val_loss: 0.5117 - val_acc: 0.8243

Epoch 00025: val_loss did not improve from 0.49854
Epoch 26/40
 - 16s - loss: 0.5154 - acc: 0.7566 - val_loss: 0.4992 - val_acc: 0.8267

Epoch 00026: val_loss did not improve from 0.49854
Epoch 27/40
 - 16s - loss: 0.5153 - acc: 0.7576 - val_loss: 0.5035 - val_acc: 0.8261

Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00027: val_loss did not improve from 0.49854
Epoch 00027: early stopping

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3808/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 172us/step
current Test accuracy: 0.8260752688172043
current auc_score ------------------>  0.8634347178864609

  32/7440 [..............................] - ETA: 8:58
 288/7440 [>.............................] - ETA: 59s 
 576/7440 [=>............................] - ETA: 28s
 864/7440 [==>...........................] - ETA: 18s
1152/7440 [===>..........................] - ETA: 13s
1440/7440 [====>.........................] - ETA: 10s
1728/7440 [=====>........................] - ETA: 8s 
2016/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2592/7440 [=========>....................] - ETA: 5s
2880/7440 [==========>...................] - ETA: 4s
3168/7440 [===========>..................] - ETA: 3s
3456/7440 [============>.................] - ETA: 3s
3744/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4320/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 497us/step
Best saved model Test accuracy: 0.8326612903225806
best saved model auc_score ------------------>  0.8619539108567464
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_15 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_117 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_15 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_118 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_15  (None, 8)                 0         
_________________________________________________________________
dense_15 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 15s - loss: 0.6747 - acc: 0.5971 - val_loss: 0.6218 - val_acc: 0.6719

Epoch 00001: val_loss improved from inf to 0.62180, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 10s - loss: 0.6485 - acc: 0.6660 - val_loss: 0.5999 - val_acc: 0.7671

Epoch 00002: val_loss improved from 0.62180 to 0.59986, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 10s - loss: 0.6387 - acc: 0.6810 - val_loss: 0.5879 - val_acc: 0.7802

Epoch 00003: val_loss improved from 0.59986 to 0.58793, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 10s - loss: 0.6327 - acc: 0.6868 - val_loss: 0.5796 - val_acc: 0.7867

Epoch 00004: val_loss improved from 0.58793 to 0.57955, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 10s - loss: 0.6294 - acc: 0.6912 - val_loss: 0.5719 - val_acc: 0.7962

Epoch 00005: val_loss improved from 0.57955 to 0.57187, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 10s - loss: 0.6262 - acc: 0.6921 - val_loss: 0.5689 - val_acc: 0.7977

Epoch 00006: val_loss improved from 0.57187 to 0.56893, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 10s - loss: 0.6233 - acc: 0.6965 - val_loss: 0.5642 - val_acc: 0.7988

Epoch 00007: val_loss improved from 0.56893 to 0.56421, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 10s - loss: 0.6214 - acc: 0.6975 - val_loss: 0.5605 - val_acc: 0.8011

Epoch 00008: val_loss improved from 0.56421 to 0.56048, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 10s - loss: 0.6190 - acc: 0.6978 - val_loss: 0.5581 - val_acc: 0.8007

Epoch 00009: val_loss improved from 0.56048 to 0.55815, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 10s - loss: 0.6178 - acc: 0.6980 - val_loss: 0.5564 - val_acc: 0.8011

Epoch 00010: val_loss improved from 0.55815 to 0.55637, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 10s - loss: 0.6158 - acc: 0.7003 - val_loss: 0.5536 - val_acc: 0.8052

Epoch 00011: val_loss improved from 0.55637 to 0.55365, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 10s - loss: 0.6138 - acc: 0.6998 - val_loss: 0.5519 - val_acc: 0.8022

Epoch 00012: val_loss improved from 0.55365 to 0.55186, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 10s - loss: 0.6126 - acc: 0.7016 - val_loss: 0.5491 - val_acc: 0.8089

Epoch 00013: val_loss improved from 0.55186 to 0.54910, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 10s - loss: 0.6115 - acc: 0.7010 - val_loss: 0.5482 - val_acc: 0.8065

Epoch 00014: val_loss improved from 0.54910 to 0.54819, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 10s - loss: 0.6096 - acc: 0.7029 - val_loss: 0.5483 - val_acc: 0.8044

Epoch 00015: val_loss did not improve from 0.54819
Epoch 16/40
 - 10s - loss: 0.6074 - acc: 0.7038 - val_loss: 0.5480 - val_acc: 0.8081

Epoch 00016: val_loss improved from 0.54819 to 0.54798, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 10s - loss: 0.6073 - acc: 0.7043 - val_loss: 0.5475 - val_acc: 0.8051

Epoch 00017: val_loss improved from 0.54798 to 0.54751, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 10s - loss: 0.6057 - acc: 0.7064 - val_loss: 0.5444 - val_acc: 0.8114

Epoch 00018: val_loss improved from 0.54751 to 0.54438, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 10s - loss: 0.6052 - acc: 0.7061 - val_loss: 0.5452 - val_acc: 0.8067

Epoch 00019: val_loss did not improve from 0.54438
Epoch 20/40
 - 10s - loss: 0.6029 - acc: 0.7069 - val_loss: 0.5421 - val_acc: 0.8128

Epoch 00020: val_loss improved from 0.54438 to 0.54208, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 10s - loss: 0.6021 - acc: 0.7073 - val_loss: 0.5411 - val_acc: 0.8082

Epoch 00021: val_loss improved from 0.54208 to 0.54107, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 10s - loss: 0.6001 - acc: 0.7099 - val_loss: 0.5391 - val_acc: 0.8137

Epoch 00022: val_loss improved from 0.54107 to 0.53906, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 10s - loss: 0.5991 - acc: 0.7081 - val_loss: 0.5414 - val_acc: 0.8071

Epoch 00023: val_loss did not improve from 0.53906
Epoch 24/40
 - 10s - loss: 0.5983 - acc: 0.7115 - val_loss: 0.5374 - val_acc: 0.8147

Epoch 00024: val_loss improved from 0.53906 to 0.53742, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 10s - loss: 0.5960 - acc: 0.7136 - val_loss: 0.5398 - val_acc: 0.8102

Epoch 00025: val_loss did not improve from 0.53742
Epoch 26/40
 - 9s - loss: 0.5955 - acc: 0.7126 - val_loss: 0.5358 - val_acc: 0.8181

Epoch 00026: val_loss improved from 0.53742 to 0.53579, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 10s - loss: 0.5941 - acc: 0.7145 - val_loss: 0.5391 - val_acc: 0.8147

Epoch 00027: val_loss did not improve from 0.53579
Epoch 28/40
 - 10s - loss: 0.5936 - acc: 0.7141 - val_loss: 0.5392 - val_acc: 0.8102

Epoch 00028: val_loss did not improve from 0.53579
Epoch 29/40
 - 10s - loss: 0.5926 - acc: 0.7142 - val_loss: 0.5349 - val_acc: 0.8172

Epoch 00029: val_loss improved from 0.53579 to 0.53485, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 10s - loss: 0.5907 - acc: 0.7149 - val_loss: 0.5314 - val_acc: 0.8145

Epoch 00030: val_loss improved from 0.53485 to 0.53143, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 31/40
 - 10s - loss: 0.5898 - acc: 0.7169 - val_loss: 0.5313 - val_acc: 0.8211

Epoch 00031: val_loss improved from 0.53143 to 0.53126, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 32/40
 - 10s - loss: 0.5885 - acc: 0.7198 - val_loss: 0.5337 - val_acc: 0.8181

Epoch 00032: val_loss did not improve from 0.53126
Epoch 33/40
 - 10s - loss: 0.5880 - acc: 0.7184 - val_loss: 0.5354 - val_acc: 0.8099

Epoch 00033: val_loss did not improve from 0.53126
Epoch 34/40
 - 10s - loss: 0.5867 - acc: 0.7189 - val_loss: 0.5322 - val_acc: 0.8167

Epoch 00034: val_loss did not improve from 0.53126
Epoch 35/40
 - 10s - loss: 0.5848 - acc: 0.7184 - val_loss: 0.5296 - val_acc: 0.8242

Epoch 00035: val_loss improved from 0.53126 to 0.52964, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 36/40
 - 10s - loss: 0.5852 - acc: 0.7184 - val_loss: 0.5333 - val_acc: 0.8134

Epoch 00036: val_loss did not improve from 0.52964
Epoch 37/40
 - 10s - loss: 0.5846 - acc: 0.7202 - val_loss: 0.5337 - val_acc: 0.8161

Epoch 00037: val_loss did not improve from 0.52964
Epoch 38/40
 - 10s - loss: 0.5823 - acc: 0.7216 - val_loss: 0.5253 - val_acc: 0.8262

Epoch 00038: val_loss improved from 0.52964 to 0.52531, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 39/40
 - 10s - loss: 0.5813 - acc: 0.7227 - val_loss: 0.5308 - val_acc: 0.8124

Epoch 00039: val_loss did not improve from 0.52531
Epoch 40/40
 - 10s - loss: 0.5805 - acc: 0.7215 - val_loss: 0.5236 - val_acc: 0.8224

Epoch 00040: val_loss improved from 0.52531 to 0.52362, saving model to keras_densenet_simple_wt_29Sept_1404.h5

  32/7440 [..............................] - ETA: 0s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2368/7440 [========>.....................] - ETA: 0s
2752/7440 [==========>...................] - ETA: 0s
3136/7440 [===========>..................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3936/7440 [==============>...............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 132us/step
current Test accuracy: 0.8224462365591397
current auc_score ------------------>  0.8655809558908544

  32/7440 [..............................] - ETA: 9:10
 416/7440 [>.............................] - ETA: 41s 
 832/7440 [==>...........................] - ETA: 19s
1248/7440 [====>.........................] - ETA: 12s
1664/7440 [=====>........................] - ETA: 8s 
2080/7440 [=======>......................] - ETA: 6s
2496/7440 [=========>....................] - ETA: 5s
2912/7440 [==========>...................] - ETA: 4s
3328/7440 [============>.................] - ETA: 3s
3744/7440 [==============>...............] - ETA: 2s
4160/7440 [===============>..............] - ETA: 2s
4576/7440 [=================>............] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 3s 446us/step
Best saved model Test accuracy: 0.8224462365591397
best saved model auc_score ------------------>  0.8655809558908544
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_16 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_16[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_119[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_120[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 24, 96, 96)   96          concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 24, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 12, 96, 96)   288         activation_121[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 12, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 12, 48, 48)   48          average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 12, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   384         activation_122[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_123[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_16[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 20, 48, 48)   80          concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 20, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_16 (Gl (None, 20)           0           activation_124[0][0]             
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 1)            21          global_average_pooling2d_16[0][0]
==================================================================================================
Total params: 6,645
Trainable params: 6,373
Non-trainable params: 272
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 24s - loss: 0.6678 - acc: 0.6482 - val_loss: 0.5944 - val_acc: 0.8132

Epoch 00001: val_loss improved from inf to 0.59444, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 17s - loss: 0.6274 - acc: 0.7122 - val_loss: 0.5585 - val_acc: 0.8337

Epoch 00002: val_loss improved from 0.59444 to 0.55851, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 17s - loss: 0.6059 - acc: 0.7278 - val_loss: 0.5382 - val_acc: 0.8383

Epoch 00003: val_loss improved from 0.55851 to 0.53821, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 17s - loss: 0.5920 - acc: 0.7337 - val_loss: 0.5242 - val_acc: 0.8461

Epoch 00004: val_loss improved from 0.53821 to 0.52420, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 17s - loss: 0.5793 - acc: 0.7383 - val_loss: 0.5162 - val_acc: 0.8511

Epoch 00005: val_loss improved from 0.52420 to 0.51620, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 17s - loss: 0.5676 - acc: 0.7449 - val_loss: 0.5060 - val_acc: 0.8599

Epoch 00006: val_loss improved from 0.51620 to 0.50601, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 17s - loss: 0.5593 - acc: 0.7478 - val_loss: 0.4989 - val_acc: 0.8624

Epoch 00007: val_loss improved from 0.50601 to 0.49887, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 17s - loss: 0.5512 - acc: 0.7512 - val_loss: 0.4932 - val_acc: 0.8560

Epoch 00008: val_loss improved from 0.49887 to 0.49321, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 17s - loss: 0.5442 - acc: 0.7560 - val_loss: 0.4866 - val_acc: 0.8401

Epoch 00009: val_loss improved from 0.49321 to 0.48656, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 17s - loss: 0.5381 - acc: 0.7579 - val_loss: 0.4827 - val_acc: 0.8466

Epoch 00010: val_loss improved from 0.48656 to 0.48265, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 17s - loss: 0.5321 - acc: 0.7590 - val_loss: 0.4778 - val_acc: 0.8453

Epoch 00011: val_loss improved from 0.48265 to 0.47783, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 17s - loss: 0.5276 - acc: 0.7582 - val_loss: 0.4809 - val_acc: 0.8415

Epoch 00012: val_loss did not improve from 0.47783
Epoch 13/40
 - 17s - loss: 0.5230 - acc: 0.7607 - val_loss: 0.4794 - val_acc: 0.8397

Epoch 00013: val_loss did not improve from 0.47783
Epoch 14/40
 - 17s - loss: 0.5189 - acc: 0.7612 - val_loss: 0.4766 - val_acc: 0.8344

Epoch 00014: val_loss improved from 0.47783 to 0.47657, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 17s - loss: 0.5152 - acc: 0.7618 - val_loss: 0.4780 - val_acc: 0.8278

Epoch 00015: val_loss did not improve from 0.47657
Epoch 16/40
 - 17s - loss: 0.5130 - acc: 0.7634 - val_loss: 0.4790 - val_acc: 0.8259

Epoch 00016: val_loss did not improve from 0.47657
Epoch 17/40
 - 17s - loss: 0.5080 - acc: 0.7658 - val_loss: 0.4719 - val_acc: 0.8312

Epoch 00017: val_loss improved from 0.47657 to 0.47187, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 17s - loss: 0.5059 - acc: 0.7662 - val_loss: 0.4728 - val_acc: 0.8249

Epoch 00018: val_loss did not improve from 0.47187
Epoch 19/40
 - 17s - loss: 0.5046 - acc: 0.7649 - val_loss: 0.4710 - val_acc: 0.8238

Epoch 00019: val_loss improved from 0.47187 to 0.47102, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 17s - loss: 0.5010 - acc: 0.7660 - val_loss: 0.4700 - val_acc: 0.8332

Epoch 00020: val_loss improved from 0.47102 to 0.46998, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 17s - loss: 0.4985 - acc: 0.7685 - val_loss: 0.4729 - val_acc: 0.8253

Epoch 00021: val_loss did not improve from 0.46998
Epoch 22/40
 - 17s - loss: 0.4972 - acc: 0.7690 - val_loss: 0.4711 - val_acc: 0.8233

Epoch 00022: val_loss did not improve from 0.46998
Epoch 23/40
 - 17s - loss: 0.4964 - acc: 0.7689 - val_loss: 0.4714 - val_acc: 0.8231

Epoch 00023: val_loss did not improve from 0.46998
Epoch 24/40
 - 17s - loss: 0.4930 - acc: 0.7705 - val_loss: 0.4660 - val_acc: 0.8243

Epoch 00024: val_loss improved from 0.46998 to 0.46600, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 17s - loss: 0.4901 - acc: 0.7718 - val_loss: 0.4750 - val_acc: 0.8086

Epoch 00025: val_loss did not improve from 0.46600
Epoch 26/40
 - 17s - loss: 0.4880 - acc: 0.7737 - val_loss: 0.4685 - val_acc: 0.8163

Epoch 00026: val_loss did not improve from 0.46600
Epoch 27/40
 - 17s - loss: 0.4877 - acc: 0.7710 - val_loss: 0.4665 - val_acc: 0.8192

Epoch 00027: val_loss did not improve from 0.46600
Epoch 28/40
 - 17s - loss: 0.4846 - acc: 0.7745 - val_loss: 0.4660 - val_acc: 0.8149

Epoch 00028: val_loss did not improve from 0.46600
Epoch 29/40
 - 17s - loss: 0.4840 - acc: 0.7740 - val_loss: 0.4632 - val_acc: 0.8180

Epoch 00029: val_loss improved from 0.46600 to 0.46317, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 17s - loss: 0.4821 - acc: 0.7737 - val_loss: 0.4615 - val_acc: 0.8223

Epoch 00030: val_loss improved from 0.46317 to 0.46152, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 31/40
 - 17s - loss: 0.4790 - acc: 0.7767 - val_loss: 0.4659 - val_acc: 0.8167

Epoch 00031: val_loss did not improve from 0.46152
Epoch 32/40
 - 17s - loss: 0.4799 - acc: 0.7754 - val_loss: 0.4618 - val_acc: 0.8171

Epoch 00032: val_loss did not improve from 0.46152
Epoch 33/40
 - 17s - loss: 0.4760 - acc: 0.7788 - val_loss: 0.4620 - val_acc: 0.8177

Epoch 00033: val_loss did not improve from 0.46152
Epoch 34/40
 - 17s - loss: 0.4767 - acc: 0.7795 - val_loss: 0.4620 - val_acc: 0.8160

Epoch 00034: val_loss did not improve from 0.46152
Epoch 35/40
 - 17s - loss: 0.4763 - acc: 0.7787 - val_loss: 0.4725 - val_acc: 0.8105

Epoch 00035: val_loss did not improve from 0.46152
Epoch 36/40
 - 17s - loss: 0.4731 - acc: 0.7794 - val_loss: 0.4667 - val_acc: 0.8126

Epoch 00036: val_loss did not improve from 0.46152
Epoch 37/40
 - 17s - loss: 0.4717 - acc: 0.7798 - val_loss: 0.4575 - val_acc: 0.8198

Epoch 00037: val_loss improved from 0.46152 to 0.45746, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 38/40
 - 17s - loss: 0.4730 - acc: 0.7785 - val_loss: 0.4645 - val_acc: 0.8075

Epoch 00038: val_loss did not improve from 0.45746
Epoch 39/40
 - 17s - loss: 0.4682 - acc: 0.7838 - val_loss: 0.4585 - val_acc: 0.8138

Epoch 00039: val_loss did not improve from 0.45746
Epoch 40/40
 - 17s - loss: 0.4680 - acc: 0.7821 - val_loss: 0.4540 - val_acc: 0.8164

Epoch 00040: val_loss improved from 0.45746 to 0.45399, saving model to keras_densenet_simple_wt_29Sept_1404.h5

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 180us/step
current Test accuracy: 0.8163978494623656
current auc_score ------------------>  0.8814131691525031

  32/7440 [..............................] - ETA: 9:48
 288/7440 [>.............................] - ETA: 1:04
 576/7440 [=>............................] - ETA: 31s 
 864/7440 [==>...........................] - ETA: 20s
1152/7440 [===>..........................] - ETA: 15s
1440/7440 [====>.........................] - ETA: 11s
1728/7440 [=====>........................] - ETA: 9s 
2016/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2592/7440 [=========>....................] - ETA: 5s
2880/7440 [==========>...................] - ETA: 4s
3168/7440 [===========>..................] - ETA: 4s
3456/7440 [============>.................] - ETA: 3s
3744/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 2s
4320/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4896/7440 [==================>...........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 526us/step
Best saved model Test accuracy: 0.8163978494623656
best saved model auc_score ------------------>  0.8814131691525031
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_17[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_125[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_126[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 24, 96, 96)   96          concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 24, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 12, 96, 96)   288         activation_127[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 12, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 12, 48, 48)   48          average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 12, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   384         activation_128[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_129[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_17[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 20, 48, 48)   80          concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 20, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_17 (Gl (None, 20)           0           activation_130[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            21          global_average_pooling2d_17[0][0]
==================================================================================================
Total params: 6,645
Trainable params: 6,373
Non-trainable params: 272
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 24s - loss: 0.6613 - acc: 0.6382 - val_loss: 0.5978 - val_acc: 0.7914

Epoch 00001: val_loss improved from inf to 0.59783, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 17s - loss: 0.6169 - acc: 0.7160 - val_loss: 0.5805 - val_acc: 0.8241

Epoch 00002: val_loss improved from 0.59783 to 0.58054, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 17s - loss: 0.6009 - acc: 0.7251 - val_loss: 0.5575 - val_acc: 0.8379

Epoch 00003: val_loss improved from 0.58054 to 0.55753, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 17s - loss: 0.5867 - acc: 0.7306 - val_loss: 0.5404 - val_acc: 0.8503

Epoch 00004: val_loss improved from 0.55753 to 0.54035, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 17s - loss: 0.5781 - acc: 0.7337 - val_loss: 0.5394 - val_acc: 0.8587

Epoch 00005: val_loss improved from 0.54035 to 0.53942, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 17s - loss: 0.5677 - acc: 0.7425 - val_loss: 0.5253 - val_acc: 0.8573

Epoch 00006: val_loss improved from 0.53942 to 0.52534, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 17s - loss: 0.5620 - acc: 0.7436 - val_loss: 0.5179 - val_acc: 0.8522

Epoch 00007: val_loss improved from 0.52534 to 0.51792, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 17s - loss: 0.5566 - acc: 0.7456 - val_loss: 0.5296 - val_acc: 0.8399

Epoch 00008: val_loss did not improve from 0.51792
Epoch 9/40
 - 17s - loss: 0.5508 - acc: 0.7494 - val_loss: 0.5032 - val_acc: 0.8465

Epoch 00009: val_loss improved from 0.51792 to 0.50322, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 17s - loss: 0.5454 - acc: 0.7520 - val_loss: 0.5006 - val_acc: 0.8397

Epoch 00010: val_loss improved from 0.50322 to 0.50063, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 17s - loss: 0.5411 - acc: 0.7525 - val_loss: 0.5226 - val_acc: 0.8280

Epoch 00011: val_loss did not improve from 0.50063
Epoch 12/40
 - 17s - loss: 0.5371 - acc: 0.7551 - val_loss: 0.5023 - val_acc: 0.8310

Epoch 00012: val_loss did not improve from 0.50063
Epoch 13/40
 - 17s - loss: 0.5336 - acc: 0.7552 - val_loss: 0.4965 - val_acc: 0.8278

Epoch 00013: val_loss improved from 0.50063 to 0.49653, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 17s - loss: 0.5301 - acc: 0.7579 - val_loss: 0.4888 - val_acc: 0.8290

Epoch 00014: val_loss improved from 0.49653 to 0.48878, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 17s - loss: 0.5259 - acc: 0.7592 - val_loss: 0.4948 - val_acc: 0.8216

Epoch 00015: val_loss did not improve from 0.48878
Epoch 16/40
 - 17s - loss: 0.5235 - acc: 0.7583 - val_loss: 0.5031 - val_acc: 0.8222

Epoch 00016: val_loss did not improve from 0.48878
Epoch 17/40
 - 17s - loss: 0.5201 - acc: 0.7631 - val_loss: 0.4889 - val_acc: 0.8200

Epoch 00017: val_loss did not improve from 0.48878
Epoch 18/40
 - 17s - loss: 0.5174 - acc: 0.7611 - val_loss: 0.4760 - val_acc: 0.8241

Epoch 00018: val_loss improved from 0.48878 to 0.47597, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 17s - loss: 0.5140 - acc: 0.7639 - val_loss: 0.4745 - val_acc: 0.8208

Epoch 00019: val_loss improved from 0.47597 to 0.47448, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 17s - loss: 0.5115 - acc: 0.7644 - val_loss: 0.4890 - val_acc: 0.8184

Epoch 00020: val_loss did not improve from 0.47448
Epoch 21/40
 - 17s - loss: 0.5099 - acc: 0.7634 - val_loss: 0.4866 - val_acc: 0.8183

Epoch 00021: val_loss did not improve from 0.47448
Epoch 22/40
 - 17s - loss: 0.5086 - acc: 0.7617 - val_loss: 0.4742 - val_acc: 0.8202

Epoch 00022: val_loss improved from 0.47448 to 0.47420, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 17s - loss: 0.5048 - acc: 0.7664 - val_loss: 0.4858 - val_acc: 0.8176

Epoch 00023: val_loss did not improve from 0.47420
Epoch 24/40
 - 17s - loss: 0.5029 - acc: 0.7665 - val_loss: 0.4744 - val_acc: 0.8237

Epoch 00024: val_loss did not improve from 0.47420
Epoch 25/40
 - 17s - loss: 0.5011 - acc: 0.7688 - val_loss: 0.4825 - val_acc: 0.8142

Epoch 00025: val_loss did not improve from 0.47420
Epoch 26/40
 - 17s - loss: 0.5008 - acc: 0.7659 - val_loss: 0.4715 - val_acc: 0.8173

Epoch 00026: val_loss improved from 0.47420 to 0.47148, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 17s - loss: 0.4982 - acc: 0.7683 - val_loss: 0.4734 - val_acc: 0.8168

Epoch 00027: val_loss did not improve from 0.47148
Epoch 28/40
 - 17s - loss: 0.4961 - acc: 0.7678 - val_loss: 0.4761 - val_acc: 0.8133

Epoch 00028: val_loss did not improve from 0.47148
Epoch 29/40
 - 17s - loss: 0.4958 - acc: 0.7675 - val_loss: 0.4723 - val_acc: 0.8190

Epoch 00029: val_loss did not improve from 0.47148
Epoch 30/40
 - 17s - loss: 0.4936 - acc: 0.7706 - val_loss: 0.4801 - val_acc: 0.8105

Epoch 00030: val_loss did not improve from 0.47148
Epoch 31/40
 - 17s - loss: 0.4922 - acc: 0.7732 - val_loss: 0.4692 - val_acc: 0.8109

Epoch 00031: val_loss improved from 0.47148 to 0.46923, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 32/40
 - 17s - loss: 0.4895 - acc: 0.7726 - val_loss: 0.4717 - val_acc: 0.8109

Epoch 00032: val_loss did not improve from 0.46923
Epoch 33/40
 - 17s - loss: 0.4879 - acc: 0.7728 - val_loss: 0.4883 - val_acc: 0.8004

Epoch 00033: val_loss did not improve from 0.46923
Epoch 34/40
 - 17s - loss: 0.4870 - acc: 0.7742 - val_loss: 0.4677 - val_acc: 0.8125

Epoch 00034: val_loss improved from 0.46923 to 0.46768, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 35/40
 - 17s - loss: 0.4860 - acc: 0.7751 - val_loss: 0.4742 - val_acc: 0.8060

Epoch 00035: val_loss did not improve from 0.46768
Epoch 36/40
 - 17s - loss: 0.4848 - acc: 0.7745 - val_loss: 0.4744 - val_acc: 0.8116

Epoch 00036: val_loss did not improve from 0.46768
Epoch 37/40
 - 17s - loss: 0.4837 - acc: 0.7758 - val_loss: 0.4678 - val_acc: 0.8112

Epoch 00037: val_loss did not improve from 0.46768
Epoch 38/40
 - 17s - loss: 0.4821 - acc: 0.7740 - val_loss: 0.4695 - val_acc: 0.8066

Epoch 00038: val_loss did not improve from 0.46768
Epoch 39/40
 - 18s - loss: 0.4798 - acc: 0.7777 - val_loss: 0.4761 - val_acc: 0.8051

Epoch 00039: val_loss did not improve from 0.46768
Epoch 40/40
 - 17s - loss: 0.4803 - acc: 0.7763 - val_loss: 0.4759 - val_acc: 0.8035

Epoch 00040: val_loss did not improve from 0.46768

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 188us/step
current Test accuracy: 0.803494623655914
current auc_score ------------------>  0.8658864825413342

  32/7440 [..............................] - ETA: 10:37
 288/7440 [>.............................] - ETA: 1:09 
 576/7440 [=>............................] - ETA: 34s 
 864/7440 [==>...........................] - ETA: 22s
1152/7440 [===>..........................] - ETA: 16s
1440/7440 [====>.........................] - ETA: 12s
1728/7440 [=====>........................] - ETA: 10s
2016/7440 [=======>......................] - ETA: 8s 
2304/7440 [========>.....................] - ETA: 7s
2592/7440 [=========>....................] - ETA: 6s
2880/7440 [==========>...................] - ETA: 5s
3168/7440 [===========>..................] - ETA: 4s
3456/7440 [============>.................] - ETA: 3s
3744/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 2s
4320/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4896/7440 [==================>...........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 554us/step
Best saved model Test accuracy: 0.8125
best saved model auc_score ------------------>  0.8678927838478439
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_131 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_18 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_132 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_18  (None, 8)                 0         
_________________________________________________________________
dense_18 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 19s - loss: 0.7290 - acc: 0.5031 - val_loss: 0.6863 - val_acc: 0.5112

Epoch 00001: val_loss improved from inf to 0.68628, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 10s - loss: 0.6859 - acc: 0.5678 - val_loss: 0.6414 - val_acc: 0.5964

Epoch 00002: val_loss improved from 0.68628 to 0.64140, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 10s - loss: 0.6637 - acc: 0.6310 - val_loss: 0.6102 - val_acc: 0.7319

Epoch 00003: val_loss improved from 0.64140 to 0.61024, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 10s - loss: 0.6511 - acc: 0.6541 - val_loss: 0.5937 - val_acc: 0.7727

Epoch 00004: val_loss improved from 0.61024 to 0.59369, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 10s - loss: 0.6450 - acc: 0.6555 - val_loss: 0.5848 - val_acc: 0.7880

Epoch 00005: val_loss improved from 0.59369 to 0.58482, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 10s - loss: 0.6410 - acc: 0.6557 - val_loss: 0.5784 - val_acc: 0.7956

Epoch 00006: val_loss improved from 0.58482 to 0.57836, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 10s - loss: 0.6375 - acc: 0.6594 - val_loss: 0.5728 - val_acc: 0.7974

Epoch 00007: val_loss improved from 0.57836 to 0.57280, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 10s - loss: 0.6357 - acc: 0.6569 - val_loss: 0.5681 - val_acc: 0.7992

Epoch 00008: val_loss improved from 0.57280 to 0.56812, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 10s - loss: 0.6331 - acc: 0.6612 - val_loss: 0.5640 - val_acc: 0.8017

Epoch 00009: val_loss improved from 0.56812 to 0.56399, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 10s - loss: 0.6315 - acc: 0.6603 - val_loss: 0.5605 - val_acc: 0.8024

Epoch 00010: val_loss improved from 0.56399 to 0.56047, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 10s - loss: 0.6289 - acc: 0.6659 - val_loss: 0.5571 - val_acc: 0.8038

Epoch 00011: val_loss improved from 0.56047 to 0.55707, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 10s - loss: 0.6276 - acc: 0.6680 - val_loss: 0.5546 - val_acc: 0.8039

Epoch 00012: val_loss improved from 0.55707 to 0.55462, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 10s - loss: 0.6255 - acc: 0.6693 - val_loss: 0.5523 - val_acc: 0.8040

Epoch 00013: val_loss improved from 0.55462 to 0.55231, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 10s - loss: 0.6238 - acc: 0.6727 - val_loss: 0.5501 - val_acc: 0.8050

Epoch 00014: val_loss improved from 0.55231 to 0.55005, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 10s - loss: 0.6223 - acc: 0.6737 - val_loss: 0.5482 - val_acc: 0.8063

Epoch 00015: val_loss improved from 0.55005 to 0.54817, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 10s - loss: 0.6208 - acc: 0.6770 - val_loss: 0.5464 - val_acc: 0.8066

Epoch 00016: val_loss improved from 0.54817 to 0.54637, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 10s - loss: 0.6189 - acc: 0.6799 - val_loss: 0.5444 - val_acc: 0.8074

Epoch 00017: val_loss improved from 0.54637 to 0.54442, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 10s - loss: 0.6182 - acc: 0.6804 - val_loss: 0.5429 - val_acc: 0.8089

Epoch 00018: val_loss improved from 0.54442 to 0.54294, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 10s - loss: 0.6168 - acc: 0.6824 - val_loss: 0.5422 - val_acc: 0.8091

Epoch 00019: val_loss improved from 0.54294 to 0.54217, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 10s - loss: 0.6155 - acc: 0.6831 - val_loss: 0.5417 - val_acc: 0.8077

Epoch 00020: val_loss improved from 0.54217 to 0.54171, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 10s - loss: 0.6146 - acc: 0.6834 - val_loss: 0.5406 - val_acc: 0.8097

Epoch 00021: val_loss improved from 0.54171 to 0.54062, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 9s - loss: 0.6136 - acc: 0.6864 - val_loss: 0.5404 - val_acc: 0.8087

Epoch 00022: val_loss improved from 0.54062 to 0.54041, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 10s - loss: 0.6112 - acc: 0.6909 - val_loss: 0.5386 - val_acc: 0.8106

Epoch 00023: val_loss improved from 0.54041 to 0.53865, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 10s - loss: 0.6106 - acc: 0.6912 - val_loss: 0.5384 - val_acc: 0.8079

Epoch 00024: val_loss improved from 0.53865 to 0.53844, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 10s - loss: 0.6096 - acc: 0.6928 - val_loss: 0.5391 - val_acc: 0.8086

Epoch 00025: val_loss did not improve from 0.53844
Epoch 26/40
 - 10s - loss: 0.6085 - acc: 0.6929 - val_loss: 0.5376 - val_acc: 0.8120

Epoch 00026: val_loss improved from 0.53844 to 0.53756, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 10s - loss: 0.6080 - acc: 0.6952 - val_loss: 0.5372 - val_acc: 0.8099

Epoch 00027: val_loss improved from 0.53756 to 0.53725, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 28/40
 - 10s - loss: 0.6070 - acc: 0.6974 - val_loss: 0.5363 - val_acc: 0.8106

Epoch 00028: val_loss improved from 0.53725 to 0.53630, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 29/40
 - 10s - loss: 0.6056 - acc: 0.6968 - val_loss: 0.5364 - val_acc: 0.8121

Epoch 00029: val_loss did not improve from 0.53630
Epoch 30/40
 - 10s - loss: 0.6045 - acc: 0.6999 - val_loss: 0.5378 - val_acc: 0.8120

Epoch 00030: val_loss did not improve from 0.53630
Epoch 31/40
 - 10s - loss: 0.6043 - acc: 0.7004 - val_loss: 0.5373 - val_acc: 0.8102

Epoch 00031: val_loss did not improve from 0.53630
Epoch 32/40
 - 10s - loss: 0.6032 - acc: 0.7005 - val_loss: 0.5359 - val_acc: 0.8118

Epoch 00032: val_loss improved from 0.53630 to 0.53588, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 33/40
 - 10s - loss: 0.6020 - acc: 0.7022 - val_loss: 0.5403 - val_acc: 0.8022

Epoch 00033: val_loss did not improve from 0.53588
Epoch 34/40
 - 10s - loss: 0.6004 - acc: 0.7033 - val_loss: 0.5378 - val_acc: 0.8152

Epoch 00034: val_loss did not improve from 0.53588
Epoch 35/40
 - 10s - loss: 0.5997 - acc: 0.7027 - val_loss: 0.5406 - val_acc: 0.8058

Epoch 00035: val_loss did not improve from 0.53588
Epoch 36/40
 - 10s - loss: 0.5983 - acc: 0.7044 - val_loss: 0.5381 - val_acc: 0.8108

Epoch 00036: val_loss did not improve from 0.53588
Epoch 37/40
 - 10s - loss: 0.5968 - acc: 0.7080 - val_loss: 0.5360 - val_acc: 0.8153

Epoch 00037: val_loss did not improve from 0.53588
Epoch 38/40
 - 9s - loss: 0.5964 - acc: 0.7093 - val_loss: 0.5374 - val_acc: 0.8192

Epoch 00038: val_loss did not improve from 0.53588
Epoch 39/40
 - 10s - loss: 0.5969 - acc: 0.7075 - val_loss: 0.5335 - val_acc: 0.8211

Epoch 00039: val_loss improved from 0.53588 to 0.53353, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 40/40
 - 10s - loss: 0.5947 - acc: 0.7095 - val_loss: 0.5335 - val_acc: 0.8204

Epoch 00040: val_loss improved from 0.53353 to 0.53349, saving model to keras_densenet_simple_wt_29Sept_1404.h5

  32/7440 [..............................] - ETA: 0s
 416/7440 [>.............................] - ETA: 0s
 832/7440 [==>...........................] - ETA: 0s
1248/7440 [====>.........................] - ETA: 0s
1664/7440 [=====>........................] - ETA: 0s
2080/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3328/7440 [============>.................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 129us/step
current Test accuracy: 0.8204301075268817
current auc_score ------------------>  0.8476875939414961

  32/7440 [..............................] - ETA: 11:01
 416/7440 [>.............................] - ETA: 49s  
 832/7440 [==>...........................] - ETA: 23s
1248/7440 [====>.........................] - ETA: 14s
1664/7440 [=====>........................] - ETA: 10s
2080/7440 [=======>......................] - ETA: 8s 
2496/7440 [=========>....................] - ETA: 6s
2912/7440 [==========>...................] - ETA: 5s
3328/7440 [============>.................] - ETA: 4s
3744/7440 [==============>...............] - ETA: 3s
4160/7440 [===============>..............] - ETA: 2s
4576/7440 [=================>............] - ETA: 2s
4992/7440 [===================>..........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 4s 510us/step
Best saved model Test accuracy: 0.8204301075268817
best saved model auc_score ------------------>  0.8476875939414961
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_19 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_133 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_19 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_134 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_19  (None, 8)                 0         
_________________________________________________________________
dense_19 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 17s - loss: 0.6795 - acc: 0.5998 - val_loss: 0.6597 - val_acc: 0.6445

Epoch 00001: val_loss improved from inf to 0.65969, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 10s - loss: 0.6535 - acc: 0.6372 - val_loss: 0.6162 - val_acc: 0.6901

Epoch 00002: val_loss improved from 0.65969 to 0.61615, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 10s - loss: 0.6386 - acc: 0.6628 - val_loss: 0.5886 - val_acc: 0.8063

Epoch 00003: val_loss improved from 0.61615 to 0.58865, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 10s - loss: 0.6310 - acc: 0.6783 - val_loss: 0.5749 - val_acc: 0.8086

Epoch 00004: val_loss improved from 0.58865 to 0.57491, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 10s - loss: 0.6258 - acc: 0.6881 - val_loss: 0.5668 - val_acc: 0.8122

Epoch 00005: val_loss improved from 0.57491 to 0.56678, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 10s - loss: 0.6222 - acc: 0.6923 - val_loss: 0.5617 - val_acc: 0.8125

Epoch 00006: val_loss improved from 0.56678 to 0.56167, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 10s - loss: 0.6190 - acc: 0.6970 - val_loss: 0.5577 - val_acc: 0.8214

Epoch 00007: val_loss improved from 0.56167 to 0.55774, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 10s - loss: 0.6149 - acc: 0.6998 - val_loss: 0.5540 - val_acc: 0.8247

Epoch 00008: val_loss improved from 0.55774 to 0.55399, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 10s - loss: 0.6121 - acc: 0.7025 - val_loss: 0.5506 - val_acc: 0.8152

Epoch 00009: val_loss improved from 0.55399 to 0.55057, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 10s - loss: 0.6095 - acc: 0.7045 - val_loss: 0.5489 - val_acc: 0.8155

Epoch 00010: val_loss improved from 0.55057 to 0.54889, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 10s - loss: 0.6081 - acc: 0.7043 - val_loss: 0.5453 - val_acc: 0.8222

Epoch 00011: val_loss improved from 0.54889 to 0.54526, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 10s - loss: 0.6075 - acc: 0.7064 - val_loss: 0.5457 - val_acc: 0.8181

Epoch 00012: val_loss did not improve from 0.54526
Epoch 13/40
 - 10s - loss: 0.6053 - acc: 0.7044 - val_loss: 0.5429 - val_acc: 0.8188

Epoch 00013: val_loss improved from 0.54526 to 0.54291, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 10s - loss: 0.6045 - acc: 0.7055 - val_loss: 0.5438 - val_acc: 0.8259

Epoch 00014: val_loss did not improve from 0.54291
Epoch 15/40
 - 10s - loss: 0.6023 - acc: 0.7084 - val_loss: 0.5406 - val_acc: 0.8195

Epoch 00015: val_loss improved from 0.54291 to 0.54058, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 10s - loss: 0.6010 - acc: 0.7076 - val_loss: 0.5416 - val_acc: 0.8226

Epoch 00016: val_loss did not improve from 0.54058
Epoch 17/40
 - 10s - loss: 0.5999 - acc: 0.7106 - val_loss: 0.5396 - val_acc: 0.8239

Epoch 00017: val_loss improved from 0.54058 to 0.53960, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 10s - loss: 0.5984 - acc: 0.7099 - val_loss: 0.5404 - val_acc: 0.8190

Epoch 00018: val_loss did not improve from 0.53960
Epoch 19/40
 - 10s - loss: 0.5981 - acc: 0.7107 - val_loss: 0.5413 - val_acc: 0.8273

Epoch 00019: val_loss did not improve from 0.53960
Epoch 20/40
 - 10s - loss: 0.5966 - acc: 0.7110 - val_loss: 0.5400 - val_acc: 0.8202

Epoch 00020: val_loss did not improve from 0.53960
Epoch 21/40
 - 9s - loss: 0.5958 - acc: 0.7106 - val_loss: 0.5373 - val_acc: 0.8255

Epoch 00021: val_loss improved from 0.53960 to 0.53726, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 10s - loss: 0.5930 - acc: 0.7148 - val_loss: 0.5376 - val_acc: 0.8274

Epoch 00022: val_loss did not improve from 0.53726
Epoch 23/40
 - 10s - loss: 0.5926 - acc: 0.7144 - val_loss: 0.5371 - val_acc: 0.8255

Epoch 00023: val_loss improved from 0.53726 to 0.53707, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 10s - loss: 0.5913 - acc: 0.7152 - val_loss: 0.5382 - val_acc: 0.8212

Epoch 00024: val_loss did not improve from 0.53707
Epoch 25/40
 - 10s - loss: 0.5893 - acc: 0.7163 - val_loss: 0.5362 - val_acc: 0.8190

Epoch 00025: val_loss improved from 0.53707 to 0.53622, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 26/40
 - 10s - loss: 0.5889 - acc: 0.7178 - val_loss: 0.5363 - val_acc: 0.8262

Epoch 00026: val_loss did not improve from 0.53622
Epoch 27/40
 - 10s - loss: 0.5880 - acc: 0.7193 - val_loss: 0.5362 - val_acc: 0.8259

Epoch 00027: val_loss did not improve from 0.53622
Epoch 28/40
 - 10s - loss: 0.5871 - acc: 0.7172 - val_loss: 0.5331 - val_acc: 0.8281

Epoch 00028: val_loss improved from 0.53622 to 0.53313, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 29/40
 - 10s - loss: 0.5855 - acc: 0.7182 - val_loss: 0.5302 - val_acc: 0.8274

Epoch 00029: val_loss improved from 0.53313 to 0.53015, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 10s - loss: 0.5835 - acc: 0.7227 - val_loss: 0.5310 - val_acc: 0.8247

Epoch 00030: val_loss did not improve from 0.53015
Epoch 31/40
 - 10s - loss: 0.5818 - acc: 0.7213 - val_loss: 0.5289 - val_acc: 0.8249

Epoch 00031: val_loss improved from 0.53015 to 0.52893, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 32/40
 - 10s - loss: 0.5813 - acc: 0.7226 - val_loss: 0.5307 - val_acc: 0.8181

Epoch 00032: val_loss did not improve from 0.52893
Epoch 33/40
 - 10s - loss: 0.5804 - acc: 0.7241 - val_loss: 0.5264 - val_acc: 0.8278

Epoch 00033: val_loss improved from 0.52893 to 0.52643, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 34/40
 - 10s - loss: 0.5783 - acc: 0.7241 - val_loss: 0.5302 - val_acc: 0.8238

Epoch 00034: val_loss did not improve from 0.52643
Epoch 35/40
 - 10s - loss: 0.5771 - acc: 0.7258 - val_loss: 0.5309 - val_acc: 0.8234

Epoch 00035: val_loss did not improve from 0.52643
Epoch 36/40
 - 10s - loss: 0.5773 - acc: 0.7220 - val_loss: 0.5262 - val_acc: 0.8180

Epoch 00036: val_loss improved from 0.52643 to 0.52619, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 37/40
 - 10s - loss: 0.5759 - acc: 0.7259 - val_loss: 0.5279 - val_acc: 0.8247

Epoch 00037: val_loss did not improve from 0.52619
Epoch 38/40
 - 10s - loss: 0.5739 - acc: 0.7291 - val_loss: 0.5256 - val_acc: 0.8177

Epoch 00038: val_loss improved from 0.52619 to 0.52559, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 39/40
 - 10s - loss: 0.5733 - acc: 0.7269 - val_loss: 0.5224 - val_acc: 0.8249

Epoch 00039: val_loss improved from 0.52559 to 0.52243, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 40/40
 - 10s - loss: 0.5717 - acc: 0.7272 - val_loss: 0.5245 - val_acc: 0.8280

Epoch 00040: val_loss did not improve from 0.52243

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 134us/step
current Test accuracy: 0.8279569892473119
current auc_score ------------------>  0.8676004451381663

  32/7440 [..............................] - ETA: 11:12
 416/7440 [>.............................] - ETA: 49s  
 832/7440 [==>...........................] - ETA: 23s
1248/7440 [====>.........................] - ETA: 15s
1664/7440 [=====>........................] - ETA: 10s
2080/7440 [=======>......................] - ETA: 8s 
2496/7440 [=========>....................] - ETA: 6s
2912/7440 [==========>...................] - ETA: 5s
3328/7440 [============>.................] - ETA: 4s
3744/7440 [==============>...............] - ETA: 3s
4160/7440 [===============>..............] - ETA: 2s
4576/7440 [=================>............] - ETA: 2s
4992/7440 [===================>..........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 4s 516us/step
Best saved model Test accuracy: 0.8248655913978494
best saved model auc_score ------------------>  0.8666598739738697
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_20 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_20[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_135[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_136[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_137[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_138[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 32, 96, 96)   0           concatenate_49[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_139[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_140[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 40, 96, 96)   0           concatenate_50[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_141[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_142[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_143[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 28, 48, 48)   0           average_pooling2d_20[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_144[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_145[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 36, 48, 48)   0           concatenate_52[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_146[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_147[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 44, 48, 48)   0           concatenate_53[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_20 (Gl (None, 44)           0           activation_148[0][0]             
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 1)            45          global_average_pooling2d_20[0][0]
==================================================================================================
Total params: 21,677
Trainable params: 20,813
Non-trainable params: 864
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 46s - loss: 0.6483 - acc: 0.7059 - val_loss: 0.5624 - val_acc: 0.8336

Epoch 00001: val_loss improved from inf to 0.56236, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 37s - loss: 0.5969 - acc: 0.7462 - val_loss: 0.5474 - val_acc: 0.8391

Epoch 00002: val_loss improved from 0.56236 to 0.54743, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 37s - loss: 0.5667 - acc: 0.7585 - val_loss: 0.5353 - val_acc: 0.8297

Epoch 00003: val_loss improved from 0.54743 to 0.53531, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 37s - loss: 0.5505 - acc: 0.7674 - val_loss: 0.5297 - val_acc: 0.8204

Epoch 00004: val_loss improved from 0.53531 to 0.52966, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 37s - loss: 0.5403 - acc: 0.7692 - val_loss: 0.5219 - val_acc: 0.8223

Epoch 00005: val_loss improved from 0.52966 to 0.52187, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 37s - loss: 0.5313 - acc: 0.7717 - val_loss: 0.5271 - val_acc: 0.8079

Epoch 00006: val_loss did not improve from 0.52187
Epoch 7/40
 - 37s - loss: 0.5233 - acc: 0.7745 - val_loss: 0.5347 - val_acc: 0.7988

Epoch 00007: val_loss did not improve from 0.52187
Epoch 8/40
 - 37s - loss: 0.5171 - acc: 0.7744 - val_loss: 0.5246 - val_acc: 0.8187

Epoch 00008: val_loss did not improve from 0.52187
Epoch 9/40
 - 37s - loss: 0.5128 - acc: 0.7763 - val_loss: 0.5425 - val_acc: 0.8040

Epoch 00009: val_loss did not improve from 0.52187
Epoch 10/40
 - 37s - loss: 0.5060 - acc: 0.7796 - val_loss: 0.5209 - val_acc: 0.8091

Epoch 00010: val_loss improved from 0.52187 to 0.52088, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 37s - loss: 0.5013 - acc: 0.7817 - val_loss: 0.5277 - val_acc: 0.7983

Epoch 00011: val_loss did not improve from 0.52088
Epoch 12/40
 - 37s - loss: 0.4966 - acc: 0.7831 - val_loss: 0.5339 - val_acc: 0.7941

Epoch 00012: val_loss did not improve from 0.52088
Epoch 13/40
 - 37s - loss: 0.4918 - acc: 0.7855 - val_loss: 0.5283 - val_acc: 0.7993

Epoch 00013: val_loss did not improve from 0.52088
Epoch 14/40
 - 37s - loss: 0.4884 - acc: 0.7871 - val_loss: 0.5472 - val_acc: 0.8079

Epoch 00014: val_loss did not improve from 0.52088
Epoch 15/40
 - 37s - loss: 0.4838 - acc: 0.7876 - val_loss: 0.5426 - val_acc: 0.7853

Epoch 00015: val_loss did not improve from 0.52088
Epoch 16/40
 - 37s - loss: 0.4799 - acc: 0.7921 - val_loss: 0.5308 - val_acc: 0.7929

Epoch 00016: val_loss did not improve from 0.52088
Epoch 17/40
 - 37s - loss: 0.4775 - acc: 0.7933 - val_loss: 0.5645 - val_acc: 0.8011

Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00017: val_loss did not improve from 0.52088
Epoch 00017: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 320us/step
current Test accuracy: 0.8010752688172043
current auc_score ------------------>  0.8562118814313794

  32/7440 [..............................] - ETA: 13:15
 192/7440 [..............................] - ETA: 2:11 
 352/7440 [>.............................] - ETA: 1:11
 512/7440 [=>............................] - ETA: 48s 
 672/7440 [=>............................] - ETA: 36s
 832/7440 [==>...........................] - ETA: 29s
 992/7440 [===>..........................] - ETA: 24s
1152/7440 [===>..........................] - ETA: 20s
1312/7440 [====>.........................] - ETA: 18s
1472/7440 [====>.........................] - ETA: 15s
1632/7440 [=====>........................] - ETA: 14s
1792/7440 [======>.......................] - ETA: 12s
1952/7440 [======>.......................] - ETA: 11s
2112/7440 [=======>......................] - ETA: 10s
2272/7440 [========>.....................] - ETA: 9s 
2432/7440 [========>.....................] - ETA: 8s
2592/7440 [=========>....................] - ETA: 8s
2752/7440 [==========>...................] - ETA: 7s
2912/7440 [==========>...................] - ETA: 6s
3072/7440 [===========>..................] - ETA: 6s
3232/7440 [============>.................] - ETA: 5s
3392/7440 [============>.................] - ETA: 5s
3552/7440 [=============>................] - ETA: 5s
3712/7440 [=============>................] - ETA: 4s
3872/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 4s
4192/7440 [===============>..............] - ETA: 3s
4352/7440 [================>.............] - ETA: 3s
4512/7440 [=================>............] - ETA: 3s
4672/7440 [=================>............] - ETA: 2s
4832/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5312/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 794us/step
Best saved model Test accuracy: 0.8091397849462365
best saved model auc_score ------------------>  0.8752563880217366
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_21[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_149[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_150[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_151[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_152[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 28, 96, 96)   0           concatenate_55[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_153[0][0]             
__________________________________________________________________________________________________
average_pooling2d_21 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_21[0][0]       
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   336         activation_154[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_155[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_21[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 20, 48, 48)   80          concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 20, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   480         activation_156[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_157[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 26, 48, 48)   0           concatenate_57[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_21 (Gl (None, 26)           0           activation_158[0][0]             
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 1)            27          global_average_pooling2d_21[0][0]
==================================================================================================
Total params: 8,507
Trainable params: 8,063
Non-trainable params: 444
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 33s - loss: 0.6657 - acc: 0.6284 - val_loss: 0.5708 - val_acc: 0.7837

Epoch 00001: val_loss improved from inf to 0.57081, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 23s - loss: 0.5927 - acc: 0.7412 - val_loss: 0.5278 - val_acc: 0.8399

Epoch 00002: val_loss improved from 0.57081 to 0.52784, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 23s - loss: 0.5696 - acc: 0.7477 - val_loss: 0.5182 - val_acc: 0.8310

Epoch 00003: val_loss improved from 0.52784 to 0.51825, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 23s - loss: 0.5577 - acc: 0.7508 - val_loss: 0.5120 - val_acc: 0.8339

Epoch 00004: val_loss improved from 0.51825 to 0.51202, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 23s - loss: 0.5485 - acc: 0.7543 - val_loss: 0.5073 - val_acc: 0.8310

Epoch 00005: val_loss improved from 0.51202 to 0.50726, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 23s - loss: 0.5412 - acc: 0.7543 - val_loss: 0.5103 - val_acc: 0.8233

Epoch 00006: val_loss did not improve from 0.50726
Epoch 7/40
 - 23s - loss: 0.5341 - acc: 0.7566 - val_loss: 0.5067 - val_acc: 0.8270

Epoch 00007: val_loss improved from 0.50726 to 0.50669, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 23s - loss: 0.5276 - acc: 0.7584 - val_loss: 0.5093 - val_acc: 0.8210

Epoch 00008: val_loss did not improve from 0.50669
Epoch 9/40
 - 23s - loss: 0.5231 - acc: 0.7612 - val_loss: 0.4990 - val_acc: 0.8148

Epoch 00009: val_loss improved from 0.50669 to 0.49899, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 23s - loss: 0.5183 - acc: 0.7633 - val_loss: 0.4932 - val_acc: 0.8118

Epoch 00010: val_loss improved from 0.49899 to 0.49325, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 24s - loss: 0.5143 - acc: 0.7650 - val_loss: 0.4952 - val_acc: 0.8120

Epoch 00011: val_loss did not improve from 0.49325
Epoch 12/40
 - 24s - loss: 0.5090 - acc: 0.7668 - val_loss: 0.4943 - val_acc: 0.8110

Epoch 00012: val_loss did not improve from 0.49325
Epoch 13/40
 - 24s - loss: 0.5061 - acc: 0.7676 - val_loss: 0.5017 - val_acc: 0.8091

Epoch 00013: val_loss did not improve from 0.49325
Epoch 14/40
 - 24s - loss: 0.5020 - acc: 0.7713 - val_loss: 0.4945 - val_acc: 0.8089

Epoch 00014: val_loss did not improve from 0.49325
Epoch 15/40
 - 23s - loss: 0.4998 - acc: 0.7700 - val_loss: 0.4968 - val_acc: 0.8035

Epoch 00015: val_loss did not improve from 0.49325
Epoch 16/40
 - 23s - loss: 0.4969 - acc: 0.7710 - val_loss: 0.4960 - val_acc: 0.8035

Epoch 00016: val_loss did not improve from 0.49325
Epoch 17/40
 - 23s - loss: 0.4928 - acc: 0.7729 - val_loss: 0.4968 - val_acc: 0.7996

Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00017: val_loss did not improve from 0.49325
Epoch 00017: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 235us/step
current Test accuracy: 0.7995967741935484
current auc_score ------------------>  0.8549959894207423

  32/7440 [..............................] - ETA: 14:24
 224/7440 [..............................] - ETA: 2:01 
 448/7440 [>.............................] - ETA: 59s 
 672/7440 [=>............................] - ETA: 39s
 896/7440 [==>...........................] - ETA: 28s
1120/7440 [===>..........................] - ETA: 22s
1344/7440 [====>.........................] - ETA: 18s
1568/7440 [=====>........................] - ETA: 15s
1792/7440 [======>.......................] - ETA: 13s
2016/7440 [=======>......................] - ETA: 11s
2240/7440 [========>.....................] - ETA: 9s 
2464/7440 [========>.....................] - ETA: 8s
2688/7440 [=========>....................] - ETA: 7s
2912/7440 [==========>...................] - ETA: 6s
3136/7440 [===========>..................] - ETA: 6s
3360/7440 [============>.................] - ETA: 5s
3584/7440 [=============>................] - ETA: 4s
3808/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 3s
4256/7440 [================>.............] - ETA: 3s
4480/7440 [=================>............] - ETA: 3s
4704/7440 [=================>............] - ETA: 2s
4928/7440 [==================>...........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 745us/step
Best saved model Test accuracy: 0.8118279569892473
best saved model auc_score ------------------>  0.8619084937565036
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_22 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_22[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_159[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_161[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_162[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 40, 96, 96)   0           concatenate_59[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_163[0][0]             
__________________________________________________________________________________________________
average_pooling2d_22 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_22[0][0]       
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_164[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_22[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_166[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 44, 48, 48)   0           concatenate_61[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_22 (Gl (None, 44)           0           activation_168[0][0]             
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 1)            45          global_average_pooling2d_22[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 41s - loss: 0.6339 - acc: 0.7078 - val_loss: 0.5416 - val_acc: 0.8367

Epoch 00001: val_loss improved from inf to 0.54158, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 30s - loss: 0.5778 - acc: 0.7535 - val_loss: 0.5292 - val_acc: 0.8349

Epoch 00002: val_loss improved from 0.54158 to 0.52923, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 30s - loss: 0.5541 - acc: 0.7596 - val_loss: 0.5088 - val_acc: 0.8270

Epoch 00003: val_loss improved from 0.52923 to 0.50877, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 30s - loss: 0.5395 - acc: 0.7654 - val_loss: 0.5219 - val_acc: 0.8246

Epoch 00004: val_loss did not improve from 0.50877
Epoch 5/40
 - 30s - loss: 0.5285 - acc: 0.7697 - val_loss: 0.5047 - val_acc: 0.8161

Epoch 00005: val_loss improved from 0.50877 to 0.50466, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 31s - loss: 0.5206 - acc: 0.7718 - val_loss: 0.5064 - val_acc: 0.8055

Epoch 00006: val_loss did not improve from 0.50466
Epoch 7/40
 - 30s - loss: 0.5112 - acc: 0.7743 - val_loss: 0.5038 - val_acc: 0.8195

Epoch 00007: val_loss improved from 0.50466 to 0.50381, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 30s - loss: 0.5044 - acc: 0.7798 - val_loss: 0.4880 - val_acc: 0.8183

Epoch 00008: val_loss improved from 0.50381 to 0.48799, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 30s - loss: 0.4988 - acc: 0.7823 - val_loss: 0.4882 - val_acc: 0.8116

Epoch 00009: val_loss did not improve from 0.48799
Epoch 10/40
 - 30s - loss: 0.4914 - acc: 0.7872 - val_loss: 0.4882 - val_acc: 0.8196

Epoch 00010: val_loss did not improve from 0.48799
Epoch 11/40
 - 30s - loss: 0.4859 - acc: 0.7885 - val_loss: 0.4880 - val_acc: 0.8194

Epoch 00011: val_loss improved from 0.48799 to 0.48795, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 31s - loss: 0.4817 - acc: 0.7932 - val_loss: 0.4945 - val_acc: 0.8165

Epoch 00012: val_loss did not improve from 0.48795
Epoch 13/40
 - 30s - loss: 0.4761 - acc: 0.7957 - val_loss: 0.4691 - val_acc: 0.8276

Epoch 00013: val_loss improved from 0.48795 to 0.46913, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 31s - loss: 0.4736 - acc: 0.7966 - val_loss: 0.4880 - val_acc: 0.8218

Epoch 00014: val_loss did not improve from 0.46913
Epoch 15/40
 - 30s - loss: 0.4690 - acc: 0.7969 - val_loss: 0.4754 - val_acc: 0.8228

Epoch 00015: val_loss did not improve from 0.46913
Epoch 16/40
 - 31s - loss: 0.4643 - acc: 0.8018 - val_loss: 0.4872 - val_acc: 0.8138

Epoch 00016: val_loss did not improve from 0.46913
Epoch 17/40
 - 30s - loss: 0.4620 - acc: 0.8022 - val_loss: 0.4675 - val_acc: 0.8177

Epoch 00017: val_loss improved from 0.46913 to 0.46752, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 30s - loss: 0.4578 - acc: 0.8053 - val_loss: 0.4936 - val_acc: 0.8060

Epoch 00018: val_loss did not improve from 0.46752
Epoch 19/40
 - 30s - loss: 0.4581 - acc: 0.8025 - val_loss: 0.4882 - val_acc: 0.8159

Epoch 00019: val_loss did not improve from 0.46752
Epoch 20/40
 - 31s - loss: 0.4530 - acc: 0.8065 - val_loss: 0.4721 - val_acc: 0.8157

Epoch 00020: val_loss did not improve from 0.46752
Epoch 21/40
 - 31s - loss: 0.4511 - acc: 0.8066 - val_loss: 0.4652 - val_acc: 0.8290

Epoch 00021: val_loss improved from 0.46752 to 0.46520, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 31s - loss: 0.4477 - acc: 0.8086 - val_loss: 0.4846 - val_acc: 0.8233

Epoch 00022: val_loss did not improve from 0.46520
Epoch 23/40
 - 30s - loss: 0.4451 - acc: 0.8095 - val_loss: 0.4801 - val_acc: 0.7996

Epoch 00023: val_loss did not improve from 0.46520
Epoch 24/40
 - 30s - loss: 0.4442 - acc: 0.8108 - val_loss: 0.4622 - val_acc: 0.8277

Epoch 00024: val_loss improved from 0.46520 to 0.46221, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 31s - loss: 0.4412 - acc: 0.8132 - val_loss: 0.4888 - val_acc: 0.8262

Epoch 00025: val_loss did not improve from 0.46221
Epoch 26/40
 - 30s - loss: 0.4403 - acc: 0.8125 - val_loss: 0.4549 - val_acc: 0.8356

Epoch 00026: val_loss improved from 0.46221 to 0.45493, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 30s - loss: 0.4354 - acc: 0.8152 - val_loss: 0.4593 - val_acc: 0.8144

Epoch 00027: val_loss did not improve from 0.45493
Epoch 28/40
 - 30s - loss: 0.4350 - acc: 0.8144 - val_loss: 0.4762 - val_acc: 0.8124

Epoch 00028: val_loss did not improve from 0.45493
Epoch 29/40
 - 30s - loss: 0.4312 - acc: 0.8171 - val_loss: 0.4470 - val_acc: 0.8165

Epoch 00029: val_loss improved from 0.45493 to 0.44701, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 31s - loss: 0.4313 - acc: 0.8176 - val_loss: 0.4659 - val_acc: 0.8124

Epoch 00030: val_loss did not improve from 0.44701
Epoch 31/40
 - 31s - loss: 0.4295 - acc: 0.8164 - val_loss: 0.4789 - val_acc: 0.8130

Epoch 00031: val_loss did not improve from 0.44701
Epoch 32/40
 - 31s - loss: 0.4249 - acc: 0.8217 - val_loss: 0.4678 - val_acc: 0.8106

Epoch 00032: val_loss did not improve from 0.44701
Epoch 33/40
 - 31s - loss: 0.4244 - acc: 0.8205 - val_loss: 0.4836 - val_acc: 0.8173

Epoch 00033: val_loss did not improve from 0.44701
Epoch 34/40
 - 31s - loss: 0.4228 - acc: 0.8217 - val_loss: 0.4922 - val_acc: 0.8128

Epoch 00034: val_loss did not improve from 0.44701
Epoch 35/40
 - 31s - loss: 0.4212 - acc: 0.8228 - val_loss: 0.4720 - val_acc: 0.8239

Epoch 00035: val_loss did not improve from 0.44701
Epoch 36/40
 - 30s - loss: 0.4198 - acc: 0.8235 - val_loss: 0.4973 - val_acc: 0.8013

Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00036: val_loss did not improve from 0.44701
Epoch 00036: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 384/7440 [>.............................] - ETA: 2s
 576/7440 [=>............................] - ETA: 2s
 768/7440 [==>...........................] - ETA: 2s
 960/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1344/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1728/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2688/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3456/7440 [============>.................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5568/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 294us/step
current Test accuracy: 0.8013440860215054
current auc_score ------------------>  0.8876221239449646

  32/7440 [..............................] - ETA: 15:26
 192/7440 [..............................] - ETA: 2:33 
 384/7440 [>.............................] - ETA: 1:15
 576/7440 [=>............................] - ETA: 49s 
 768/7440 [==>...........................] - ETA: 36s
 960/7440 [==>...........................] - ETA: 28s
1152/7440 [===>..........................] - ETA: 23s
1344/7440 [====>.........................] - ETA: 19s
1536/7440 [=====>........................] - ETA: 17s
1728/7440 [=====>........................] - ETA: 14s
1920/7440 [======>.......................] - ETA: 13s
2112/7440 [=======>......................] - ETA: 11s
2304/7440 [========>.....................] - ETA: 10s
2496/7440 [=========>....................] - ETA: 9s 
2688/7440 [=========>....................] - ETA: 8s
2880/7440 [==========>...................] - ETA: 7s
3072/7440 [===========>..................] - ETA: 6s
3264/7440 [============>.................] - ETA: 6s
3456/7440 [============>.................] - ETA: 5s
3648/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 4s
4224/7440 [================>.............] - ETA: 3s
4416/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 829us/step
Best saved model Test accuracy: 0.8165322580645161
best saved model auc_score ------------------>  0.8971771303040813
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_23[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_169[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_170[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 40, 96, 96)   0           concatenate_63[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_173[0][0]             
__________________________________________________________________________________________________
average_pooling2d_23 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_23[0][0]       
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_174[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_175[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_23[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_176[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_177[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 44, 48, 48)   0           concatenate_65[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_23 (Gl (None, 44)           0           activation_178[0][0]             
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 1)            45          global_average_pooling2d_23[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 42s - loss: 0.6561 - acc: 0.6716 - val_loss: 0.5620 - val_acc: 0.8372

Epoch 00001: val_loss improved from inf to 0.56201, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 30s - loss: 0.5904 - acc: 0.7463 - val_loss: 0.5348 - val_acc: 0.8278

Epoch 00002: val_loss improved from 0.56201 to 0.53479, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 31s - loss: 0.5631 - acc: 0.7587 - val_loss: 0.5357 - val_acc: 0.8212

Epoch 00003: val_loss did not improve from 0.53479
Epoch 4/40
 - 30s - loss: 0.5433 - acc: 0.7653 - val_loss: 0.5121 - val_acc: 0.8203

Epoch 00004: val_loss improved from 0.53479 to 0.51205, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 30s - loss: 0.5324 - acc: 0.7699 - val_loss: 0.5106 - val_acc: 0.8078

Epoch 00005: val_loss improved from 0.51205 to 0.51057, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 30s - loss: 0.5206 - acc: 0.7757 - val_loss: 0.5163 - val_acc: 0.8040

Epoch 00006: val_loss did not improve from 0.51057
Epoch 7/40
 - 31s - loss: 0.5131 - acc: 0.7771 - val_loss: 0.4955 - val_acc: 0.8027

Epoch 00007: val_loss improved from 0.51057 to 0.49549, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 30s - loss: 0.5046 - acc: 0.7810 - val_loss: 0.4982 - val_acc: 0.8250

Epoch 00008: val_loss did not improve from 0.49549
Epoch 9/40
 - 31s - loss: 0.4996 - acc: 0.7834 - val_loss: 0.4917 - val_acc: 0.8101

Epoch 00009: val_loss improved from 0.49549 to 0.49170, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 31s - loss: 0.4933 - acc: 0.7842 - val_loss: 0.4828 - val_acc: 0.8155

Epoch 00010: val_loss improved from 0.49170 to 0.48281, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 30s - loss: 0.4883 - acc: 0.7861 - val_loss: 0.4797 - val_acc: 0.8183

Epoch 00011: val_loss improved from 0.48281 to 0.47966, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 30s - loss: 0.4807 - acc: 0.7895 - val_loss: 0.4752 - val_acc: 0.8210

Epoch 00012: val_loss improved from 0.47966 to 0.47519, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 31s - loss: 0.4769 - acc: 0.7933 - val_loss: 0.4538 - val_acc: 0.8305

Epoch 00013: val_loss improved from 0.47519 to 0.45382, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 30s - loss: 0.4728 - acc: 0.7934 - val_loss: 0.4581 - val_acc: 0.8230

Epoch 00014: val_loss did not improve from 0.45382
Epoch 15/40
 - 31s - loss: 0.4689 - acc: 0.7940 - val_loss: 0.4648 - val_acc: 0.8176

Epoch 00015: val_loss did not improve from 0.45382
Epoch 16/40
 - 31s - loss: 0.4645 - acc: 0.7987 - val_loss: 0.4348 - val_acc: 0.8403

Epoch 00016: val_loss improved from 0.45382 to 0.43484, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 30s - loss: 0.4591 - acc: 0.8011 - val_loss: 0.4754 - val_acc: 0.8101

Epoch 00017: val_loss did not improve from 0.43484
Epoch 18/40
 - 31s - loss: 0.4553 - acc: 0.8035 - val_loss: 0.4378 - val_acc: 0.8493

Epoch 00018: val_loss did not improve from 0.43484
Epoch 19/40
 - 31s - loss: 0.4522 - acc: 0.8043 - val_loss: 0.4303 - val_acc: 0.8434

Epoch 00019: val_loss improved from 0.43484 to 0.43031, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 31s - loss: 0.4486 - acc: 0.8063 - val_loss: 0.4378 - val_acc: 0.8267

Epoch 00020: val_loss did not improve from 0.43031
Epoch 21/40
 - 31s - loss: 0.4456 - acc: 0.8076 - val_loss: 0.4464 - val_acc: 0.8379

Epoch 00021: val_loss did not improve from 0.43031
Epoch 22/40
 - 30s - loss: 0.4405 - acc: 0.8118 - val_loss: 0.4399 - val_acc: 0.8469

Epoch 00022: val_loss did not improve from 0.43031
Epoch 23/40
 - 31s - loss: 0.4393 - acc: 0.8106 - val_loss: 0.4441 - val_acc: 0.8454

Epoch 00023: val_loss did not improve from 0.43031
Epoch 24/40
 - 30s - loss: 0.4353 - acc: 0.8146 - val_loss: 0.4415 - val_acc: 0.8488

Epoch 00024: val_loss did not improve from 0.43031
Epoch 25/40
 - 31s - loss: 0.4325 - acc: 0.8156 - val_loss: 0.4925 - val_acc: 0.8188

Epoch 00025: val_loss did not improve from 0.43031
Epoch 26/40
 - 31s - loss: 0.4319 - acc: 0.8141 - val_loss: 0.4625 - val_acc: 0.8011

Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00026: val_loss did not improve from 0.43031
Epoch 00026: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
1024/7440 [===>..........................] - ETA: 2s
1216/7440 [===>..........................] - ETA: 1s
1408/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1984/7440 [=======>......................] - ETA: 1s
2176/7440 [=======>......................] - ETA: 1s
2368/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3136/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 309us/step
current Test accuracy: 0.8010752688172043
current auc_score ------------------>  0.8852700456700195

  32/7440 [..............................] - ETA: 17:01
 192/7440 [..............................] - ETA: 2:48 
 384/7440 [>.............................] - ETA: 1:23
 576/7440 [=>............................] - ETA: 54s 
 768/7440 [==>...........................] - ETA: 40s
 960/7440 [==>...........................] - ETA: 31s
1152/7440 [===>..........................] - ETA: 25s
1344/7440 [====>.........................] - ETA: 21s
1536/7440 [=====>........................] - ETA: 18s
1728/7440 [=====>........................] - ETA: 16s
1920/7440 [======>.......................] - ETA: 14s
2112/7440 [=======>......................] - ETA: 12s
2304/7440 [========>.....................] - ETA: 11s
2496/7440 [=========>....................] - ETA: 10s
2688/7440 [=========>....................] - ETA: 9s 
2880/7440 [==========>...................] - ETA: 8s
3072/7440 [===========>..................] - ETA: 7s
3264/7440 [============>.................] - ETA: 6s
3456/7440 [============>.................] - ETA: 6s
3648/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 5s
4032/7440 [===============>..............] - ETA: 4s
4224/7440 [================>.............] - ETA: 4s
4416/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 891us/step
Best saved model Test accuracy: 0.8434139784946236
best saved model auc_score ------------------>  0.9104298184761245
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_24 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_24[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_179[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_180[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_181[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_182[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 40, 96, 96)   0           concatenate_67[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_183[0][0]             
__________________________________________________________________________________________________
average_pooling2d_24 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_24[0][0]       
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_184[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_185[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_24[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_186[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_187[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 44, 48, 48)   0           concatenate_69[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_24 (Gl (None, 44)           0           activation_188[0][0]             
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 1)            45          global_average_pooling2d_24[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 43s - loss: 0.6496 - acc: 0.6901 - val_loss: 0.5691 - val_acc: 0.8351

Epoch 00001: val_loss improved from inf to 0.56912, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 31s - loss: 0.5934 - acc: 0.7450 - val_loss: 0.5362 - val_acc: 0.8485

Epoch 00002: val_loss improved from 0.56912 to 0.53624, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 31s - loss: 0.5617 - acc: 0.7583 - val_loss: 0.5456 - val_acc: 0.8226

Epoch 00003: val_loss did not improve from 0.53624
Epoch 4/40
 - 31s - loss: 0.5455 - acc: 0.7633 - val_loss: 0.5264 - val_acc: 0.8128

Epoch 00004: val_loss improved from 0.53624 to 0.52638, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 31s - loss: 0.5342 - acc: 0.7669 - val_loss: 0.5316 - val_acc: 0.7973

Epoch 00005: val_loss did not improve from 0.52638
Epoch 6/40
 - 31s - loss: 0.5243 - acc: 0.7721 - val_loss: 0.4966 - val_acc: 0.8243

Epoch 00006: val_loss improved from 0.52638 to 0.49662, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 31s - loss: 0.5150 - acc: 0.7753 - val_loss: 0.5025 - val_acc: 0.8192

Epoch 00007: val_loss did not improve from 0.49662
Epoch 8/40
 - 31s - loss: 0.5092 - acc: 0.7774 - val_loss: 0.4994 - val_acc: 0.8075

Epoch 00008: val_loss did not improve from 0.49662
Epoch 9/40
 - 31s - loss: 0.5038 - acc: 0.7798 - val_loss: 0.4969 - val_acc: 0.8116

Epoch 00009: val_loss did not improve from 0.49662
Epoch 10/40
 - 31s - loss: 0.4988 - acc: 0.7802 - val_loss: 0.4861 - val_acc: 0.8176

Epoch 00010: val_loss improved from 0.49662 to 0.48608, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 31s - loss: 0.4944 - acc: 0.7817 - val_loss: 0.4744 - val_acc: 0.8242

Epoch 00011: val_loss improved from 0.48608 to 0.47444, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 31s - loss: 0.4892 - acc: 0.7849 - val_loss: 0.4783 - val_acc: 0.8136

Epoch 00012: val_loss did not improve from 0.47444
Epoch 13/40
 - 31s - loss: 0.4836 - acc: 0.7889 - val_loss: 0.4810 - val_acc: 0.8167

Epoch 00013: val_loss did not improve from 0.47444
Epoch 14/40
 - 31s - loss: 0.4796 - acc: 0.7898 - val_loss: 0.5048 - val_acc: 0.7863

Epoch 00014: val_loss did not improve from 0.47444
Epoch 15/40
 - 31s - loss: 0.4759 - acc: 0.7919 - val_loss: 0.4691 - val_acc: 0.8159

Epoch 00015: val_loss improved from 0.47444 to 0.46906, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 31s - loss: 0.4700 - acc: 0.7964 - val_loss: 0.4891 - val_acc: 0.8047

Epoch 00016: val_loss did not improve from 0.46906
Epoch 17/40
 - 31s - loss: 0.4673 - acc: 0.7955 - val_loss: 0.5049 - val_acc: 0.8095

Epoch 00017: val_loss did not improve from 0.46906
Epoch 18/40
 - 31s - loss: 0.4638 - acc: 0.7961 - val_loss: 0.4666 - val_acc: 0.8223

Epoch 00018: val_loss improved from 0.46906 to 0.46656, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 31s - loss: 0.4619 - acc: 0.7986 - val_loss: 0.4725 - val_acc: 0.8083

Epoch 00019: val_loss did not improve from 0.46656
Epoch 20/40
 - 31s - loss: 0.4570 - acc: 0.8003 - val_loss: 0.4891 - val_acc: 0.8108

Epoch 00020: val_loss did not improve from 0.46656
Epoch 21/40
 - 31s - loss: 0.4543 - acc: 0.8032 - val_loss: 0.4637 - val_acc: 0.8204

Epoch 00021: val_loss improved from 0.46656 to 0.46371, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 31s - loss: 0.4499 - acc: 0.8054 - val_loss: 0.4599 - val_acc: 0.8218

Epoch 00022: val_loss improved from 0.46371 to 0.45986, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 31s - loss: 0.4490 - acc: 0.8047 - val_loss: 0.5608 - val_acc: 0.7888

Epoch 00023: val_loss did not improve from 0.45986
Epoch 24/40
 - 31s - loss: 0.4449 - acc: 0.8054 - val_loss: 0.4892 - val_acc: 0.8210

Epoch 00024: val_loss did not improve from 0.45986
Epoch 25/40
 - 31s - loss: 0.4427 - acc: 0.8086 - val_loss: 0.4589 - val_acc: 0.8134

Epoch 00025: val_loss improved from 0.45986 to 0.45891, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 26/40
 - 31s - loss: 0.4416 - acc: 0.8089 - val_loss: 0.4517 - val_acc: 0.8149

Epoch 00026: val_loss improved from 0.45891 to 0.45165, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 32s - loss: 0.4379 - acc: 0.8104 - val_loss: 0.4425 - val_acc: 0.8308

Epoch 00027: val_loss improved from 0.45165 to 0.44248, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 28/40
 - 31s - loss: 0.4338 - acc: 0.8136 - val_loss: 0.4699 - val_acc: 0.8196

Epoch 00028: val_loss did not improve from 0.44248
Epoch 29/40
 - 31s - loss: 0.4355 - acc: 0.8135 - val_loss: 0.4553 - val_acc: 0.8276

Epoch 00029: val_loss did not improve from 0.44248
Epoch 30/40
 - 31s - loss: 0.4316 - acc: 0.8150 - val_loss: 0.4483 - val_acc: 0.8222

Epoch 00030: val_loss did not improve from 0.44248
Epoch 31/40
 - 31s - loss: 0.4293 - acc: 0.8153 - val_loss: 0.4391 - val_acc: 0.8308

Epoch 00031: val_loss improved from 0.44248 to 0.43914, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 32/40
 - 31s - loss: 0.4283 - acc: 0.8171 - val_loss: 0.4529 - val_acc: 0.8207

Epoch 00032: val_loss did not improve from 0.43914
Epoch 33/40
 - 31s - loss: 0.4260 - acc: 0.8179 - val_loss: 0.4552 - val_acc: 0.8121

Epoch 00033: val_loss did not improve from 0.43914
Epoch 34/40
 - 31s - loss: 0.4233 - acc: 0.8185 - val_loss: 0.4406 - val_acc: 0.8349

Epoch 00034: val_loss did not improve from 0.43914
Epoch 35/40
 - 31s - loss: 0.4227 - acc: 0.8197 - val_loss: 0.4464 - val_acc: 0.8313

Epoch 00035: val_loss did not improve from 0.43914
Epoch 36/40
 - 31s - loss: 0.4182 - acc: 0.8224 - val_loss: 0.4462 - val_acc: 0.8363

Epoch 00036: val_loss did not improve from 0.43914
Epoch 37/40
 - 31s - loss: 0.4179 - acc: 0.8215 - val_loss: 0.4596 - val_acc: 0.8301

Epoch 00037: val_loss did not improve from 0.43914
Epoch 38/40
 - 31s - loss: 0.4168 - acc: 0.8246 - val_loss: 0.4795 - val_acc: 0.7973

Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00038: val_loss did not improve from 0.43914
Epoch 00038: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 1s
1504/7440 [=====>........................] - ETA: 1s
1664/7440 [=====>........................] - ETA: 1s
1856/7440 [======>.......................] - ETA: 1s
2016/7440 [=======>......................] - ETA: 1s
2208/7440 [=======>......................] - ETA: 1s
2368/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3424/7440 [============>.................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4384/7440 [================>.............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 313us/step
current Test accuracy: 0.7973118279569893
current auc_score ------------------>  0.8766187203722973

  32/7440 [..............................] - ETA: 18:44
 192/7440 [..............................] - ETA: 3:05 
 384/7440 [>.............................] - ETA: 1:31
 576/7440 [=>............................] - ETA: 59s 
 768/7440 [==>...........................] - ETA: 44s
 960/7440 [==>...........................] - ETA: 34s
1152/7440 [===>..........................] - ETA: 28s
1344/7440 [====>.........................] - ETA: 23s
1536/7440 [=====>........................] - ETA: 20s
1728/7440 [=====>........................] - ETA: 17s
1920/7440 [======>.......................] - ETA: 15s
2112/7440 [=======>......................] - ETA: 13s
2304/7440 [========>.....................] - ETA: 12s
2496/7440 [=========>....................] - ETA: 11s
2688/7440 [=========>....................] - ETA: 10s
2880/7440 [==========>...................] - ETA: 9s 
3072/7440 [===========>..................] - ETA: 8s
3264/7440 [============>.................] - ETA: 7s
3456/7440 [============>.................] - ETA: 6s
3648/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
4032/7440 [===============>..............] - ETA: 5s
4224/7440 [================>.............] - ETA: 4s
4416/7440 [================>.............] - ETA: 4s
4608/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 957us/step
Best saved model Test accuracy: 0.8307795698924731
best saved model auc_score ------------------>  0.8979545323158746
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_25[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_189 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_189[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_190 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_190[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_191 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_191[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_192 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_192[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 40, 96, 96)   0           concatenate_71[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_193[0][0]             
__________________________________________________________________________________________________
average_pooling2d_25 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_25[0][0]       
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_194[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_195[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_25[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_196[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_197[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 44, 48, 48)   0           concatenate_73[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_25 (Gl (None, 44)           0           activation_198[0][0]             
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 1)            45          global_average_pooling2d_25[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 44s - loss: 0.6407 - acc: 0.7105 - val_loss: 0.5564 - val_acc: 0.8445

Epoch 00001: val_loss improved from inf to 0.55642, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 31s - loss: 0.5898 - acc: 0.7450 - val_loss: 0.5313 - val_acc: 0.8262

Epoch 00002: val_loss improved from 0.55642 to 0.53128, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 31s - loss: 0.5632 - acc: 0.7553 - val_loss: 0.5204 - val_acc: 0.8200

Epoch 00003: val_loss improved from 0.53128 to 0.52042, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 31s - loss: 0.5461 - acc: 0.7647 - val_loss: 0.5374 - val_acc: 0.7919

Epoch 00004: val_loss did not improve from 0.52042
Epoch 5/40
 - 31s - loss: 0.5347 - acc: 0.7706 - val_loss: 0.5170 - val_acc: 0.8013

Epoch 00005: val_loss improved from 0.52042 to 0.51703, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 31s - loss: 0.5224 - acc: 0.7757 - val_loss: 0.5170 - val_acc: 0.7929

Epoch 00006: val_loss improved from 0.51703 to 0.51698, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 31s - loss: 0.5135 - acc: 0.7795 - val_loss: 0.5276 - val_acc: 0.7891

Epoch 00007: val_loss did not improve from 0.51698
Epoch 8/40
 - 31s - loss: 0.5069 - acc: 0.7814 - val_loss: 0.5387 - val_acc: 0.7793

Epoch 00008: val_loss did not improve from 0.51698
Epoch 9/40
 - 31s - loss: 0.4997 - acc: 0.7849 - val_loss: 0.5091 - val_acc: 0.7965

Epoch 00009: val_loss improved from 0.51698 to 0.50907, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 31s - loss: 0.4941 - acc: 0.7883 - val_loss: 0.5180 - val_acc: 0.7863

Epoch 00010: val_loss did not improve from 0.50907
Epoch 11/40
 - 31s - loss: 0.4870 - acc: 0.7905 - val_loss: 0.5072 - val_acc: 0.7984

Epoch 00011: val_loss improved from 0.50907 to 0.50721, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 31s - loss: 0.4818 - acc: 0.7923 - val_loss: 0.5085 - val_acc: 0.7919

Epoch 00012: val_loss did not improve from 0.50721
Epoch 13/40
 - 31s - loss: 0.4769 - acc: 0.7943 - val_loss: 0.5094 - val_acc: 0.7940

Epoch 00013: val_loss did not improve from 0.50721
Epoch 14/40
 - 31s - loss: 0.4748 - acc: 0.7959 - val_loss: 0.4919 - val_acc: 0.8011

Epoch 00014: val_loss improved from 0.50721 to 0.49191, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 31s - loss: 0.4691 - acc: 0.7994 - val_loss: 0.5049 - val_acc: 0.7925

Epoch 00015: val_loss did not improve from 0.49191
Epoch 16/40
 - 31s - loss: 0.4650 - acc: 0.8014 - val_loss: 0.5764 - val_acc: 0.7762

Epoch 00016: val_loss did not improve from 0.49191
Epoch 17/40
 - 31s - loss: 0.4620 - acc: 0.8023 - val_loss: 0.5131 - val_acc: 0.7734

Epoch 00017: val_loss did not improve from 0.49191
Epoch 18/40
 - 31s - loss: 0.4587 - acc: 0.8044 - val_loss: 0.4937 - val_acc: 0.7863

Epoch 00018: val_loss did not improve from 0.49191
Epoch 19/40
 - 31s - loss: 0.4560 - acc: 0.8049 - val_loss: 0.4867 - val_acc: 0.7926

Epoch 00019: val_loss improved from 0.49191 to 0.48671, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 31s - loss: 0.4516 - acc: 0.8062 - val_loss: 0.4829 - val_acc: 0.7949

Epoch 00020: val_loss improved from 0.48671 to 0.48288, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 31s - loss: 0.4504 - acc: 0.8084 - val_loss: 0.5154 - val_acc: 0.7968

Epoch 00021: val_loss did not improve from 0.48288
Epoch 22/40
 - 31s - loss: 0.4458 - acc: 0.8101 - val_loss: 0.4915 - val_acc: 0.8160

Epoch 00022: val_loss did not improve from 0.48288
Epoch 23/40
 - 31s - loss: 0.4422 - acc: 0.8114 - val_loss: 0.4776 - val_acc: 0.8267

Epoch 00023: val_loss improved from 0.48288 to 0.47758, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 31s - loss: 0.4415 - acc: 0.8118 - val_loss: 0.4709 - val_acc: 0.8147

Epoch 00024: val_loss improved from 0.47758 to 0.47092, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 32s - loss: 0.4401 - acc: 0.8129 - val_loss: 0.5031 - val_acc: 0.7758

Epoch 00025: val_loss did not improve from 0.47092
Epoch 26/40
 - 31s - loss: 0.4370 - acc: 0.8149 - val_loss: 0.4947 - val_acc: 0.7866

Epoch 00026: val_loss did not improve from 0.47092
Epoch 27/40
 - 31s - loss: 0.4367 - acc: 0.8124 - val_loss: 0.4868 - val_acc: 0.8008

Epoch 00027: val_loss did not improve from 0.47092
Epoch 28/40
 - 31s - loss: 0.4340 - acc: 0.8168 - val_loss: 0.5141 - val_acc: 0.7879

Epoch 00028: val_loss did not improve from 0.47092
Epoch 29/40
 - 31s - loss: 0.4302 - acc: 0.8188 - val_loss: 0.4673 - val_acc: 0.8101

Epoch 00029: val_loss improved from 0.47092 to 0.46733, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 31s - loss: 0.4313 - acc: 0.8167 - val_loss: 0.4745 - val_acc: 0.7930

Epoch 00030: val_loss did not improve from 0.46733
Epoch 31/40
 - 31s - loss: 0.4257 - acc: 0.8200 - val_loss: 0.5593 - val_acc: 0.7741

Epoch 00031: val_loss did not improve from 0.46733
Epoch 32/40
 - 31s - loss: 0.4258 - acc: 0.8199 - val_loss: 0.5160 - val_acc: 0.7925

Epoch 00032: val_loss did not improve from 0.46733
Epoch 33/40
 - 31s - loss: 0.4251 - acc: 0.8204 - val_loss: 0.4759 - val_acc: 0.8109

Epoch 00033: val_loss did not improve from 0.46733
Epoch 34/40
 - 31s - loss: 0.4213 - acc: 0.8226 - val_loss: 0.4677 - val_acc: 0.8019

Epoch 00034: val_loss did not improve from 0.46733
Epoch 35/40
 - 31s - loss: 0.4202 - acc: 0.8231 - val_loss: 0.4616 - val_acc: 0.8358

Epoch 00035: val_loss improved from 0.46733 to 0.46162, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 36/40
 - 31s - loss: 0.4184 - acc: 0.8228 - val_loss: 0.4785 - val_acc: 0.7901

Epoch 00036: val_loss did not improve from 0.46162
Epoch 37/40
 - 31s - loss: 0.4189 - acc: 0.8248 - val_loss: 0.4927 - val_acc: 0.8058

Epoch 00037: val_loss did not improve from 0.46162
Epoch 38/40
 - 31s - loss: 0.4141 - acc: 0.8256 - val_loss: 0.4731 - val_acc: 0.7831

Epoch 00038: val_loss did not improve from 0.46162
Epoch 39/40
 - 31s - loss: 0.4141 - acc: 0.8280 - val_loss: 0.4658 - val_acc: 0.8171

Epoch 00039: val_loss did not improve from 0.46162
Epoch 40/40
 - 31s - loss: 0.4131 - acc: 0.8281 - val_loss: 0.5329 - val_acc: 0.7984

Epoch 00040: val_loss did not improve from 0.46162

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 704/7440 [=>............................] - ETA: 2s
 864/7440 [==>...........................] - ETA: 2s
1024/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 1s
1344/7440 [====>.........................] - ETA: 1s
1504/7440 [=====>........................] - ETA: 1s
1664/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
1984/7440 [=======>......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2464/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3424/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
3936/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 316us/step
current Test accuracy: 0.7983870967741935
current auc_score ------------------>  0.8719175627240143

  32/7440 [..............................] - ETA: 19:48
 192/7440 [..............................] - ETA: 3:15 
 352/7440 [>.............................] - ETA: 1:45
 512/7440 [=>............................] - ETA: 1:11
 672/7440 [=>............................] - ETA: 53s 
 832/7440 [==>...........................] - ETA: 42s
 992/7440 [===>..........................] - ETA: 35s
1152/7440 [===>..........................] - ETA: 29s
1312/7440 [====>.........................] - ETA: 25s
1472/7440 [====>.........................] - ETA: 22s
1632/7440 [=====>........................] - ETA: 20s
1792/7440 [======>.......................] - ETA: 17s
1952/7440 [======>.......................] - ETA: 16s
2112/7440 [=======>......................] - ETA: 14s
2272/7440 [========>.....................] - ETA: 13s
2432/7440 [========>.....................] - ETA: 12s
2592/7440 [=========>....................] - ETA: 11s
2752/7440 [==========>...................] - ETA: 10s
2912/7440 [==========>...................] - ETA: 9s 
3072/7440 [===========>..................] - ETA: 8s
3232/7440 [============>.................] - ETA: 8s
3392/7440 [============>.................] - ETA: 7s
3552/7440 [=============>................] - ETA: 6s
3712/7440 [=============>................] - ETA: 6s
3872/7440 [==============>...............] - ETA: 5s
4032/7440 [===============>..............] - ETA: 5s
4192/7440 [===============>..............] - ETA: 4s
4352/7440 [================>.............] - ETA: 4s
4512/7440 [=================>............] - ETA: 4s
4672/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 2s
5312/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 1ms/step
Best saved model Test accuracy: 0.835752688172043
best saved model auc_score ------------------>  0.9058685975257255
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_26 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_26[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_199[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_200[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_201[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_202 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_202[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 40, 96, 96)   0           concatenate_75[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_203 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_203[0][0]             
__________________________________________________________________________________________________
average_pooling2d_26 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_26[0][0]       
__________________________________________________________________________________________________
activation_204 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_204[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_205 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_205[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_26[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_206 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_206[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_207 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_207[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 44, 48, 48)   0           concatenate_77[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_208 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_26 (Gl (None, 44)           0           activation_208[0][0]             
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 1)            45          global_average_pooling2d_26[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 45s - loss: 0.6421 - acc: 0.7008 - val_loss: 0.5351 - val_acc: 0.8543

Epoch 00001: val_loss improved from inf to 0.53512, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 32s - loss: 0.5812 - acc: 0.7522 - val_loss: 0.5122 - val_acc: 0.8419

Epoch 00002: val_loss improved from 0.53512 to 0.51220, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 31s - loss: 0.5579 - acc: 0.7603 - val_loss: 0.5622 - val_acc: 0.8040

Epoch 00003: val_loss did not improve from 0.51220
Epoch 4/40
 - 31s - loss: 0.5436 - acc: 0.7661 - val_loss: 0.5105 - val_acc: 0.8179

Epoch 00004: val_loss improved from 0.51220 to 0.51048, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 32s - loss: 0.5310 - acc: 0.7709 - val_loss: 0.4953 - val_acc: 0.8147

Epoch 00005: val_loss improved from 0.51048 to 0.49528, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 31s - loss: 0.5215 - acc: 0.7742 - val_loss: 0.4914 - val_acc: 0.8179

Epoch 00006: val_loss improved from 0.49528 to 0.49144, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 31s - loss: 0.5126 - acc: 0.7758 - val_loss: 0.4769 - val_acc: 0.8203

Epoch 00007: val_loss improved from 0.49144 to 0.47687, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 32s - loss: 0.5047 - acc: 0.7822 - val_loss: 0.4762 - val_acc: 0.8181

Epoch 00008: val_loss improved from 0.47687 to 0.47621, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 32s - loss: 0.4972 - acc: 0.7865 - val_loss: 0.4691 - val_acc: 0.8335

Epoch 00009: val_loss improved from 0.47621 to 0.46909, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 31s - loss: 0.4900 - acc: 0.7878 - val_loss: 0.4538 - val_acc: 0.8349

Epoch 00010: val_loss improved from 0.46909 to 0.45375, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 31s - loss: 0.4866 - acc: 0.7901 - val_loss: 0.4562 - val_acc: 0.8219

Epoch 00011: val_loss did not improve from 0.45375
Epoch 12/40
 - 31s - loss: 0.4788 - acc: 0.7931 - val_loss: 0.4648 - val_acc: 0.8156

Epoch 00012: val_loss did not improve from 0.45375
Epoch 13/40
 - 31s - loss: 0.4743 - acc: 0.7963 - val_loss: 0.4572 - val_acc: 0.8305

Epoch 00013: val_loss did not improve from 0.45375
Epoch 14/40
 - 31s - loss: 0.4692 - acc: 0.7987 - val_loss: 0.4777 - val_acc: 0.7964

Epoch 00014: val_loss did not improve from 0.45375
Epoch 15/40
 - 31s - loss: 0.4635 - acc: 0.8024 - val_loss: 0.4443 - val_acc: 0.8363

Epoch 00015: val_loss improved from 0.45375 to 0.44429, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 32s - loss: 0.4605 - acc: 0.8044 - val_loss: 0.4502 - val_acc: 0.8340

Epoch 00016: val_loss did not improve from 0.44429
Epoch 17/40
 - 32s - loss: 0.4553 - acc: 0.8054 - val_loss: 0.4456 - val_acc: 0.8298

Epoch 00017: val_loss did not improve from 0.44429
Epoch 18/40
 - 32s - loss: 0.4522 - acc: 0.8064 - val_loss: 0.5150 - val_acc: 0.8009

Epoch 00018: val_loss did not improve from 0.44429
Epoch 19/40
 - 32s - loss: 0.4490 - acc: 0.8087 - val_loss: 0.4417 - val_acc: 0.8274

Epoch 00019: val_loss improved from 0.44429 to 0.44167, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 32s - loss: 0.4456 - acc: 0.8101 - val_loss: 0.4492 - val_acc: 0.8259

Epoch 00020: val_loss did not improve from 0.44167
Epoch 21/40
 - 32s - loss: 0.4442 - acc: 0.8115 - val_loss: 0.4252 - val_acc: 0.8431

Epoch 00021: val_loss improved from 0.44167 to 0.42516, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 31s - loss: 0.4417 - acc: 0.8136 - val_loss: 0.4273 - val_acc: 0.8419

Epoch 00022: val_loss did not improve from 0.42516
Epoch 23/40
 - 31s - loss: 0.4396 - acc: 0.8128 - val_loss: 0.4282 - val_acc: 0.8398

Epoch 00023: val_loss did not improve from 0.42516
Epoch 24/40
 - 31s - loss: 0.4351 - acc: 0.8173 - val_loss: 0.4277 - val_acc: 0.8304

Epoch 00024: val_loss did not improve from 0.42516
Epoch 25/40
 - 31s - loss: 0.4357 - acc: 0.8158 - val_loss: 0.4273 - val_acc: 0.8480

Epoch 00025: val_loss did not improve from 0.42516
Epoch 26/40
 - 31s - loss: 0.4319 - acc: 0.8155 - val_loss: 0.4334 - val_acc: 0.8261

Epoch 00026: val_loss did not improve from 0.42516
Epoch 27/40
 - 31s - loss: 0.4288 - acc: 0.8202 - val_loss: 0.4487 - val_acc: 0.8281

Epoch 00027: val_loss did not improve from 0.42516
Epoch 28/40
 - 31s - loss: 0.4289 - acc: 0.8199 - val_loss: 0.4723 - val_acc: 0.8210

Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00028: val_loss did not improve from 0.42516
Epoch 00028: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4384/7440 [================>.............] - ETA: 0s
4544/7440 [=================>............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 320us/step
current Test accuracy: 0.8209677419354838
current auc_score ------------------>  0.9087448693490576

  32/7440 [..............................] - ETA: 21:58
 192/7440 [..............................] - ETA: 3:37 
 352/7440 [>.............................] - ETA: 1:57
 512/7440 [=>............................] - ETA: 1:19
 672/7440 [=>............................] - ETA: 59s 
 832/7440 [==>...........................] - ETA: 47s
 992/7440 [===>..........................] - ETA: 39s
1152/7440 [===>..........................] - ETA: 33s
1312/7440 [====>.........................] - ETA: 28s
1472/7440 [====>.........................] - ETA: 25s
1632/7440 [=====>........................] - ETA: 22s
1792/7440 [======>.......................] - ETA: 19s
1952/7440 [======>.......................] - ETA: 17s
2112/7440 [=======>......................] - ETA: 16s
2272/7440 [========>.....................] - ETA: 14s
2432/7440 [========>.....................] - ETA: 13s
2592/7440 [=========>....................] - ETA: 12s
2752/7440 [==========>...................] - ETA: 11s
2912/7440 [==========>...................] - ETA: 10s
3072/7440 [===========>..................] - ETA: 9s 
3232/7440 [============>.................] - ETA: 8s
3392/7440 [============>.................] - ETA: 8s
3552/7440 [=============>................] - ETA: 7s
3712/7440 [=============>................] - ETA: 6s
3872/7440 [==============>...............] - ETA: 6s
4032/7440 [===============>..............] - ETA: 5s
4192/7440 [===============>..............] - ETA: 5s
4352/7440 [================>.............] - ETA: 5s
4512/7440 [=================>............] - ETA: 4s
4672/7440 [=================>............] - ETA: 4s
4832/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 3s
5312/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8431451612903226
best saved model auc_score ------------------>  0.913668343161059
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_27[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_209 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_209[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_210 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_210[0][0]             
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_79[0][0]             
__________________________________________________________________________________________________
activation_211 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_211[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_212 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_212[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 40, 96, 96)   0           concatenate_79[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_80[0][0]             
__________________________________________________________________________________________________
activation_213 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_213[0][0]             
__________________________________________________________________________________________________
average_pooling2d_27 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_27[0][0]       
__________________________________________________________________________________________________
activation_214 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_214[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_215 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_215[0][0]             
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_27[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_81[0][0]             
__________________________________________________________________________________________________
activation_216 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_216[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_217 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_217[0][0]             
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 44, 48, 48)   0           concatenate_81[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_82[0][0]             
__________________________________________________________________________________________________
activation_218 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_27 (Gl (None, 44)           0           activation_218[0][0]             
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 1)            45          global_average_pooling2d_27[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 49s - loss: 0.6478 - acc: 0.6982 - val_loss: 0.5497 - val_acc: 0.8509

Epoch 00001: val_loss improved from inf to 0.54975, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 32s - loss: 0.5937 - acc: 0.7410 - val_loss: 0.5259 - val_acc: 0.8560

Epoch 00002: val_loss improved from 0.54975 to 0.52589, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 31s - loss: 0.5709 - acc: 0.7500 - val_loss: 0.5174 - val_acc: 0.8427

Epoch 00003: val_loss improved from 0.52589 to 0.51738, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 32s - loss: 0.5526 - acc: 0.7583 - val_loss: 0.5110 - val_acc: 0.8353

Epoch 00004: val_loss improved from 0.51738 to 0.51098, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 32s - loss: 0.5364 - acc: 0.7682 - val_loss: 0.5130 - val_acc: 0.8212

Epoch 00005: val_loss did not improve from 0.51098
Epoch 6/40
 - 32s - loss: 0.5246 - acc: 0.7745 - val_loss: 0.5085 - val_acc: 0.8089

Epoch 00006: val_loss improved from 0.51098 to 0.50850, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 32s - loss: 0.5151 - acc: 0.7753 - val_loss: 0.5033 - val_acc: 0.8180

Epoch 00007: val_loss improved from 0.50850 to 0.50325, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 32s - loss: 0.5054 - acc: 0.7804 - val_loss: 0.5049 - val_acc: 0.8015

Epoch 00008: val_loss did not improve from 0.50325
Epoch 9/40
 - 32s - loss: 0.4974 - acc: 0.7829 - val_loss: 0.4888 - val_acc: 0.8138

Epoch 00009: val_loss improved from 0.50325 to 0.48884, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 32s - loss: 0.4913 - acc: 0.7886 - val_loss: 0.4987 - val_acc: 0.8051

Epoch 00010: val_loss did not improve from 0.48884
Epoch 11/40
 - 32s - loss: 0.4846 - acc: 0.7895 - val_loss: 0.5037 - val_acc: 0.8077

Epoch 00011: val_loss did not improve from 0.48884
Epoch 12/40
 - 32s - loss: 0.4808 - acc: 0.7923 - val_loss: 0.5189 - val_acc: 0.7997

Epoch 00012: val_loss did not improve from 0.48884
Epoch 13/40
 - 32s - loss: 0.4750 - acc: 0.7950 - val_loss: 0.5070 - val_acc: 0.8004

Epoch 00013: val_loss did not improve from 0.48884
Epoch 14/40
 - 32s - loss: 0.4713 - acc: 0.7962 - val_loss: 0.4910 - val_acc: 0.8001

Epoch 00014: val_loss did not improve from 0.48884
Epoch 15/40
 - 32s - loss: 0.4658 - acc: 0.7996 - val_loss: 0.4746 - val_acc: 0.8063

Epoch 00015: val_loss improved from 0.48884 to 0.47461, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 32s - loss: 0.4615 - acc: 0.8026 - val_loss: 0.4579 - val_acc: 0.8238

Epoch 00016: val_loss improved from 0.47461 to 0.45789, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 32s - loss: 0.4585 - acc: 0.8042 - val_loss: 0.5014 - val_acc: 0.8027

Epoch 00017: val_loss did not improve from 0.45789
Epoch 18/40
 - 32s - loss: 0.4558 - acc: 0.8046 - val_loss: 0.6027 - val_acc: 0.7618

Epoch 00018: val_loss did not improve from 0.45789
Epoch 19/40
 - 32s - loss: 0.4521 - acc: 0.8078 - val_loss: 0.4673 - val_acc: 0.8313

Epoch 00019: val_loss did not improve from 0.45789
Epoch 20/40
 - 32s - loss: 0.4504 - acc: 0.8077 - val_loss: 0.5305 - val_acc: 0.7852

Epoch 00020: val_loss did not improve from 0.45789
Epoch 21/40
 - 32s - loss: 0.4462 - acc: 0.8103 - val_loss: 0.4976 - val_acc: 0.7917

Epoch 00021: val_loss did not improve from 0.45789
Epoch 22/40
 - 32s - loss: 0.4459 - acc: 0.8095 - val_loss: 0.4280 - val_acc: 0.8438

Epoch 00022: val_loss improved from 0.45789 to 0.42795, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 23/40
 - 32s - loss: 0.4407 - acc: 0.8139 - val_loss: 0.4522 - val_acc: 0.8218

Epoch 00023: val_loss did not improve from 0.42795
Epoch 24/40
 - 32s - loss: 0.4397 - acc: 0.8135 - val_loss: 0.5231 - val_acc: 0.8011

Epoch 00024: val_loss did not improve from 0.42795
Epoch 25/40
 - 32s - loss: 0.4367 - acc: 0.8147 - val_loss: 0.5033 - val_acc: 0.8019

Epoch 00025: val_loss did not improve from 0.42795
Epoch 26/40
 - 32s - loss: 0.4345 - acc: 0.8180 - val_loss: 0.4563 - val_acc: 0.8173

Epoch 00026: val_loss did not improve from 0.42795
Epoch 27/40
 - 32s - loss: 0.4326 - acc: 0.8189 - val_loss: 0.5129 - val_acc: 0.8024

Epoch 00027: val_loss did not improve from 0.42795
Epoch 28/40
 - 32s - loss: 0.4286 - acc: 0.8200 - val_loss: 0.6117 - val_acc: 0.7683

Epoch 00028: val_loss did not improve from 0.42795
Epoch 29/40
 - 32s - loss: 0.4274 - acc: 0.8223 - val_loss: 0.5611 - val_acc: 0.7874

Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00029: val_loss did not improve from 0.42795
Epoch 00029: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 321us/step
current Test accuracy: 0.7873655913978495
current auc_score ------------------>  0.8856351890391951

  32/7440 [..............................] - ETA: 22:41
 192/7440 [..............................] - ETA: 3:44 
 352/7440 [>.............................] - ETA: 2:00
 512/7440 [=>............................] - ETA: 1:21
 672/7440 [=>............................] - ETA: 1:01
 832/7440 [==>...........................] - ETA: 48s 
 992/7440 [===>..........................] - ETA: 40s
1152/7440 [===>..........................] - ETA: 34s
1312/7440 [====>.........................] - ETA: 29s
1472/7440 [====>.........................] - ETA: 25s
1632/7440 [=====>........................] - ETA: 22s
1792/7440 [======>.......................] - ETA: 20s
1952/7440 [======>.......................] - ETA: 18s
2112/7440 [=======>......................] - ETA: 16s
2272/7440 [========>.....................] - ETA: 15s
2432/7440 [========>.....................] - ETA: 13s
2592/7440 [=========>....................] - ETA: 12s
2752/7440 [==========>...................] - ETA: 11s
2912/7440 [==========>...................] - ETA: 10s
3072/7440 [===========>..................] - ETA: 9s 
3232/7440 [============>.................] - ETA: 9s
3392/7440 [============>.................] - ETA: 8s
3552/7440 [=============>................] - ETA: 7s
3712/7440 [=============>................] - ETA: 7s
3872/7440 [==============>...............] - ETA: 6s
4032/7440 [===============>..............] - ETA: 6s
4192/7440 [===============>..............] - ETA: 5s
4352/7440 [================>.............] - ETA: 5s
4512/7440 [=================>............] - ETA: 4s
4672/7440 [=================>............] - ETA: 4s
4832/7440 [==================>...........] - ETA: 4s
4992/7440 [===================>..........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 3s
5312/7440 [====================>.........] - ETA: 3s
5472/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8438172043010753
best saved model auc_score ------------------>  0.9124155249161752
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_28 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_28[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_219 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_219[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_220 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_220[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_83[0][0]             
__________________________________________________________________________________________________
activation_221 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_221[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_222 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_222[0][0]             
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 28, 96, 96)   0           concatenate_83[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_84[0][0]             
__________________________________________________________________________________________________
activation_223 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_223[0][0]             
__________________________________________________________________________________________________
average_pooling2d_28 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_28[0][0]       
__________________________________________________________________________________________________
activation_224 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   336         activation_224[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_225 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_225[0][0]             
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_28[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 20, 48, 48)   80          concatenate_85[0][0]             
__________________________________________________________________________________________________
activation_226 (Activation)     (None, 20, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   480         activation_226[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_227 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_227[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 26, 48, 48)   0           concatenate_85[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_86[0][0]             
__________________________________________________________________________________________________
activation_228 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_28 (Gl (None, 26)           0           activation_228[0][0]             
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 1)            27          global_average_pooling2d_28[0][0]
==================================================================================================
Total params: 8,507
Trainable params: 8,063
Non-trainable params: 444
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 41s - loss: 0.6590 - acc: 0.6721 - val_loss: 0.5650 - val_acc: 0.7984

Epoch 00001: val_loss improved from inf to 0.56502, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 25s - loss: 0.6053 - acc: 0.7182 - val_loss: 0.5386 - val_acc: 0.8137

Epoch 00002: val_loss improved from 0.56502 to 0.53864, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 25s - loss: 0.5803 - acc: 0.7313 - val_loss: 0.5296 - val_acc: 0.8429

Epoch 00003: val_loss improved from 0.53864 to 0.52961, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 25s - loss: 0.5649 - acc: 0.7426 - val_loss: 0.5056 - val_acc: 0.8497

Epoch 00004: val_loss improved from 0.52961 to 0.50561, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 25s - loss: 0.5550 - acc: 0.7500 - val_loss: 0.4941 - val_acc: 0.8440

Epoch 00005: val_loss improved from 0.50561 to 0.49410, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 25s - loss: 0.5452 - acc: 0.7538 - val_loss: 0.4840 - val_acc: 0.8491

Epoch 00006: val_loss improved from 0.49410 to 0.48404, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 25s - loss: 0.5387 - acc: 0.7556 - val_loss: 0.4863 - val_acc: 0.8310

Epoch 00007: val_loss did not improve from 0.48404
Epoch 8/40
 - 25s - loss: 0.5316 - acc: 0.7611 - val_loss: 0.4770 - val_acc: 0.8441

Epoch 00008: val_loss improved from 0.48404 to 0.47701, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 25s - loss: 0.5273 - acc: 0.7604 - val_loss: 0.4776 - val_acc: 0.8382

Epoch 00009: val_loss did not improve from 0.47701
Epoch 10/40
 - 25s - loss: 0.5214 - acc: 0.7633 - val_loss: 0.4696 - val_acc: 0.8401

Epoch 00010: val_loss improved from 0.47701 to 0.46961, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 25s - loss: 0.5162 - acc: 0.7670 - val_loss: 0.4694 - val_acc: 0.8301

Epoch 00011: val_loss improved from 0.46961 to 0.46939, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 25s - loss: 0.5107 - acc: 0.7679 - val_loss: 0.4781 - val_acc: 0.8192

Epoch 00012: val_loss did not improve from 0.46939
Epoch 13/40
 - 25s - loss: 0.5070 - acc: 0.7689 - val_loss: 0.4560 - val_acc: 0.8333

Epoch 00013: val_loss improved from 0.46939 to 0.45598, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 25s - loss: 0.5033 - acc: 0.7731 - val_loss: 0.4708 - val_acc: 0.8246

Epoch 00014: val_loss did not improve from 0.45598
Epoch 15/40
 - 25s - loss: 0.4987 - acc: 0.7742 - val_loss: 0.4602 - val_acc: 0.8323

Epoch 00015: val_loss did not improve from 0.45598
Epoch 16/40
 - 25s - loss: 0.4957 - acc: 0.7743 - val_loss: 0.4560 - val_acc: 0.8309

Epoch 00016: val_loss did not improve from 0.45598
Epoch 17/40
 - 25s - loss: 0.4915 - acc: 0.7751 - val_loss: 0.4479 - val_acc: 0.8285

Epoch 00017: val_loss improved from 0.45598 to 0.44788, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 25s - loss: 0.4888 - acc: 0.7786 - val_loss: 0.4421 - val_acc: 0.8341

Epoch 00018: val_loss improved from 0.44788 to 0.44208, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 25s - loss: 0.4863 - acc: 0.7774 - val_loss: 0.4484 - val_acc: 0.8285

Epoch 00019: val_loss did not improve from 0.44208
Epoch 20/40
 - 25s - loss: 0.4836 - acc: 0.7779 - val_loss: 0.4383 - val_acc: 0.8368

Epoch 00020: val_loss improved from 0.44208 to 0.43826, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 25s - loss: 0.4824 - acc: 0.7808 - val_loss: 0.4478 - val_acc: 0.8227

Epoch 00021: val_loss did not improve from 0.43826
Epoch 22/40
 - 25s - loss: 0.4776 - acc: 0.7832 - val_loss: 0.4401 - val_acc: 0.8285

Epoch 00022: val_loss did not improve from 0.43826
Epoch 23/40
 - 25s - loss: 0.4761 - acc: 0.7818 - val_loss: 0.4354 - val_acc: 0.8296

Epoch 00023: val_loss improved from 0.43826 to 0.43540, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 25s - loss: 0.4727 - acc: 0.7862 - val_loss: 0.4365 - val_acc: 0.8435

Epoch 00024: val_loss did not improve from 0.43540
Epoch 25/40
 - 25s - loss: 0.4708 - acc: 0.7858 - val_loss: 0.4337 - val_acc: 0.8356

Epoch 00025: val_loss improved from 0.43540 to 0.43372, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 26/40
 - 25s - loss: 0.4684 - acc: 0.7875 - val_loss: 0.4393 - val_acc: 0.8280

Epoch 00026: val_loss did not improve from 0.43372
Epoch 27/40
 - 25s - loss: 0.4664 - acc: 0.7882 - val_loss: 0.4364 - val_acc: 0.8427

Epoch 00027: val_loss did not improve from 0.43372
Epoch 28/40
 - 25s - loss: 0.4646 - acc: 0.7895 - val_loss: 0.4299 - val_acc: 0.8417

Epoch 00028: val_loss improved from 0.43372 to 0.42989, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 29/40
 - 25s - loss: 0.4634 - acc: 0.7881 - val_loss: 0.4333 - val_acc: 0.8383

Epoch 00029: val_loss did not improve from 0.42989
Epoch 30/40
 - 25s - loss: 0.4610 - acc: 0.7888 - val_loss: 0.4337 - val_acc: 0.8355

Epoch 00030: val_loss did not improve from 0.42989
Epoch 31/40
 - 25s - loss: 0.4587 - acc: 0.7930 - val_loss: 0.4374 - val_acc: 0.8348

Epoch 00031: val_loss did not improve from 0.42989
Epoch 32/40
 - 25s - loss: 0.4565 - acc: 0.7931 - val_loss: 0.4306 - val_acc: 0.8402

Epoch 00032: val_loss did not improve from 0.42989
Epoch 33/40
 - 24s - loss: 0.4548 - acc: 0.7925 - val_loss: 0.4257 - val_acc: 0.8496

Epoch 00033: val_loss improved from 0.42989 to 0.42575, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 34/40
 - 25s - loss: 0.4550 - acc: 0.7938 - val_loss: 0.4345 - val_acc: 0.8305

Epoch 00034: val_loss did not improve from 0.42575
Epoch 35/40
 - 25s - loss: 0.4510 - acc: 0.7949 - val_loss: 0.4308 - val_acc: 0.8425

Epoch 00035: val_loss did not improve from 0.42575
Epoch 36/40
 - 25s - loss: 0.4522 - acc: 0.7951 - val_loss: 0.4281 - val_acc: 0.8411

Epoch 00036: val_loss did not improve from 0.42575
Epoch 37/40
 - 25s - loss: 0.4490 - acc: 0.7970 - val_loss: 0.4394 - val_acc: 0.8328

Epoch 00037: val_loss did not improve from 0.42575
Epoch 38/40
 - 25s - loss: 0.4479 - acc: 0.7960 - val_loss: 0.4278 - val_acc: 0.8362

Epoch 00038: val_loss did not improve from 0.42575
Epoch 39/40
 - 25s - loss: 0.4469 - acc: 0.7975 - val_loss: 0.4380 - val_acc: 0.8332

Epoch 00039: val_loss did not improve from 0.42575
Epoch 40/40
 - 25s - loss: 0.4453 - acc: 0.7995 - val_loss: 0.4174 - val_acc: 0.8512

Epoch 00040: val_loss improved from 0.42575 to 0.41736, saving model to keras_densenet_simple_wt_29Sept_1404.h5

  32/7440 [..............................] - ETA: 1s
 224/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2176/7440 [=======>......................] - ETA: 1s
2400/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
3008/7440 [===========>..................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 0s
3968/7440 [===============>..............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4544/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 263us/step
current Test accuracy: 0.8512096774193548
current auc_score ------------------>  0.8987713536246964

  32/7440 [..............................] - ETA: 23:48
 224/7440 [..............................] - ETA: 3:20 
 416/7440 [>.............................] - ETA: 1:46
 608/7440 [=>............................] - ETA: 1:11
 800/7440 [==>...........................] - ETA: 53s 
 992/7440 [===>..........................] - ETA: 41s
1184/7440 [===>..........................] - ETA: 34s
1376/7440 [====>.........................] - ETA: 28s
1568/7440 [=====>........................] - ETA: 24s
1760/7440 [======>.......................] - ETA: 21s
1952/7440 [======>.......................] - ETA: 18s
2144/7440 [=======>......................] - ETA: 16s
2336/7440 [========>.....................] - ETA: 14s
2528/7440 [=========>....................] - ETA: 13s
2720/7440 [=========>....................] - ETA: 11s
2912/7440 [==========>...................] - ETA: 10s
3104/7440 [===========>..................] - ETA: 9s 
3296/7440 [============>.................] - ETA: 8s
3488/7440 [=============>................] - ETA: 8s
3680/7440 [=============>................] - ETA: 7s
3872/7440 [==============>...............] - ETA: 6s
4064/7440 [===============>..............] - ETA: 6s
4256/7440 [================>.............] - ETA: 5s
4448/7440 [================>.............] - ETA: 4s
4640/7440 [=================>............] - ETA: 4s
4832/7440 [==================>...........] - ETA: 4s
5024/7440 [===================>..........] - ETA: 3s
5216/7440 [====================>.........] - ETA: 3s
5408/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5984/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6368/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8512096774193548
best saved model auc_score ------------------>  0.8987713536246964
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_29[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_229 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_229[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_230 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_230[0][0]             
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_87[0][0]             
__________________________________________________________________________________________________
activation_231 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_231[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_232 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_232[0][0]             
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 36, 96, 96)   0           concatenate_87[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 36, 96, 96)   144         concatenate_88[0][0]             
__________________________________________________________________________________________________
activation_233 (Activation)     (None, 36, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 18, 96, 96)   648         activation_233[0][0]             
__________________________________________________________________________________________________
average_pooling2d_29 (AveragePo (None, 18, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 18, 48, 48)   72          average_pooling2d_29[0][0]       
__________________________________________________________________________________________________
activation_234 (Activation)     (None, 18, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   720         activation_234[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_235 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_235[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 28, 48, 48)   0           average_pooling2d_29[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_89[0][0]             
__________________________________________________________________________________________________
activation_236 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1120        activation_236[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_237 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_237[0][0]             
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 38, 48, 48)   0           concatenate_89[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 38, 48, 48)   152         concatenate_90[0][0]             
__________________________________________________________________________________________________
activation_238 (Activation)     (None, 38, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_29 (Gl (None, 38)           0           activation_238[0][0]             
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 1)            39          global_average_pooling2d_29[0][0]
==================================================================================================
Total params: 20,183
Trainable params: 19,539
Non-trainable params: 644
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 45s - loss: 0.6687 - acc: 0.6599 - val_loss: 0.5637 - val_acc: 0.8144

Epoch 00001: val_loss improved from inf to 0.56369, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 29s - loss: 0.6009 - acc: 0.7321 - val_loss: 0.5380 - val_acc: 0.8556

Epoch 00002: val_loss improved from 0.56369 to 0.53802, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 29s - loss: 0.5726 - acc: 0.7496 - val_loss: 0.5308 - val_acc: 0.8441

Epoch 00003: val_loss improved from 0.53802 to 0.53084, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 29s - loss: 0.5561 - acc: 0.7555 - val_loss: 0.5347 - val_acc: 0.8308

Epoch 00004: val_loss did not improve from 0.53084
Epoch 5/40
 - 29s - loss: 0.5452 - acc: 0.7597 - val_loss: 0.5327 - val_acc: 0.8228

Epoch 00005: val_loss did not improve from 0.53084
Epoch 6/40
 - 29s - loss: 0.5373 - acc: 0.7641 - val_loss: 0.5557 - val_acc: 0.7983

Epoch 00006: val_loss did not improve from 0.53084
Epoch 7/40
 - 29s - loss: 0.5313 - acc: 0.7647 - val_loss: 0.5322 - val_acc: 0.8148

Epoch 00007: val_loss did not improve from 0.53084
Epoch 8/40
 - 29s - loss: 0.5256 - acc: 0.7658 - val_loss: 0.5592 - val_acc: 0.7863

Epoch 00008: val_loss did not improve from 0.53084
Epoch 9/40
 - 29s - loss: 0.5203 - acc: 0.7676 - val_loss: 0.5395 - val_acc: 0.8007

Epoch 00009: val_loss did not improve from 0.53084
Epoch 10/40
 - 29s - loss: 0.5153 - acc: 0.7699 - val_loss: 0.5356 - val_acc: 0.7944

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00010: val_loss did not improve from 0.53084
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1344/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1728/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2688/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3456/7440 [============>.................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4384/7440 [================>.............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 313us/step
current Test accuracy: 0.7943548387096774
current auc_score ------------------>  0.8521867412417621

  32/7440 [..............................] - ETA: 25:36
 192/7440 [..............................] - ETA: 4:12 
 352/7440 [>.............................] - ETA: 2:15
 544/7440 [=>............................] - ETA: 1:26
 736/7440 [=>............................] - ETA: 1:02
 928/7440 [==>...........................] - ETA: 48s 
1120/7440 [===>..........................] - ETA: 39s
1312/7440 [====>.........................] - ETA: 32s
1504/7440 [=====>........................] - ETA: 28s
1696/7440 [=====>........................] - ETA: 24s
1888/7440 [======>.......................] - ETA: 21s
2080/7440 [=======>......................] - ETA: 18s
2272/7440 [========>.....................] - ETA: 16s
2464/7440 [========>.....................] - ETA: 14s
2656/7440 [=========>....................] - ETA: 13s
2848/7440 [==========>...................] - ETA: 12s
3040/7440 [===========>..................] - ETA: 10s
3232/7440 [============>.................] - ETA: 9s 
3424/7440 [============>.................] - ETA: 9s
3616/7440 [=============>................] - ETA: 8s
3808/7440 [==============>...............] - ETA: 7s
4000/7440 [===============>..............] - ETA: 6s
4192/7440 [===============>..............] - ETA: 6s
4384/7440 [================>.............] - ETA: 5s
4576/7440 [=================>............] - ETA: 5s
4768/7440 [==================>...........] - ETA: 4s
4960/7440 [===================>..........] - ETA: 4s
5152/7440 [===================>..........] - ETA: 3s
5344/7440 [====================>.........] - ETA: 3s
5536/7440 [=====================>........] - ETA: 2s
5728/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 2s
6112/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6496/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 9s 1ms/step
Best saved model Test accuracy: 0.8440860215053764
best saved model auc_score ------------------>  0.8761658139669326
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_30 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_30[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_239 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_239[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_240 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_240[0][0]             
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_91[0][0]             
__________________________________________________________________________________________________
activation_241 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_241[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_242 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_242[0][0]             
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 40, 96, 96)   0           concatenate_91[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_92[0][0]             
__________________________________________________________________________________________________
activation_243 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_243[0][0]             
__________________________________________________________________________________________________
average_pooling2d_30 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_30[0][0]       
__________________________________________________________________________________________________
activation_244 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_244[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_245 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_245[0][0]             
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_30[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_93[0][0]             
__________________________________________________________________________________________________
activation_246 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_246[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_247 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_247[0][0]             
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 44, 48, 48)   0           concatenate_93[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_94[0][0]             
__________________________________________________________________________________________________
activation_248 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_30 (Gl (None, 44)           0           activation_248[0][0]             
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 1)            45          global_average_pooling2d_30[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 49s - loss: 0.6860 - acc: 0.6300 - val_loss: 0.6018 - val_acc: 0.7871

Epoch 00001: val_loss improved from inf to 0.60177, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 33s - loss: 0.6035 - acc: 0.7422 - val_loss: 0.5602 - val_acc: 0.8339

Epoch 00002: val_loss improved from 0.60177 to 0.56017, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 32s - loss: 0.5768 - acc: 0.7566 - val_loss: 0.5521 - val_acc: 0.8198

Epoch 00003: val_loss improved from 0.56017 to 0.55212, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 32s - loss: 0.5581 - acc: 0.7634 - val_loss: 0.5370 - val_acc: 0.8039

Epoch 00004: val_loss improved from 0.55212 to 0.53701, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 32s - loss: 0.5432 - acc: 0.7709 - val_loss: 0.5192 - val_acc: 0.8124

Epoch 00005: val_loss improved from 0.53701 to 0.51918, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 32s - loss: 0.5314 - acc: 0.7726 - val_loss: 0.5102 - val_acc: 0.8081

Epoch 00006: val_loss improved from 0.51918 to 0.51019, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 32s - loss: 0.5204 - acc: 0.7779 - val_loss: 0.4995 - val_acc: 0.8134

Epoch 00007: val_loss improved from 0.51019 to 0.49954, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 32s - loss: 0.5123 - acc: 0.7800 - val_loss: 0.4945 - val_acc: 0.8165

Epoch 00008: val_loss improved from 0.49954 to 0.49453, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 32s - loss: 0.5040 - acc: 0.7830 - val_loss: 0.5079 - val_acc: 0.8054

Epoch 00009: val_loss did not improve from 0.49453
Epoch 10/40
 - 32s - loss: 0.4969 - acc: 0.7864 - val_loss: 0.5409 - val_acc: 0.7757

Epoch 00010: val_loss did not improve from 0.49453
Epoch 11/40
 - 32s - loss: 0.4928 - acc: 0.7853 - val_loss: 0.5100 - val_acc: 0.7949

Epoch 00011: val_loss did not improve from 0.49453
Epoch 12/40
 - 32s - loss: 0.4861 - acc: 0.7912 - val_loss: 0.4918 - val_acc: 0.8003

Epoch 00012: val_loss improved from 0.49453 to 0.49185, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 32s - loss: 0.4830 - acc: 0.7904 - val_loss: 0.4878 - val_acc: 0.8056

Epoch 00013: val_loss improved from 0.49185 to 0.48778, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 32s - loss: 0.4785 - acc: 0.7942 - val_loss: 0.4827 - val_acc: 0.8073

Epoch 00014: val_loss improved from 0.48778 to 0.48266, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 32s - loss: 0.4739 - acc: 0.7952 - val_loss: 0.5147 - val_acc: 0.7745

Epoch 00015: val_loss did not improve from 0.48266
Epoch 16/40
 - 32s - loss: 0.4694 - acc: 0.7977 - val_loss: 0.4836 - val_acc: 0.8032

Epoch 00016: val_loss did not improve from 0.48266
Epoch 17/40
 - 33s - loss: 0.4666 - acc: 0.7993 - val_loss: 0.4788 - val_acc: 0.7978

Epoch 00017: val_loss improved from 0.48266 to 0.47882, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 33s - loss: 0.4611 - acc: 0.8031 - val_loss: 0.4934 - val_acc: 0.7859

Epoch 00018: val_loss did not improve from 0.47882
Epoch 19/40
 - 33s - loss: 0.4607 - acc: 0.8009 - val_loss: 0.4754 - val_acc: 0.7984

Epoch 00019: val_loss improved from 0.47882 to 0.47543, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 32s - loss: 0.4557 - acc: 0.8047 - val_loss: 0.4605 - val_acc: 0.8261

Epoch 00020: val_loss improved from 0.47543 to 0.46052, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 21/40
 - 32s - loss: 0.4535 - acc: 0.8065 - val_loss: 0.5016 - val_acc: 0.7923

Epoch 00021: val_loss did not improve from 0.46052
Epoch 22/40
 - 32s - loss: 0.4508 - acc: 0.8075 - val_loss: 0.4824 - val_acc: 0.7989

Epoch 00022: val_loss did not improve from 0.46052
Epoch 23/40
 - 32s - loss: 0.4490 - acc: 0.8079 - val_loss: 0.4700 - val_acc: 0.8067

Epoch 00023: val_loss did not improve from 0.46052
Epoch 24/40
 - 32s - loss: 0.4454 - acc: 0.8084 - val_loss: 0.5042 - val_acc: 0.7991

Epoch 00024: val_loss did not improve from 0.46052
Epoch 25/40
 - 32s - loss: 0.4429 - acc: 0.8100 - val_loss: 0.4818 - val_acc: 0.8152

Epoch 00025: val_loss did not improve from 0.46052
Epoch 26/40
 - 32s - loss: 0.4399 - acc: 0.8147 - val_loss: 0.5120 - val_acc: 0.7897

Epoch 00026: val_loss did not improve from 0.46052
Epoch 27/40
 - 32s - loss: 0.4361 - acc: 0.8143 - val_loss: 0.4639 - val_acc: 0.8116

Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00027: val_loss did not improve from 0.46052
Epoch 00027: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 337us/step
current Test accuracy: 0.8115591397849462
current auc_score ------------------>  0.8869395305815702

  32/7440 [..............................] - ETA: 27:29
 192/7440 [..............................] - ETA: 4:31 
 352/7440 [>.............................] - ETA: 2:25
 512/7440 [=>............................] - ETA: 1:38
 672/7440 [=>............................] - ETA: 1:14
 832/7440 [==>...........................] - ETA: 58s 
 992/7440 [===>..........................] - ETA: 48s
1152/7440 [===>..........................] - ETA: 41s
1312/7440 [====>.........................] - ETA: 35s
1472/7440 [====>.........................] - ETA: 30s
1632/7440 [=====>........................] - ETA: 27s
1792/7440 [======>.......................] - ETA: 24s
1952/7440 [======>.......................] - ETA: 21s
2112/7440 [=======>......................] - ETA: 19s
2272/7440 [========>.....................] - ETA: 17s
2432/7440 [========>.....................] - ETA: 16s
2592/7440 [=========>....................] - ETA: 14s
2752/7440 [==========>...................] - ETA: 13s
2912/7440 [==========>...................] - ETA: 12s
3072/7440 [===========>..................] - ETA: 11s
3232/7440 [============>.................] - ETA: 10s
3392/7440 [============>.................] - ETA: 9s 
3552/7440 [=============>................] - ETA: 9s
3712/7440 [=============>................] - ETA: 8s
3872/7440 [==============>...............] - ETA: 7s
4032/7440 [===============>..............] - ETA: 7s
4192/7440 [===============>..............] - ETA: 6s
4352/7440 [================>.............] - ETA: 6s
4512/7440 [=================>............] - ETA: 5s
4672/7440 [=================>............] - ETA: 5s
4832/7440 [==================>...........] - ETA: 4s
4992/7440 [===================>..........] - ETA: 4s
5152/7440 [===================>..........] - ETA: 3s
5312/7440 [====================>.........] - ETA: 3s
5472/7440 [=====================>........] - ETA: 3s
5632/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 2s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 10s 1ms/step
Best saved model Test accuracy: 0.8260752688172043
best saved model auc_score ------------------>  0.8848267863336802
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_31[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_249 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_249[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_250 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_250[0][0]             
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_95[0][0]             
__________________________________________________________________________________________________
activation_251 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_251[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_252 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_252[0][0]             
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 40, 96, 96)   0           concatenate_95[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_96[0][0]             
__________________________________________________________________________________________________
activation_253 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_253[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_254 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_254[0][0]             
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 52, 96, 96)   0           concatenate_96[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_97[0][0]             
__________________________________________________________________________________________________
activation_255 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_255[0][0]             
__________________________________________________________________________________________________
average_pooling2d_31 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_31[0][0]       
__________________________________________________________________________________________________
activation_256 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_256[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_257 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_257[0][0]             
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 38, 48, 48)   0           average_pooling2d_31[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_98[0][0]             
__________________________________________________________________________________________________
activation_258 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_258[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_259 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_259[0][0]             
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 50, 48, 48)   0           concatenate_98[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_99[0][0]             
__________________________________________________________________________________________________
activation_260 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_260[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_261 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_261[0][0]             
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 62, 48, 48)   0           concatenate_99[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 62, 48, 48)   248         concatenate_100[0][0]            
__________________________________________________________________________________________________
activation_262 (Activation)     (None, 62, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_31 (Gl (None, 62)           0           activation_262[0][0]             
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 1)            63          global_average_pooling2d_31[0][0]
==================================================================================================
Total params: 44,711
Trainable params: 43,511
Non-trainable params: 1,200
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 64s - loss: 0.6448 - acc: 0.7193 - val_loss: 0.5504 - val_acc: 0.8173

Epoch 00001: val_loss improved from inf to 0.55044, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 44s - loss: 0.5820 - acc: 0.7522 - val_loss: 0.5577 - val_acc: 0.8051

Epoch 00002: val_loss did not improve from 0.55044
Epoch 3/40
 - 44s - loss: 0.5557 - acc: 0.7643 - val_loss: 0.5191 - val_acc: 0.8210

Epoch 00003: val_loss improved from 0.55044 to 0.51910, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 44s - loss: 0.5366 - acc: 0.7738 - val_loss: 0.5015 - val_acc: 0.8341

Epoch 00004: val_loss improved from 0.51910 to 0.50150, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 44s - loss: 0.5215 - acc: 0.7785 - val_loss: 0.5206 - val_acc: 0.7960

Epoch 00005: val_loss did not improve from 0.50150
Epoch 6/40
 - 44s - loss: 0.5122 - acc: 0.7825 - val_loss: 0.5126 - val_acc: 0.7931

Epoch 00006: val_loss did not improve from 0.50150
Epoch 7/40
 - 44s - loss: 0.5005 - acc: 0.7901 - val_loss: 0.4764 - val_acc: 0.8317

Epoch 00007: val_loss improved from 0.50150 to 0.47637, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 44s - loss: 0.4924 - acc: 0.7961 - val_loss: 0.5151 - val_acc: 0.8034

Epoch 00008: val_loss did not improve from 0.47637
Epoch 9/40
 - 44s - loss: 0.4843 - acc: 0.7988 - val_loss: 0.4876 - val_acc: 0.8195

Epoch 00009: val_loss did not improve from 0.47637
Epoch 10/40
 - 44s - loss: 0.4771 - acc: 0.8023 - val_loss: 0.4889 - val_acc: 0.8102

Epoch 00010: val_loss did not improve from 0.47637
Epoch 11/40
 - 44s - loss: 0.4697 - acc: 0.8082 - val_loss: 0.4680 - val_acc: 0.8422

Epoch 00011: val_loss improved from 0.47637 to 0.46796, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 45s - loss: 0.4637 - acc: 0.8092 - val_loss: 0.4711 - val_acc: 0.8352

Epoch 00012: val_loss did not improve from 0.46796
Epoch 13/40
 - 44s - loss: 0.4597 - acc: 0.8140 - val_loss: 0.4568 - val_acc: 0.8388

Epoch 00013: val_loss improved from 0.46796 to 0.45683, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 45s - loss: 0.4557 - acc: 0.8147 - val_loss: 0.4819 - val_acc: 0.8129

Epoch 00014: val_loss did not improve from 0.45683
Epoch 15/40
 - 44s - loss: 0.4505 - acc: 0.8178 - val_loss: 0.4560 - val_acc: 0.8323

Epoch 00015: val_loss improved from 0.45683 to 0.45597, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 44s - loss: 0.4462 - acc: 0.8189 - val_loss: 0.4962 - val_acc: 0.8276

Epoch 00016: val_loss did not improve from 0.45597
Epoch 17/40
 - 44s - loss: 0.4434 - acc: 0.8212 - val_loss: 0.5247 - val_acc: 0.8185

Epoch 00017: val_loss did not improve from 0.45597
Epoch 18/40
 - 44s - loss: 0.4382 - acc: 0.8219 - val_loss: 0.4313 - val_acc: 0.8452

Epoch 00018: val_loss improved from 0.45597 to 0.43127, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 44s - loss: 0.4365 - acc: 0.8234 - val_loss: 0.4766 - val_acc: 0.8386

Epoch 00019: val_loss did not improve from 0.43127
Epoch 20/40
 - 44s - loss: 0.4317 - acc: 0.8238 - val_loss: 0.4504 - val_acc: 0.8278

Epoch 00020: val_loss did not improve from 0.43127
Epoch 21/40
 - 44s - loss: 0.4296 - acc: 0.8271 - val_loss: 0.4832 - val_acc: 0.8074

Epoch 00021: val_loss did not improve from 0.43127
Epoch 22/40
 - 44s - loss: 0.4264 - acc: 0.8288 - val_loss: 0.4657 - val_acc: 0.8442

Epoch 00022: val_loss did not improve from 0.43127
Epoch 23/40
 - 44s - loss: 0.4224 - acc: 0.8294 - val_loss: 0.4597 - val_acc: 0.8371

Epoch 00023: val_loss did not improve from 0.43127
Epoch 24/40
 - 44s - loss: 0.4200 - acc: 0.8322 - val_loss: 0.5030 - val_acc: 0.8302

Epoch 00024: val_loss did not improve from 0.43127
Epoch 25/40
 - 44s - loss: 0.4184 - acc: 0.8331 - val_loss: 0.4438 - val_acc: 0.8462

Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00025: val_loss did not improve from 0.43127
Epoch 00025: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 436us/step
current Test accuracy: 0.8462365591397849
current auc_score ------------------>  0.9131603725864261

  32/7440 [..............................] - ETA: 29:36
 160/7440 [..............................] - ETA: 5:52 
 288/7440 [>.............................] - ETA: 3:13
 416/7440 [>.............................] - ETA: 2:12
 544/7440 [=>............................] - ETA: 1:40
 672/7440 [=>............................] - ETA: 1:20
 800/7440 [==>...........................] - ETA: 1:06
 928/7440 [==>...........................] - ETA: 56s 
1056/7440 [===>..........................] - ETA: 49s
1184/7440 [===>..........................] - ETA: 43s
1312/7440 [====>.........................] - ETA: 38s
1440/7440 [====>.........................] - ETA: 34s
1568/7440 [=====>........................] - ETA: 31s
1696/7440 [=====>........................] - ETA: 28s
1824/7440 [======>.......................] - ETA: 26s
1952/7440 [======>.......................] - ETA: 23s
2080/7440 [=======>......................] - ETA: 22s
2208/7440 [=======>......................] - ETA: 20s
2336/7440 [========>.....................] - ETA: 18s
2464/7440 [========>.....................] - ETA: 17s
2592/7440 [=========>....................] - ETA: 16s
2720/7440 [=========>....................] - ETA: 15s
2848/7440 [==========>...................] - ETA: 14s
2976/7440 [===========>..................] - ETA: 13s
3104/7440 [===========>..................] - ETA: 12s
3232/7440 [============>.................] - ETA: 11s
3360/7440 [============>.................] - ETA: 11s
3488/7440 [=============>................] - ETA: 10s
3616/7440 [=============>................] - ETA: 9s 
3744/7440 [==============>...............] - ETA: 9s
3872/7440 [==============>...............] - ETA: 8s
4000/7440 [===============>..............] - ETA: 8s
4128/7440 [===============>..............] - ETA: 7s
4256/7440 [================>.............] - ETA: 7s
4384/7440 [================>.............] - ETA: 6s
4512/7440 [=================>............] - ETA: 6s
4640/7440 [=================>............] - ETA: 5s
4768/7440 [==================>...........] - ETA: 5s
4896/7440 [==================>...........] - ETA: 5s
5024/7440 [===================>..........] - ETA: 4s
5152/7440 [===================>..........] - ETA: 4s
5280/7440 [====================>.........] - ETA: 4s
5408/7440 [====================>.........] - ETA: 3s
5536/7440 [=====================>........] - ETA: 3s
5664/7440 [=====================>........] - ETA: 3s
5792/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 2s
6048/7440 [=======================>......] - ETA: 2s
6176/7440 [=======================>......] - ETA: 2s
6304/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 1s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 11s 1ms/step
Best saved model Test accuracy: 0.8451612903225807
best saved model auc_score ------------------>  0.915169311481096
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_32 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_32[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_263 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_263[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_264 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_264[0][0]             
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_101[0][0]            
__________________________________________________________________________________________________
activation_265 (Activation)     (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 56, 96, 96)   1680        activation_265[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_266 (Activation)     (None, 56, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_266[0][0]             
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 44, 96, 96)   0           concatenate_101[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 44, 96, 96)   176         concatenate_102[0][0]            
__________________________________________________________________________________________________
activation_267 (Activation)     (None, 44, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 56, 96, 96)   2464        activation_267[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_268 (Activation)     (None, 56, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_268[0][0]             
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 58, 96, 96)   0           concatenate_102[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 58, 96, 96)   232         concatenate_103[0][0]            
__________________________________________________________________________________________________
activation_269 (Activation)     (None, 58, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 29, 96, 96)   1682        activation_269[0][0]             
__________________________________________________________________________________________________
average_pooling2d_32 (AveragePo (None, 29, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 29, 48, 48)   116         average_pooling2d_32[0][0]       
__________________________________________________________________________________________________
activation_270 (Activation)     (None, 29, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   1624        activation_270[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_271 (Activation)     (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_271[0][0]             
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 43, 48, 48)   0           average_pooling2d_32[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_104[0][0]            
__________________________________________________________________________________________________
activation_272 (Activation)     (None, 43, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 56, 48, 48)   2408        activation_272[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_273 (Activation)     (None, 56, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_273[0][0]             
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 57, 48, 48)   0           concatenate_104[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 57, 48, 48)   228         concatenate_105[0][0]            
__________________________________________________________________________________________________
activation_274 (Activation)     (None, 57, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 56, 48, 48)   3192        activation_274[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_275 (Activation)     (None, 56, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_275[0][0]             
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 71, 48, 48)   0           concatenate_105[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 71, 48, 48)   284         concatenate_106[0][0]            
__________________________________________________________________________________________________
activation_276 (Activation)     (None, 71, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_32 (Gl (None, 71)           0           activation_276[0][0]             
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 1)            72          global_average_pooling2d_32[0][0]
==================================================================================================
Total params: 59,378
Trainable params: 58,010
Non-trainable params: 1,368
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 68s - loss: 0.6283 - acc: 0.7391 - val_loss: 0.5540 - val_acc: 0.8321

Epoch 00001: val_loss improved from inf to 0.55397, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 48s - loss: 0.5712 - acc: 0.7649 - val_loss: 0.5613 - val_acc: 0.8087

Epoch 00002: val_loss did not improve from 0.55397
Epoch 3/40
 - 48s - loss: 0.5481 - acc: 0.7748 - val_loss: 0.5525 - val_acc: 0.8176

Epoch 00003: val_loss improved from 0.55397 to 0.55247, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 48s - loss: 0.5313 - acc: 0.7821 - val_loss: 0.5493 - val_acc: 0.8125

Epoch 00004: val_loss improved from 0.55247 to 0.54926, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 48s - loss: 0.5192 - acc: 0.7877 - val_loss: 0.5394 - val_acc: 0.7973

Epoch 00005: val_loss improved from 0.54926 to 0.53937, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 48s - loss: 0.5077 - acc: 0.7921 - val_loss: 0.5276 - val_acc: 0.8190

Epoch 00006: val_loss improved from 0.53937 to 0.52760, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 48s - loss: 0.4980 - acc: 0.7980 - val_loss: 0.5364 - val_acc: 0.7962

Epoch 00007: val_loss did not improve from 0.52760
Epoch 8/40
 - 48s - loss: 0.4920 - acc: 0.8019 - val_loss: 0.5444 - val_acc: 0.7983

Epoch 00008: val_loss did not improve from 0.52760
Epoch 9/40
 - 48s - loss: 0.4840 - acc: 0.8036 - val_loss: 0.5172 - val_acc: 0.8032

Epoch 00009: val_loss improved from 0.52760 to 0.51725, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 48s - loss: 0.4774 - acc: 0.8079 - val_loss: 0.5660 - val_acc: 0.8094

Epoch 00010: val_loss did not improve from 0.51725
Epoch 11/40
 - 48s - loss: 0.4721 - acc: 0.8100 - val_loss: 0.5262 - val_acc: 0.7935

Epoch 00011: val_loss did not improve from 0.51725
Epoch 12/40
 - 48s - loss: 0.4676 - acc: 0.8122 - val_loss: 0.5148 - val_acc: 0.7970

Epoch 00012: val_loss improved from 0.51725 to 0.51484, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 48s - loss: 0.4636 - acc: 0.8141 - val_loss: 0.4880 - val_acc: 0.8192

Epoch 00013: val_loss improved from 0.51484 to 0.48801, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 48s - loss: 0.4563 - acc: 0.8186 - val_loss: 0.4911 - val_acc: 0.8019

Epoch 00014: val_loss did not improve from 0.48801
Epoch 15/40
 - 48s - loss: 0.4508 - acc: 0.8229 - val_loss: 0.5185 - val_acc: 0.8059

Epoch 00015: val_loss did not improve from 0.48801
Epoch 16/40
 - 48s - loss: 0.4485 - acc: 0.8252 - val_loss: 0.5359 - val_acc: 0.8102

Epoch 00016: val_loss did not improve from 0.48801
Epoch 17/40
 - 48s - loss: 0.4441 - acc: 0.8264 - val_loss: 0.5748 - val_acc: 0.7964

Epoch 00017: val_loss did not improve from 0.48801
Epoch 18/40
 - 48s - loss: 0.4400 - acc: 0.8285 - val_loss: 0.5161 - val_acc: 0.8034

Epoch 00018: val_loss did not improve from 0.48801
Epoch 19/40
 - 48s - loss: 0.4375 - acc: 0.8288 - val_loss: 0.5354 - val_acc: 0.7964

Epoch 00019: val_loss did not improve from 0.48801
Epoch 20/40
 - 48s - loss: 0.4341 - acc: 0.8314 - val_loss: 0.4969 - val_acc: 0.8159

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00020: val_loss did not improve from 0.48801
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 452us/step
current Test accuracy: 0.8158602150537635
current auc_score ------------------>  0.8906913371488032

  32/7440 [..............................] - ETA: 32:31
 128/7440 [..............................] - ETA: 8:04 
 256/7440 [>.............................] - ETA: 3:59
 384/7440 [>.............................] - ETA: 2:38
 512/7440 [=>............................] - ETA: 1:57
 640/7440 [=>............................] - ETA: 1:32
 768/7440 [==>...........................] - ETA: 1:16
 896/7440 [==>...........................] - ETA: 1:04
1024/7440 [===>..........................] - ETA: 55s 
1152/7440 [===>..........................] - ETA: 48s
1280/7440 [====>.........................] - ETA: 43s
1408/7440 [====>.........................] - ETA: 38s
1536/7440 [=====>........................] - ETA: 35s
1664/7440 [=====>........................] - ETA: 31s
1792/7440 [======>.......................] - ETA: 29s
1920/7440 [======>.......................] - ETA: 26s
2048/7440 [=======>......................] - ETA: 24s
2176/7440 [=======>......................] - ETA: 22s
2304/7440 [========>.....................] - ETA: 21s
2432/7440 [========>.....................] - ETA: 19s
2560/7440 [=========>....................] - ETA: 18s
2688/7440 [=========>....................] - ETA: 17s
2816/7440 [==========>...................] - ETA: 15s
2944/7440 [==========>...................] - ETA: 14s
3072/7440 [===========>..................] - ETA: 14s
3200/7440 [===========>..................] - ETA: 13s
3328/7440 [============>.................] - ETA: 12s
3456/7440 [============>.................] - ETA: 11s
3584/7440 [=============>................] - ETA: 10s
3712/7440 [=============>................] - ETA: 10s
3840/7440 [==============>...............] - ETA: 9s 
3968/7440 [===============>..............] - ETA: 8s
4096/7440 [===============>..............] - ETA: 8s
4224/7440 [================>.............] - ETA: 7s
4352/7440 [================>.............] - ETA: 7s
4480/7440 [=================>............] - ETA: 6s
4608/7440 [=================>............] - ETA: 6s
4736/7440 [==================>...........] - ETA: 6s
4864/7440 [==================>...........] - ETA: 5s
4992/7440 [===================>..........] - ETA: 5s
5120/7440 [===================>..........] - ETA: 4s
5248/7440 [====================>.........] - ETA: 4s
5376/7440 [====================>.........] - ETA: 4s
5504/7440 [=====================>........] - ETA: 3s
5632/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 3s
5888/7440 [======================>.......] - ETA: 2s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 12s 2ms/step
Best saved model Test accuracy: 0.8192204301075269
best saved model auc_score ------------------>  0.8940557434385477
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_33[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_277 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_277[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_278 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_278[0][0]             
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_107[0][0]            
__________________________________________________________________________________________________
activation_279 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_279[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_280 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_280[0][0]             
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 48, 96, 96)   0           concatenate_107[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_108[0][0]            
__________________________________________________________________________________________________
activation_281 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_281[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_282 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_282[0][0]             
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 64, 96, 96)   0           concatenate_108[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_109[0][0]            
__________________________________________________________________________________________________
activation_283 (Activation)     (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_283[0][0]             
__________________________________________________________________________________________________
average_pooling2d_33 (AveragePo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_33[0][0]       
__________________________________________________________________________________________________
activation_284 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_284[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_285 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_285[0][0]             
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 48, 48, 48)   0           average_pooling2d_33[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_110[0][0]            
__________________________________________________________________________________________________
activation_286 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_286[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_287 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_287[0][0]             
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 64, 48, 48)   0           concatenate_110[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_111[0][0]            
__________________________________________________________________________________________________
activation_288 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_288[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_289 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_289[0][0]             
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 80, 48, 48)   0           concatenate_111[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 80, 48, 48)   320         concatenate_112[0][0]            
__________________________________________________________________________________________________
activation_290 (Activation)     (None, 80, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_33 (Gl (None, 80)           0           activation_290[0][0]             
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 1)            81          global_average_pooling2d_33[0][0]
==================================================================================================
Total params: 76,145
Trainable params: 74,609
Non-trainable params: 1,536
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 73s - loss: 0.6575 - acc: 0.7113 - val_loss: 0.5672 - val_acc: 0.8403

Epoch 00001: val_loss improved from inf to 0.56723, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 52s - loss: 0.5826 - acc: 0.7665 - val_loss: 0.5848 - val_acc: 0.7785

Epoch 00002: val_loss did not improve from 0.56723
Epoch 3/40
 - 52s - loss: 0.5574 - acc: 0.7757 - val_loss: 0.5535 - val_acc: 0.8078

Epoch 00003: val_loss improved from 0.56723 to 0.55351, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 52s - loss: 0.5376 - acc: 0.7841 - val_loss: 0.5317 - val_acc: 0.8159

Epoch 00004: val_loss improved from 0.55351 to 0.53172, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 52s - loss: 0.5214 - acc: 0.7928 - val_loss: 0.5299 - val_acc: 0.8124

Epoch 00005: val_loss improved from 0.53172 to 0.52989, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 52s - loss: 0.5105 - acc: 0.7966 - val_loss: 0.5256 - val_acc: 0.8218

Epoch 00006: val_loss improved from 0.52989 to 0.52559, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 52s - loss: 0.5016 - acc: 0.8015 - val_loss: 0.5341 - val_acc: 0.8015

Epoch 00007: val_loss did not improve from 0.52559
Epoch 8/40
 - 52s - loss: 0.4927 - acc: 0.8074 - val_loss: 0.5427 - val_acc: 0.8081

Epoch 00008: val_loss did not improve from 0.52559
Epoch 9/40
 - 52s - loss: 0.4842 - acc: 0.8105 - val_loss: 0.6159 - val_acc: 0.7965

Epoch 00009: val_loss did not improve from 0.52559
Epoch 10/40
 - 52s - loss: 0.4777 - acc: 0.8151 - val_loss: 0.6139 - val_acc: 0.7868

Epoch 00010: val_loss did not improve from 0.52559
Epoch 11/40
 - 52s - loss: 0.4700 - acc: 0.8190 - val_loss: 0.5164 - val_acc: 0.8304

Epoch 00011: val_loss improved from 0.52559 to 0.51638, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 52s - loss: 0.4655 - acc: 0.8189 - val_loss: 0.5460 - val_acc: 0.7866

Epoch 00012: val_loss did not improve from 0.51638
Epoch 13/40
 - 52s - loss: 0.4594 - acc: 0.8242 - val_loss: 0.5931 - val_acc: 0.7922

Epoch 00013: val_loss did not improve from 0.51638
Epoch 14/40
 - 52s - loss: 0.4527 - acc: 0.8279 - val_loss: 0.5821 - val_acc: 0.7765

Epoch 00014: val_loss did not improve from 0.51638
Epoch 15/40
 - 52s - loss: 0.4498 - acc: 0.8284 - val_loss: 0.5742 - val_acc: 0.8003

Epoch 00015: val_loss did not improve from 0.51638
Epoch 16/40
 - 52s - loss: 0.4458 - acc: 0.8296 - val_loss: 0.5356 - val_acc: 0.7989

Epoch 00016: val_loss did not improve from 0.51638
Epoch 17/40
 - 52s - loss: 0.4410 - acc: 0.8333 - val_loss: 0.5890 - val_acc: 0.7751

Epoch 00017: val_loss did not improve from 0.51638
Epoch 18/40
 - 52s - loss: 0.4376 - acc: 0.8352 - val_loss: 0.5211 - val_acc: 0.8165

Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00018: val_loss did not improve from 0.51638
Epoch 00018: early stopping

  32/7440 [..............................] - ETA: 4s
 128/7440 [..............................] - ETA: 4s
 256/7440 [>.............................] - ETA: 3s
 384/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 640/7440 [=>............................] - ETA: 3s
 768/7440 [==>...........................] - ETA: 3s
 896/7440 [==>...........................] - ETA: 3s
1024/7440 [===>..........................] - ETA: 3s
1152/7440 [===>..........................] - ETA: 3s
1280/7440 [====>.........................] - ETA: 3s
1408/7440 [====>.........................] - ETA: 3s
1536/7440 [=====>........................] - ETA: 2s
1664/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2048/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2304/7440 [========>.....................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 2s
2560/7440 [=========>....................] - ETA: 2s
2688/7440 [=========>....................] - ETA: 2s
2816/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3456/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4608/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 494us/step
current Test accuracy: 0.8165322580645161
current auc_score ------------------>  0.8809776057925771

  32/7440 [..............................] - ETA: 34:12
 128/7440 [..............................] - ETA: 8:29 
 224/7440 [..............................] - ETA: 4:49
 352/7440 [>.............................] - ETA: 3:01
 480/7440 [>.............................] - ETA: 2:11
 608/7440 [=>............................] - ETA: 1:42
 736/7440 [=>............................] - ETA: 1:24
 864/7440 [==>...........................] - ETA: 1:10
 992/7440 [===>..........................] - ETA: 1:00
1120/7440 [===>..........................] - ETA: 53s 
1248/7440 [====>.........................] - ETA: 47s
1376/7440 [====>.........................] - ETA: 42s
1504/7440 [=====>........................] - ETA: 37s
1632/7440 [=====>........................] - ETA: 34s
1760/7440 [======>.......................] - ETA: 31s
1888/7440 [======>.......................] - ETA: 28s
2016/7440 [=======>......................] - ETA: 26s
2144/7440 [=======>......................] - ETA: 24s
2272/7440 [========>.....................] - ETA: 22s
2400/7440 [========>.....................] - ETA: 21s
2528/7440 [=========>....................] - ETA: 19s
2656/7440 [=========>....................] - ETA: 18s
2784/7440 [==========>...................] - ETA: 17s
2912/7440 [==========>...................] - ETA: 16s
3040/7440 [===========>..................] - ETA: 15s
3168/7440 [===========>..................] - ETA: 14s
3296/7440 [============>.................] - ETA: 13s
3424/7440 [============>.................] - ETA: 12s
3552/7440 [=============>................] - ETA: 11s
3680/7440 [=============>................] - ETA: 10s
3808/7440 [==============>...............] - ETA: 10s
3936/7440 [==============>...............] - ETA: 9s 
4064/7440 [===============>..............] - ETA: 9s
4192/7440 [===============>..............] - ETA: 8s
4320/7440 [================>.............] - ETA: 7s
4448/7440 [================>.............] - ETA: 7s
4576/7440 [=================>............] - ETA: 6s
4704/7440 [=================>............] - ETA: 6s
4832/7440 [==================>...........] - ETA: 6s
4960/7440 [===================>..........] - ETA: 5s
5088/7440 [===================>..........] - ETA: 5s
5216/7440 [====================>.........] - ETA: 4s
5344/7440 [====================>.........] - ETA: 4s
5472/7440 [=====================>........] - ETA: 4s
5600/7440 [=====================>........] - ETA: 3s
5728/7440 [======================>.......] - ETA: 3s
5856/7440 [======================>.......] - ETA: 3s
5984/7440 [=======================>......] - ETA: 2s
6112/7440 [=======================>......] - ETA: 2s
6240/7440 [========================>.....] - ETA: 2s
6368/7440 [========================>.....] - ETA: 2s
6496/7440 [=========================>....] - ETA: 1s
6624/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7008/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.8303763440860215
best saved model auc_score ------------------>  0.8901821381084519
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_34 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_34[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_291 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_291[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_292 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_292[0][0]             
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_113[0][0]            
__________________________________________________________________________________________________
activation_293 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_293[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_294 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_294[0][0]             
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 40, 96, 96)   0           concatenate_113[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_114[0][0]            
__________________________________________________________________________________________________
activation_295 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_295[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_296 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_296[0][0]             
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 52, 96, 96)   0           concatenate_114[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_115[0][0]            
__________________________________________________________________________________________________
activation_297 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_297[0][0]             
__________________________________________________________________________________________________
average_pooling2d_34 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_34[0][0]       
__________________________________________________________________________________________________
activation_298 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_298[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_299 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_299[0][0]             
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 38, 48, 48)   0           average_pooling2d_34[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_116[0][0]            
__________________________________________________________________________________________________
activation_300 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_300[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_301 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_301[0][0]             
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 50, 48, 48)   0           concatenate_116[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_117[0][0]            
__________________________________________________________________________________________________
activation_302 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_302[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_303 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_303[0][0]             
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 62, 48, 48)   0           concatenate_117[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 62, 48, 48)   248         concatenate_118[0][0]            
__________________________________________________________________________________________________
activation_304 (Activation)     (None, 62, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_34 (Gl (None, 62)           0           activation_304[0][0]             
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 1)            63          global_average_pooling2d_34[0][0]
==================================================================================================
Total params: 44,711
Trainable params: 43,511
Non-trainable params: 1,200
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 69s - loss: 0.6470 - acc: 0.7126 - val_loss: 0.5500 - val_acc: 0.8446

Epoch 00001: val_loss improved from inf to 0.55003, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 45s - loss: 0.5758 - acc: 0.7643 - val_loss: 0.5693 - val_acc: 0.8003

Epoch 00002: val_loss did not improve from 0.55003
Epoch 3/40
 - 45s - loss: 0.5526 - acc: 0.7721 - val_loss: 0.5339 - val_acc: 0.7984

Epoch 00003: val_loss improved from 0.55003 to 0.53387, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 45s - loss: 0.5373 - acc: 0.7760 - val_loss: 0.5278 - val_acc: 0.8034

Epoch 00004: val_loss improved from 0.53387 to 0.52776, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 45s - loss: 0.5230 - acc: 0.7836 - val_loss: 0.5229 - val_acc: 0.8035

Epoch 00005: val_loss improved from 0.52776 to 0.52292, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 45s - loss: 0.5121 - acc: 0.7897 - val_loss: 0.5318 - val_acc: 0.7839

Epoch 00006: val_loss did not improve from 0.52292
Epoch 7/40
 - 45s - loss: 0.5029 - acc: 0.7921 - val_loss: 0.5450 - val_acc: 0.7644

Epoch 00007: val_loss did not improve from 0.52292
Epoch 8/40
 - 45s - loss: 0.4960 - acc: 0.7958 - val_loss: 0.5336 - val_acc: 0.7899

Epoch 00008: val_loss did not improve from 0.52292
Epoch 9/40
 - 45s - loss: 0.4866 - acc: 0.8015 - val_loss: 0.4947 - val_acc: 0.8070

Epoch 00009: val_loss improved from 0.52292 to 0.49469, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 45s - loss: 0.4823 - acc: 0.8015 - val_loss: 0.5093 - val_acc: 0.7917

Epoch 00010: val_loss did not improve from 0.49469
Epoch 11/40
 - 45s - loss: 0.4742 - acc: 0.8063 - val_loss: 0.5161 - val_acc: 0.7813

Epoch 00011: val_loss did not improve from 0.49469
Epoch 12/40
 - 45s - loss: 0.4707 - acc: 0.8065 - val_loss: 0.5030 - val_acc: 0.8110

Epoch 00012: val_loss did not improve from 0.49469
Epoch 13/40
 - 45s - loss: 0.4675 - acc: 0.8093 - val_loss: 0.4884 - val_acc: 0.8039

Epoch 00013: val_loss improved from 0.49469 to 0.48835, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 45s - loss: 0.4623 - acc: 0.8139 - val_loss: 0.5303 - val_acc: 0.7698

Epoch 00014: val_loss did not improve from 0.48835
Epoch 15/40
 - 45s - loss: 0.4557 - acc: 0.8161 - val_loss: 0.4899 - val_acc: 0.7978

Epoch 00015: val_loss did not improve from 0.48835
Epoch 16/40
 - 45s - loss: 0.4550 - acc: 0.8151 - val_loss: 0.4971 - val_acc: 0.7825

Epoch 00016: val_loss did not improve from 0.48835
Epoch 17/40
 - 45s - loss: 0.4488 - acc: 0.8166 - val_loss: 0.4853 - val_acc: 0.8034

Epoch 00017: val_loss improved from 0.48835 to 0.48530, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 45s - loss: 0.4469 - acc: 0.8187 - val_loss: 0.4649 - val_acc: 0.8149

Epoch 00018: val_loss improved from 0.48530 to 0.46495, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 19/40
 - 45s - loss: 0.4416 - acc: 0.8221 - val_loss: 0.4840 - val_acc: 0.8034

Epoch 00019: val_loss did not improve from 0.46495
Epoch 20/40
 - 45s - loss: 0.4379 - acc: 0.8258 - val_loss: 0.4874 - val_acc: 0.7905

Epoch 00020: val_loss did not improve from 0.46495
Epoch 21/40
 - 45s - loss: 0.4340 - acc: 0.8274 - val_loss: 0.4737 - val_acc: 0.8028

Epoch 00021: val_loss did not improve from 0.46495
Epoch 22/40
 - 45s - loss: 0.4330 - acc: 0.8258 - val_loss: 0.4662 - val_acc: 0.8052

Epoch 00022: val_loss did not improve from 0.46495
Epoch 23/40
 - 45s - loss: 0.4285 - acc: 0.8318 - val_loss: 0.4888 - val_acc: 0.7917

Epoch 00023: val_loss did not improve from 0.46495
Epoch 24/40
 - 45s - loss: 0.4265 - acc: 0.8314 - val_loss: 0.4546 - val_acc: 0.8403

Epoch 00024: val_loss improved from 0.46495 to 0.45463, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 45s - loss: 0.4216 - acc: 0.8341 - val_loss: 0.4904 - val_acc: 0.8062

Epoch 00025: val_loss did not improve from 0.45463
Epoch 26/40
 - 45s - loss: 0.4200 - acc: 0.8356 - val_loss: 0.4904 - val_acc: 0.8031

Epoch 00026: val_loss did not improve from 0.45463
Epoch 27/40
 - 45s - loss: 0.4182 - acc: 0.8362 - val_loss: 0.4878 - val_acc: 0.8077

Epoch 00027: val_loss did not improve from 0.45463
Epoch 28/40
 - 45s - loss: 0.4166 - acc: 0.8362 - val_loss: 0.4684 - val_acc: 0.8274

Epoch 00028: val_loss did not improve from 0.45463
Epoch 29/40
 - 45s - loss: 0.4104 - acc: 0.8417 - val_loss: 0.4948 - val_acc: 0.8091

Epoch 00029: val_loss did not improve from 0.45463
Epoch 30/40
 - 45s - loss: 0.4109 - acc: 0.8401 - val_loss: 0.5254 - val_acc: 0.7796

Epoch 00030: val_loss did not improve from 0.45463
Epoch 31/40
 - 45s - loss: 0.4079 - acc: 0.8410 - val_loss: 0.4626 - val_acc: 0.8239

Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00031: val_loss did not improve from 0.45463
Epoch 00031: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 442us/step
current Test accuracy: 0.8239247311827957
current auc_score ------------------>  0.9041007630939992

  32/7440 [..............................] - ETA: 36:44
 128/7440 [..............................] - ETA: 9:06 
 256/7440 [>.............................] - ETA: 4:30
 384/7440 [>.............................] - ETA: 2:58
 512/7440 [=>............................] - ETA: 2:11
 640/7440 [=>............................] - ETA: 1:44
 768/7440 [==>...........................] - ETA: 1:25
 896/7440 [==>...........................] - ETA: 1:12
1024/7440 [===>..........................] - ETA: 1:02
1152/7440 [===>..........................] - ETA: 54s 
1280/7440 [====>.........................] - ETA: 48s
1408/7440 [====>.........................] - ETA: 43s
1536/7440 [=====>........................] - ETA: 39s
1664/7440 [=====>........................] - ETA: 35s
1792/7440 [======>.......................] - ETA: 32s
1920/7440 [======>.......................] - ETA: 29s
2048/7440 [=======>......................] - ETA: 27s
2176/7440 [=======>......................] - ETA: 25s
2304/7440 [========>.....................] - ETA: 23s
2432/7440 [========>.....................] - ETA: 21s
2560/7440 [=========>....................] - ETA: 20s
2688/7440 [=========>....................] - ETA: 18s
2816/7440 [==========>...................] - ETA: 17s
2944/7440 [==========>...................] - ETA: 16s
3072/7440 [===========>..................] - ETA: 15s
3200/7440 [===========>..................] - ETA: 14s
3328/7440 [============>.................] - ETA: 13s
3456/7440 [============>.................] - ETA: 12s
3584/7440 [=============>................] - ETA: 11s
3712/7440 [=============>................] - ETA: 11s
3840/7440 [==============>...............] - ETA: 10s
3968/7440 [===============>..............] - ETA: 9s 
4096/7440 [===============>..............] - ETA: 9s
4224/7440 [================>.............] - ETA: 8s
4352/7440 [================>.............] - ETA: 8s
4480/7440 [=================>............] - ETA: 7s
4608/7440 [=================>............] - ETA: 7s
4736/7440 [==================>...........] - ETA: 6s
4864/7440 [==================>...........] - ETA: 6s
4992/7440 [===================>..........] - ETA: 5s
5120/7440 [===================>..........] - ETA: 5s
5248/7440 [====================>.........] - ETA: 4s
5376/7440 [====================>.........] - ETA: 4s
5504/7440 [=====================>........] - ETA: 4s
5632/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 3s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.8403225806451613
best saved model auc_score ------------------>  0.9093756503642039
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_35[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_305 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_305[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_306 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_306[0][0]             
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_119[0][0]            
__________________________________________________________________________________________________
activation_307 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_307[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_308 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_308[0][0]             
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 36, 96, 96)   0           concatenate_119[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_120[0][0]            
__________________________________________________________________________________________________
activation_309 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_309[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_310 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_310[0][0]             
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 46, 96, 96)   0           concatenate_120[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_121[0][0]            
__________________________________________________________________________________________________
activation_311 (Activation)     (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_311[0][0]             
__________________________________________________________________________________________________
average_pooling2d_35 (AveragePo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_35[0][0]       
__________________________________________________________________________________________________
activation_312 (Activation)     (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_312[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_313 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_313[0][0]             
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 33, 48, 48)   0           average_pooling2d_35[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_122[0][0]            
__________________________________________________________________________________________________
activation_314 (Activation)     (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_314[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_315 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_315[0][0]             
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 43, 48, 48)   0           concatenate_122[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_123[0][0]            
__________________________________________________________________________________________________
activation_316 (Activation)     (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_316[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_317 (Activation)     (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_317[0][0]             
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 53, 48, 48)   0           concatenate_123[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 53, 48, 48)   212         concatenate_124[0][0]            
__________________________________________________________________________________________________
activation_318 (Activation)     (None, 53, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_35 (Gl (None, 53)           0           activation_318[0][0]             
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 1)            54          global_average_pooling2d_35[0][0]
==================================================================================================
Total params: 32,144
Trainable params: 31,112
Non-trainable params: 1,032
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 67s - loss: 0.6546 - acc: 0.6947 - val_loss: 0.5617 - val_acc: 0.8289

Epoch 00001: val_loss improved from inf to 0.56173, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 42s - loss: 0.5917 - acc: 0.7467 - val_loss: 0.5414 - val_acc: 0.8442

Epoch 00002: val_loss improved from 0.56173 to 0.54142, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 42s - loss: 0.5676 - acc: 0.7567 - val_loss: 0.5223 - val_acc: 0.8349

Epoch 00003: val_loss improved from 0.54142 to 0.52230, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 42s - loss: 0.5478 - acc: 0.7664 - val_loss: 0.5205 - val_acc: 0.8290

Epoch 00004: val_loss improved from 0.52230 to 0.52047, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 42s - loss: 0.5309 - acc: 0.7745 - val_loss: 0.5049 - val_acc: 0.8316

Epoch 00005: val_loss improved from 0.52047 to 0.50488, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 42s - loss: 0.5199 - acc: 0.7795 - val_loss: 0.5188 - val_acc: 0.8258

Epoch 00006: val_loss did not improve from 0.50488
Epoch 7/40
 - 42s - loss: 0.5120 - acc: 0.7835 - val_loss: 0.4828 - val_acc: 0.8309

Epoch 00007: val_loss improved from 0.50488 to 0.48279, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 42s - loss: 0.5025 - acc: 0.7884 - val_loss: 0.4987 - val_acc: 0.8403

Epoch 00008: val_loss did not improve from 0.48279
Epoch 9/40
 - 42s - loss: 0.4956 - acc: 0.7914 - val_loss: 0.4786 - val_acc: 0.8245

Epoch 00009: val_loss improved from 0.48279 to 0.47860, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 42s - loss: 0.4907 - acc: 0.7923 - val_loss: 0.4890 - val_acc: 0.8226

Epoch 00010: val_loss did not improve from 0.47860
Epoch 11/40
 - 42s - loss: 0.4856 - acc: 0.7960 - val_loss: 0.4903 - val_acc: 0.8181

Epoch 00011: val_loss did not improve from 0.47860
Epoch 12/40
 - 42s - loss: 0.4784 - acc: 0.8000 - val_loss: 0.4883 - val_acc: 0.8249

Epoch 00012: val_loss did not improve from 0.47860
Epoch 13/40
 - 42s - loss: 0.4721 - acc: 0.8019 - val_loss: 0.4859 - val_acc: 0.8098

Epoch 00013: val_loss did not improve from 0.47860
Epoch 14/40
 - 42s - loss: 0.4675 - acc: 0.8043 - val_loss: 0.4777 - val_acc: 0.8192

Epoch 00014: val_loss improved from 0.47860 to 0.47769, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 42s - loss: 0.4638 - acc: 0.8047 - val_loss: 0.4853 - val_acc: 0.8163

Epoch 00015: val_loss did not improve from 0.47769
Epoch 16/40
 - 42s - loss: 0.4598 - acc: 0.8074 - val_loss: 0.5066 - val_acc: 0.7950

Epoch 00016: val_loss did not improve from 0.47769
Epoch 17/40
 - 42s - loss: 0.4565 - acc: 0.8098 - val_loss: 0.4958 - val_acc: 0.8161

Epoch 00017: val_loss did not improve from 0.47769
Epoch 18/40
 - 42s - loss: 0.4524 - acc: 0.8099 - val_loss: 0.4800 - val_acc: 0.8089

Epoch 00018: val_loss did not improve from 0.47769
Epoch 19/40
 - 42s - loss: 0.4499 - acc: 0.8143 - val_loss: 0.4719 - val_acc: 0.8242

Epoch 00019: val_loss improved from 0.47769 to 0.47192, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 42s - loss: 0.4467 - acc: 0.8164 - val_loss: 0.4763 - val_acc: 0.8090

Epoch 00020: val_loss did not improve from 0.47192
Epoch 21/40
 - 42s - loss: 0.4425 - acc: 0.8179 - val_loss: 0.4716 - val_acc: 0.8403

Epoch 00021: val_loss improved from 0.47192 to 0.47159, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 22/40
 - 42s - loss: 0.4412 - acc: 0.8182 - val_loss: 0.4735 - val_acc: 0.8383

Epoch 00022: val_loss did not improve from 0.47159
Epoch 23/40
 - 42s - loss: 0.4358 - acc: 0.8226 - val_loss: 0.4601 - val_acc: 0.8344

Epoch 00023: val_loss improved from 0.47159 to 0.46010, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 24/40
 - 42s - loss: 0.4332 - acc: 0.8215 - val_loss: 0.4502 - val_acc: 0.8218

Epoch 00024: val_loss improved from 0.46010 to 0.45020, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 25/40
 - 42s - loss: 0.4304 - acc: 0.8244 - val_loss: 0.4607 - val_acc: 0.8137

Epoch 00025: val_loss did not improve from 0.45020
Epoch 26/40
 - 42s - loss: 0.4276 - acc: 0.8263 - val_loss: 0.4660 - val_acc: 0.8266

Epoch 00026: val_loss did not improve from 0.45020
Epoch 27/40
 - 42s - loss: 0.4249 - acc: 0.8278 - val_loss: 0.4876 - val_acc: 0.8220

Epoch 00027: val_loss did not improve from 0.45020
Epoch 28/40
 - 42s - loss: 0.4224 - acc: 0.8288 - val_loss: 0.4723 - val_acc: 0.8238

Epoch 00028: val_loss did not improve from 0.45020
Epoch 29/40
 - 42s - loss: 0.4191 - acc: 0.8291 - val_loss: 0.4686 - val_acc: 0.8378

Epoch 00029: val_loss did not improve from 0.45020
Epoch 30/40
 - 42s - loss: 0.4184 - acc: 0.8302 - val_loss: 0.4505 - val_acc: 0.8386

Epoch 00030: val_loss did not improve from 0.45020
Epoch 31/40
 - 42s - loss: 0.4156 - acc: 0.8328 - val_loss: 0.4761 - val_acc: 0.7926

Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00031: val_loss did not improve from 0.45020
Epoch 00031: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 408us/step
current Test accuracy: 0.7926075268817204
current auc_score ------------------>  0.8942470950398891

  32/7440 [..............................] - ETA: 39:16
 128/7440 [..............................] - ETA: 9:44 
 256/7440 [>.............................] - ETA: 4:48
 384/7440 [>.............................] - ETA: 3:10
 512/7440 [=>............................] - ETA: 2:20
 640/7440 [=>............................] - ETA: 1:51
 768/7440 [==>...........................] - ETA: 1:31
 896/7440 [==>...........................] - ETA: 1:17
1024/7440 [===>..........................] - ETA: 1:06
1152/7440 [===>..........................] - ETA: 58s 
1280/7440 [====>.........................] - ETA: 51s
1408/7440 [====>.........................] - ETA: 46s
1536/7440 [=====>........................] - ETA: 41s
1664/7440 [=====>........................] - ETA: 37s
1792/7440 [======>.......................] - ETA: 34s
1920/7440 [======>.......................] - ETA: 31s
2048/7440 [=======>......................] - ETA: 29s
2176/7440 [=======>......................] - ETA: 26s
2304/7440 [========>.....................] - ETA: 24s
2432/7440 [========>.....................] - ETA: 23s
2560/7440 [=========>....................] - ETA: 21s
2688/7440 [=========>....................] - ETA: 20s
2816/7440 [==========>...................] - ETA: 18s
2944/7440 [==========>...................] - ETA: 17s
3072/7440 [===========>..................] - ETA: 16s
3200/7440 [===========>..................] - ETA: 15s
3328/7440 [============>.................] - ETA: 14s
3456/7440 [============>.................] - ETA: 13s
3584/7440 [=============>................] - ETA: 12s
3712/7440 [=============>................] - ETA: 11s
3840/7440 [==============>...............] - ETA: 11s
3968/7440 [===============>..............] - ETA: 10s
4096/7440 [===============>..............] - ETA: 9s 
4224/7440 [================>.............] - ETA: 9s
4352/7440 [================>.............] - ETA: 8s
4480/7440 [=================>............] - ETA: 7s
4608/7440 [=================>............] - ETA: 7s
4736/7440 [==================>...........] - ETA: 6s
4864/7440 [==================>...........] - ETA: 6s
4992/7440 [===================>..........] - ETA: 6s
5120/7440 [===================>..........] - ETA: 5s
5248/7440 [====================>.........] - ETA: 5s
5376/7440 [====================>.........] - ETA: 4s
5504/7440 [=====================>........] - ETA: 4s
5632/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 3s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.8217741935483871
best saved model auc_score ------------------>  0.9016586816394958
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_36 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_36[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_319 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_319[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_320 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_320[0][0]             
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_125[0][0]            
__________________________________________________________________________________________________
activation_321 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_321[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_322 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_322[0][0]             
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 48, 96, 96)   0           concatenate_125[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_126[0][0]            
__________________________________________________________________________________________________
activation_323 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_323[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_324 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_324[0][0]             
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 64, 96, 96)   0           concatenate_126[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_127[0][0]            
__________________________________________________________________________________________________
activation_325 (Activation)     (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_325[0][0]             
__________________________________________________________________________________________________
average_pooling2d_36 (AveragePo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_36[0][0]       
__________________________________________________________________________________________________
activation_326 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_326[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_327 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_327[0][0]             
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 48, 48, 48)   0           average_pooling2d_36[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_128[0][0]            
__________________________________________________________________________________________________
activation_328 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_328[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_329 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_329[0][0]             
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 64, 48, 48)   0           concatenate_128[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_129[0][0]            
__________________________________________________________________________________________________
activation_330 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_330[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_331 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_331[0][0]             
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 80, 48, 48)   0           concatenate_129[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 80, 48, 48)   320         concatenate_130[0][0]            
__________________________________________________________________________________________________
activation_332 (Activation)     (None, 80, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_36 (Gl (None, 80)           0           activation_332[0][0]             
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 1)            81          global_average_pooling2d_36[0][0]
==================================================================================================
Total params: 76,145
Trainable params: 74,609
Non-trainable params: 1,536
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 79s - loss: 0.6536 - acc: 0.7135 - val_loss: 0.5575 - val_acc: 0.8483

Epoch 00001: val_loss improved from inf to 0.55749, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 53s - loss: 0.5780 - acc: 0.7671 - val_loss: 0.5482 - val_acc: 0.8161

Epoch 00002: val_loss improved from 0.55749 to 0.54821, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 53s - loss: 0.5503 - acc: 0.7770 - val_loss: 0.5494 - val_acc: 0.8090

Epoch 00003: val_loss did not improve from 0.54821
Epoch 4/40
 - 53s - loss: 0.5321 - acc: 0.7848 - val_loss: 0.5702 - val_acc: 0.7883

Epoch 00004: val_loss did not improve from 0.54821
Epoch 5/40
 - 53s - loss: 0.5193 - acc: 0.7899 - val_loss: 0.5974 - val_acc: 0.8067

Epoch 00005: val_loss did not improve from 0.54821
Epoch 6/40
 - 53s - loss: 0.5064 - acc: 0.7983 - val_loss: 0.5384 - val_acc: 0.8476

Epoch 00006: val_loss improved from 0.54821 to 0.53843, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 53s - loss: 0.4972 - acc: 0.8019 - val_loss: 0.5509 - val_acc: 0.8114

Epoch 00007: val_loss did not improve from 0.53843
Epoch 8/40
 - 53s - loss: 0.4878 - acc: 0.8073 - val_loss: 0.4952 - val_acc: 0.8398

Epoch 00008: val_loss improved from 0.53843 to 0.49524, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 53s - loss: 0.4805 - acc: 0.8108 - val_loss: 0.5672 - val_acc: 0.8226

Epoch 00009: val_loss did not improve from 0.49524
Epoch 10/40
 - 53s - loss: 0.4711 - acc: 0.8140 - val_loss: 0.5808 - val_acc: 0.7886

Epoch 00010: val_loss did not improve from 0.49524
Epoch 11/40
 - 53s - loss: 0.4667 - acc: 0.8185 - val_loss: 0.5270 - val_acc: 0.8227

Epoch 00011: val_loss did not improve from 0.49524
Epoch 12/40
 - 53s - loss: 0.4616 - acc: 0.8205 - val_loss: 0.6430 - val_acc: 0.7965

Epoch 00012: val_loss did not improve from 0.49524
Epoch 13/40
 - 53s - loss: 0.4557 - acc: 0.8229 - val_loss: 0.5093 - val_acc: 0.8058

Epoch 00013: val_loss did not improve from 0.49524
Epoch 14/40
 - 53s - loss: 0.4491 - acc: 0.8275 - val_loss: 0.4735 - val_acc: 0.8465

Epoch 00014: val_loss improved from 0.49524 to 0.47347, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 53s - loss: 0.4452 - acc: 0.8304 - val_loss: 0.5844 - val_acc: 0.7949

Epoch 00015: val_loss did not improve from 0.47347
Epoch 16/40
 - 53s - loss: 0.4424 - acc: 0.8321 - val_loss: 0.4930 - val_acc: 0.8173

Epoch 00016: val_loss did not improve from 0.47347
Epoch 17/40
 - 53s - loss: 0.4349 - acc: 0.8353 - val_loss: 0.7660 - val_acc: 0.7702

Epoch 00017: val_loss did not improve from 0.47347
Epoch 18/40
 - 53s - loss: 0.4320 - acc: 0.8377 - val_loss: 0.6660 - val_acc: 0.7848

Epoch 00018: val_loss did not improve from 0.47347
Epoch 19/40
 - 53s - loss: 0.4279 - acc: 0.8413 - val_loss: 0.5419 - val_acc: 0.7884

Epoch 00019: val_loss did not improve from 0.47347
Epoch 20/40
 - 53s - loss: 0.4222 - acc: 0.8424 - val_loss: 0.5289 - val_acc: 0.8050

Epoch 00020: val_loss did not improve from 0.47347
Epoch 21/40
 - 53s - loss: 0.4188 - acc: 0.8449 - val_loss: 0.5095 - val_acc: 0.8038

Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00021: val_loss did not improve from 0.47347
Epoch 00021: early stopping

  32/7440 [..............................] - ETA: 5s
 128/7440 [..............................] - ETA: 4s
 224/7440 [..............................] - ETA: 4s
 320/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 640/7440 [=>............................] - ETA: 3s
 768/7440 [==>...........................] - ETA: 3s
 896/7440 [==>...........................] - ETA: 3s
1024/7440 [===>..........................] - ETA: 3s
1152/7440 [===>..........................] - ETA: 3s
1280/7440 [====>.........................] - ETA: 3s
1408/7440 [====>.........................] - ETA: 3s
1536/7440 [=====>........................] - ETA: 3s
1664/7440 [=====>........................] - ETA: 3s
1792/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2048/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2304/7440 [========>.....................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 2s
2560/7440 [=========>....................] - ETA: 2s
2688/7440 [=========>....................] - ETA: 2s
2816/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3456/7440 [============>.................] - ETA: 2s
3584/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4608/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 511us/step
current Test accuracy: 0.803763440860215
current auc_score ------------------>  0.8860632009480867

  32/7440 [..............................] - ETA: 43:42
 128/7440 [..............................] - ETA: 10:50
 224/7440 [..............................] - ETA: 6:08 
 320/7440 [>.............................] - ETA: 4:15
 448/7440 [>.............................] - ETA: 3:00
 576/7440 [=>............................] - ETA: 2:18
 672/7440 [=>............................] - ETA: 1:57
 800/7440 [==>...........................] - ETA: 1:37
 928/7440 [==>...........................] - ETA: 1:22
1056/7440 [===>..........................] - ETA: 1:11
1184/7440 [===>..........................] - ETA: 1:03
1312/7440 [====>.........................] - ETA: 56s 
1440/7440 [====>.........................] - ETA: 50s
1568/7440 [=====>........................] - ETA: 45s
1696/7440 [=====>........................] - ETA: 41s
1824/7440 [======>.......................] - ETA: 37s
1952/7440 [======>.......................] - ETA: 34s
2080/7440 [=======>......................] - ETA: 31s
2208/7440 [=======>......................] - ETA: 29s
2304/7440 [========>.....................] - ETA: 27s
2432/7440 [========>.....................] - ETA: 25s
2560/7440 [=========>....................] - ETA: 24s
2688/7440 [=========>....................] - ETA: 22s
2816/7440 [==========>...................] - ETA: 20s
2944/7440 [==========>...................] - ETA: 19s
3072/7440 [===========>..................] - ETA: 18s
3200/7440 [===========>..................] - ETA: 17s
3328/7440 [============>.................] - ETA: 16s
3456/7440 [============>.................] - ETA: 15s
3584/7440 [=============>................] - ETA: 14s
3712/7440 [=============>................] - ETA: 13s
3840/7440 [==============>...............] - ETA: 12s
3968/7440 [===============>..............] - ETA: 11s
4064/7440 [===============>..............] - ETA: 11s
4192/7440 [===============>..............] - ETA: 10s
4320/7440 [================>.............] - ETA: 9s 
4416/7440 [================>.............] - ETA: 9s
4512/7440 [=================>............] - ETA: 8s
4640/7440 [=================>............] - ETA: 8s
4768/7440 [==================>...........] - ETA: 7s
4896/7440 [==================>...........] - ETA: 7s
5024/7440 [===================>..........] - ETA: 6s
5152/7440 [===================>..........] - ETA: 6s
5280/7440 [====================>.........] - ETA: 5s
5408/7440 [====================>.........] - ETA: 5s
5536/7440 [=====================>........] - ETA: 4s
5664/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 15s 2ms/step
Best saved model Test accuracy: 0.8465053763440861
best saved model auc_score ------------------>  0.906364897676032
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_37[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_333 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_333[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_334 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_334[0][0]             
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_131[0][0]            
__________________________________________________________________________________________________
activation_335 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_335[0][0]             
__________________________________________________________________________________________________
average_pooling2d_37 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_37[0][0]       
__________________________________________________________________________________________________
activation_336 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_336[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_337 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_337[0][0]             
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 26, 48, 48)   0           average_pooling2d_37[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_132[0][0]            
__________________________________________________________________________________________________
activation_338 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_37 (Gl (None, 26)           0           activation_338[0][0]             
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 1)            27          global_average_pooling2d_37[0][0]
==================================================================================================
Total params: 13,235
Trainable params: 12,875
Non-trainable params: 360
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 48s - loss: 0.6501 - acc: 0.6855 - val_loss: 0.5482 - val_acc: 0.8183

Epoch 00001: val_loss improved from inf to 0.54818, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 22s - loss: 0.6083 - acc: 0.7248 - val_loss: 0.5304 - val_acc: 0.8327

Epoch 00002: val_loss improved from 0.54818 to 0.53039, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 22s - loss: 0.5911 - acc: 0.7362 - val_loss: 0.5302 - val_acc: 0.8590

Epoch 00003: val_loss improved from 0.53039 to 0.53020, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 22s - loss: 0.5767 - acc: 0.7469 - val_loss: 0.5172 - val_acc: 0.8620

Epoch 00004: val_loss improved from 0.53020 to 0.51725, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 23s - loss: 0.5644 - acc: 0.7545 - val_loss: 0.5132 - val_acc: 0.8577

Epoch 00005: val_loss improved from 0.51725 to 0.51316, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 22s - loss: 0.5545 - acc: 0.7570 - val_loss: 0.5089 - val_acc: 0.8539

Epoch 00006: val_loss improved from 0.51316 to 0.50889, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 23s - loss: 0.5465 - acc: 0.7614 - val_loss: 0.5009 - val_acc: 0.8492

Epoch 00007: val_loss improved from 0.50889 to 0.50086, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 22s - loss: 0.5392 - acc: 0.7643 - val_loss: 0.5180 - val_acc: 0.8409

Epoch 00008: val_loss did not improve from 0.50086
Epoch 9/40
 - 22s - loss: 0.5328 - acc: 0.7661 - val_loss: 0.5132 - val_acc: 0.8394

Epoch 00009: val_loss did not improve from 0.50086
Epoch 10/40
 - 22s - loss: 0.5283 - acc: 0.7666 - val_loss: 0.5092 - val_acc: 0.8388

Epoch 00010: val_loss did not improve from 0.50086
Epoch 11/40
 - 23s - loss: 0.5237 - acc: 0.7673 - val_loss: 0.4991 - val_acc: 0.8441

Epoch 00011: val_loss improved from 0.50086 to 0.49908, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 22s - loss: 0.5179 - acc: 0.7695 - val_loss: 0.4982 - val_acc: 0.8405

Epoch 00012: val_loss improved from 0.49908 to 0.49820, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 22s - loss: 0.5150 - acc: 0.7696 - val_loss: 0.4967 - val_acc: 0.8329

Epoch 00013: val_loss improved from 0.49820 to 0.49671, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 22s - loss: 0.5114 - acc: 0.7704 - val_loss: 0.5052 - val_acc: 0.8208

Epoch 00014: val_loss did not improve from 0.49671
Epoch 15/40
 - 22s - loss: 0.5086 - acc: 0.7715 - val_loss: 0.4947 - val_acc: 0.8336

Epoch 00015: val_loss improved from 0.49671 to 0.49469, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 16/40
 - 22s - loss: 0.5040 - acc: 0.7730 - val_loss: 0.4916 - val_acc: 0.8266

Epoch 00016: val_loss improved from 0.49469 to 0.49160, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 22s - loss: 0.5029 - acc: 0.7727 - val_loss: 0.5318 - val_acc: 0.8054

Epoch 00017: val_loss did not improve from 0.49160
Epoch 18/40
 - 22s - loss: 0.4999 - acc: 0.7764 - val_loss: 0.4933 - val_acc: 0.8243

Epoch 00018: val_loss did not improve from 0.49160
Epoch 19/40
 - 23s - loss: 0.4970 - acc: 0.7764 - val_loss: 0.4861 - val_acc: 0.8282

Epoch 00019: val_loss improved from 0.49160 to 0.48610, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 20/40
 - 22s - loss: 0.4939 - acc: 0.7762 - val_loss: 0.4894 - val_acc: 0.8250

Epoch 00020: val_loss did not improve from 0.48610
Epoch 21/40
 - 22s - loss: 0.4917 - acc: 0.7774 - val_loss: 0.4977 - val_acc: 0.8223

Epoch 00021: val_loss did not improve from 0.48610
Epoch 22/40
 - 22s - loss: 0.4896 - acc: 0.7784 - val_loss: 0.4957 - val_acc: 0.8116

Epoch 00022: val_loss did not improve from 0.48610
Epoch 23/40
 - 22s - loss: 0.4864 - acc: 0.7799 - val_loss: 0.4870 - val_acc: 0.8249

Epoch 00023: val_loss did not improve from 0.48610
Epoch 24/40
 - 22s - loss: 0.4861 - acc: 0.7805 - val_loss: 0.4919 - val_acc: 0.8191

Epoch 00024: val_loss did not improve from 0.48610
Epoch 25/40
 - 22s - loss: 0.4832 - acc: 0.7816 - val_loss: 0.4950 - val_acc: 0.8272

Epoch 00025: val_loss did not improve from 0.48610
Epoch 26/40
 - 23s - loss: 0.4828 - acc: 0.7810 - val_loss: 0.4840 - val_acc: 0.8246

Epoch 00026: val_loss improved from 0.48610 to 0.48399, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 27/40
 - 22s - loss: 0.4813 - acc: 0.7819 - val_loss: 0.4929 - val_acc: 0.8114

Epoch 00027: val_loss did not improve from 0.48399
Epoch 28/40
 - 22s - loss: 0.4777 - acc: 0.7828 - val_loss: 0.4971 - val_acc: 0.8184

Epoch 00028: val_loss did not improve from 0.48399
Epoch 29/40
 - 22s - loss: 0.4766 - acc: 0.7846 - val_loss: 0.4807 - val_acc: 0.8177

Epoch 00029: val_loss improved from 0.48399 to 0.48067, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 30/40
 - 22s - loss: 0.4739 - acc: 0.7849 - val_loss: 0.4773 - val_acc: 0.8137

Epoch 00030: val_loss improved from 0.48067 to 0.47734, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 31/40
 - 22s - loss: 0.4733 - acc: 0.7868 - val_loss: 0.4827 - val_acc: 0.8113

Epoch 00031: val_loss did not improve from 0.47734
Epoch 32/40
 - 22s - loss: 0.4737 - acc: 0.7855 - val_loss: 0.4758 - val_acc: 0.8194

Epoch 00032: val_loss improved from 0.47734 to 0.47580, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 33/40
 - 22s - loss: 0.4698 - acc: 0.7886 - val_loss: 0.5036 - val_acc: 0.8090

Epoch 00033: val_loss did not improve from 0.47580
Epoch 34/40
 - 22s - loss: 0.4685 - acc: 0.7864 - val_loss: 0.4846 - val_acc: 0.8210

Epoch 00034: val_loss did not improve from 0.47580
Epoch 35/40
 - 22s - loss: 0.4687 - acc: 0.7855 - val_loss: 0.4902 - val_acc: 0.8043

Epoch 00035: val_loss did not improve from 0.47580
Epoch 36/40
 - 22s - loss: 0.4657 - acc: 0.7891 - val_loss: 0.4950 - val_acc: 0.8089

Epoch 00036: val_loss did not improve from 0.47580
Epoch 37/40
 - 22s - loss: 0.4648 - acc: 0.7886 - val_loss: 0.5142 - val_acc: 0.7837

Epoch 00037: val_loss did not improve from 0.47580
Epoch 38/40
 - 22s - loss: 0.4633 - acc: 0.7907 - val_loss: 0.4985 - val_acc: 0.8091

Epoch 00038: val_loss did not improve from 0.47580
Epoch 39/40
 - 22s - loss: 0.4622 - acc: 0.7906 - val_loss: 0.4898 - val_acc: 0.8202

Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00039: val_loss did not improve from 0.47580
Epoch 00039: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 272us/step
current Test accuracy: 0.8201612903225807
current auc_score ------------------>  0.8676190166493235

  32/7440 [..............................] - ETA: 43:15
 192/7440 [..............................] - ETA: 7:05 
 384/7440 [>.............................] - ETA: 3:28
 576/7440 [=>............................] - ETA: 2:15
 768/7440 [==>...........................] - ETA: 1:39
 960/7440 [==>...........................] - ETA: 1:17
1152/7440 [===>..........................] - ETA: 1:02
1344/7440 [====>.........................] - ETA: 52s 
1536/7440 [=====>........................] - ETA: 44s
1728/7440 [=====>........................] - ETA: 38s
1920/7440 [======>.......................] - ETA: 33s
2112/7440 [=======>......................] - ETA: 29s
2304/7440 [========>.....................] - ETA: 26s
2496/7440 [=========>....................] - ETA: 23s
2688/7440 [=========>....................] - ETA: 21s
2880/7440 [==========>...................] - ETA: 19s
3072/7440 [===========>..................] - ETA: 17s
3264/7440 [============>.................] - ETA: 15s
3456/7440 [============>.................] - ETA: 14s
3648/7440 [=============>................] - ETA: 12s
3840/7440 [==============>...............] - ETA: 11s
4032/7440 [===============>..............] - ETA: 10s
4224/7440 [================>.............] - ETA: 9s 
4384/7440 [================>.............] - ETA: 8s
4576/7440 [=================>............] - ETA: 7s
4768/7440 [==================>...........] - ETA: 7s
4960/7440 [===================>..........] - ETA: 6s
5152/7440 [===================>..........] - ETA: 5s
5344/7440 [====================>.........] - ETA: 4s
5536/7440 [=====================>........] - ETA: 4s
5728/7440 [======================>.......] - ETA: 3s
5920/7440 [======================>.......] - ETA: 3s
6112/7440 [=======================>......] - ETA: 2s
6304/7440 [========================>.....] - ETA: 2s
6496/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7072/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.8193548387096774
best saved model auc_score ------------------>  0.8724796580529541
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_38 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_38[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_339 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_339[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_340 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_340[0][0]             
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_133[0][0]            
__________________________________________________________________________________________________
activation_341 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_341[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_342 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_342[0][0]             
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 28, 96, 96)   0           concatenate_133[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_134[0][0]            
__________________________________________________________________________________________________
activation_343 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_343[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_344 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_344[0][0]             
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 34, 96, 96)   0           concatenate_134[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 34, 96, 96)   136         concatenate_135[0][0]            
__________________________________________________________________________________________________
activation_345 (Activation)     (None, 34, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 17, 96, 96)   578         activation_345[0][0]             
__________________________________________________________________________________________________
average_pooling2d_38 (AveragePo (None, 17, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 17, 48, 48)   68          average_pooling2d_38[0][0]       
__________________________________________________________________________________________________
activation_346 (Activation)     (None, 17, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   408         activation_346[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_347 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_347[0][0]             
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 23, 48, 48)   0           average_pooling2d_38[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 23, 48, 48)   92          concatenate_136[0][0]            
__________________________________________________________________________________________________
activation_348 (Activation)     (None, 23, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   552         activation_348[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_349 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_349[0][0]             
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 29, 48, 48)   0           concatenate_136[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 29, 48, 48)   116         concatenate_137[0][0]            
__________________________________________________________________________________________________
activation_350 (Activation)     (None, 29, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 24, 48, 48)   696         activation_350[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_351 (Activation)     (None, 24, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_351[0][0]             
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 35, 48, 48)   0           concatenate_137[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 35, 48, 48)   140         concatenate_138[0][0]            
__________________________________________________________________________________________________
activation_352 (Activation)     (None, 35, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_38 (Gl (None, 35)           0           activation_352[0][0]             
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 1)            36          global_average_pooling2d_38[0][0]
==================================================================================================
Total params: 13,310
Trainable params: 12,614
Non-trainable params: 696
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 64s - loss: 0.6492 - acc: 0.6793 - val_loss: 0.5545 - val_acc: 0.8368

Epoch 00001: val_loss improved from inf to 0.55446, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 35s - loss: 0.5918 - acc: 0.7438 - val_loss: 0.5191 - val_acc: 0.8526

Epoch 00002: val_loss improved from 0.55446 to 0.51913, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 35s - loss: 0.5662 - acc: 0.7540 - val_loss: 0.5157 - val_acc: 0.8341

Epoch 00003: val_loss improved from 0.51913 to 0.51574, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 35s - loss: 0.5501 - acc: 0.7583 - val_loss: 0.5110 - val_acc: 0.8239

Epoch 00004: val_loss improved from 0.51574 to 0.51101, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 35s - loss: 0.5394 - acc: 0.7587 - val_loss: 0.5077 - val_acc: 0.8175

Epoch 00005: val_loss improved from 0.51101 to 0.50767, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 35s - loss: 0.5311 - acc: 0.7611 - val_loss: 0.5046 - val_acc: 0.8164

Epoch 00006: val_loss improved from 0.50767 to 0.50463, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 35s - loss: 0.5246 - acc: 0.7646 - val_loss: 0.5013 - val_acc: 0.8112

Epoch 00007: val_loss improved from 0.50463 to 0.50129, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 35s - loss: 0.5189 - acc: 0.7646 - val_loss: 0.4953 - val_acc: 0.8171

Epoch 00008: val_loss improved from 0.50129 to 0.49525, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 35s - loss: 0.5123 - acc: 0.7708 - val_loss: 0.4927 - val_acc: 0.8147

Epoch 00009: val_loss improved from 0.49525 to 0.49267, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 10/40
 - 35s - loss: 0.5067 - acc: 0.7724 - val_loss: 0.4938 - val_acc: 0.8066

Epoch 00010: val_loss did not improve from 0.49267
Epoch 11/40
 - 35s - loss: 0.5018 - acc: 0.7712 - val_loss: 0.4886 - val_acc: 0.8062

Epoch 00011: val_loss improved from 0.49267 to 0.48863, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 35s - loss: 0.4971 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.8059

Epoch 00012: val_loss improved from 0.48863 to 0.48843, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 35s - loss: 0.4916 - acc: 0.7768 - val_loss: 0.4762 - val_acc: 0.8199

Epoch 00013: val_loss improved from 0.48843 to 0.47620, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 35s - loss: 0.4867 - acc: 0.7816 - val_loss: 0.4758 - val_acc: 0.8145

Epoch 00014: val_loss improved from 0.47620 to 0.47580, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 35s - loss: 0.4830 - acc: 0.7831 - val_loss: 0.4844 - val_acc: 0.8227

Epoch 00015: val_loss did not improve from 0.47580
Epoch 16/40
 - 35s - loss: 0.4794 - acc: 0.7858 - val_loss: 0.4813 - val_acc: 0.8077

Epoch 00016: val_loss did not improve from 0.47580
Epoch 17/40
 - 35s - loss: 0.4755 - acc: 0.7856 - val_loss: 0.4776 - val_acc: 0.8306

Epoch 00017: val_loss did not improve from 0.47580
Epoch 18/40
 - 36s - loss: 0.4736 - acc: 0.7873 - val_loss: 0.4844 - val_acc: 0.8263

Epoch 00018: val_loss did not improve from 0.47580
Epoch 19/40
 - 35s - loss: 0.4694 - acc: 0.7890 - val_loss: 0.5078 - val_acc: 0.7960

Epoch 00019: val_loss did not improve from 0.47580
Epoch 20/40
 - 35s - loss: 0.4652 - acc: 0.7931 - val_loss: 0.4788 - val_acc: 0.8180

Epoch 00020: val_loss did not improve from 0.47580
Epoch 21/40
 - 35s - loss: 0.4620 - acc: 0.7933 - val_loss: 0.4842 - val_acc: 0.8118

Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00021: val_loss did not improve from 0.47580
Epoch 00021: early stopping

  32/7440 [..............................] - ETA: 4s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 352us/step
current Test accuracy: 0.8118279569892473
current auc_score ------------------>  0.8751923994103363

  32/7440 [..............................] - ETA: 45:46
 160/7440 [..............................] - ETA: 9:02 
 320/7440 [>.............................] - ETA: 4:26
 480/7440 [>.............................] - ETA: 2:54
 640/7440 [=>............................] - ETA: 2:08
 800/7440 [==>...........................] - ETA: 1:40
 960/7440 [==>...........................] - ETA: 1:22
1120/7440 [===>..........................] - ETA: 1:09
1280/7440 [====>.........................] - ETA: 59s 
1440/7440 [====>.........................] - ETA: 51s
1600/7440 [=====>........................] - ETA: 45s
1760/7440 [======>.......................] - ETA: 40s
1920/7440 [======>.......................] - ETA: 36s
2080/7440 [=======>......................] - ETA: 32s
2240/7440 [========>.....................] - ETA: 29s
2400/7440 [========>.....................] - ETA: 26s
2560/7440 [=========>....................] - ETA: 24s
2720/7440 [=========>....................] - ETA: 22s
2880/7440 [==========>...................] - ETA: 20s
3040/7440 [===========>..................] - ETA: 18s
3200/7440 [===========>..................] - ETA: 17s
3360/7440 [============>.................] - ETA: 15s
3520/7440 [=============>................] - ETA: 14s
3680/7440 [=============>................] - ETA: 13s
3840/7440 [==============>...............] - ETA: 12s
4000/7440 [===============>..............] - ETA: 11s
4160/7440 [===============>..............] - ETA: 10s
4320/7440 [================>.............] - ETA: 9s 
4480/7440 [=================>............] - ETA: 8s
4640/7440 [=================>............] - ETA: 8s
4800/7440 [==================>...........] - ETA: 7s
4960/7440 [===================>..........] - ETA: 6s
5120/7440 [===================>..........] - ETA: 6s
5280/7440 [====================>.........] - ETA: 5s
5440/7440 [====================>.........] - ETA: 5s
5600/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 4s
5920/7440 [======================>.......] - ETA: 3s
6080/7440 [=======================>......] - ETA: 3s
6240/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6560/7440 [=========================>....] - ETA: 1s
6720/7440 [==========================>...] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 15s 2ms/step
Best saved model Test accuracy: 0.8145161290322581
best saved model auc_score ------------------>  0.8779610359579142
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_39[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_353 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_353[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_354 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_354[0][0]             
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_139[0][0]            
__________________________________________________________________________________________________
activation_355 (Activation)     (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 56, 96, 96)   1680        activation_355[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_356 (Activation)     (None, 56, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_356[0][0]             
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 44, 96, 96)   0           concatenate_139[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 44, 96, 96)   176         concatenate_140[0][0]            
__________________________________________________________________________________________________
activation_357 (Activation)     (None, 44, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 56, 96, 96)   2464        activation_357[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_358 (Activation)     (None, 56, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_358[0][0]             
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 58, 96, 96)   0           concatenate_140[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 58, 96, 96)   232         concatenate_141[0][0]            
__________________________________________________________________________________________________
activation_359 (Activation)     (None, 58, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 29, 96, 96)   1682        activation_359[0][0]             
__________________________________________________________________________________________________
average_pooling2d_39 (AveragePo (None, 29, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 29, 48, 48)   116         average_pooling2d_39[0][0]       
__________________________________________________________________________________________________
activation_360 (Activation)     (None, 29, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   1624        activation_360[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_361 (Activation)     (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_361[0][0]             
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 43, 48, 48)   0           average_pooling2d_39[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_142[0][0]            
__________________________________________________________________________________________________
activation_362 (Activation)     (None, 43, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 56, 48, 48)   2408        activation_362[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_363 (Activation)     (None, 56, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_363[0][0]             
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 57, 48, 48)   0           concatenate_142[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 57, 48, 48)   228         concatenate_143[0][0]            
__________________________________________________________________________________________________
activation_364 (Activation)     (None, 57, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 56, 48, 48)   3192        activation_364[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_365 (Activation)     (None, 56, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_365[0][0]             
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 71, 48, 48)   0           concatenate_143[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 71, 48, 48)   284         concatenate_144[0][0]            
__________________________________________________________________________________________________
activation_366 (Activation)     (None, 71, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_39 (Gl (None, 71)           0           activation_366[0][0]             
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 1)            72          global_average_pooling2d_39[0][0]
==================================================================================================
Total params: 59,378
Trainable params: 58,010
Non-trainable params: 1,368
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 79s - loss: 0.6615 - acc: 0.6961 - val_loss: 0.5562 - val_acc: 0.8382

Epoch 00001: val_loss improved from inf to 0.55621, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 49s - loss: 0.5865 - acc: 0.7571 - val_loss: 0.5729 - val_acc: 0.7892

Epoch 00002: val_loss did not improve from 0.55621
Epoch 3/40
 - 50s - loss: 0.5626 - acc: 0.7673 - val_loss: 0.5671 - val_acc: 0.7898

Epoch 00003: val_loss did not improve from 0.55621
Epoch 4/40
 - 50s - loss: 0.5453 - acc: 0.7758 - val_loss: 0.5640 - val_acc: 0.7983

Epoch 00004: val_loss did not improve from 0.55621
Epoch 5/40
 - 50s - loss: 0.5288 - acc: 0.7853 - val_loss: 0.5673 - val_acc: 0.7794

Epoch 00005: val_loss did not improve from 0.55621
Epoch 6/40
 - 50s - loss: 0.5186 - acc: 0.7871 - val_loss: 0.5462 - val_acc: 0.7852

Epoch 00006: val_loss improved from 0.55621 to 0.54619, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 7/40
 - 50s - loss: 0.5074 - acc: 0.7954 - val_loss: 0.5763 - val_acc: 0.7660

Epoch 00007: val_loss did not improve from 0.54619
Epoch 8/40
 - 50s - loss: 0.4986 - acc: 0.8009 - val_loss: 0.5406 - val_acc: 0.7902

Epoch 00008: val_loss improved from 0.54619 to 0.54062, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 50s - loss: 0.4920 - acc: 0.8040 - val_loss: 0.5686 - val_acc: 0.7616

Epoch 00009: val_loss did not improve from 0.54062
Epoch 10/40
 - 50s - loss: 0.4852 - acc: 0.8049 - val_loss: 0.5380 - val_acc: 0.7875

Epoch 00010: val_loss improved from 0.54062 to 0.53797, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 11/40
 - 50s - loss: 0.4798 - acc: 0.8073 - val_loss: 0.5520 - val_acc: 0.7832

Epoch 00011: val_loss did not improve from 0.53797
Epoch 12/40
 - 50s - loss: 0.4728 - acc: 0.8100 - val_loss: 0.5090 - val_acc: 0.8148

Epoch 00012: val_loss improved from 0.53797 to 0.50898, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 50s - loss: 0.4667 - acc: 0.8163 - val_loss: 0.5333 - val_acc: 0.7923

Epoch 00013: val_loss did not improve from 0.50898
Epoch 14/40
 - 50s - loss: 0.4626 - acc: 0.8180 - val_loss: 0.5567 - val_acc: 0.7687

Epoch 00014: val_loss did not improve from 0.50898
Epoch 15/40
 - 50s - loss: 0.4581 - acc: 0.8206 - val_loss: 0.5228 - val_acc: 0.8159

Epoch 00015: val_loss did not improve from 0.50898
Epoch 16/40
 - 50s - loss: 0.4522 - acc: 0.8233 - val_loss: 0.5426 - val_acc: 0.7804

Epoch 00016: val_loss did not improve from 0.50898
Epoch 17/40
 - 50s - loss: 0.4498 - acc: 0.8238 - val_loss: 0.4992 - val_acc: 0.7969

Epoch 00017: val_loss improved from 0.50898 to 0.49917, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 18/40
 - 50s - loss: 0.4464 - acc: 0.8268 - val_loss: 0.5700 - val_acc: 0.7652

Epoch 00018: val_loss did not improve from 0.49917
Epoch 19/40
 - 50s - loss: 0.4416 - acc: 0.8274 - val_loss: 0.5685 - val_acc: 0.7690

Epoch 00019: val_loss did not improve from 0.49917
Epoch 20/40
 - 50s - loss: 0.4377 - acc: 0.8289 - val_loss: 0.5656 - val_acc: 0.7598

Epoch 00020: val_loss did not improve from 0.49917
Epoch 21/40
 - 50s - loss: 0.4330 - acc: 0.8335 - val_loss: 0.5275 - val_acc: 0.7973

Epoch 00021: val_loss did not improve from 0.49917
Epoch 22/40
 - 50s - loss: 0.4285 - acc: 0.8347 - val_loss: 0.5680 - val_acc: 0.7698

Epoch 00022: val_loss did not improve from 0.49917
Epoch 23/40
 - 50s - loss: 0.4246 - acc: 0.8380 - val_loss: 0.5192 - val_acc: 0.7972

Epoch 00023: val_loss did not improve from 0.49917
Epoch 24/40
 - 50s - loss: 0.4258 - acc: 0.8380 - val_loss: 0.5354 - val_acc: 0.7964

Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00024: val_loss did not improve from 0.49917
Epoch 00024: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 4s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 3s
1440/7440 [====>.........................] - ETA: 3s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 493us/step
current Test accuracy: 0.7963709677419355
current auc_score ------------------>  0.8792927650595446

  32/7440 [..............................] - ETA: 49:56
 128/7440 [..............................] - ETA: 12:22
 256/7440 [>.............................] - ETA: 6:06 
 384/7440 [>.............................] - ETA: 4:01
 512/7440 [=>............................] - ETA: 2:58
 640/7440 [=>............................] - ETA: 2:20
 768/7440 [==>...........................] - ETA: 1:55
 896/7440 [==>...........................] - ETA: 1:37
1024/7440 [===>..........................] - ETA: 1:24
1152/7440 [===>..........................] - ETA: 1:13
1280/7440 [====>.........................] - ETA: 1:05
1408/7440 [====>.........................] - ETA: 58s 
1536/7440 [=====>........................] - ETA: 52s
1664/7440 [=====>........................] - ETA: 47s
1792/7440 [======>.......................] - ETA: 43s
1920/7440 [======>.......................] - ETA: 39s
2048/7440 [=======>......................] - ETA: 36s
2176/7440 [=======>......................] - ETA: 33s
2304/7440 [========>.....................] - ETA: 31s
2432/7440 [========>.....................] - ETA: 29s
2560/7440 [=========>....................] - ETA: 27s
2688/7440 [=========>....................] - ETA: 25s
2816/7440 [==========>...................] - ETA: 23s
2944/7440 [==========>...................] - ETA: 22s
3072/7440 [===========>..................] - ETA: 20s
3200/7440 [===========>..................] - ETA: 19s
3328/7440 [============>.................] - ETA: 18s
3456/7440 [============>.................] - ETA: 16s
3584/7440 [=============>................] - ETA: 15s
3712/7440 [=============>................] - ETA: 14s
3840/7440 [==============>...............] - ETA: 13s
3968/7440 [===============>..............] - ETA: 13s
4096/7440 [===============>..............] - ETA: 12s
4224/7440 [================>.............] - ETA: 11s
4352/7440 [================>.............] - ETA: 10s
4480/7440 [=================>............] - ETA: 10s
4608/7440 [=================>............] - ETA: 9s 
4736/7440 [==================>...........] - ETA: 8s
4864/7440 [==================>...........] - ETA: 8s
4992/7440 [===================>..........] - ETA: 7s
5120/7440 [===================>..........] - ETA: 7s
5248/7440 [====================>.........] - ETA: 6s
5376/7440 [====================>.........] - ETA: 5s
5504/7440 [=====================>........] - ETA: 5s
5632/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 4s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 17s 2ms/step
Best saved model Test accuracy: 0.7969086021505376
best saved model auc_score ------------------>  0.8820120100589663
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_40 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_40[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_367 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_367[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_368 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_368[0][0]             
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_145[0][0]            
__________________________________________________________________________________________________
activation_369 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_369[0][0]             
__________________________________________________________________________________________________
average_pooling2d_40 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_40[0][0]       
__________________________________________________________________________________________________
activation_370 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_370[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_371 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_371[0][0]             
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 26, 48, 48)   0           average_pooling2d_40[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_146[0][0]            
__________________________________________________________________________________________________
activation_372 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_40 (Gl (None, 26)           0           activation_372[0][0]             
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 1)            27          global_average_pooling2d_40[0][0]
==================================================================================================
Total params: 13,235
Trainable params: 12,875
Non-trainable params: 360
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/40
 - 53s - loss: 0.6933 - acc: 0.5918 - val_loss: 0.5679 - val_acc: 0.8114

Epoch 00001: val_loss improved from inf to 0.56789, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 2/40
 - 23s - loss: 0.6270 - acc: 0.7022 - val_loss: 0.5551 - val_acc: 0.8308

Epoch 00002: val_loss improved from 0.56789 to 0.55507, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 3/40
 - 23s - loss: 0.6071 - acc: 0.7289 - val_loss: 0.5453 - val_acc: 0.8368

Epoch 00003: val_loss improved from 0.55507 to 0.54527, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 4/40
 - 23s - loss: 0.5903 - acc: 0.7396 - val_loss: 0.5336 - val_acc: 0.8337

Epoch 00004: val_loss improved from 0.54527 to 0.53360, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 5/40
 - 23s - loss: 0.5754 - acc: 0.7494 - val_loss: 0.5309 - val_acc: 0.8216

Epoch 00005: val_loss improved from 0.53360 to 0.53088, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 6/40
 - 23s - loss: 0.5637 - acc: 0.7544 - val_loss: 0.5361 - val_acc: 0.8222

Epoch 00006: val_loss did not improve from 0.53088
Epoch 7/40
 - 23s - loss: 0.5544 - acc: 0.7554 - val_loss: 0.5221 - val_acc: 0.8140

Epoch 00007: val_loss improved from 0.53088 to 0.52212, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 8/40
 - 23s - loss: 0.5457 - acc: 0.7601 - val_loss: 0.5168 - val_acc: 0.8206

Epoch 00008: val_loss improved from 0.52212 to 0.51685, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 9/40
 - 23s - loss: 0.5400 - acc: 0.7601 - val_loss: 0.5367 - val_acc: 0.8046

Epoch 00009: val_loss did not improve from 0.51685
Epoch 10/40
 - 23s - loss: 0.5336 - acc: 0.7627 - val_loss: 0.5244 - val_acc: 0.7997

Epoch 00010: val_loss did not improve from 0.51685
Epoch 11/40
 - 23s - loss: 0.5282 - acc: 0.7647 - val_loss: 0.5138 - val_acc: 0.8036

Epoch 00011: val_loss improved from 0.51685 to 0.51383, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 12/40
 - 23s - loss: 0.5233 - acc: 0.7672 - val_loss: 0.5108 - val_acc: 0.8069

Epoch 00012: val_loss improved from 0.51383 to 0.51076, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 13/40
 - 23s - loss: 0.5189 - acc: 0.7679 - val_loss: 0.5078 - val_acc: 0.8032

Epoch 00013: val_loss improved from 0.51076 to 0.50780, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 14/40
 - 23s - loss: 0.5161 - acc: 0.7701 - val_loss: 0.5033 - val_acc: 0.7965

Epoch 00014: val_loss improved from 0.50780 to 0.50332, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 15/40
 - 24s - loss: 0.5124 - acc: 0.7717 - val_loss: 0.5043 - val_acc: 0.7945

Epoch 00015: val_loss did not improve from 0.50332
Epoch 16/40
 - 23s - loss: 0.5086 - acc: 0.7709 - val_loss: 0.4928 - val_acc: 0.8062

Epoch 00016: val_loss improved from 0.50332 to 0.49281, saving model to keras_densenet_simple_wt_29Sept_1404.h5
Epoch 17/40
 - 23s - loss: 0.5058 - acc: 0.7736 - val_loss: 0.5145 - val_acc: 0.7984

Epoch 00017: val_loss did not improve from 0.49281
Epoch 18/40
 - 23s - loss: 0.5030 - acc: 0.7733 - val_loss: 0.4972 - val_acc: 0.7923

Epoch 00018: val_loss did not improve from 0.49281
Epoch 19/40
 - 23s - loss: 0.4999 - acc: 0.7752 - val_loss: 0.4955 - val_acc: 0.7934

Epoch 00019: val_loss did not improve from 0.49281
Epoch 20/40
 - 23s - loss: 0.4972 - acc: 0.7746 - val_loss: 0.5030 - val_acc: 0.7888

Epoch 00020: val_loss did not improve from 0.49281
Epoch 21/40
 - 23s - loss: 0.4930 - acc: 0.7772 - val_loss: 0.5033 - val_acc: 0.7851

Epoch 00021: val_loss did not improve from 0.49281
Epoch 22/40
 - 23s - loss: 0.4911 - acc: 0.7765 - val_loss: 0.5045 - val_acc: 0.7785

Epoch 00022: val_loss did not improve from 0.49281
Epoch 23/40
 - 23s - loss: 0.4882 - acc: 0.7801 - val_loss: 0.5030 - val_acc: 0.7944

Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.486832740847579e-06.

Epoch 00023: val_loss did not improve from 0.49281
Epoch 00023: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 384/7440 [>.............................] - ETA: 2s
 576/7440 [=>............................] - ETA: 2s
 768/7440 [==>...........................] - ETA: 2s
 960/7440 [==>...........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 1s
1344/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1728/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2688/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3456/7440 [============>.................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 320us/step
current Test accuracy: 0.7943548387096774
current auc_score ------------------>  0.8615255737657532

  32/7440 [..............................] - ETA: 51:05
 192/7440 [..............................] - ETA: 8:22 
 384/7440 [>.............................] - ETA: 4:05
 576/7440 [=>............................] - ETA: 2:39
 768/7440 [==>...........................] - ETA: 1:57
 960/7440 [==>...........................] - ETA: 1:31
1152/7440 [===>..........................] - ETA: 1:14
1344/7440 [====>.........................] - ETA: 1:01
1536/7440 [=====>........................] - ETA: 52s 
1728/7440 [=====>........................] - ETA: 45s
1920/7440 [======>.......................] - ETA: 39s
2112/7440 [=======>......................] - ETA: 35s
2304/7440 [========>.....................] - ETA: 31s
2496/7440 [=========>....................] - ETA: 27s
2688/7440 [=========>....................] - ETA: 24s
2880/7440 [==========>...................] - ETA: 22s
3072/7440 [===========>..................] - ETA: 20s
3264/7440 [============>.................] - ETA: 18s
3456/7440 [============>.................] - ETA: 16s
3648/7440 [=============>................] - ETA: 14s
3840/7440 [==============>...............] - ETA: 13s
4032/7440 [===============>..............] - ETA: 12s
4224/7440 [================>.............] - ETA: 11s
4416/7440 [================>.............] - ETA: 9s 
4608/7440 [=================>............] - ETA: 8s
4800/7440 [==================>...........] - ETA: 8s
4992/7440 [===================>..........] - ETA: 7s
5184/7440 [===================>..........] - ETA: 6s
5376/7440 [====================>.........] - ETA: 5s
5568/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 4s
5952/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6336/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6720/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 16s 2ms/step
Best saved model Test accuracy: 0.8061827956989247
best saved model auc_score ------------------>  0.8689852873164527
best model <keras.engine.training.Model object at 0x7ef287171be0>
best run {'depth': 6, 'growth_rate': 3}
Evalutation of best performing model:
[0.4546257483382379, 0.8403225806451613]
val roc_auc_score 0.909
----------trials-------------
{'depth': [0], 'growth_rate': [1]} -0.8749584489536363
{'depth': [6], 'growth_rate': [4]} -0.8853878699849693
{'depth': [2], 'growth_rate': [5]} -0.8927244840443982
{'depth': [5], 'growth_rate': [5]} -0.8938374378540872
{'depth': [4], 'growth_rate': [3]} -0.894537663313678
{'depth': [2], 'growth_rate': [2]} -0.8655757529772228
{'depth': [3], 'growth_rate': [3]} -0.8717293906810035
{'depth': [5], 'growth_rate': [2]} -0.8819493583073188
{'depth': [2], 'growth_rate': [4]} -0.8797923892935599
{'depth': [0], 'growth_rate': [5]} -0.8705430179789571
{'depth': [3], 'growth_rate': [1]} -0.8955411752803792
{'depth': [4], 'growth_rate': [1]} -0.8799135738235634
{'depth': [1], 'growth_rate': [3]} -0.8770868380737658
{'depth': [1], 'growth_rate': [0]} -0.8619539108567464
{'depth': [0], 'growth_rate': [1]} -0.8655809558908544
{'depth': [1], 'growth_rate': [1]} -0.8814131691525031
{'depth': [1], 'growth_rate': [1]} -0.8678927838478439
{'depth': [0], 'growth_rate': [3]} -0.8476875939414961
{'depth': [0], 'growth_rate': [4]} -0.8666598739738697
{'depth': [6], 'growth_rate': [1]} -0.8752563880217366
{'depth': [3], 'growth_rate': [0]} -0.8619084937565036
{'depth': [4], 'growth_rate': [3]} -0.8971771303040813
{'depth': [4], 'growth_rate': [3]} -0.9104298184761245
{'depth': [4], 'growth_rate': [3]} -0.8979545323158746
{'depth': [4], 'growth_rate': [3]} -0.9058685975257255
{'depth': [4], 'growth_rate': [3]} -0.913668343161059
{'depth': [4], 'growth_rate': [3]} -0.9124155249161752
{'depth': [4], 'growth_rate': [0]} -0.8987713536246964
{'depth': [4], 'growth_rate': [2]} -0.8761658139669326
{'depth': [4], 'growth_rate': [3]} -0.8848267863336802
{'depth': [6], 'growth_rate': [3]} -0.915169311481096
{'depth': [6], 'growth_rate': [4]} -0.8940557434385477
{'depth': [6], 'growth_rate': [5]} -0.8901821381084519
{'depth': [6], 'growth_rate': [3]} -0.9093756503642039
{'depth': [5], 'growth_rate': [2]} -0.9016586816394958
{'depth': [6], 'growth_rate': [5]} -0.906364897676032
{'depth': [2], 'growth_rate': [3]} -0.8724796580529541
{'depth': [6], 'growth_rate': [0]} -0.8779610359579142
{'depth': [5], 'growth_rate': [4]} -0.8820120100589663
{'depth': [2], 'growth_rate': [3]} -0.8689852873164527
python evaluate_saved_model_simple.py
