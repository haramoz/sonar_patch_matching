python hello-world.py
python hyperas_simple.py
python hyperas_contrastive_loss.py
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
python keras_densenet_simple.py
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from sklearn.model_selection import train_test_split
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import pickle
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'depth': hp.choice('depth', [7,10,13,16,19,22,25]),
        'nb_filter': hp.choice('nb_filter', [8,16,32]),
        'growth_rate': hp.choice('growth_rate', [6,8,10,12,14,16]),
    }

>>> Functions
  1: def process_data():
  2:     random_seed = 7
  3: 
  4:     f = h5py.File('matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  5:     X_train = f['X_train'].value
  6:     y_train = f['y_train'].value
  7:     X_test = f['X_val'].value
  8:     y_test = f['y_val'].value
  9:     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed)
 10:  
 11:     return X_train,y_train,X_val,y_val,X_test,y_test
 12: 
 13: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val,X_test,y_test = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 40
   4:     es_patience = 5
   5:     lr_patience = 5
   6:     dropout = None
   7:     #depth = space['depth']
   8:     depth = 22
   9:     nb_dense_block = 3
  10:     nb_filter = space['nb_filter']
  11:     #growth_rate = space['growth_rate']
  12:     growth_rate = 16
  13:     bn = True
  14:     reduction_ = 0.5
  15:     bs = 32
  16:     lr = 1E-4 #########################################################CHange file name##########################################
  17:     weight_file = 'keras_densenet_simple_wt_29Sept_2040.h5'
  18:     
  19:     nb_classes = 1
  20:     img_dim = (2,96,96) 
  21:     n_channels = 2 
  22: 
  23:     
  24:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  25:                  growth_rate=growth_rate, nb_filter=nb_filter,
  26:                  dropout_rate=dropout,activation='sigmoid',
  27:                  input_shape=img_dim,include_top=True,
  28:                  bottleneck=bn,reduction=reduction_,
  29:                  classes=nb_classes,pooling='avg',
  30:                  weights=None)
  31:     
  32: 
  33:     model.summary()
  34:     opt = Adam(lr=lr)
  35:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  36: 
  37:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  38:     #es = EarlyStopping(monitor='val_acc', patience=es_patience,verbose=1,restore_best_weights=True)
  39:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  40: 
  41:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  42: 
  43:     model.fit(X_train,y_train,
  44:           batch_size=bs,
  45:           epochs=epochs,
  46:           callbacks=[es,lr_reducer,checkpointer],
  47:           validation_data=(X_val,y_val),
  48:           verbose=2)
  49:     
  50:     score, acc = model.evaluate(X_test, y_test)
  51:     print('current Test accuracy:', acc)
  52:     pred = model.predict(X_test)
  53:     auc_score = roc_auc_score(y_test,pred)
  54:     print("current auc_score ------------------> ",auc_score)
  55: 
  56:     model = load_model(weight_file) #This is the best model
  57:     score, acc = model.evaluate(X_test, y_test)
  58:     print('Best saved model Test accuracy:', acc)
  59:     pred = model.predict(X_test)
  60:     auc_score = roc_auc_score(y_test,pred)
  61:     print("best saved model auc_score ------------------> ",auc_score)
  62: 
  63:     
  64:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  65: 
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 32, 96, 96)   576         input_1[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 32, 96, 96)   128         initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_1[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 48, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 48, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_3[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 64, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 64, 96, 96)   256         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 64, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   4096        activation_5[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 80, 96, 96)   0           concatenate_2[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 80, 96, 96)   320         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 80, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 40, 96, 96)   3200        activation_7[0][0]               
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 40, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 40, 48, 48)   160         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 40, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_8[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 56, 48, 48)   0           average_pooling2d_1[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 56, 48, 48)   224         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3584        activation_10[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 72, 48, 48)   0           concatenate_4[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 72, 48, 48)   288         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4608        activation_12[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_13[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 88, 48, 48)   0           concatenate_5[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 88, 48, 48)   352         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 88, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3872        activation_14[0][0]              
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_15[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 60, 24, 24)   0           average_pooling2d_2[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 60, 24, 24)   240         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 60, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3840        activation_17[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 76, 24, 24)   0           concatenate_7[0][0]              
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 76, 24, 24)   304         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 76, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4864        activation_19[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 92, 24, 24)   0           concatenate_8[0][0]              
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 92, 24, 24)   368         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 92, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 92)           0           activation_21[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            93          global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 127,485
Trainable params: 124,829
Non-trainable params: 2,656
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 53s - loss: 0.6439 - acc: 0.7455 - val_loss: 0.5894 - val_acc: 0.7705

Epoch 00001: val_loss improved from inf to 0.58945, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 48s - loss: 0.5528 - acc: 0.7960 - val_loss: 0.5206 - val_acc: 0.8084

Epoch 00002: val_loss improved from 0.58945 to 0.52057, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 48s - loss: 0.5089 - acc: 0.8198 - val_loss: 0.5108 - val_acc: 0.8229

Epoch 00003: val_loss improved from 0.52057 to 0.51078, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 48s - loss: 0.4772 - acc: 0.8391 - val_loss: 0.5072 - val_acc: 0.8164

Epoch 00004: val_loss improved from 0.51078 to 0.50716, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 48s - loss: 0.4493 - acc: 0.8524 - val_loss: 0.4554 - val_acc: 0.8416

Epoch 00005: val_loss improved from 0.50716 to 0.45537, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 6/40
 - 48s - loss: 0.4257 - acc: 0.8648 - val_loss: 0.4296 - val_acc: 0.8656

Epoch 00006: val_loss improved from 0.45537 to 0.42960, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 48s - loss: 0.4054 - acc: 0.8762 - val_loss: 0.4192 - val_acc: 0.8734

Epoch 00007: val_loss improved from 0.42960 to 0.41922, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 48s - loss: 0.3822 - acc: 0.8878 - val_loss: 0.4623 - val_acc: 0.8454

Epoch 00008: val_loss did not improve from 0.41922
Epoch 9/40
 - 48s - loss: 0.3672 - acc: 0.8947 - val_loss: 0.3672 - val_acc: 0.8971

Epoch 00009: val_loss improved from 0.41922 to 0.36719, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 48s - loss: 0.3511 - acc: 0.9023 - val_loss: 0.4220 - val_acc: 0.8507

Epoch 00010: val_loss did not improve from 0.36719
Epoch 11/40
 - 48s - loss: 0.3388 - acc: 0.9074 - val_loss: 0.3375 - val_acc: 0.9085

Epoch 00011: val_loss improved from 0.36719 to 0.33749, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 48s - loss: 0.3253 - acc: 0.9145 - val_loss: 0.3957 - val_acc: 0.8715

Epoch 00012: val_loss did not improve from 0.33749
Epoch 13/40
 - 48s - loss: 0.3115 - acc: 0.9196 - val_loss: 0.3390 - val_acc: 0.9143

Epoch 00013: val_loss did not improve from 0.33749
Epoch 14/40
 - 48s - loss: 0.3008 - acc: 0.9255 - val_loss: 0.3639 - val_acc: 0.8934

Epoch 00014: val_loss did not improve from 0.33749
Epoch 15/40
 - 48s - loss: 0.2900 - acc: 0.9298 - val_loss: 0.3261 - val_acc: 0.9174

Epoch 00015: val_loss improved from 0.33749 to 0.32611, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 16/40
 - 48s - loss: 0.2864 - acc: 0.9315 - val_loss: 0.3309 - val_acc: 0.9065

Epoch 00016: val_loss did not improve from 0.32611
Epoch 17/40
 - 48s - loss: 0.2737 - acc: 0.9357 - val_loss: 0.3299 - val_acc: 0.9036

Epoch 00017: val_loss did not improve from 0.32611
Epoch 18/40
 - 48s - loss: 0.2672 - acc: 0.9379 - val_loss: 0.2895 - val_acc: 0.9275

Epoch 00018: val_loss improved from 0.32611 to 0.28950, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 19/40
 - 48s - loss: 0.2602 - acc: 0.9418 - val_loss: 0.3110 - val_acc: 0.9134

Epoch 00019: val_loss did not improve from 0.28950
Epoch 20/40
 - 48s - loss: 0.2533 - acc: 0.9444 - val_loss: 0.2976 - val_acc: 0.9256

Epoch 00020: val_loss did not improve from 0.28950
Epoch 21/40
 - 48s - loss: 0.2453 - acc: 0.9467 - val_loss: 0.3121 - val_acc: 0.9149

Epoch 00021: val_loss did not improve from 0.28950
Epoch 22/40
 - 48s - loss: 0.2412 - acc: 0.9482 - val_loss: 0.3089 - val_acc: 0.9219

Epoch 00022: val_loss did not improve from 0.28950
Epoch 23/40
 - 48s - loss: 0.2317 - acc: 0.9527 - val_loss: 0.2820 - val_acc: 0.9291

Epoch 00023: val_loss improved from 0.28950 to 0.28196, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 24/40
 - 48s - loss: 0.2284 - acc: 0.9553 - val_loss: 0.2727 - val_acc: 0.9307

Epoch 00024: val_loss improved from 0.28196 to 0.27267, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 25/40
 - 48s - loss: 0.2196 - acc: 0.9579 - val_loss: 0.3703 - val_acc: 0.9094

Epoch 00025: val_loss did not improve from 0.27267
Epoch 26/40
 - 48s - loss: 0.2174 - acc: 0.9569 - val_loss: 0.2844 - val_acc: 0.9297

Epoch 00026: val_loss did not improve from 0.27267
Epoch 27/40
 - 48s - loss: 0.2134 - acc: 0.9579 - val_loss: 0.2750 - val_acc: 0.9317

Epoch 00027: val_loss did not improve from 0.27267
Epoch 28/40
 - 48s - loss: 0.2066 - acc: 0.9618 - val_loss: 0.3675 - val_acc: 0.9081

Epoch 00028: val_loss did not improve from 0.27267
Epoch 29/40
 - 48s - loss: 0.2036 - acc: 0.9628 - val_loss: 0.2724 - val_acc: 0.9315

Epoch 00029: val_loss improved from 0.27267 to 0.27235, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 30/40
 - 49s - loss: 0.1996 - acc: 0.9649 - val_loss: 0.2999 - val_acc: 0.9182

Epoch 00030: val_loss did not improve from 0.27235
Epoch 31/40
 - 48s - loss: 0.1916 - acc: 0.9688 - val_loss: 0.2581 - val_acc: 0.9391

Epoch 00031: val_loss improved from 0.27235 to 0.25810, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 32/40
 - 48s - loss: 0.1852 - acc: 0.9697 - val_loss: 0.2592 - val_acc: 0.9394

Epoch 00032: val_loss did not improve from 0.25810
Epoch 33/40
 - 48s - loss: 0.1870 - acc: 0.9696 - val_loss: 0.3797 - val_acc: 0.8996

Epoch 00033: val_loss did not improve from 0.25810
Epoch 34/40
 - 48s - loss: 0.1835 - acc: 0.9708 - val_loss: 0.2745 - val_acc: 0.9378

Epoch 00034: val_loss did not improve from 0.25810
Epoch 35/40
 - 48s - loss: 0.1778 - acc: 0.9727 - val_loss: 0.2785 - val_acc: 0.9324

Epoch 00035: val_loss did not improve from 0.25810
Epoch 36/40
 - 48s - loss: 0.1745 - acc: 0.9739 - val_loss: 0.3293 - val_acc: 0.9177

Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00036: val_loss did not improve from 0.25810
Epoch 00036: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 468us/step
current Test accuracy: 0.8364247311827957
current auc_score ------------------>  0.9163345834778587

  32/7440 [..............................] - ETA: 2:04
 160/7440 [..............................] - ETA: 27s 
 288/7440 [>.............................] - ETA: 16s
 416/7440 [>.............................] - ETA: 12s
 544/7440 [=>............................] - ETA: 9s 
 672/7440 [=>............................] - ETA: 8s
 800/7440 [==>...........................] - ETA: 7s
 928/7440 [==>...........................] - ETA: 6s
1056/7440 [===>..........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 5s
1312/7440 [====>.........................] - ETA: 5s
1440/7440 [====>.........................] - ETA: 4s
1568/7440 [=====>........................] - ETA: 4s
1696/7440 [=====>........................] - ETA: 4s
1824/7440 [======>.......................] - ETA: 4s
1952/7440 [======>.......................] - ETA: 4s
2080/7440 [=======>......................] - ETA: 3s
2208/7440 [=======>......................] - ETA: 3s
2336/7440 [========>.....................] - ETA: 3s
2464/7440 [========>.....................] - ETA: 3s
2592/7440 [=========>....................] - ETA: 3s
2720/7440 [=========>....................] - ETA: 3s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 2s
3744/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4000/7440 [===============>..............] - ETA: 2s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 530us/step
Best saved model Test accuracy: 0.805241935483871
best saved model auc_score ------------------>  0.90915824806336
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 8, 96, 96)    144         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 8, 96, 96)    32          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 96, 96)    0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   512         activation_22[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   1536        activation_24[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 40, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   2560        activation_26[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 56, 96, 96)   0           concatenate_11[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 56, 96, 96)   224         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 56, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 28, 96, 96)   1568        activation_28[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 28, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 28, 48, 48)   112         average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 28, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1792        activation_29[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 44, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 44, 48, 48)   176         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 44, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2816        activation_31[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 60, 48, 48)   0           concatenate_13[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 60, 48, 48)   240         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 60, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   3840        activation_33[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 76, 48, 48)   0           concatenate_14[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 76, 48, 48)   304         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 76, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 38, 48, 48)   2888        activation_35[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 38, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 38, 24, 24)   152         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 38, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2432        activation_36[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 54, 24, 24)   0           average_pooling2d_4[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 54, 24, 24)   216         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 54, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3456        activation_38[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_39[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 70, 24, 24)   0           concatenate_16[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 70, 24, 24)   280         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 70, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4480        activation_40[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_41[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 86, 24, 24)   0           concatenate_17[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 86, 24, 24)   344         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 86, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 86)           0           activation_42[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            87          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 115,695
Trainable params: 113,375
Non-trainable params: 2,320
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 47s - loss: 0.6188 - acc: 0.7591 - val_loss: 0.5399 - val_acc: 0.8017

Epoch 00001: val_loss improved from inf to 0.53986, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 43s - loss: 0.5243 - acc: 0.8103 - val_loss: 0.5319 - val_acc: 0.8030

Epoch 00002: val_loss improved from 0.53986 to 0.53191, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 43s - loss: 0.4874 - acc: 0.8280 - val_loss: 0.4978 - val_acc: 0.8238

Epoch 00003: val_loss improved from 0.53191 to 0.49778, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 43s - loss: 0.4613 - acc: 0.8414 - val_loss: 0.4798 - val_acc: 0.8247

Epoch 00004: val_loss improved from 0.49778 to 0.47985, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 43s - loss: 0.4360 - acc: 0.8578 - val_loss: 0.5719 - val_acc: 0.7730

Epoch 00005: val_loss did not improve from 0.47985
Epoch 6/40
 - 43s - loss: 0.4149 - acc: 0.8688 - val_loss: 0.4235 - val_acc: 0.8601

Epoch 00006: val_loss improved from 0.47985 to 0.42353, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 43s - loss: 0.3968 - acc: 0.8770 - val_loss: 0.4220 - val_acc: 0.8681

Epoch 00007: val_loss improved from 0.42353 to 0.42201, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 43s - loss: 0.3806 - acc: 0.8849 - val_loss: 0.4812 - val_acc: 0.8318

Epoch 00008: val_loss did not improve from 0.42201
Epoch 9/40
 - 43s - loss: 0.3667 - acc: 0.8898 - val_loss: 0.6099 - val_acc: 0.7536

Epoch 00009: val_loss did not improve from 0.42201
Epoch 10/40
 - 43s - loss: 0.3553 - acc: 0.8975 - val_loss: 0.3787 - val_acc: 0.8883

Epoch 00010: val_loss improved from 0.42201 to 0.37868, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 11/40
 - 43s - loss: 0.3383 - acc: 0.9052 - val_loss: 0.3747 - val_acc: 0.8862

Epoch 00011: val_loss improved from 0.37868 to 0.37474, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 43s - loss: 0.3260 - acc: 0.9109 - val_loss: 0.3673 - val_acc: 0.8906

Epoch 00012: val_loss improved from 0.37474 to 0.36725, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 13/40
 - 43s - loss: 0.3137 - acc: 0.9161 - val_loss: 0.4450 - val_acc: 0.8352

Epoch 00013: val_loss did not improve from 0.36725
Epoch 14/40
 - 43s - loss: 0.3047 - acc: 0.9209 - val_loss: 0.5303 - val_acc: 0.8434

Epoch 00014: val_loss did not improve from 0.36725
Epoch 15/40
 - 43s - loss: 0.2971 - acc: 0.9237 - val_loss: 0.4567 - val_acc: 0.8591

Epoch 00015: val_loss did not improve from 0.36725
Epoch 16/40
 - 43s - loss: 0.2852 - acc: 0.9274 - val_loss: 0.3672 - val_acc: 0.8887

Epoch 00016: val_loss improved from 0.36725 to 0.36718, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 17/40
 - 43s - loss: 0.2771 - acc: 0.9321 - val_loss: 0.3315 - val_acc: 0.9090

Epoch 00017: val_loss improved from 0.36718 to 0.33153, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 18/40
 - 43s - loss: 0.2690 - acc: 0.9360 - val_loss: 0.4342 - val_acc: 0.8552

Epoch 00018: val_loss did not improve from 0.33153
Epoch 19/40
 - 43s - loss: 0.2600 - acc: 0.9377 - val_loss: 0.5800 - val_acc: 0.7993

Epoch 00019: val_loss did not improve from 0.33153
Epoch 20/40
 - 43s - loss: 0.2531 - acc: 0.9421 - val_loss: 0.2997 - val_acc: 0.9174

Epoch 00020: val_loss improved from 0.33153 to 0.29971, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 21/40
 - 43s - loss: 0.2485 - acc: 0.9429 - val_loss: 0.3358 - val_acc: 0.9006

Epoch 00021: val_loss did not improve from 0.29971
Epoch 22/40
 - 43s - loss: 0.2386 - acc: 0.9478 - val_loss: 0.3053 - val_acc: 0.9168

Epoch 00022: val_loss did not improve from 0.29971
Epoch 23/40
 - 43s - loss: 0.2352 - acc: 0.9492 - val_loss: 0.3797 - val_acc: 0.9078

Epoch 00023: val_loss did not improve from 0.29971
Epoch 24/40
 - 43s - loss: 0.2262 - acc: 0.9528 - val_loss: 0.3338 - val_acc: 0.9007

Epoch 00024: val_loss did not improve from 0.29971
Epoch 25/40
 - 43s - loss: 0.2208 - acc: 0.9545 - val_loss: 0.5005 - val_acc: 0.8439

Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00025: val_loss did not improve from 0.29971
Epoch 00025: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 544/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 418us/step
current Test accuracy: 0.6858870967741936
current auc_score ------------------>  0.8546848624118395

  32/7440 [..............................] - ETA: 3:15
 160/7440 [..............................] - ETA: 41s 
 288/7440 [>.............................] - ETA: 23s
 416/7440 [>.............................] - ETA: 17s
 544/7440 [=>............................] - ETA: 13s
 672/7440 [=>............................] - ETA: 11s
 800/7440 [==>...........................] - ETA: 9s 
 928/7440 [==>...........................] - ETA: 8s
1056/7440 [===>..........................] - ETA: 7s
1184/7440 [===>..........................] - ETA: 7s
1312/7440 [====>.........................] - ETA: 6s
1440/7440 [====>.........................] - ETA: 6s
1568/7440 [=====>........................] - ETA: 5s
1696/7440 [=====>........................] - ETA: 5s
1824/7440 [======>.......................] - ETA: 4s
1952/7440 [======>.......................] - ETA: 4s
2080/7440 [=======>......................] - ETA: 4s
2208/7440 [=======>......................] - ETA: 4s
2336/7440 [========>.....................] - ETA: 3s
2464/7440 [========>.....................] - ETA: 3s
2592/7440 [=========>....................] - ETA: 3s
2720/7440 [=========>....................] - ETA: 3s
2848/7440 [==========>...................] - ETA: 3s
2976/7440 [===========>..................] - ETA: 3s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 2s
3744/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4000/7440 [===============>..............] - ETA: 2s
4128/7440 [===============>..............] - ETA: 2s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 532us/step
Best saved model Test accuracy: 0.8241935483870968
best saved model auc_score ------------------>  0.9081203029251936
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_43[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_44[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_45[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_46[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 48, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_47[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_48[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 64, 96, 96)   0           concatenate_20[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_49[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_50[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_51[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 48, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_52[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_53[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 64, 48, 48)   0           concatenate_22[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_54[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_55[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 80, 48, 48)   0           concatenate_23[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 80, 48, 48)   320         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 80, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 40, 48, 48)   3200        activation_56[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 40, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 40, 24, 24)   160         average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 40, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2560        activation_57[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_58[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 56, 24, 24)   0           average_pooling2d_6[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 56, 24, 24)   224         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 56, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3584        activation_59[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_60[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 72, 24, 24)   0           concatenate_25[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 72, 24, 24)   288         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4608        activation_61[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_62[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 88, 24, 24)   0           concatenate_26[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 88, 24, 24)   352         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 88, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 88)           0           activation_63[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            89          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 119,545
Trainable params: 117,113
Non-trainable params: 2,432
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 50s - loss: 0.6273 - acc: 0.7545 - val_loss: 0.5597 - val_acc: 0.7888

Epoch 00001: val_loss improved from inf to 0.55974, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 45s - loss: 0.5427 - acc: 0.8020 - val_loss: 0.5574 - val_acc: 0.7897

Epoch 00002: val_loss improved from 0.55974 to 0.55744, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 45s - loss: 0.4992 - acc: 0.8240 - val_loss: 0.4689 - val_acc: 0.8341

Epoch 00003: val_loss improved from 0.55744 to 0.46890, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 45s - loss: 0.4637 - acc: 0.8442 - val_loss: 0.4559 - val_acc: 0.8493

Epoch 00004: val_loss improved from 0.46890 to 0.45595, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 45s - loss: 0.4368 - acc: 0.8605 - val_loss: 0.4185 - val_acc: 0.8640

Epoch 00005: val_loss improved from 0.45595 to 0.41847, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 6/40
 - 45s - loss: 0.4167 - acc: 0.8686 - val_loss: 0.4249 - val_acc: 0.8619

Epoch 00006: val_loss did not improve from 0.41847
Epoch 7/40
 - 45s - loss: 0.3948 - acc: 0.8806 - val_loss: 0.3896 - val_acc: 0.8759

Epoch 00007: val_loss improved from 0.41847 to 0.38961, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 45s - loss: 0.3775 - acc: 0.8899 - val_loss: 0.4166 - val_acc: 0.8701

Epoch 00008: val_loss did not improve from 0.38961
Epoch 9/40
 - 45s - loss: 0.3637 - acc: 0.8961 - val_loss: 0.6018 - val_acc: 0.7464

Epoch 00009: val_loss did not improve from 0.38961
Epoch 10/40
 - 45s - loss: 0.3493 - acc: 0.9020 - val_loss: 0.3716 - val_acc: 0.8825

Epoch 00010: val_loss improved from 0.38961 to 0.37162, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 11/40
 - 45s - loss: 0.3355 - acc: 0.9097 - val_loss: 0.3328 - val_acc: 0.9078

Epoch 00011: val_loss improved from 0.37162 to 0.33283, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 45s - loss: 0.3220 - acc: 0.9143 - val_loss: 0.3306 - val_acc: 0.9143

Epoch 00012: val_loss improved from 0.33283 to 0.33056, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 13/40
 - 45s - loss: 0.3114 - acc: 0.9187 - val_loss: 0.3319 - val_acc: 0.9057

Epoch 00013: val_loss did not improve from 0.33056
Epoch 14/40
 - 45s - loss: 0.3028 - acc: 0.9222 - val_loss: 0.3373 - val_acc: 0.9034

Epoch 00014: val_loss did not improve from 0.33056
Epoch 15/40
 - 45s - loss: 0.2922 - acc: 0.9258 - val_loss: 0.2946 - val_acc: 0.9244

Epoch 00015: val_loss improved from 0.33056 to 0.29461, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 16/40
 - 45s - loss: 0.2869 - acc: 0.9281 - val_loss: 0.3108 - val_acc: 0.9149

Epoch 00016: val_loss did not improve from 0.29461
Epoch 17/40
 - 45s - loss: 0.2751 - acc: 0.9347 - val_loss: 0.3325 - val_acc: 0.9130

Epoch 00017: val_loss did not improve from 0.29461
Epoch 18/40
 - 45s - loss: 0.2654 - acc: 0.9385 - val_loss: 0.2997 - val_acc: 0.9198

Epoch 00018: val_loss did not improve from 0.29461
Epoch 19/40
 - 45s - loss: 0.2588 - acc: 0.9423 - val_loss: 0.4961 - val_acc: 0.8677

Epoch 00019: val_loss did not improve from 0.29461
Epoch 20/40
 - 45s - loss: 0.2532 - acc: 0.9432 - val_loss: 0.3961 - val_acc: 0.8686

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00020: val_loss did not improve from 0.29461
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 445us/step
current Test accuracy: 0.7783602150537634
current auc_score ------------------>  0.8205113307896867

  32/7440 [..............................] - ETA: 4:35
 160/7440 [..............................] - ETA: 56s 
 288/7440 [>.............................] - ETA: 32s
 416/7440 [>.............................] - ETA: 23s
 544/7440 [=>............................] - ETA: 18s
 672/7440 [=>............................] - ETA: 14s
 800/7440 [==>...........................] - ETA: 12s
 928/7440 [==>...........................] - ETA: 11s
1056/7440 [===>..........................] - ETA: 9s 
1184/7440 [===>..........................] - ETA: 8s
1312/7440 [====>.........................] - ETA: 8s
1440/7440 [====>.........................] - ETA: 7s
1568/7440 [=====>........................] - ETA: 6s
1696/7440 [=====>........................] - ETA: 6s
1824/7440 [======>.......................] - ETA: 6s
1952/7440 [======>.......................] - ETA: 5s
2080/7440 [=======>......................] - ETA: 5s
2208/7440 [=======>......................] - ETA: 5s
2336/7440 [========>.....................] - ETA: 4s
2464/7440 [========>.....................] - ETA: 4s
2592/7440 [=========>....................] - ETA: 4s
2720/7440 [=========>....................] - ETA: 4s
2848/7440 [==========>...................] - ETA: 3s
2976/7440 [===========>..................] - ETA: 3s
3104/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3360/7440 [============>.................] - ETA: 3s
3488/7440 [=============>................] - ETA: 3s
3616/7440 [=============>................] - ETA: 2s
3744/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4000/7440 [===============>..............] - ETA: 2s
4128/7440 [===============>..............] - ETA: 2s
4256/7440 [================>.............] - ETA: 2s
4384/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 2s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 594us/step
Best saved model Test accuracy: 0.8376344086021505
best saved model auc_score ------------------>  0.9054346239449647
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_64[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_65[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_66[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_67[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 48, 96, 96)   0           concatenate_28[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_68[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_69[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 64, 96, 96)   0           concatenate_29[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_70[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_71[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_72[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 48, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_73[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_74[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 64, 48, 48)   0           concatenate_31[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_75[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_76[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 80, 48, 48)   0           concatenate_32[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 80, 48, 48)   320         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 80, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 40, 48, 48)   3200        activation_77[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 40, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 40, 24, 24)   160         average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 40, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2560        activation_78[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_79[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 56, 24, 24)   0           average_pooling2d_8[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 56, 24, 24)   224         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 56, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3584        activation_80[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_81[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 72, 24, 24)   0           concatenate_34[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 72, 24, 24)   288         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4608        activation_82[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_83[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 88, 24, 24)   0           concatenate_35[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 88, 24, 24)   352         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 88, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 88)           0           activation_84[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            89          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 119,545
Trainable params: 117,113
Non-trainable params: 2,432
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 51s - loss: 0.6203 - acc: 0.7590 - val_loss: 0.5606 - val_acc: 0.7871

Epoch 00001: val_loss improved from inf to 0.56059, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 45s - loss: 0.5389 - acc: 0.7994 - val_loss: 0.5248 - val_acc: 0.8051

Epoch 00002: val_loss improved from 0.56059 to 0.52478, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 45s - loss: 0.4918 - acc: 0.8261 - val_loss: 0.4869 - val_acc: 0.8299

Epoch 00003: val_loss improved from 0.52478 to 0.48688, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 45s - loss: 0.4562 - acc: 0.8457 - val_loss: 0.4428 - val_acc: 0.8503

Epoch 00004: val_loss improved from 0.48688 to 0.44279, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 45s - loss: 0.4307 - acc: 0.8610 - val_loss: 0.4208 - val_acc: 0.8647

Epoch 00005: val_loss improved from 0.44279 to 0.42085, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 6/40
 - 45s - loss: 0.4046 - acc: 0.8745 - val_loss: 0.4271 - val_acc: 0.8461

Epoch 00006: val_loss did not improve from 0.42085
Epoch 7/40
 - 45s - loss: 0.3876 - acc: 0.8828 - val_loss: 0.4010 - val_acc: 0.8686

Epoch 00007: val_loss improved from 0.42085 to 0.40097, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 45s - loss: 0.3697 - acc: 0.8906 - val_loss: 0.3795 - val_acc: 0.8847

Epoch 00008: val_loss improved from 0.40097 to 0.37951, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 9/40
 - 45s - loss: 0.3512 - acc: 0.8998 - val_loss: 0.3634 - val_acc: 0.8907

Epoch 00009: val_loss improved from 0.37951 to 0.36339, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 45s - loss: 0.3391 - acc: 0.9038 - val_loss: 0.3667 - val_acc: 0.8922

Epoch 00010: val_loss did not improve from 0.36339
Epoch 11/40
 - 45s - loss: 0.3265 - acc: 0.9103 - val_loss: 0.3329 - val_acc: 0.9085

Epoch 00011: val_loss improved from 0.36339 to 0.33289, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 45s - loss: 0.3141 - acc: 0.9166 - val_loss: 0.3828 - val_acc: 0.8874

Epoch 00012: val_loss did not improve from 0.33289
Epoch 13/40
 - 45s - loss: 0.3011 - acc: 0.9235 - val_loss: 0.3100 - val_acc: 0.9178

Epoch 00013: val_loss improved from 0.33289 to 0.30998, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 14/40
 - 45s - loss: 0.2939 - acc: 0.9249 - val_loss: 0.4742 - val_acc: 0.8425

Epoch 00014: val_loss did not improve from 0.30998
Epoch 15/40
 - 45s - loss: 0.2816 - acc: 0.9322 - val_loss: 0.3577 - val_acc: 0.8893

Epoch 00015: val_loss did not improve from 0.30998
Epoch 16/40
 - 45s - loss: 0.2707 - acc: 0.9351 - val_loss: 0.3093 - val_acc: 0.9139

Epoch 00016: val_loss improved from 0.30998 to 0.30925, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 17/40
 - 45s - loss: 0.2689 - acc: 0.9356 - val_loss: 0.2792 - val_acc: 0.9277

Epoch 00017: val_loss improved from 0.30925 to 0.27921, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 18/40
 - 45s - loss: 0.2576 - acc: 0.9402 - val_loss: 0.3965 - val_acc: 0.8803

Epoch 00018: val_loss did not improve from 0.27921
Epoch 19/40
 - 45s - loss: 0.2491 - acc: 0.9426 - val_loss: 0.2879 - val_acc: 0.9253

Epoch 00019: val_loss did not improve from 0.27921
Epoch 20/40
 - 45s - loss: 0.2423 - acc: 0.9480 - val_loss: 0.2861 - val_acc: 0.9249

Epoch 00020: val_loss did not improve from 0.27921
Epoch 21/40
 - 45s - loss: 0.2354 - acc: 0.9484 - val_loss: 0.2790 - val_acc: 0.9334

Epoch 00021: val_loss improved from 0.27921 to 0.27903, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 22/40
 - 45s - loss: 0.2309 - acc: 0.9499 - val_loss: 0.3036 - val_acc: 0.9147

Epoch 00022: val_loss did not improve from 0.27903
Epoch 23/40
 - 45s - loss: 0.2236 - acc: 0.9523 - val_loss: 0.2984 - val_acc: 0.9191

Epoch 00023: val_loss did not improve from 0.27903
Epoch 24/40
 - 45s - loss: 0.2173 - acc: 0.9557 - val_loss: 0.4322 - val_acc: 0.8761

Epoch 00024: val_loss did not improve from 0.27903
Epoch 25/40
 - 45s - loss: 0.2161 - acc: 0.9567 - val_loss: 0.3122 - val_acc: 0.9154

Epoch 00025: val_loss did not improve from 0.27903
Epoch 26/40
 - 45s - loss: 0.2067 - acc: 0.9591 - val_loss: 0.3157 - val_acc: 0.9158

Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00026: val_loss did not improve from 0.27903
Epoch 00026: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 451us/step
current Test accuracy: 0.7709677419354839
current auc_score ------------------>  0.8769337134350792

  32/7440 [..............................] - ETA: 6:01
 160/7440 [..............................] - ETA: 1:13
 288/7440 [>.............................] - ETA: 41s 
 416/7440 [>.............................] - ETA: 29s
 544/7440 [=>............................] - ETA: 22s
 672/7440 [=>............................] - ETA: 18s
 800/7440 [==>...........................] - ETA: 15s
 928/7440 [==>...........................] - ETA: 13s
1056/7440 [===>..........................] - ETA: 12s
1184/7440 [===>..........................] - ETA: 11s
1312/7440 [====>.........................] - ETA: 10s
1440/7440 [====>.........................] - ETA: 9s 
1568/7440 [=====>........................] - ETA: 8s
1696/7440 [=====>........................] - ETA: 7s
1824/7440 [======>.......................] - ETA: 7s
1952/7440 [======>.......................] - ETA: 6s
2080/7440 [=======>......................] - ETA: 6s
2208/7440 [=======>......................] - ETA: 6s
2336/7440 [========>.....................] - ETA: 5s
2464/7440 [========>.....................] - ETA: 5s
2592/7440 [=========>....................] - ETA: 5s
2720/7440 [=========>....................] - ETA: 4s
2848/7440 [==========>...................] - ETA: 4s
2976/7440 [===========>..................] - ETA: 4s
3104/7440 [===========>..................] - ETA: 4s
3232/7440 [============>.................] - ETA: 3s
3360/7440 [============>.................] - ETA: 3s
3488/7440 [=============>................] - ETA: 3s
3616/7440 [=============>................] - ETA: 3s
3744/7440 [==============>...............] - ETA: 3s
3872/7440 [==============>...............] - ETA: 3s
4000/7440 [===============>..............] - ETA: 2s
4128/7440 [===============>..............] - ETA: 2s
4256/7440 [================>.............] - ETA: 2s
4384/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 2s
4640/7440 [=================>............] - ETA: 2s
4768/7440 [==================>...........] - ETA: 2s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5920/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 672us/step
Best saved model Test accuracy: 0.7842741935483871
best saved model auc_score ------------------>  0.8656969013758816
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 8, 96, 96)    144         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 8, 96, 96)    32          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 96, 96)    0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   512         activation_85[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_86[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   1536        activation_87[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 40, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   2560        activation_89[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_90[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 56, 96, 96)   0           concatenate_38[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 56, 96, 96)   224         concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 56, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 28, 96, 96)   1568        activation_91[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 28, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 28, 48, 48)   112         average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 28, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1792        activation_92[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_93[0][0]              
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 44, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 44, 48, 48)   176         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 44, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2816        activation_94[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_95[0][0]              
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 60, 48, 48)   0           concatenate_40[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 60, 48, 48)   240         concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 60, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   3840        activation_96[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_97[0][0]              
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 76, 48, 48)   0           concatenate_41[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 76, 48, 48)   304         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 76, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 38, 48, 48)   2888        activation_98[0][0]              
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 38, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 38, 24, 24)   152         average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 38, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2432        activation_99[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_100[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 54, 24, 24)   0           average_pooling2d_10[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 54, 24, 24)   216         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 54, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3456        activation_101[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_102[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 70, 24, 24)   0           concatenate_43[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 70, 24, 24)   280         concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 70, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4480        activation_103[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_104[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 86, 24, 24)   0           concatenate_44[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 86, 24, 24)   344         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 86, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 86)           0           activation_105[0][0]             
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            87          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 115,695
Trainable params: 113,375
Non-trainable params: 2,320
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 52s - loss: 0.6217 - acc: 0.7606 - val_loss: 0.6005 - val_acc: 0.7691

Epoch 00001: val_loss improved from inf to 0.60049, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 44s - loss: 0.5397 - acc: 0.7998 - val_loss: 0.5609 - val_acc: 0.7815

Epoch 00002: val_loss improved from 0.60049 to 0.56086, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 44s - loss: 0.5030 - acc: 0.8169 - val_loss: 0.5508 - val_acc: 0.7978

Epoch 00003: val_loss improved from 0.56086 to 0.55077, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 44s - loss: 0.4752 - acc: 0.8344 - val_loss: 0.4666 - val_acc: 0.8337

Epoch 00004: val_loss improved from 0.55077 to 0.46661, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 44s - loss: 0.4526 - acc: 0.8444 - val_loss: 0.6697 - val_acc: 0.7349

Epoch 00005: val_loss did not improve from 0.46661
Epoch 6/40
 - 44s - loss: 0.4274 - acc: 0.8584 - val_loss: 0.4538 - val_acc: 0.8449

Epoch 00006: val_loss improved from 0.46661 to 0.45380, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 44s - loss: 0.4128 - acc: 0.8675 - val_loss: 0.4163 - val_acc: 0.8621

Epoch 00007: val_loss improved from 0.45380 to 0.41631, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 44s - loss: 0.3928 - acc: 0.8771 - val_loss: 0.3991 - val_acc: 0.8768

Epoch 00008: val_loss improved from 0.41631 to 0.39913, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 9/40
 - 44s - loss: 0.3779 - acc: 0.8867 - val_loss: 0.4484 - val_acc: 0.8409

Epoch 00009: val_loss did not improve from 0.39913
Epoch 10/40
 - 44s - loss: 0.3664 - acc: 0.8890 - val_loss: 0.3660 - val_acc: 0.8889

Epoch 00010: val_loss improved from 0.39913 to 0.36596, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 11/40
 - 44s - loss: 0.3511 - acc: 0.8967 - val_loss: 0.3616 - val_acc: 0.9021

Epoch 00011: val_loss improved from 0.36596 to 0.36159, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 44s - loss: 0.3406 - acc: 0.9018 - val_loss: 0.3447 - val_acc: 0.8996

Epoch 00012: val_loss improved from 0.36159 to 0.34473, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 13/40
 - 44s - loss: 0.3292 - acc: 0.9091 - val_loss: 0.3622 - val_acc: 0.8840

Epoch 00013: val_loss did not improve from 0.34473
Epoch 14/40
 - 44s - loss: 0.3154 - acc: 0.9164 - val_loss: 0.3769 - val_acc: 0.8870

Epoch 00014: val_loss did not improve from 0.34473
Epoch 15/40
 - 44s - loss: 0.3106 - acc: 0.9165 - val_loss: 0.3434 - val_acc: 0.9029

Epoch 00015: val_loss improved from 0.34473 to 0.34336, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 16/40
 - 45s - loss: 0.2986 - acc: 0.9230 - val_loss: 0.5167 - val_acc: 0.8121

Epoch 00016: val_loss did not improve from 0.34336
Epoch 17/40
 - 44s - loss: 0.2906 - acc: 0.9246 - val_loss: 0.3621 - val_acc: 0.8945

Epoch 00017: val_loss did not improve from 0.34336
Epoch 18/40
 - 44s - loss: 0.2851 - acc: 0.9275 - val_loss: 0.3373 - val_acc: 0.9037

Epoch 00018: val_loss improved from 0.34336 to 0.33732, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 19/40
 - 44s - loss: 0.2775 - acc: 0.9315 - val_loss: 0.3414 - val_acc: 0.8967

Epoch 00019: val_loss did not improve from 0.33732
Epoch 20/40
 - 44s - loss: 0.2703 - acc: 0.9336 - val_loss: 0.4822 - val_acc: 0.8427

Epoch 00020: val_loss did not improve from 0.33732
Epoch 21/40
 - 45s - loss: 0.2629 - acc: 0.9367 - val_loss: 0.3262 - val_acc: 0.9036

Epoch 00021: val_loss improved from 0.33732 to 0.32622, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 22/40
 - 45s - loss: 0.2592 - acc: 0.9387 - val_loss: 0.3291 - val_acc: 0.9046

Epoch 00022: val_loss did not improve from 0.32622
Epoch 23/40
 - 44s - loss: 0.2510 - acc: 0.9422 - val_loss: 0.3415 - val_acc: 0.9026

Epoch 00023: val_loss did not improve from 0.32622
Epoch 24/40
 - 44s - loss: 0.2433 - acc: 0.9445 - val_loss: 0.3468 - val_acc: 0.8985

Epoch 00024: val_loss did not improve from 0.32622
Epoch 25/40
 - 44s - loss: 0.2433 - acc: 0.9431 - val_loss: 0.2832 - val_acc: 0.9246

Epoch 00025: val_loss improved from 0.32622 to 0.28315, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 26/40
 - 44s - loss: 0.2318 - acc: 0.9495 - val_loss: 0.2996 - val_acc: 0.9198

Epoch 00026: val_loss did not improve from 0.28315
Epoch 27/40
 - 44s - loss: 0.2288 - acc: 0.9505 - val_loss: 0.3452 - val_acc: 0.8980

Epoch 00027: val_loss did not improve from 0.28315
Epoch 28/40
 - 44s - loss: 0.2237 - acc: 0.9543 - val_loss: 0.3092 - val_acc: 0.9168

Epoch 00028: val_loss did not improve from 0.28315
Epoch 29/40
 - 44s - loss: 0.2163 - acc: 0.9561 - val_loss: 0.3057 - val_acc: 0.9163

Epoch 00029: val_loss did not improve from 0.28315
Epoch 30/40
 - 44s - loss: 0.2118 - acc: 0.9561 - val_loss: 0.2779 - val_acc: 0.9243

Epoch 00030: val_loss improved from 0.28315 to 0.27795, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 31/40
 - 44s - loss: 0.2101 - acc: 0.9569 - val_loss: 0.2966 - val_acc: 0.9251

Epoch 00031: val_loss did not improve from 0.27795
Epoch 32/40
 - 44s - loss: 0.2033 - acc: 0.9609 - val_loss: 0.4848 - val_acc: 0.8524

Epoch 00032: val_loss did not improve from 0.27795
Epoch 33/40
 - 44s - loss: 0.2014 - acc: 0.9604 - val_loss: 0.3085 - val_acc: 0.9185

Epoch 00033: val_loss did not improve from 0.27795
Epoch 34/40
 - 44s - loss: 0.1965 - acc: 0.9635 - val_loss: 0.2875 - val_acc: 0.9272

Epoch 00034: val_loss did not improve from 0.27795
Epoch 35/40
 - 44s - loss: 0.1920 - acc: 0.9647 - val_loss: 0.2937 - val_acc: 0.9253

Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00035: val_loss did not improve from 0.27795
Epoch 00035: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 469us/step
current Test accuracy: 0.8388440860215054
current auc_score ------------------>  0.918422216441207

  32/7440 [..............................] - ETA: 7:37
 128/7440 [..............................] - ETA: 1:55
 256/7440 [>.............................] - ETA: 58s 
 384/7440 [>.............................] - ETA: 39s
 512/7440 [=>............................] - ETA: 29s
 640/7440 [=>............................] - ETA: 24s
 768/7440 [==>...........................] - ETA: 20s
 896/7440 [==>...........................] - ETA: 17s
1024/7440 [===>..........................] - ETA: 15s
1152/7440 [===>..........................] - ETA: 13s
1280/7440 [====>.........................] - ETA: 12s
1408/7440 [====>.........................] - ETA: 11s
1536/7440 [=====>........................] - ETA: 10s
1664/7440 [=====>........................] - ETA: 9s 
1792/7440 [======>.......................] - ETA: 8s
1920/7440 [======>.......................] - ETA: 8s
2048/7440 [=======>......................] - ETA: 7s
2176/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2432/7440 [========>.....................] - ETA: 6s
2560/7440 [=========>....................] - ETA: 6s
2688/7440 [=========>....................] - ETA: 5s
2816/7440 [==========>...................] - ETA: 5s
2944/7440 [==========>...................] - ETA: 5s
3072/7440 [===========>..................] - ETA: 4s
3200/7440 [===========>..................] - ETA: 4s
3328/7440 [============>.................] - ETA: 4s
3456/7440 [============>.................] - ETA: 4s
3584/7440 [=============>................] - ETA: 3s
3712/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
3968/7440 [===============>..............] - ETA: 3s
4096/7440 [===============>..............] - ETA: 3s
4224/7440 [================>.............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4480/7440 [=================>............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4736/7440 [==================>...........] - ETA: 2s
4864/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5120/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5888/7440 [======================>.......] - ETA: 1s
6016/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 727us/step
Best saved model Test accuracy: 0.8388440860215054
best saved model auc_score ------------------>  0.9051229188345473
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 32, 96, 96)   576         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 32, 96, 96)   128         initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_106[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_107[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 48, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_108[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_109[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 64, 96, 96)   0           concatenate_46[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 64, 96, 96)   256         concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   4096        activation_110[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_111[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 80, 96, 96)   0           concatenate_47[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 80, 96, 96)   320         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 80, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 40, 96, 96)   3200        activation_112[0][0]             
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 40, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 40, 48, 48)   160         average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_113[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_114[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 56, 48, 48)   0           average_pooling2d_11[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 56, 48, 48)   224         concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 56, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3584        activation_115[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_116[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 72, 48, 48)   0           concatenate_49[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 72, 48, 48)   288         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4608        activation_117[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_118[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 88, 48, 48)   0           concatenate_50[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 88, 48, 48)   352         concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 88, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3872        activation_119[0][0]             
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_120[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_121[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 60, 24, 24)   0           average_pooling2d_12[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 60, 24, 24)   240         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 60, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3840        activation_122[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_123[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 76, 24, 24)   0           concatenate_52[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 76, 24, 24)   304         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 76, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4864        activation_124[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_125[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 92, 24, 24)   0           concatenate_53[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 92, 24, 24)   368         concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 92, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 92)           0           activation_126[0][0]             
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            93          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 127,485
Trainable params: 124,829
Non-trainable params: 2,656
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 59s - loss: 0.6222 - acc: 0.7621 - val_loss: 0.5543 - val_acc: 0.7964

Epoch 00001: val_loss improved from inf to 0.55430, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 51s - loss: 0.5412 - acc: 0.8039 - val_loss: 0.5185 - val_acc: 0.8122

Epoch 00002: val_loss improved from 0.55430 to 0.51846, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 50s - loss: 0.5050 - acc: 0.8238 - val_loss: 0.5330 - val_acc: 0.7989

Epoch 00003: val_loss did not improve from 0.51846
Epoch 4/40
 - 51s - loss: 0.4716 - acc: 0.8416 - val_loss: 0.4810 - val_acc: 0.8365

Epoch 00004: val_loss improved from 0.51846 to 0.48097, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 51s - loss: 0.4462 - acc: 0.8568 - val_loss: 0.4847 - val_acc: 0.8401

Epoch 00005: val_loss did not improve from 0.48097
Epoch 6/40
 - 51s - loss: 0.4254 - acc: 0.8664 - val_loss: 0.4317 - val_acc: 0.8591

Epoch 00006: val_loss improved from 0.48097 to 0.43165, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 51s - loss: 0.4054 - acc: 0.8750 - val_loss: 0.3940 - val_acc: 0.8845

Epoch 00007: val_loss improved from 0.43165 to 0.39402, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 50s - loss: 0.3859 - acc: 0.8868 - val_loss: 0.3881 - val_acc: 0.8842

Epoch 00008: val_loss improved from 0.39402 to 0.38809, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 9/40
 - 51s - loss: 0.3721 - acc: 0.8925 - val_loss: 0.3740 - val_acc: 0.8958

Epoch 00009: val_loss improved from 0.38809 to 0.37396, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 51s - loss: 0.3570 - acc: 0.8990 - val_loss: 0.3565 - val_acc: 0.8983

Epoch 00010: val_loss improved from 0.37396 to 0.35648, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 11/40
 - 50s - loss: 0.3423 - acc: 0.9057 - val_loss: 0.3721 - val_acc: 0.8919

Epoch 00011: val_loss did not improve from 0.35648
Epoch 12/40
 - 51s - loss: 0.3346 - acc: 0.9095 - val_loss: 0.3763 - val_acc: 0.8849

Epoch 00012: val_loss did not improve from 0.35648
Epoch 13/40
 - 51s - loss: 0.3222 - acc: 0.9152 - val_loss: 0.3394 - val_acc: 0.9081

Epoch 00013: val_loss improved from 0.35648 to 0.33941, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 14/40
 - 51s - loss: 0.3121 - acc: 0.9188 - val_loss: 0.3224 - val_acc: 0.9165

Epoch 00014: val_loss improved from 0.33941 to 0.32236, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 15/40
 - 50s - loss: 0.2993 - acc: 0.9248 - val_loss: 0.3196 - val_acc: 0.9178

Epoch 00015: val_loss improved from 0.32236 to 0.31963, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 16/40
 - 50s - loss: 0.2907 - acc: 0.9285 - val_loss: 0.4186 - val_acc: 0.8609

Epoch 00016: val_loss did not improve from 0.31963
Epoch 17/40
 - 50s - loss: 0.2801 - acc: 0.9325 - val_loss: 0.3080 - val_acc: 0.9177

Epoch 00017: val_loss improved from 0.31963 to 0.30800, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 18/40
 - 50s - loss: 0.2747 - acc: 0.9360 - val_loss: 0.3012 - val_acc: 0.9237

Epoch 00018: val_loss improved from 0.30800 to 0.30118, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 19/40
 - 50s - loss: 0.2678 - acc: 0.9377 - val_loss: 0.3550 - val_acc: 0.8948

Epoch 00019: val_loss did not improve from 0.30118
Epoch 20/40
 - 50s - loss: 0.2607 - acc: 0.9414 - val_loss: 0.3095 - val_acc: 0.9197

Epoch 00020: val_loss did not improve from 0.30118
Epoch 21/40
 - 51s - loss: 0.2532 - acc: 0.9428 - val_loss: 0.3051 - val_acc: 0.9196

Epoch 00021: val_loss did not improve from 0.30118
Epoch 22/40
 - 50s - loss: 0.2436 - acc: 0.9482 - val_loss: 0.3083 - val_acc: 0.9184

Epoch 00022: val_loss did not improve from 0.30118
Epoch 23/40
 - 50s - loss: 0.2412 - acc: 0.9477 - val_loss: 0.3499 - val_acc: 0.9070

Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00023: val_loss did not improve from 0.30118
Epoch 00023: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 3s
1440/7440 [====>.........................] - ETA: 3s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 506us/step
current Test accuracy: 0.7865591397849462
current auc_score ------------------>  0.8680469562955255

  32/7440 [..............................] - ETA: 9:36
 128/7440 [..............................] - ETA: 2:25
 224/7440 [..............................] - ETA: 1:23
 352/7440 [>.............................] - ETA: 53s 
 480/7440 [>.............................] - ETA: 39s
 608/7440 [=>............................] - ETA: 31s
 736/7440 [=>............................] - ETA: 26s
 864/7440 [==>...........................] - ETA: 22s
 992/7440 [===>..........................] - ETA: 19s
1120/7440 [===>..........................] - ETA: 17s
1248/7440 [====>.........................] - ETA: 15s
1376/7440 [====>.........................] - ETA: 14s
1504/7440 [=====>........................] - ETA: 12s
1632/7440 [=====>........................] - ETA: 11s
1760/7440 [======>.......................] - ETA: 10s
1888/7440 [======>.......................] - ETA: 10s
2016/7440 [=======>......................] - ETA: 9s 
2144/7440 [=======>......................] - ETA: 8s
2272/7440 [========>.....................] - ETA: 8s
2400/7440 [========>.....................] - ETA: 7s
2528/7440 [=========>....................] - ETA: 7s
2656/7440 [=========>....................] - ETA: 6s
2784/7440 [==========>...................] - ETA: 6s
2912/7440 [==========>...................] - ETA: 6s
3040/7440 [===========>..................] - ETA: 5s
3168/7440 [===========>..................] - ETA: 5s
3296/7440 [============>.................] - ETA: 5s
3424/7440 [============>.................] - ETA: 4s
3552/7440 [=============>................] - ETA: 4s
3680/7440 [=============>................] - ETA: 4s
3808/7440 [==============>...............] - ETA: 4s
3936/7440 [==============>...............] - ETA: 4s
4064/7440 [===============>..............] - ETA: 3s
4192/7440 [===============>..............] - ETA: 3s
4320/7440 [================>.............] - ETA: 3s
4448/7440 [================>.............] - ETA: 3s
4576/7440 [=================>............] - ETA: 3s
4704/7440 [=================>............] - ETA: 2s
4832/7440 [==================>...........] - ETA: 2s
4960/7440 [===================>..........] - ETA: 2s
5088/7440 [===================>..........] - ETA: 2s
5216/7440 [====================>.........] - ETA: 2s
5344/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5728/7440 [======================>.......] - ETA: 1s
5856/7440 [======================>.......] - ETA: 1s
5984/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 1s
6368/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 845us/step
Best saved model Test accuracy: 0.8329301075268817
best saved model auc_score ------------------>  0.9128310353798127
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 8, 96, 96)    144         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 8, 96, 96)    32          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 8, 96, 96)    0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   512         activation_127[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_128[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   1536        activation_129[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_130[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 40, 96, 96)   0           concatenate_55[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   2560        activation_131[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_132[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 56, 96, 96)   0           concatenate_56[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 56, 96, 96)   224         concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 56, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 28, 96, 96)   1568        activation_133[0][0]             
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 28, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 28, 48, 48)   112         average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 28, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1792        activation_134[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_135[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 44, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 44, 48, 48)   176         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 44, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2816        activation_136[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_137[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 60, 48, 48)   0           concatenate_58[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 60, 48, 48)   240         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 60, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   3840        activation_138[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_139[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 76, 48, 48)   0           concatenate_59[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 76, 48, 48)   304         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 76, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 38, 48, 48)   2888        activation_140[0][0]             
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 38, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 38, 24, 24)   152         average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 38, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2432        activation_141[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_142[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 54, 24, 24)   0           average_pooling2d_14[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 54, 24, 24)   216         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 54, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3456        activation_143[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_144[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 70, 24, 24)   0           concatenate_61[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 70, 24, 24)   280         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 70, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4480        activation_145[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_146[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 86, 24, 24)   0           concatenate_62[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 86, 24, 24)   344         concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 86, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 86)           0           activation_147[0][0]             
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            87          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 115,695
Trainable params: 113,375
Non-trainable params: 2,320
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 54s - loss: 0.6276 - acc: 0.7548 - val_loss: 0.5727 - val_acc: 0.7864

Epoch 00001: val_loss improved from inf to 0.57272, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 44s - loss: 0.5435 - acc: 0.7986 - val_loss: 0.5295 - val_acc: 0.8021

Epoch 00002: val_loss improved from 0.57272 to 0.52954, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 44s - loss: 0.5030 - acc: 0.8196 - val_loss: 0.4688 - val_acc: 0.8406

Epoch 00003: val_loss improved from 0.52954 to 0.46882, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 45s - loss: 0.4718 - acc: 0.8375 - val_loss: 0.4550 - val_acc: 0.8530

Epoch 00004: val_loss improved from 0.46882 to 0.45504, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 44s - loss: 0.4454 - acc: 0.8514 - val_loss: 0.5003 - val_acc: 0.8286

Epoch 00005: val_loss did not improve from 0.45504
Epoch 6/40
 - 44s - loss: 0.4234 - acc: 0.8634 - val_loss: 0.4069 - val_acc: 0.8719

Epoch 00006: val_loss improved from 0.45504 to 0.40690, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 44s - loss: 0.4055 - acc: 0.8712 - val_loss: 0.4320 - val_acc: 0.8593

Epoch 00007: val_loss did not improve from 0.40690
Epoch 8/40
 - 44s - loss: 0.3896 - acc: 0.8818 - val_loss: 0.3948 - val_acc: 0.8804

Epoch 00008: val_loss improved from 0.40690 to 0.39482, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 9/40
 - 44s - loss: 0.3747 - acc: 0.8871 - val_loss: 0.3891 - val_acc: 0.8731

Epoch 00009: val_loss improved from 0.39482 to 0.38911, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 44s - loss: 0.3603 - acc: 0.8941 - val_loss: 0.3908 - val_acc: 0.8859

Epoch 00010: val_loss did not improve from 0.38911
Epoch 11/40
 - 44s - loss: 0.3464 - acc: 0.9007 - val_loss: 0.3711 - val_acc: 0.8899

Epoch 00011: val_loss improved from 0.38911 to 0.37106, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 45s - loss: 0.3357 - acc: 0.9064 - val_loss: 0.4205 - val_acc: 0.8542

Epoch 00012: val_loss did not improve from 0.37106
Epoch 13/40
 - 44s - loss: 0.3303 - acc: 0.9079 - val_loss: 0.4825 - val_acc: 0.8525

Epoch 00013: val_loss did not improve from 0.37106
Epoch 14/40
 - 44s - loss: 0.3194 - acc: 0.9128 - val_loss: 0.3209 - val_acc: 0.9101

Epoch 00014: val_loss improved from 0.37106 to 0.32085, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 15/40
 - 44s - loss: 0.3074 - acc: 0.9178 - val_loss: 0.5443 - val_acc: 0.7873

Epoch 00015: val_loss did not improve from 0.32085
Epoch 16/40
 - 44s - loss: 0.3018 - acc: 0.9207 - val_loss: 0.3277 - val_acc: 0.9045

Epoch 00016: val_loss did not improve from 0.32085
Epoch 17/40
 - 44s - loss: 0.2944 - acc: 0.9239 - val_loss: 0.3090 - val_acc: 0.9155

Epoch 00017: val_loss improved from 0.32085 to 0.30900, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 18/40
 - 44s - loss: 0.2866 - acc: 0.9272 - val_loss: 0.3041 - val_acc: 0.9165

Epoch 00018: val_loss improved from 0.30900 to 0.30413, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 19/40
 - 44s - loss: 0.2822 - acc: 0.9278 - val_loss: 0.2924 - val_acc: 0.9224

Epoch 00019: val_loss improved from 0.30413 to 0.29240, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 20/40
 - 44s - loss: 0.2687 - acc: 0.9353 - val_loss: 0.4104 - val_acc: 0.8672

Epoch 00020: val_loss did not improve from 0.29240
Epoch 21/40
 - 44s - loss: 0.2670 - acc: 0.9347 - val_loss: 0.4361 - val_acc: 0.8566

Epoch 00021: val_loss did not improve from 0.29240
Epoch 22/40
 - 44s - loss: 0.2638 - acc: 0.9350 - val_loss: 0.3517 - val_acc: 0.9024

Epoch 00022: val_loss did not improve from 0.29240
Epoch 23/40
 - 44s - loss: 0.2574 - acc: 0.9376 - val_loss: 0.3185 - val_acc: 0.9069

Epoch 00023: val_loss did not improve from 0.29240
Epoch 24/40
 - 44s - loss: 0.2500 - acc: 0.9410 - val_loss: 0.4341 - val_acc: 0.8512

Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00024: val_loss did not improve from 0.29240
Epoch 00024: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 454us/step
current Test accuracy: 0.703494623655914
current auc_score ------------------>  0.8923194155393688

  32/7440 [..............................] - ETA: 11:45
 160/7440 [..............................] - ETA: 2:21 
 288/7440 [>.............................] - ETA: 1:18
 416/7440 [>.............................] - ETA: 54s 
 544/7440 [=>............................] - ETA: 41s
 672/7440 [=>............................] - ETA: 33s
 800/7440 [==>...........................] - ETA: 28s
 928/7440 [==>...........................] - ETA: 24s
1056/7440 [===>..........................] - ETA: 21s
1184/7440 [===>..........................] - ETA: 18s
1312/7440 [====>.........................] - ETA: 17s
1440/7440 [====>.........................] - ETA: 15s
1568/7440 [=====>........................] - ETA: 14s
1696/7440 [=====>........................] - ETA: 12s
1824/7440 [======>.......................] - ETA: 11s
1952/7440 [======>.......................] - ETA: 11s
2080/7440 [=======>......................] - ETA: 10s
2208/7440 [=======>......................] - ETA: 9s 
2336/7440 [========>.....................] - ETA: 9s
2464/7440 [========>.....................] - ETA: 8s
2592/7440 [=========>....................] - ETA: 7s
2720/7440 [=========>....................] - ETA: 7s
2848/7440 [==========>...................] - ETA: 7s
2976/7440 [===========>..................] - ETA: 6s
3104/7440 [===========>..................] - ETA: 6s
3232/7440 [============>.................] - ETA: 5s
3360/7440 [============>.................] - ETA: 5s
3488/7440 [=============>................] - ETA: 5s
3616/7440 [=============>................] - ETA: 4s
3744/7440 [==============>...............] - ETA: 4s
3872/7440 [==============>...............] - ETA: 4s
4000/7440 [===============>..............] - ETA: 4s
4128/7440 [===============>..............] - ETA: 3s
4256/7440 [================>.............] - ETA: 3s
4384/7440 [================>.............] - ETA: 3s
4512/7440 [=================>............] - ETA: 3s
4640/7440 [=================>............] - ETA: 3s
4768/7440 [==================>...........] - ETA: 2s
4896/7440 [==================>...........] - ETA: 2s
5024/7440 [===================>..........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5280/7440 [====================>.........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 2s
5536/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5920/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 873us/step
Best saved model Test accuracy: 0.8272849462365591
best saved model auc_score ------------------>  0.895890926696728
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 32, 96, 96)   576         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 32, 96, 96)   128         initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_148[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_149[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 48, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_150[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_151[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 64, 96, 96)   0           concatenate_64[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 64, 96, 96)   256         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   4096        activation_152[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_153[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 80, 96, 96)   0           concatenate_65[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 80, 96, 96)   320         concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 80, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 40, 96, 96)   3200        activation_154[0][0]             
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 40, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 40, 48, 48)   160         average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_155[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_156[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 56, 48, 48)   0           average_pooling2d_15[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 56, 48, 48)   224         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 56, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3584        activation_157[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_158[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 72, 48, 48)   0           concatenate_67[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 72, 48, 48)   288         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4608        activation_159[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 88, 48, 48)   0           concatenate_68[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 88, 48, 48)   352         concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 88, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3872        activation_161[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_162[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_163[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 60, 24, 24)   0           average_pooling2d_16[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 60, 24, 24)   240         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 60, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3840        activation_164[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 76, 24, 24)   0           concatenate_70[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 76, 24, 24)   304         concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 76, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4864        activation_166[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 92, 24, 24)   0           concatenate_71[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 92, 24, 24)   368         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 92, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 92)           0           activation_168[0][0]             
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            93          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 127,485
Trainable params: 124,829
Non-trainable params: 2,656
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 61s - loss: 0.6333 - acc: 0.7519 - val_loss: 0.5578 - val_acc: 0.7958

Epoch 00001: val_loss improved from inf to 0.55779, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 50s - loss: 0.5475 - acc: 0.7995 - val_loss: 0.5199 - val_acc: 0.8161

Epoch 00002: val_loss improved from 0.55779 to 0.51994, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 50s - loss: 0.5048 - acc: 0.8206 - val_loss: 0.5128 - val_acc: 0.8203

Epoch 00003: val_loss improved from 0.51994 to 0.51283, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 52s - loss: 0.4708 - acc: 0.8401 - val_loss: 0.4618 - val_acc: 0.8414

Epoch 00004: val_loss improved from 0.51283 to 0.46180, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 51s - loss: 0.4471 - acc: 0.8511 - val_loss: 0.4346 - val_acc: 0.8646

Epoch 00005: val_loss improved from 0.46180 to 0.43465, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 6/40
 - 51s - loss: 0.4236 - acc: 0.8656 - val_loss: 0.4159 - val_acc: 0.8650

Epoch 00006: val_loss improved from 0.43465 to 0.41589, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 51s - loss: 0.4038 - acc: 0.8775 - val_loss: 0.4143 - val_acc: 0.8665

Epoch 00007: val_loss improved from 0.41589 to 0.41433, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 51s - loss: 0.3845 - acc: 0.8870 - val_loss: 0.4525 - val_acc: 0.8555

Epoch 00008: val_loss did not improve from 0.41433
Epoch 9/40
 - 51s - loss: 0.3686 - acc: 0.8924 - val_loss: 0.3790 - val_acc: 0.8881

Epoch 00009: val_loss improved from 0.41433 to 0.37899, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 51s - loss: 0.3554 - acc: 0.9002 - val_loss: 0.3798 - val_acc: 0.8764

Epoch 00010: val_loss did not improve from 0.37899
Epoch 11/40
 - 51s - loss: 0.3430 - acc: 0.9028 - val_loss: 0.3775 - val_acc: 0.8889

Epoch 00011: val_loss improved from 0.37899 to 0.37746, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 12/40
 - 51s - loss: 0.3306 - acc: 0.9116 - val_loss: 0.5624 - val_acc: 0.8366

Epoch 00012: val_loss did not improve from 0.37746
Epoch 13/40
 - 51s - loss: 0.3177 - acc: 0.9143 - val_loss: 0.5527 - val_acc: 0.7998

Epoch 00013: val_loss did not improve from 0.37746
Epoch 14/40
 - 51s - loss: 0.3108 - acc: 0.9180 - val_loss: 0.3299 - val_acc: 0.9144

Epoch 00014: val_loss improved from 0.37746 to 0.32986, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 15/40
 - 51s - loss: 0.2997 - acc: 0.9255 - val_loss: 0.4225 - val_acc: 0.8608

Epoch 00015: val_loss did not improve from 0.32986
Epoch 16/40
 - 51s - loss: 0.2881 - acc: 0.9300 - val_loss: 0.3458 - val_acc: 0.9055

Epoch 00016: val_loss did not improve from 0.32986
Epoch 17/40
 - 51s - loss: 0.2754 - acc: 0.9344 - val_loss: 0.3040 - val_acc: 0.9263

Epoch 00017: val_loss improved from 0.32986 to 0.30397, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 18/40
 - 51s - loss: 0.2703 - acc: 0.9386 - val_loss: 0.3269 - val_acc: 0.9135

Epoch 00018: val_loss did not improve from 0.30397
Epoch 19/40
 - 51s - loss: 0.2643 - acc: 0.9399 - val_loss: 0.3299 - val_acc: 0.9142

Epoch 00019: val_loss did not improve from 0.30397
Epoch 20/40
 - 51s - loss: 0.2533 - acc: 0.9430 - val_loss: 0.4197 - val_acc: 0.8876

Epoch 00020: val_loss did not improve from 0.30397
Epoch 21/40
 - 51s - loss: 0.2477 - acc: 0.9468 - val_loss: 0.3039 - val_acc: 0.9237

Epoch 00021: val_loss improved from 0.30397 to 0.30390, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 22/40
 - 51s - loss: 0.2427 - acc: 0.9468 - val_loss: 0.3162 - val_acc: 0.9090

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00022: val_loss did not improve from 0.30390
Epoch 23/40
 - 51s - loss: 0.2174 - acc: 0.9600 - val_loss: 0.2652 - val_acc: 0.9376

Epoch 00023: val_loss improved from 0.30390 to 0.26522, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 24/40
 - 51s - loss: 0.2100 - acc: 0.9632 - val_loss: 0.2777 - val_acc: 0.9293

Epoch 00024: val_loss did not improve from 0.26522
Epoch 25/40
 - 51s - loss: 0.2063 - acc: 0.9654 - val_loss: 0.2941 - val_acc: 0.9255

Epoch 00025: val_loss did not improve from 0.26522
Epoch 26/40
 - 51s - loss: 0.2036 - acc: 0.9661 - val_loss: 0.2744 - val_acc: 0.9320

Epoch 00026: val_loss did not improve from 0.26522
Epoch 27/40
 - 51s - loss: 0.1997 - acc: 0.9671 - val_loss: 0.3565 - val_acc: 0.9081

Epoch 00027: val_loss did not improve from 0.26522
Epoch 28/40
 - 51s - loss: 0.1985 - acc: 0.9671 - val_loss: 0.2640 - val_acc: 0.9356

Epoch 00028: val_loss improved from 0.26522 to 0.26399, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 29/40
 - 51s - loss: 0.1950 - acc: 0.9694 - val_loss: 0.2689 - val_acc: 0.9334

Epoch 00029: val_loss did not improve from 0.26399
Epoch 30/40
 - 51s - loss: 0.1927 - acc: 0.9704 - val_loss: 0.2591 - val_acc: 0.9385

Epoch 00030: val_loss improved from 0.26399 to 0.25910, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 31/40
 - 51s - loss: 0.1912 - acc: 0.9700 - val_loss: 0.2949 - val_acc: 0.9290

Epoch 00031: val_loss did not improve from 0.25910
Epoch 32/40
 - 51s - loss: 0.1866 - acc: 0.9722 - val_loss: 0.2616 - val_acc: 0.9405

Epoch 00032: val_loss did not improve from 0.25910
Epoch 33/40
 - 51s - loss: 0.1838 - acc: 0.9737 - val_loss: 0.2588 - val_acc: 0.9394

Epoch 00033: val_loss improved from 0.25910 to 0.25879, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 34/40
 - 51s - loss: 0.1830 - acc: 0.9736 - val_loss: 0.2835 - val_acc: 0.9337

Epoch 00034: val_loss did not improve from 0.25879
Epoch 35/40
 - 51s - loss: 0.1798 - acc: 0.9752 - val_loss: 0.2929 - val_acc: 0.9241

Epoch 00035: val_loss did not improve from 0.25879
Epoch 36/40
 - 51s - loss: 0.1766 - acc: 0.9763 - val_loss: 0.2632 - val_acc: 0.9330

Epoch 00036: val_loss did not improve from 0.25879
Epoch 37/40
 - 51s - loss: 0.1761 - acc: 0.9766 - val_loss: 0.2588 - val_acc: 0.9386

Epoch 00037: val_loss improved from 0.25879 to 0.25878, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 38/40
 - 51s - loss: 0.1744 - acc: 0.9768 - val_loss: 0.2585 - val_acc: 0.9406

Epoch 00038: val_loss improved from 0.25878 to 0.25850, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 39/40
 - 51s - loss: 0.1735 - acc: 0.9778 - val_loss: 0.2580 - val_acc: 0.9388

Epoch 00039: val_loss improved from 0.25850 to 0.25797, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 40/40
 - 51s - loss: 0.1683 - acc: 0.9789 - val_loss: 0.2621 - val_acc: 0.9394

Epoch 00040: val_loss did not improve from 0.25797

  32/7440 [..............................] - ETA: 3s
 128/7440 [..............................] - ETA: 3s
 256/7440 [>.............................] - ETA: 3s
 352/7440 [>.............................] - ETA: 3s
 448/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 640/7440 [=>............................] - ETA: 3s
 736/7440 [=>............................] - ETA: 3s
 864/7440 [==>...........................] - ETA: 3s
 960/7440 [==>...........................] - ETA: 3s
1088/7440 [===>..........................] - ETA: 3s
1216/7440 [===>..........................] - ETA: 3s
1344/7440 [====>.........................] - ETA: 3s
1440/7440 [====>.........................] - ETA: 3s
1536/7440 [=====>........................] - ETA: 3s
1632/7440 [=====>........................] - ETA: 3s
1728/7440 [=====>........................] - ETA: 2s
1856/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2048/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2272/7440 [========>.....................] - ETA: 2s
2400/7440 [========>.....................] - ETA: 2s
2496/7440 [=========>....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2688/7440 [=========>....................] - ETA: 2s
2784/7440 [==========>...................] - ETA: 2s
2880/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3168/7440 [===========>..................] - ETA: 2s
3264/7440 [============>.................] - ETA: 2s
3392/7440 [============>.................] - ETA: 2s
3520/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 2s
3712/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4576/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5056/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5344/7440 [====================>.........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 526us/step
current Test accuracy: 0.8048387096774193
current auc_score ------------------>  0.899249046132501

  32/7440 [..............................] - ETA: 14:28
 128/7440 [..............................] - ETA: 3:37 
 224/7440 [..............................] - ETA: 2:04
 320/7440 [>.............................] - ETA: 1:26
 416/7440 [>.............................] - ETA: 1:06
 512/7440 [=>............................] - ETA: 54s 
 608/7440 [=>............................] - ETA: 45s
 704/7440 [=>............................] - ETA: 39s
 800/7440 [==>...........................] - ETA: 34s
 896/7440 [==>...........................] - ETA: 30s
 992/7440 [===>..........................] - ETA: 27s
1088/7440 [===>..........................] - ETA: 25s
1216/7440 [===>..........................] - ETA: 22s
1312/7440 [====>.........................] - ETA: 20s
1408/7440 [====>.........................] - ETA: 19s
1504/7440 [=====>........................] - ETA: 17s
1600/7440 [=====>........................] - ETA: 16s
1696/7440 [=====>........................] - ETA: 15s
1792/7440 [======>.......................] - ETA: 14s
1920/7440 [======>.......................] - ETA: 13s
2016/7440 [=======>......................] - ETA: 12s
2112/7440 [=======>......................] - ETA: 12s
2208/7440 [=======>......................] - ETA: 11s
2304/7440 [========>.....................] - ETA: 11s
2400/7440 [========>.....................] - ETA: 10s
2496/7440 [=========>....................] - ETA: 10s
2592/7440 [=========>....................] - ETA: 9s 
2688/7440 [=========>....................] - ETA: 9s
2784/7440 [==========>...................] - ETA: 8s
2880/7440 [==========>...................] - ETA: 8s
3008/7440 [===========>..................] - ETA: 7s
3136/7440 [===========>..................] - ETA: 7s
3264/7440 [============>.................] - ETA: 6s
3360/7440 [============>.................] - ETA: 6s
3488/7440 [=============>................] - ETA: 6s
3616/7440 [=============>................] - ETA: 5s
3712/7440 [=============>................] - ETA: 5s
3808/7440 [==============>...............] - ETA: 5s
3936/7440 [==============>...............] - ETA: 5s
4064/7440 [===============>..............] - ETA: 4s
4192/7440 [===============>..............] - ETA: 4s
4320/7440 [================>.............] - ETA: 4s
4448/7440 [================>.............] - ETA: 4s
4576/7440 [=================>............] - ETA: 3s
4704/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 3s
4928/7440 [==================>...........] - ETA: 3s
5056/7440 [===================>..........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 2s
5312/7440 [====================>.........] - ETA: 2s
5440/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 2s
5696/7440 [=====================>........] - ETA: 2s
5824/7440 [======================>.......] - ETA: 1s
5920/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6368/7440 [========================>.....] - ETA: 1s
6464/7440 [=========================>....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.7955645161290322
best saved model auc_score ------------------>  0.898679760665973
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 8, 96, 96)    144         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 8, 96, 96)    32          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 8, 96, 96)    0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   512         activation_169[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_170[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   1536        activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 40, 96, 96)   0           concatenate_73[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   2560        activation_173[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 56, 96, 96)   0           concatenate_74[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 56, 96, 96)   224         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 56, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 28, 96, 96)   1568        activation_175[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 28, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 28, 48, 48)   112         average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 28, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1792        activation_176[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_177[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 44, 48, 48)   0           average_pooling2d_17[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 44, 48, 48)   176         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 44, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2816        activation_178[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_179[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 60, 48, 48)   0           concatenate_76[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 60, 48, 48)   240         concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 60, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   3840        activation_180[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_181[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 76, 48, 48)   0           concatenate_77[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 76, 48, 48)   304         concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 76, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 38, 48, 48)   2888        activation_182[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 38, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 38, 24, 24)   152         average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 38, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2432        activation_183[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_184[0][0]             
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 54, 24, 24)   0           average_pooling2d_18[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 54, 24, 24)   216         concatenate_79[0][0]             
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 54, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3456        activation_185[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_186[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 70, 24, 24)   0           concatenate_79[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 70, 24, 24)   280         concatenate_80[0][0]             
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 70, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4480        activation_187[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_188[0][0]             
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 86, 24, 24)   0           concatenate_80[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 86, 24, 24)   344         concatenate_81[0][0]             
__________________________________________________________________________________________________
activation_189 (Activation)     (None, 86, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 86)           0           activation_189[0][0]             
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            87          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 115,695
Trainable params: 113,375
Non-trainable params: 2,320
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 58s - loss: 0.6212 - acc: 0.7561 - val_loss: 0.5590 - val_acc: 0.7902

Epoch 00001: val_loss improved from inf to 0.55897, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 46s - loss: 0.5412 - acc: 0.7995 - val_loss: 0.5253 - val_acc: 0.8055

Epoch 00002: val_loss improved from 0.55897 to 0.52528, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 46s - loss: 0.5056 - acc: 0.8154 - val_loss: 0.5542 - val_acc: 0.8007

Epoch 00003: val_loss did not improve from 0.52528
Epoch 4/40
 - 46s - loss: 0.4759 - acc: 0.8338 - val_loss: 0.4915 - val_acc: 0.8288

Epoch 00004: val_loss improved from 0.52528 to 0.49152, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 46s - loss: 0.4502 - acc: 0.8495 - val_loss: 0.5464 - val_acc: 0.8020

Epoch 00005: val_loss did not improve from 0.49152
Epoch 6/40
 - 46s - loss: 0.4260 - acc: 0.8618 - val_loss: 0.4263 - val_acc: 0.8632

Epoch 00006: val_loss improved from 0.49152 to 0.42634, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 7/40
 - 46s - loss: 0.4083 - acc: 0.8679 - val_loss: 0.4164 - val_acc: 0.8675

Epoch 00007: val_loss improved from 0.42634 to 0.41640, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 8/40
 - 46s - loss: 0.3943 - acc: 0.8779 - val_loss: 0.3890 - val_acc: 0.8827

Epoch 00008: val_loss improved from 0.41640 to 0.38904, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 9/40
 - 46s - loss: 0.3738 - acc: 0.8884 - val_loss: 0.3802 - val_acc: 0.8844

Epoch 00009: val_loss improved from 0.38904 to 0.38017, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 46s - loss: 0.3594 - acc: 0.8952 - val_loss: 0.3533 - val_acc: 0.8946

Epoch 00010: val_loss improved from 0.38017 to 0.35330, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 11/40
 - 48s - loss: 0.3448 - acc: 0.9006 - val_loss: 0.5443 - val_acc: 0.8134

Epoch 00011: val_loss did not improve from 0.35330
Epoch 12/40
 - 46s - loss: 0.3336 - acc: 0.9081 - val_loss: 0.3475 - val_acc: 0.8965

Epoch 00012: val_loss improved from 0.35330 to 0.34747, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 13/40
 - 46s - loss: 0.3246 - acc: 0.9107 - val_loss: 0.3219 - val_acc: 0.9106

Epoch 00013: val_loss improved from 0.34747 to 0.32191, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 14/40
 - 46s - loss: 0.3148 - acc: 0.9143 - val_loss: 0.3206 - val_acc: 0.9104

Epoch 00014: val_loss improved from 0.32191 to 0.32060, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 15/40
 - 46s - loss: 0.3016 - acc: 0.9215 - val_loss: 0.3087 - val_acc: 0.9217

Epoch 00015: val_loss improved from 0.32060 to 0.30867, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 16/40
 - 46s - loss: 0.2958 - acc: 0.9234 - val_loss: 0.3453 - val_acc: 0.9000

Epoch 00016: val_loss did not improve from 0.30867
Epoch 17/40
 - 46s - loss: 0.2844 - acc: 0.9286 - val_loss: 0.2937 - val_acc: 0.9223

Epoch 00017: val_loss improved from 0.30867 to 0.29368, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 18/40
 - 46s - loss: 0.2774 - acc: 0.9311 - val_loss: 0.3128 - val_acc: 0.9149

Epoch 00018: val_loss did not improve from 0.29368
Epoch 19/40
 - 46s - loss: 0.2703 - acc: 0.9337 - val_loss: 0.2980 - val_acc: 0.9219

Epoch 00019: val_loss did not improve from 0.29368
Epoch 20/40
 - 46s - loss: 0.2591 - acc: 0.9390 - val_loss: 0.3331 - val_acc: 0.8960

Epoch 00020: val_loss did not improve from 0.29368
Epoch 21/40
 - 46s - loss: 0.2556 - acc: 0.9386 - val_loss: 0.3130 - val_acc: 0.9120

Epoch 00021: val_loss did not improve from 0.29368
Epoch 22/40
 - 46s - loss: 0.2491 - acc: 0.9434 - val_loss: 0.2928 - val_acc: 0.9217

Epoch 00022: val_loss improved from 0.29368 to 0.29276, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 23/40
 - 46s - loss: 0.2417 - acc: 0.9462 - val_loss: 0.2890 - val_acc: 0.9277

Epoch 00023: val_loss improved from 0.29276 to 0.28903, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 24/40
 - 46s - loss: 0.2347 - acc: 0.9485 - val_loss: 0.2848 - val_acc: 0.9219

Epoch 00024: val_loss improved from 0.28903 to 0.28475, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 25/40
 - 46s - loss: 0.2319 - acc: 0.9496 - val_loss: 0.2720 - val_acc: 0.9335

Epoch 00025: val_loss improved from 0.28475 to 0.27202, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 26/40
 - 46s - loss: 0.2249 - acc: 0.9523 - val_loss: 0.3005 - val_acc: 0.9198

Epoch 00026: val_loss did not improve from 0.27202
Epoch 27/40
 - 46s - loss: 0.2192 - acc: 0.9554 - val_loss: 0.3193 - val_acc: 0.9124

Epoch 00027: val_loss did not improve from 0.27202
Epoch 28/40
 - 46s - loss: 0.2146 - acc: 0.9567 - val_loss: 0.2971 - val_acc: 0.9185

Epoch 00028: val_loss did not improve from 0.27202
Epoch 29/40
 - 46s - loss: 0.2119 - acc: 0.9559 - val_loss: 0.3336 - val_acc: 0.9037

Epoch 00029: val_loss did not improve from 0.27202
Epoch 30/40
 - 46s - loss: 0.2055 - acc: 0.9601 - val_loss: 0.2608 - val_acc: 0.9354

Epoch 00030: val_loss improved from 0.27202 to 0.26076, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 31/40
 - 46s - loss: 0.2013 - acc: 0.9604 - val_loss: 0.5102 - val_acc: 0.8333

Epoch 00031: val_loss did not improve from 0.26076
Epoch 32/40
 - 46s - loss: 0.1972 - acc: 0.9623 - val_loss: 0.2758 - val_acc: 0.9310

Epoch 00032: val_loss did not improve from 0.26076
Epoch 33/40
 - 46s - loss: 0.1957 - acc: 0.9623 - val_loss: 0.2683 - val_acc: 0.9356

Epoch 00033: val_loss did not improve from 0.26076
Epoch 34/40
 - 46s - loss: 0.1910 - acc: 0.9642 - val_loss: 0.2812 - val_acc: 0.9346

Epoch 00034: val_loss did not improve from 0.26076
Epoch 35/40
 - 46s - loss: 0.1871 - acc: 0.9662 - val_loss: 0.3209 - val_acc: 0.9163

Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00035: val_loss did not improve from 0.26076
Epoch 00035: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 475us/step
current Test accuracy: 0.7426075268817204
current auc_score ------------------>  0.8261815672332062

  32/7440 [..............................] - ETA: 16:56
 128/7440 [..............................] - ETA: 4:13 
 256/7440 [>.............................] - ETA: 2:06
 384/7440 [>.............................] - ETA: 1:23
 512/7440 [=>............................] - ETA: 1:02
 640/7440 [=>............................] - ETA: 49s 
 768/7440 [==>...........................] - ETA: 41s
 896/7440 [==>...........................] - ETA: 35s
1024/7440 [===>..........................] - ETA: 30s
1152/7440 [===>..........................] - ETA: 26s
1280/7440 [====>.........................] - ETA: 24s
1408/7440 [====>.........................] - ETA: 21s
1536/7440 [=====>........................] - ETA: 19s
1664/7440 [=====>........................] - ETA: 17s
1792/7440 [======>.......................] - ETA: 16s
1920/7440 [======>.......................] - ETA: 15s
2048/7440 [=======>......................] - ETA: 14s
2176/7440 [=======>......................] - ETA: 13s
2304/7440 [========>.....................] - ETA: 12s
2432/7440 [========>.....................] - ETA: 11s
2560/7440 [=========>....................] - ETA: 10s
2688/7440 [=========>....................] - ETA: 10s
2816/7440 [==========>...................] - ETA: 9s 
2944/7440 [==========>...................] - ETA: 8s
3072/7440 [===========>..................] - ETA: 8s
3200/7440 [===========>..................] - ETA: 7s
3328/7440 [============>.................] - ETA: 7s
3456/7440 [============>.................] - ETA: 6s
3584/7440 [=============>................] - ETA: 6s
3712/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
3968/7440 [===============>..............] - ETA: 5s
4096/7440 [===============>..............] - ETA: 5s
4224/7440 [================>.............] - ETA: 4s
4352/7440 [================>.............] - ETA: 4s
4480/7440 [=================>............] - ETA: 4s
4608/7440 [=================>............] - ETA: 4s
4736/7440 [==================>...........] - ETA: 3s
4864/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 3s
5248/7440 [====================>.........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5504/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5888/7440 [======================>.......] - ETA: 1s
6016/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.7940860215053763
best saved model auc_score ------------------>  0.8944055671175859
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_190 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_190[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_191 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_191[0][0]             
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_82[0][0]             
__________________________________________________________________________________________________
activation_192 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_192[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_193[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 48, 96, 96)   0           concatenate_82[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_83[0][0]             
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_194[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_195[0][0]             
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 64, 96, 96)   0           concatenate_83[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_84[0][0]             
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_196[0][0]             
__________________________________________________________________________________________________
average_pooling2d_19 (AveragePo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_19[0][0]       
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_197[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_198[0][0]             
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 48, 48, 48)   0           average_pooling2d_19[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_85[0][0]             
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_199[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_200[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 64, 48, 48)   0           concatenate_85[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_86[0][0]             
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_201[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_202 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_202[0][0]             
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 80, 48, 48)   0           concatenate_86[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 80, 48, 48)   320         concatenate_87[0][0]             
__________________________________________________________________________________________________
activation_203 (Activation)     (None, 80, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 40, 48, 48)   3200        activation_203[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 40, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 40, 24, 24)   160         average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_204 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2560        activation_204[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_205 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_205[0][0]             
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 56, 24, 24)   0           average_pooling2d_20[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 56, 24, 24)   224         concatenate_88[0][0]             
__________________________________________________________________________________________________
activation_206 (Activation)     (None, 56, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3584        activation_206[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_207 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_207[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 72, 24, 24)   0           concatenate_88[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 72, 24, 24)   288         concatenate_89[0][0]             
__________________________________________________________________________________________________
activation_208 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4608        activation_208[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_209 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_209[0][0]             
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 88, 24, 24)   0           concatenate_89[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 88, 24, 24)   352         concatenate_90[0][0]             
__________________________________________________________________________________________________
activation_210 (Activation)     (None, 88, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 88)           0           activation_210[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            89          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 119,545
Trainable params: 117,113
Non-trainable params: 2,432
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 62s - loss: 0.6233 - acc: 0.7624 - val_loss: 0.5805 - val_acc: 0.7874

Epoch 00001: val_loss improved from inf to 0.58051, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 2/40
 - 48s - loss: 0.5369 - acc: 0.8067 - val_loss: 0.5078 - val_acc: 0.8175

Epoch 00002: val_loss improved from 0.58051 to 0.50777, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 3/40
 - 48s - loss: 0.4983 - acc: 0.8258 - val_loss: 0.5045 - val_acc: 0.8220

Epoch 00003: val_loss improved from 0.50777 to 0.50453, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 4/40
 - 48s - loss: 0.4655 - acc: 0.8441 - val_loss: 0.4633 - val_acc: 0.8421

Epoch 00004: val_loss improved from 0.50453 to 0.46329, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 5/40
 - 48s - loss: 0.4444 - acc: 0.8524 - val_loss: 0.4303 - val_acc: 0.8635

Epoch 00005: val_loss improved from 0.46329 to 0.43025, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 6/40
 - 48s - loss: 0.4238 - acc: 0.8641 - val_loss: 0.4653 - val_acc: 0.8483

Epoch 00006: val_loss did not improve from 0.43025
Epoch 7/40
 - 48s - loss: 0.4071 - acc: 0.8723 - val_loss: 0.4750 - val_acc: 0.8302

Epoch 00007: val_loss did not improve from 0.43025
Epoch 8/40
 - 48s - loss: 0.3926 - acc: 0.8819 - val_loss: 0.3815 - val_acc: 0.8825

Epoch 00008: val_loss improved from 0.43025 to 0.38154, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 9/40
 - 48s - loss: 0.3780 - acc: 0.8881 - val_loss: 0.3764 - val_acc: 0.8818

Epoch 00009: val_loss improved from 0.38154 to 0.37641, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 10/40
 - 48s - loss: 0.3609 - acc: 0.8949 - val_loss: 0.3518 - val_acc: 0.9036

Epoch 00010: val_loss improved from 0.37641 to 0.35180, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 11/40
 - 48s - loss: 0.3503 - acc: 0.9011 - val_loss: 0.3821 - val_acc: 0.8839

Epoch 00011: val_loss did not improve from 0.35180
Epoch 12/40
 - 48s - loss: 0.3397 - acc: 0.9047 - val_loss: 0.3381 - val_acc: 0.9051

Epoch 00012: val_loss improved from 0.35180 to 0.33812, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 13/40
 - 48s - loss: 0.3252 - acc: 0.9121 - val_loss: 0.3617 - val_acc: 0.8932

Epoch 00013: val_loss did not improve from 0.33812
Epoch 14/40
 - 48s - loss: 0.3157 - acc: 0.9182 - val_loss: 0.3611 - val_acc: 0.9006

Epoch 00014: val_loss did not improve from 0.33812
Epoch 15/40
 - 48s - loss: 0.3064 - acc: 0.9211 - val_loss: 0.3202 - val_acc: 0.9099

Epoch 00015: val_loss improved from 0.33812 to 0.32018, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 16/40
 - 48s - loss: 0.2991 - acc: 0.9227 - val_loss: 0.3168 - val_acc: 0.9133

Epoch 00016: val_loss improved from 0.32018 to 0.31682, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 17/40
 - 48s - loss: 0.2863 - acc: 0.9303 - val_loss: 0.3443 - val_acc: 0.9025

Epoch 00017: val_loss did not improve from 0.31682
Epoch 18/40
 - 48s - loss: 0.2812 - acc: 0.9318 - val_loss: 0.3040 - val_acc: 0.9185

Epoch 00018: val_loss improved from 0.31682 to 0.30400, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 19/40
 - 48s - loss: 0.2729 - acc: 0.9351 - val_loss: 0.3233 - val_acc: 0.9116

Epoch 00019: val_loss did not improve from 0.30400
Epoch 20/40
 - 48s - loss: 0.2689 - acc: 0.9365 - val_loss: 0.3964 - val_acc: 0.8843

Epoch 00020: val_loss did not improve from 0.30400
Epoch 21/40
 - 48s - loss: 0.2581 - acc: 0.9406 - val_loss: 0.3280 - val_acc: 0.9068

Epoch 00021: val_loss did not improve from 0.30400
Epoch 22/40
 - 48s - loss: 0.2529 - acc: 0.9443 - val_loss: 0.3656 - val_acc: 0.8963

Epoch 00022: val_loss did not improve from 0.30400
Epoch 23/40
 - 48s - loss: 0.2450 - acc: 0.9466 - val_loss: 0.2803 - val_acc: 0.9288

Epoch 00023: val_loss improved from 0.30400 to 0.28026, saving model to keras_densenet_simple_wt_29Sept_2040.h5
Epoch 24/40
 - 48s - loss: 0.2390 - acc: 0.9487 - val_loss: 0.3801 - val_acc: 0.8902

Epoch 00024: val_loss did not improve from 0.28026
Epoch 25/40
 - 48s - loss: 0.2343 - acc: 0.9522 - val_loss: 0.3170 - val_acc: 0.9125

Epoch 00025: val_loss did not improve from 0.28026
Epoch 26/40
 - 48s - loss: 0.2332 - acc: 0.9506 - val_loss: 0.3185 - val_acc: 0.9120

Epoch 00026: val_loss did not improve from 0.28026
Epoch 27/40
 - 48s - loss: 0.2224 - acc: 0.9543 - val_loss: 0.3048 - val_acc: 0.9213

Epoch 00027: val_loss did not improve from 0.28026
Epoch 28/40
 - 48s - loss: 0.2212 - acc: 0.9547 - val_loss: 0.2913 - val_acc: 0.9258

Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00028: val_loss did not improve from 0.28026
Epoch 00028: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 3s
1440/7440 [====>.........................] - ETA: 3s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 498us/step
current Test accuracy: 0.7995967741935484
current auc_score ------------------>  0.8970846340617411

  32/7440 [..............................] - ETA: 19:36
 128/7440 [..............................] - ETA: 4:53 
 256/7440 [>.............................] - ETA: 2:25
 384/7440 [>.............................] - ETA: 1:36
 512/7440 [=>............................] - ETA: 1:12
 640/7440 [=>............................] - ETA: 57s 
 768/7440 [==>...........................] - ETA: 47s
 896/7440 [==>...........................] - ETA: 40s
1024/7440 [===>..........................] - ETA: 34s
1152/7440 [===>..........................] - ETA: 30s
1280/7440 [====>.........................] - ETA: 27s
1408/7440 [====>.........................] - ETA: 24s
1536/7440 [=====>........................] - ETA: 22s
1664/7440 [=====>........................] - ETA: 20s
1792/7440 [======>.......................] - ETA: 18s
1920/7440 [======>.......................] - ETA: 17s
2048/7440 [=======>......................] - ETA: 16s
2176/7440 [=======>......................] - ETA: 14s
2304/7440 [========>.....................] - ETA: 13s
2432/7440 [========>.....................] - ETA: 12s
2560/7440 [=========>....................] - ETA: 12s
2688/7440 [=========>....................] - ETA: 11s
2816/7440 [==========>...................] - ETA: 10s
2944/7440 [==========>...................] - ETA: 9s 
3072/7440 [===========>..................] - ETA: 9s
3200/7440 [===========>..................] - ETA: 8s
3328/7440 [============>.................] - ETA: 8s
3456/7440 [============>.................] - ETA: 7s
3584/7440 [=============>................] - ETA: 7s
3712/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 6s
3968/7440 [===============>..............] - ETA: 6s
4096/7440 [===============>..............] - ETA: 5s
4224/7440 [================>.............] - ETA: 5s
4352/7440 [================>.............] - ETA: 5s
4480/7440 [=================>............] - ETA: 4s
4608/7440 [=================>............] - ETA: 4s
4736/7440 [==================>...........] - ETA: 4s
4864/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 3s
5248/7440 [====================>.........] - ETA: 3s
5376/7440 [====================>.........] - ETA: 2s
5504/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5888/7440 [======================>.......] - ETA: 2s
6016/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 9s 1ms/step
Best saved model Test accuracy: 0.821505376344086
best saved model auc_score ------------------>  0.9072122499710951
best model <keras.engine.training.Model object at 0x7f7ba40a1a90>
best run {'depth': 6, 'growth_rate': 2, 'nb_filter': 2}
Evalutation of best performing model:
[0.5329862432454222, 0.8329301075268817]
val roc_auc_score 0.913
----------trials-------------
{'depth': [0], 'growth_rate': [1], 'nb_filter': [2]} -0.90915824806336
{'depth': [4], 'growth_rate': [1], 'nb_filter': [0]} -0.9081203029251936
{'depth': [6], 'growth_rate': [2], 'nb_filter': [1]} -0.9054346239449647
{'depth': [5], 'growth_rate': [5], 'nb_filter': [1]} -0.8656969013758816
{'depth': [6], 'growth_rate': [3], 'nb_filter': [0]} -0.9051229188345473
{'depth': [6], 'growth_rate': [2], 'nb_filter': [2]} -0.9128310353798127
{'depth': [6], 'growth_rate': [0], 'nb_filter': [0]} -0.895890926696728
{'depth': [3], 'growth_rate': [5], 'nb_filter': [2]} -0.898679760665973
{'depth': [6], 'growth_rate': [2], 'nb_filter': [0]} -0.8944055671175859
{'depth': [3], 'growth_rate': [0], 'nb_filter': [1]} -0.9072122499710951
python evaluate_saved_model_simple.py
