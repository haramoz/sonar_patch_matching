python hello-world.py
python hyperas_simple.py
python hyperas_contrastive_loss.py
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
python keras_densenet_simple.py
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from sklearn.model_selection import train_test_split
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import pickle
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'depth': hp.choice('depth', [7,13,19,25,31]),
        'nb_dense_block': hp.choice('nb_dense_block', [2,3]),
        'growth_rate': hp.choice('growth_rate', [6,10,14,18]),
    }

>>> Functions
  1: def process_data():
  2:     random_seed = 7
  3: 
  4:     f = h5py.File('matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  5:     X_train = f['X_train'].value
  6:     y_train = f['y_train'].value
  7:     X_test = f['X_val'].value
  8:     y_test = f['y_val'].value
  9:     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed)
 10:  
 11:     return X_train,y_train,X_val,y_val,X_test,y_test
 12: 
 13: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val,X_test,y_test = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 40
   4:     es_patience = 7
   5:     lr_patience = 5
   6:     dropout = None
   7:     depth = space['depth']
   8:     nb_dense_block = space['nb_dense_block']
   9:     nb_filter = 16
  10:     growth_rate = space['growth_rate']
  11:     bn = True
  12:     reduction_ = 0.5
  13:     bs = 32
  14:     lr = 1E-4 #########################################################CHange file name##########################################
  15:     weight_file = 'keras_densenet_simple_wt_29Sept_2200.h5'
  16:     nb_classes = 1
  17:     img_dim = (2,96,96) 
  18:     n_channels = 2 
  19: 
  20:     
  21:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  22:                  growth_rate=growth_rate, nb_filter=nb_filter,
  23:                  dropout_rate=dropout,activation='sigmoid',
  24:                  input_shape=img_dim,include_top=True,
  25:                  bottleneck=bn,reduction=reduction_,
  26:                  classes=nb_classes,pooling='avg',
  27:                  weights=None)
  28:     
  29: 
  30:     model.summary()
  31:     opt = Adam(lr=lr)
  32:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  33: 
  34:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  35:     #es = EarlyStopping(monitor='val_acc', patience=es_patience,verbose=1,restore_best_weights=True)
  36:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  37: 
  38:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  39: 
  40:     model.fit(X_train,y_train,
  41:           batch_size=bs,
  42:           epochs=epochs,
  43:           callbacks=[es,lr_reducer,checkpointer],
  44:           validation_data=(X_val,y_val),
  45:           verbose=2)
  46:     
  47:     score, acc = model.evaluate(X_test, y_test)
  48:     print('current Test accuracy:', acc)
  49:     pred = model.predict(X_test)
  50:     auc_score = roc_auc_score(y_test,pred)
  51:     print("current auc_score ------------------> ",auc_score)
  52: 
  53:     model = load_model(weight_file) #This is the best model
  54:     score, acc = model.evaluate(X_test, y_test)
  55:     print('Best saved model Test accuracy:', acc)
  56:     pred = model.predict(X_test)
  57:     auc_score = roc_auc_score(y_test,pred)
  58:     print("best saved model auc_score ------------------> ",auc_score)
  59: 
  60:     
  61:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  62: 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_1 (Activation)    (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_1 (Average (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_2 (Activation)    (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_1 ( (None, 8)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 9s - loss: 0.6755 - acc: 0.6102 - val_loss: 0.6458 - val_acc: 0.6844

Epoch 00001: val_loss improved from inf to 0.64585, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 6s - loss: 0.6396 - acc: 0.6844 - val_loss: 0.6326 - val_acc: 0.6867

Epoch 00002: val_loss improved from 0.64585 to 0.63259, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 5s - loss: 0.6307 - acc: 0.6825 - val_loss: 0.6251 - val_acc: 0.6904

Epoch 00003: val_loss improved from 0.63259 to 0.62507, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 6s - loss: 0.6237 - acc: 0.6832 - val_loss: 0.6195 - val_acc: 0.6935

Epoch 00004: val_loss improved from 0.62507 to 0.61951, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 6s - loss: 0.6179 - acc: 0.6869 - val_loss: 0.6150 - val_acc: 0.6948

Epoch 00005: val_loss improved from 0.61951 to 0.61500, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 6s - loss: 0.6129 - acc: 0.6914 - val_loss: 0.6098 - val_acc: 0.6993

Epoch 00006: val_loss improved from 0.61500 to 0.60983, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 6s - loss: 0.6085 - acc: 0.6969 - val_loss: 0.6061 - val_acc: 0.7038

Epoch 00007: val_loss improved from 0.60983 to 0.60607, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 6s - loss: 0.6042 - acc: 0.7008 - val_loss: 0.6022 - val_acc: 0.7065

Epoch 00008: val_loss improved from 0.60607 to 0.60218, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 6s - loss: 0.6007 - acc: 0.7042 - val_loss: 0.5985 - val_acc: 0.7096

Epoch 00009: val_loss improved from 0.60218 to 0.59854, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 6s - loss: 0.5970 - acc: 0.7071 - val_loss: 0.5950 - val_acc: 0.7118

Epoch 00010: val_loss improved from 0.59854 to 0.59496, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 6s - loss: 0.5938 - acc: 0.7097 - val_loss: 0.5909 - val_acc: 0.7161

Epoch 00011: val_loss improved from 0.59496 to 0.59094, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 6s - loss: 0.5895 - acc: 0.7147 - val_loss: 0.5890 - val_acc: 0.7134

Epoch 00012: val_loss improved from 0.59094 to 0.58904, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 5s - loss: 0.5845 - acc: 0.7163 - val_loss: 0.5843 - val_acc: 0.7213

Epoch 00013: val_loss improved from 0.58904 to 0.58430, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 5s - loss: 0.5823 - acc: 0.7163 - val_loss: 0.5843 - val_acc: 0.7179

Epoch 00014: val_loss improved from 0.58430 to 0.58426, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 5s - loss: 0.5781 - acc: 0.7203 - val_loss: 0.5871 - val_acc: 0.7037

Epoch 00015: val_loss did not improve from 0.58426
Epoch 16/40
 - 5s - loss: 0.5751 - acc: 0.7208 - val_loss: 0.5728 - val_acc: 0.7280

Epoch 00016: val_loss improved from 0.58426 to 0.57278, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 6s - loss: 0.5716 - acc: 0.7229 - val_loss: 0.5804 - val_acc: 0.7149

Epoch 00017: val_loss did not improve from 0.57278
Epoch 18/40
 - 6s - loss: 0.5678 - acc: 0.7248 - val_loss: 0.5649 - val_acc: 0.7292

Epoch 00018: val_loss improved from 0.57278 to 0.56489, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 5s - loss: 0.5637 - acc: 0.7269 - val_loss: 0.5631 - val_acc: 0.7298

Epoch 00019: val_loss improved from 0.56489 to 0.56309, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 6s - loss: 0.5617 - acc: 0.7282 - val_loss: 0.5596 - val_acc: 0.7278

Epoch 00020: val_loss improved from 0.56309 to 0.55963, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 5s - loss: 0.5585 - acc: 0.7310 - val_loss: 0.5721 - val_acc: 0.7150

Epoch 00021: val_loss did not improve from 0.55963
Epoch 22/40
 - 5s - loss: 0.5565 - acc: 0.7312 - val_loss: 0.5651 - val_acc: 0.7239

Epoch 00022: val_loss did not improve from 0.55963
Epoch 23/40
 - 5s - loss: 0.5545 - acc: 0.7329 - val_loss: 0.5535 - val_acc: 0.7347

Epoch 00023: val_loss improved from 0.55963 to 0.55350, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 5s - loss: 0.5504 - acc: 0.7342 - val_loss: 0.5514 - val_acc: 0.7356

Epoch 00024: val_loss improved from 0.55350 to 0.55136, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 5s - loss: 0.5487 - acc: 0.7377 - val_loss: 0.5485 - val_acc: 0.7310

Epoch 00025: val_loss improved from 0.55136 to 0.54851, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 6s - loss: 0.5489 - acc: 0.7355 - val_loss: 0.5515 - val_acc: 0.7326

Epoch 00026: val_loss did not improve from 0.54851
Epoch 27/40
 - 6s - loss: 0.5474 - acc: 0.7376 - val_loss: 0.5472 - val_acc: 0.7374

Epoch 00027: val_loss improved from 0.54851 to 0.54717, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 6s - loss: 0.5449 - acc: 0.7381 - val_loss: 0.5449 - val_acc: 0.7380

Epoch 00028: val_loss improved from 0.54717 to 0.54494, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 6s - loss: 0.5452 - acc: 0.7375 - val_loss: 0.5443 - val_acc: 0.7323

Epoch 00029: val_loss improved from 0.54494 to 0.54429, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 30/40
 - 6s - loss: 0.5412 - acc: 0.7408 - val_loss: 0.5579 - val_acc: 0.7250

Epoch 00030: val_loss did not improve from 0.54429
Epoch 31/40
 - 6s - loss: 0.5409 - acc: 0.7420 - val_loss: 0.5585 - val_acc: 0.7229

Epoch 00031: val_loss did not improve from 0.54429
Epoch 32/40
 - 6s - loss: 0.5410 - acc: 0.7407 - val_loss: 0.5545 - val_acc: 0.7252

Epoch 00032: val_loss did not improve from 0.54429
Epoch 33/40
 - 6s - loss: 0.5377 - acc: 0.7439 - val_loss: 0.5385 - val_acc: 0.7407

Epoch 00033: val_loss improved from 0.54429 to 0.53848, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 5s - loss: 0.5363 - acc: 0.7435 - val_loss: 0.5606 - val_acc: 0.7194

Epoch 00034: val_loss did not improve from 0.53848
Epoch 35/40
 - 5s - loss: 0.5360 - acc: 0.7464 - val_loss: 0.5374 - val_acc: 0.7430

Epoch 00035: val_loss improved from 0.53848 to 0.53736, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 6s - loss: 0.5361 - acc: 0.7441 - val_loss: 0.5336 - val_acc: 0.7470

Epoch 00036: val_loss improved from 0.53736 to 0.53360, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 5s - loss: 0.5338 - acc: 0.7463 - val_loss: 0.5331 - val_acc: 0.7477

Epoch 00037: val_loss improved from 0.53360 to 0.53308, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 38/40
 - 6s - loss: 0.5338 - acc: 0.7470 - val_loss: 0.5378 - val_acc: 0.7391

Epoch 00038: val_loss did not improve from 0.53308
Epoch 39/40
 - 6s - loss: 0.5327 - acc: 0.7476 - val_loss: 0.5442 - val_acc: 0.7383

Epoch 00039: val_loss did not improve from 0.53308
Epoch 40/40
 - 5s - loss: 0.5325 - acc: 0.7474 - val_loss: 0.5301 - val_acc: 0.7439

Epoch 00040: val_loss improved from 0.53308 to 0.53012, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 0s
 800/7440 [==>...........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
2304/7440 [========>.....................] - ETA: 0s
3040/7440 [===========>..................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 70us/step
current Test accuracy: 0.8165322580645161
current auc_score ------------------>  0.8631193273788879

  32/7440 [..............................] - ETA: 15s
 640/7440 [=>............................] - ETA: 1s 
1280/7440 [====>.........................] - ETA: 0s
1920/7440 [======>.......................] - ETA: 0s
2560/7440 [=========>....................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 90us/step
Best saved model Test accuracy: 0.8165322580645161
best saved model auc_score ------------------>  0.8631193273788879
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_3[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 30, 96, 96)   120         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 30, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 15, 96, 96)   450         activation_5[0][0]               
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 15, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 15, 48, 48)   60          average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 15, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   840         activation_6[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 29, 48, 48)   0           average_pooling2d_2[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 29, 48, 48)   116         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 29, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 29)           0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            30          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 17,424
Trainable params: 17,020
Non-trainable params: 404
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 16s - loss: 0.6177 - acc: 0.7039 - val_loss: 0.5706 - val_acc: 0.7511

Epoch 00001: val_loss improved from inf to 0.57057, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 15s - loss: 0.5549 - acc: 0.7581 - val_loss: 0.5436 - val_acc: 0.7572

Epoch 00002: val_loss improved from 0.57057 to 0.54361, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 15s - loss: 0.5309 - acc: 0.7656 - val_loss: 0.5213 - val_acc: 0.7647

Epoch 00003: val_loss improved from 0.54361 to 0.52134, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 15s - loss: 0.5173 - acc: 0.7701 - val_loss: 0.5122 - val_acc: 0.7708

Epoch 00004: val_loss improved from 0.52134 to 0.51222, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 15s - loss: 0.5054 - acc: 0.7720 - val_loss: 0.4970 - val_acc: 0.7765

Epoch 00005: val_loss improved from 0.51222 to 0.49702, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 15s - loss: 0.4975 - acc: 0.7751 - val_loss: 0.5036 - val_acc: 0.7701

Epoch 00006: val_loss did not improve from 0.49702
Epoch 7/40
 - 15s - loss: 0.4897 - acc: 0.7786 - val_loss: 0.4906 - val_acc: 0.7756

Epoch 00007: val_loss improved from 0.49702 to 0.49061, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 15s - loss: 0.4833 - acc: 0.7810 - val_loss: 0.4841 - val_acc: 0.7806

Epoch 00008: val_loss improved from 0.49061 to 0.48413, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 15s - loss: 0.4783 - acc: 0.7850 - val_loss: 0.4729 - val_acc: 0.7838

Epoch 00009: val_loss improved from 0.48413 to 0.47294, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 15s - loss: 0.4762 - acc: 0.7827 - val_loss: 0.4746 - val_acc: 0.7762

Epoch 00010: val_loss did not improve from 0.47294
Epoch 11/40
 - 15s - loss: 0.4701 - acc: 0.7870 - val_loss: 0.4955 - val_acc: 0.7759

Epoch 00011: val_loss did not improve from 0.47294
Epoch 12/40
 - 15s - loss: 0.4669 - acc: 0.7862 - val_loss: 0.4675 - val_acc: 0.7812

Epoch 00012: val_loss improved from 0.47294 to 0.46745, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 15s - loss: 0.4633 - acc: 0.7888 - val_loss: 0.4612 - val_acc: 0.7829

Epoch 00013: val_loss improved from 0.46745 to 0.46121, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 15s - loss: 0.4602 - acc: 0.7927 - val_loss: 0.4565 - val_acc: 0.7942

Epoch 00014: val_loss improved from 0.46121 to 0.45649, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 15s - loss: 0.4593 - acc: 0.7935 - val_loss: 0.4543 - val_acc: 0.7907

Epoch 00015: val_loss improved from 0.45649 to 0.45428, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 15s - loss: 0.4558 - acc: 0.7940 - val_loss: 0.4461 - val_acc: 0.7982

Epoch 00016: val_loss improved from 0.45428 to 0.44613, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 15s - loss: 0.4524 - acc: 0.7966 - val_loss: 0.4479 - val_acc: 0.7978

Epoch 00017: val_loss did not improve from 0.44613
Epoch 18/40
 - 15s - loss: 0.4491 - acc: 0.7989 - val_loss: 0.4508 - val_acc: 0.7982

Epoch 00018: val_loss did not improve from 0.44613
Epoch 19/40
 - 15s - loss: 0.4470 - acc: 0.7991 - val_loss: 0.4411 - val_acc: 0.7998

Epoch 00019: val_loss improved from 0.44613 to 0.44108, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 15s - loss: 0.4460 - acc: 0.8019 - val_loss: 0.4631 - val_acc: 0.7941

Epoch 00020: val_loss did not improve from 0.44108
Epoch 21/40
 - 15s - loss: 0.4423 - acc: 0.8004 - val_loss: 0.4655 - val_acc: 0.7840

Epoch 00021: val_loss did not improve from 0.44108
Epoch 22/40
 - 15s - loss: 0.4402 - acc: 0.8018 - val_loss: 0.4725 - val_acc: 0.7745

Epoch 00022: val_loss did not improve from 0.44108
Epoch 23/40
 - 15s - loss: 0.4394 - acc: 0.8043 - val_loss: 0.4377 - val_acc: 0.8005

Epoch 00023: val_loss improved from 0.44108 to 0.43769, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 15s - loss: 0.4374 - acc: 0.8054 - val_loss: 0.4352 - val_acc: 0.8065

Epoch 00024: val_loss improved from 0.43769 to 0.43516, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 15s - loss: 0.4334 - acc: 0.8049 - val_loss: 0.4337 - val_acc: 0.8087

Epoch 00025: val_loss improved from 0.43516 to 0.43368, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 15s - loss: 0.4315 - acc: 0.8055 - val_loss: 0.4333 - val_acc: 0.8012

Epoch 00026: val_loss improved from 0.43368 to 0.43333, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 15s - loss: 0.4268 - acc: 0.8117 - val_loss: 0.4777 - val_acc: 0.7731

Epoch 00027: val_loss did not improve from 0.43333
Epoch 28/40
 - 15s - loss: 0.4272 - acc: 0.8089 - val_loss: 0.4212 - val_acc: 0.8086

Epoch 00028: val_loss improved from 0.43333 to 0.42116, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 15s - loss: 0.4265 - acc: 0.8092 - val_loss: 0.4378 - val_acc: 0.7949

Epoch 00029: val_loss did not improve from 0.42116
Epoch 30/40
 - 15s - loss: 0.4224 - acc: 0.8134 - val_loss: 0.4193 - val_acc: 0.8128

Epoch 00030: val_loss improved from 0.42116 to 0.41926, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 15s - loss: 0.4213 - acc: 0.8132 - val_loss: 0.4679 - val_acc: 0.8022

Epoch 00031: val_loss did not improve from 0.41926
Epoch 32/40
 - 15s - loss: 0.4211 - acc: 0.8140 - val_loss: 0.4124 - val_acc: 0.8164

Epoch 00032: val_loss improved from 0.41926 to 0.41242, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 15s - loss: 0.4164 - acc: 0.8153 - val_loss: 0.4489 - val_acc: 0.7888

Epoch 00033: val_loss did not improve from 0.41242
Epoch 34/40
 - 15s - loss: 0.4170 - acc: 0.8168 - val_loss: 0.4349 - val_acc: 0.8042

Epoch 00034: val_loss did not improve from 0.41242
Epoch 35/40
 - 15s - loss: 0.4167 - acc: 0.8152 - val_loss: 0.4205 - val_acc: 0.8135

Epoch 00035: val_loss did not improve from 0.41242
Epoch 36/40
 - 15s - loss: 0.4122 - acc: 0.8160 - val_loss: 0.4807 - val_acc: 0.7652

Epoch 00036: val_loss did not improve from 0.41242
Epoch 37/40
 - 15s - loss: 0.4124 - acc: 0.8180 - val_loss: 0.4252 - val_acc: 0.8171

Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00037: val_loss did not improve from 0.41242
Epoch 38/40
 - 15s - loss: 0.4032 - acc: 0.8231 - val_loss: 0.4110 - val_acc: 0.8139

Epoch 00038: val_loss improved from 0.41242 to 0.41101, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 15s - loss: 0.4019 - acc: 0.8255 - val_loss: 0.4014 - val_acc: 0.8247

Epoch 00039: val_loss improved from 0.41101 to 0.40140, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 15s - loss: 0.4025 - acc: 0.8231 - val_loss: 0.4072 - val_acc: 0.8214

Epoch 00040: val_loss did not improve from 0.40140

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 167us/step
current Test accuracy: 0.8040322580645162
current auc_score ------------------>  0.8821385058966356

  32/7440 [..............................] - ETA: 44s
 352/7440 [>.............................] - ETA: 4s 
 672/7440 [=>............................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 188us/step
Best saved model Test accuracy: 0.8306451612903226
best saved model auc_score ------------------>  0.898872954965892
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_9[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 56, 96, 96)   1680        activation_11[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 56, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_12[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 44, 96, 96)   0           concatenate_3[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 44, 96, 96)   176         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 44, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 56, 96, 96)   2464        activation_13[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 56, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 58, 96, 96)   0           concatenate_4[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_3_bn (BatchNormalizatio (None, 58, 96, 96)   232         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 58, 96, 96)   0           dense_0_3_bn[0][0]               
__________________________________________________________________________________________________
dense_0_3_bottleneck_conv2D (Co (None, 56, 96, 96)   3248        activation_15[0][0]              
__________________________________________________________________________________________________
dense_0_3_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_3_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 56, 96, 96)   0           dense_0_3_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_3_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 72, 96, 96)   0           concatenate_5[0][0]              
                                                                 dense_0_3_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 72, 96, 96)   288         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 72, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 36, 96, 96)   2592        activation_17[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 36, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 36, 48, 48)   144         average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 36, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   2016        activation_18[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 50, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 50, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 56, 48, 48)   2800        activation_20[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 56, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 64, 48, 48)   0           concatenate_7[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 56, 48, 48)   3584        activation_22[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 56, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 78, 48, 48)   0           concatenate_8[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_3_bn (BatchNormalizatio (None, 78, 48, 48)   312         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 78, 48, 48)   0           dense_1_3_bn[0][0]               
__________________________________________________________________________________________________
dense_1_3_bottleneck_conv2D (Co (None, 56, 48, 48)   4368        activation_24[0][0]              
__________________________________________________________________________________________________
dense_1_3_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_3_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 56, 48, 48)   0           dense_1_3_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_3_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 92, 48, 48)   0           concatenate_9[0][0]              
                                                                 dense_1_3_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 92, 48, 48)   368         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 92, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 46, 48, 48)   4232        activation_26[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 46, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 46, 24, 24)   184         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 46, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 56, 24, 24)   2576        activation_27[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 56, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 60, 24, 24)   0           average_pooling2d_4[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 60, 24, 24)   240         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 60, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 56, 24, 24)   3360        activation_29[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 56, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 74, 24, 24)   0           concatenate_11[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 74, 24, 24)   296         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 74, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 56, 24, 24)   4144        activation_31[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 56, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 88, 24, 24)   0           concatenate_12[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_3_bn (BatchNormalizatio (None, 88, 24, 24)   352         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 88, 24, 24)   0           dense_2_3_bn[0][0]               
__________________________________________________________________________________________________
dense_2_3_bottleneck_conv2D (Co (None, 56, 24, 24)   4928        activation_33[0][0]              
__________________________________________________________________________________________________
dense_2_3_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_3_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 56, 24, 24)   0           dense_2_3_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_3_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 102, 24, 24)  0           concatenate_13[0][0]             
                                                                 dense_2_3_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 102, 24, 24)  408         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 102, 24, 24)  0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 102)          0           activation_35[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            103         global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 134,279
Trainable params: 131,115
Non-trainable params: 3,164
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 62s - loss: 0.6253 - acc: 0.7684 - val_loss: 0.5620 - val_acc: 0.8021

Epoch 00001: val_loss improved from inf to 0.56199, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 57s - loss: 0.5404 - acc: 0.8139 - val_loss: 0.5113 - val_acc: 0.8308

Epoch 00002: val_loss improved from 0.56199 to 0.51133, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 57s - loss: 0.4995 - acc: 0.8359 - val_loss: 0.5121 - val_acc: 0.8351

Epoch 00003: val_loss did not improve from 0.51133
Epoch 4/40
 - 57s - loss: 0.4660 - acc: 0.8543 - val_loss: 0.4513 - val_acc: 0.8665

Epoch 00004: val_loss improved from 0.51133 to 0.45128, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 57s - loss: 0.4391 - acc: 0.8671 - val_loss: 0.4537 - val_acc: 0.8566

Epoch 00005: val_loss did not improve from 0.45128
Epoch 6/40
 - 57s - loss: 0.4151 - acc: 0.8803 - val_loss: 0.4218 - val_acc: 0.8744

Epoch 00006: val_loss improved from 0.45128 to 0.42176, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 57s - loss: 0.3943 - acc: 0.8908 - val_loss: 0.3959 - val_acc: 0.8883

Epoch 00007: val_loss improved from 0.42176 to 0.39589, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 57s - loss: 0.3732 - acc: 0.9000 - val_loss: 0.4043 - val_acc: 0.8839

Epoch 00008: val_loss did not improve from 0.39589
Epoch 9/40
 - 57s - loss: 0.3563 - acc: 0.9080 - val_loss: 0.3631 - val_acc: 0.9030

Epoch 00009: val_loss improved from 0.39589 to 0.36311, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 57s - loss: 0.3419 - acc: 0.9141 - val_loss: 0.3639 - val_acc: 0.9015

Epoch 00010: val_loss did not improve from 0.36311
Epoch 11/40
 - 57s - loss: 0.3303 - acc: 0.9180 - val_loss: 0.4013 - val_acc: 0.8878

Epoch 00011: val_loss did not improve from 0.36311
Epoch 12/40
 - 57s - loss: 0.3159 - acc: 0.9251 - val_loss: 0.3494 - val_acc: 0.9120

Epoch 00012: val_loss improved from 0.36311 to 0.34942, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 57s - loss: 0.3049 - acc: 0.9292 - val_loss: 0.3364 - val_acc: 0.9084

Epoch 00013: val_loss improved from 0.34942 to 0.33644, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 56s - loss: 0.2951 - acc: 0.9335 - val_loss: 0.3123 - val_acc: 0.9255

Epoch 00014: val_loss improved from 0.33644 to 0.31231, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 56s - loss: 0.2844 - acc: 0.9383 - val_loss: 0.3378 - val_acc: 0.9115

Epoch 00015: val_loss did not improve from 0.31231
Epoch 16/40
 - 58s - loss: 0.2744 - acc: 0.9423 - val_loss: 0.3833 - val_acc: 0.9049

Epoch 00016: val_loss did not improve from 0.31231
Epoch 17/40
 - 57s - loss: 0.2665 - acc: 0.9447 - val_loss: 0.3045 - val_acc: 0.9260

Epoch 00017: val_loss improved from 0.31231 to 0.30445, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 57s - loss: 0.2588 - acc: 0.9479 - val_loss: 0.3391 - val_acc: 0.9071

Epoch 00018: val_loss did not improve from 0.30445
Epoch 19/40
 - 57s - loss: 0.2538 - acc: 0.9495 - val_loss: 0.3034 - val_acc: 0.9248

Epoch 00019: val_loss improved from 0.30445 to 0.30342, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 57s - loss: 0.2454 - acc: 0.9534 - val_loss: 0.3375 - val_acc: 0.9147

Epoch 00020: val_loss did not improve from 0.30342
Epoch 21/40
 - 57s - loss: 0.2370 - acc: 0.9569 - val_loss: 0.3930 - val_acc: 0.9027

Epoch 00021: val_loss did not improve from 0.30342
Epoch 22/40
 - 57s - loss: 0.2333 - acc: 0.9564 - val_loss: 0.2963 - val_acc: 0.9268

Epoch 00022: val_loss improved from 0.30342 to 0.29625, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 56s - loss: 0.2270 - acc: 0.9586 - val_loss: 0.4011 - val_acc: 0.8965

Epoch 00023: val_loss did not improve from 0.29625
Epoch 24/40
 - 57s - loss: 0.2202 - acc: 0.9628 - val_loss: 0.3214 - val_acc: 0.9291

Epoch 00024: val_loss did not improve from 0.29625
Epoch 25/40
 - 57s - loss: 0.2158 - acc: 0.9643 - val_loss: 0.3620 - val_acc: 0.9130

Epoch 00025: val_loss did not improve from 0.29625
Epoch 26/40
 - 57s - loss: 0.2123 - acc: 0.9645 - val_loss: 0.2850 - val_acc: 0.9360

Epoch 00026: val_loss improved from 0.29625 to 0.28496, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 57s - loss: 0.2036 - acc: 0.9684 - val_loss: 0.3614 - val_acc: 0.9193

Epoch 00027: val_loss did not improve from 0.28496
Epoch 28/40
 - 57s - loss: 0.1993 - acc: 0.9698 - val_loss: 0.3083 - val_acc: 0.9243

Epoch 00028: val_loss did not improve from 0.28496
Epoch 29/40
 - 57s - loss: 0.2002 - acc: 0.9684 - val_loss: 0.3100 - val_acc: 0.9310

Epoch 00029: val_loss did not improve from 0.28496
Epoch 30/40
 - 57s - loss: 0.1911 - acc: 0.9723 - val_loss: 0.2841 - val_acc: 0.9325

Epoch 00030: val_loss improved from 0.28496 to 0.28413, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 57s - loss: 0.1917 - acc: 0.9715 - val_loss: 0.3091 - val_acc: 0.9233

Epoch 00031: val_loss did not improve from 0.28413
Epoch 32/40
 - 57s - loss: 0.1875 - acc: 0.9740 - val_loss: 0.3193 - val_acc: 0.9221

Epoch 00032: val_loss did not improve from 0.28413
Epoch 33/40
 - 57s - loss: 0.1803 - acc: 0.9769 - val_loss: 0.3095 - val_acc: 0.9287

Epoch 00033: val_loss did not improve from 0.28413
Epoch 34/40
 - 57s - loss: 0.1793 - acc: 0.9763 - val_loss: 0.3403 - val_acc: 0.9253

Epoch 00034: val_loss did not improve from 0.28413
Epoch 35/40
 - 57s - loss: 0.1776 - acc: 0.9768 - val_loss: 0.3421 - val_acc: 0.9149

Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00035: val_loss did not improve from 0.28413
Epoch 36/40
 - 56s - loss: 0.1528 - acc: 0.9882 - val_loss: 0.2597 - val_acc: 0.9440

Epoch 00036: val_loss improved from 0.28413 to 0.25968, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 56s - loss: 0.1480 - acc: 0.9898 - val_loss: 0.2533 - val_acc: 0.9479

Epoch 00037: val_loss improved from 0.25968 to 0.25326, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 38/40
 - 56s - loss: 0.1452 - acc: 0.9909 - val_loss: 0.2729 - val_acc: 0.9428

Epoch 00038: val_loss did not improve from 0.25326
Epoch 39/40
 - 56s - loss: 0.1461 - acc: 0.9902 - val_loss: 0.2600 - val_acc: 0.9465

Epoch 00039: val_loss did not improve from 0.25326
Epoch 40/40
 - 56s - loss: 0.1443 - acc: 0.9911 - val_loss: 0.2512 - val_acc: 0.9483

Epoch 00040: val_loss improved from 0.25326 to 0.25123, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 4s
 128/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 3s
 320/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 608/7440 [=>............................] - ETA: 3s
 704/7440 [=>............................] - ETA: 3s
 832/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1024/7440 [===>..........................] - ETA: 3s
1120/7440 [===>..........................] - ETA: 3s
1216/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 3s
1408/7440 [====>.........................] - ETA: 3s
1504/7440 [=====>........................] - ETA: 3s
1600/7440 [=====>........................] - ETA: 3s
1696/7440 [=====>........................] - ETA: 3s
1792/7440 [======>.......................] - ETA: 2s
1888/7440 [======>.......................] - ETA: 2s
1984/7440 [=======>......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2272/7440 [========>.....................] - ETA: 2s
2368/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2560/7440 [=========>....................] - ETA: 2s
2656/7440 [=========>....................] - ETA: 2s
2752/7440 [==========>...................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3040/7440 [===========>..................] - ETA: 2s
3136/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3424/7440 [============>.................] - ETA: 2s
3520/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 2s
3712/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4576/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5056/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5344/7440 [====================>.........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 531us/step
current Test accuracy: 0.7924731182795699
current auc_score ------------------>  0.8955412836744132

  32/7440 [..............................] - ETA: 3:10
 128/7440 [..............................] - ETA: 50s 
 224/7440 [..............................] - ETA: 29s
 320/7440 [>.............................] - ETA: 21s
 416/7440 [>.............................] - ETA: 17s
 512/7440 [=>............................] - ETA: 14s
 608/7440 [=>............................] - ETA: 12s
 704/7440 [=>............................] - ETA: 11s
 800/7440 [==>...........................] - ETA: 10s
 896/7440 [==>...........................] - ETA: 9s 
 992/7440 [===>..........................] - ETA: 8s
1088/7440 [===>..........................] - ETA: 8s
1184/7440 [===>..........................] - ETA: 7s
1280/7440 [====>.........................] - ETA: 7s
1376/7440 [====>.........................] - ETA: 6s
1472/7440 [====>.........................] - ETA: 6s
1568/7440 [=====>........................] - ETA: 6s
1664/7440 [=====>........................] - ETA: 5s
1760/7440 [======>.......................] - ETA: 5s
1856/7440 [======>.......................] - ETA: 5s
1952/7440 [======>.......................] - ETA: 5s
2048/7440 [=======>......................] - ETA: 5s
2144/7440 [=======>......................] - ETA: 4s
2240/7440 [========>.....................] - ETA: 4s
2336/7440 [========>.....................] - ETA: 4s
2432/7440 [========>.....................] - ETA: 4s
2528/7440 [=========>....................] - ETA: 4s
2624/7440 [=========>....................] - ETA: 4s
2720/7440 [=========>....................] - ETA: 3s
2816/7440 [==========>...................] - ETA: 3s
2912/7440 [==========>...................] - ETA: 3s
3008/7440 [===========>..................] - ETA: 3s
3104/7440 [===========>..................] - ETA: 3s
3200/7440 [===========>..................] - ETA: 3s
3296/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3488/7440 [=============>................] - ETA: 3s
3584/7440 [=============>................] - ETA: 2s
3680/7440 [=============>................] - ETA: 2s
3776/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
3968/7440 [===============>..............] - ETA: 2s
4064/7440 [===============>..............] - ETA: 2s
4160/7440 [===============>..............] - ETA: 2s
4256/7440 [================>.............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4448/7440 [================>.............] - ETA: 2s
4544/7440 [=================>............] - ETA: 2s
4640/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4928/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5696/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5888/7440 [======================>.......] - ETA: 1s
5984/7440 [=======================>......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 642us/step
Best saved model Test accuracy: 0.7924731182795699
best saved model auc_score ------------------>  0.8955412836744132
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_36[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 26, 96, 96)   104         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 26, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 13, 96, 96)   338         activation_38[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 13, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 13, 48, 48)   52          average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 13, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   520         activation_39[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_40[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 23, 48, 48)   92          concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 23, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 11, 48, 48)   253         activation_41[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 11, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 11, 24, 24)   44          average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 11, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   440         activation_42[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_43[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 21, 24, 24)   0           average_pooling2d_6[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 21, 24, 24)   84          concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 21, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 21)           0           activation_44[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            22          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 14,221
Trainable params: 13,761
Non-trainable params: 460
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 19s - loss: 0.6061 - acc: 0.7234 - val_loss: 0.5594 - val_acc: 0.7518

Epoch 00001: val_loss improved from inf to 0.55937, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 16s - loss: 0.5346 - acc: 0.7723 - val_loss: 0.5231 - val_acc: 0.7657

Epoch 00002: val_loss improved from 0.55937 to 0.52314, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 16s - loss: 0.5078 - acc: 0.7818 - val_loss: 0.5176 - val_acc: 0.7711

Epoch 00003: val_loss improved from 0.52314 to 0.51757, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 16s - loss: 0.4899 - acc: 0.7856 - val_loss: 0.4882 - val_acc: 0.7772

Epoch 00004: val_loss improved from 0.51757 to 0.48822, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 16s - loss: 0.4755 - acc: 0.7891 - val_loss: 0.4653 - val_acc: 0.7944

Epoch 00005: val_loss improved from 0.48822 to 0.46534, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 16s - loss: 0.4651 - acc: 0.7936 - val_loss: 0.4877 - val_acc: 0.7850

Epoch 00006: val_loss did not improve from 0.46534
Epoch 7/40
 - 16s - loss: 0.4568 - acc: 0.7977 - val_loss: 0.4711 - val_acc: 0.7811

Epoch 00007: val_loss did not improve from 0.46534
Epoch 8/40
 - 16s - loss: 0.4486 - acc: 0.7994 - val_loss: 0.4386 - val_acc: 0.8085

Epoch 00008: val_loss improved from 0.46534 to 0.43859, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 16s - loss: 0.4420 - acc: 0.8057 - val_loss: 0.4364 - val_acc: 0.8056

Epoch 00009: val_loss improved from 0.43859 to 0.43636, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 16s - loss: 0.4362 - acc: 0.8085 - val_loss: 0.4927 - val_acc: 0.7792

Epoch 00010: val_loss did not improve from 0.43636
Epoch 11/40
 - 16s - loss: 0.4299 - acc: 0.8109 - val_loss: 0.4361 - val_acc: 0.8097

Epoch 00011: val_loss improved from 0.43636 to 0.43606, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 16s - loss: 0.4251 - acc: 0.8144 - val_loss: 0.4350 - val_acc: 0.8104

Epoch 00012: val_loss improved from 0.43606 to 0.43496, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 16s - loss: 0.4225 - acc: 0.8141 - val_loss: 0.4277 - val_acc: 0.8195

Epoch 00013: val_loss improved from 0.43496 to 0.42768, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 16s - loss: 0.4164 - acc: 0.8190 - val_loss: 0.4217 - val_acc: 0.8058

Epoch 00014: val_loss improved from 0.42768 to 0.42173, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 16s - loss: 0.4113 - acc: 0.8221 - val_loss: 0.4147 - val_acc: 0.8192

Epoch 00015: val_loss improved from 0.42173 to 0.41469, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 16s - loss: 0.4119 - acc: 0.8214 - val_loss: 0.4120 - val_acc: 0.8233

Epoch 00016: val_loss improved from 0.41469 to 0.41197, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 16s - loss: 0.4072 - acc: 0.8251 - val_loss: 0.4265 - val_acc: 0.8133

Epoch 00017: val_loss did not improve from 0.41197
Epoch 18/40
 - 16s - loss: 0.4040 - acc: 0.8247 - val_loss: 0.4048 - val_acc: 0.8291

Epoch 00018: val_loss improved from 0.41197 to 0.40477, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 16s - loss: 0.3992 - acc: 0.8256 - val_loss: 0.3965 - val_acc: 0.8291

Epoch 00019: val_loss improved from 0.40477 to 0.39648, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 16s - loss: 0.3984 - acc: 0.8287 - val_loss: 0.3846 - val_acc: 0.8382

Epoch 00020: val_loss improved from 0.39648 to 0.38459, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 16s - loss: 0.3942 - acc: 0.8311 - val_loss: 0.3944 - val_acc: 0.8289

Epoch 00021: val_loss did not improve from 0.38459
Epoch 22/40
 - 16s - loss: 0.3926 - acc: 0.8301 - val_loss: 0.3836 - val_acc: 0.8340

Epoch 00022: val_loss improved from 0.38459 to 0.38361, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 16s - loss: 0.3900 - acc: 0.8325 - val_loss: 0.4043 - val_acc: 0.8240

Epoch 00023: val_loss did not improve from 0.38361
Epoch 24/40
 - 16s - loss: 0.3881 - acc: 0.8330 - val_loss: 0.3840 - val_acc: 0.8322

Epoch 00024: val_loss did not improve from 0.38361
Epoch 25/40
 - 16s - loss: 0.3854 - acc: 0.8370 - val_loss: 0.3886 - val_acc: 0.8370

Epoch 00025: val_loss did not improve from 0.38361
Epoch 26/40
 - 16s - loss: 0.3816 - acc: 0.8371 - val_loss: 0.3831 - val_acc: 0.8367

Epoch 00026: val_loss improved from 0.38361 to 0.38311, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 16s - loss: 0.3815 - acc: 0.8381 - val_loss: 0.3767 - val_acc: 0.8448

Epoch 00027: val_loss improved from 0.38311 to 0.37672, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 16s - loss: 0.3767 - acc: 0.8408 - val_loss: 0.4552 - val_acc: 0.8040

Epoch 00028: val_loss did not improve from 0.37672
Epoch 29/40
 - 16s - loss: 0.3752 - acc: 0.8414 - val_loss: 0.3811 - val_acc: 0.8396

Epoch 00029: val_loss did not improve from 0.37672
Epoch 30/40
 - 16s - loss: 0.3754 - acc: 0.8426 - val_loss: 0.3992 - val_acc: 0.8237

Epoch 00030: val_loss did not improve from 0.37672
Epoch 31/40
 - 16s - loss: 0.3715 - acc: 0.8446 - val_loss: 0.4223 - val_acc: 0.8089

Epoch 00031: val_loss did not improve from 0.37672
Epoch 32/40
 - 16s - loss: 0.3690 - acc: 0.8455 - val_loss: 0.3979 - val_acc: 0.8271

Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00032: val_loss did not improve from 0.37672
Epoch 33/40
 - 15s - loss: 0.3628 - acc: 0.8481 - val_loss: 0.3629 - val_acc: 0.8523

Epoch 00033: val_loss improved from 0.37672 to 0.36286, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 16s - loss: 0.3604 - acc: 0.8505 - val_loss: 0.3556 - val_acc: 0.8602

Epoch 00034: val_loss improved from 0.36286 to 0.35565, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 35/40
 - 16s - loss: 0.3601 - acc: 0.8506 - val_loss: 0.3594 - val_acc: 0.8488

Epoch 00035: val_loss did not improve from 0.35565
Epoch 36/40
 - 16s - loss: 0.3555 - acc: 0.8518 - val_loss: 0.3685 - val_acc: 0.8429

Epoch 00036: val_loss did not improve from 0.35565
Epoch 37/40
 - 16s - loss: 0.3572 - acc: 0.8524 - val_loss: 0.3618 - val_acc: 0.8530

Epoch 00037: val_loss did not improve from 0.35565
Epoch 38/40
 - 16s - loss: 0.3571 - acc: 0.8525 - val_loss: 0.3531 - val_acc: 0.8545

Epoch 00038: val_loss improved from 0.35565 to 0.35308, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 15s - loss: 0.3560 - acc: 0.8526 - val_loss: 0.3532 - val_acc: 0.8533

Epoch 00039: val_loss did not improve from 0.35308
Epoch 40/40
 - 15s - loss: 0.3564 - acc: 0.8517 - val_loss: 0.3610 - val_acc: 0.8498

Epoch 00040: val_loss did not improve from 0.35308

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 0s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 163us/step
current Test accuracy: 0.8541666666666666
current auc_score ------------------>  0.9192767950052029

  32/7440 [..............................] - ETA: 2:58
 320/7440 [>.............................] - ETA: 18s 
 608/7440 [=>............................] - ETA: 9s 
 896/7440 [==>...........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 5s
1472/7440 [====>.........................] - ETA: 4s
1792/7440 [======>.......................] - ETA: 3s
2112/7440 [=======>......................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
3008/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3936/7440 [==============>...............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 276us/step
Best saved model Test accuracy: 0.8365591397849462
best saved model auc_score ------------------>  0.9000390218522373
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_45[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_46[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_47[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_48[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 28, 96, 96)   0           concatenate_18[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_49[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_50[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 34, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 34, 96, 96)   136         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 34, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 17, 96, 96)   578         activation_51[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 17, 48, 48)   68          average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   408         activation_52[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_53[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 23, 48, 48)   92          concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 23, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   552         activation_54[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_55[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 29, 48, 48)   0           concatenate_21[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 29, 48, 48)   116         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 29, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 24, 48, 48)   696         activation_56[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 24, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_57[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 35, 48, 48)   0           concatenate_22[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 35, 48, 48)   140         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 35, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 17, 48, 48)   595         activation_58[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 17, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 17, 24, 24)   68          average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 24, 24, 24)   408         activation_59[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 24, 24, 24)   96          dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 24, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 6, 24, 24)    1296        activation_60[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 23, 24, 24)   0           average_pooling2d_8[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 23, 24, 24)   92          concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 23, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 24, 24, 24)   552         activation_61[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 24, 24, 24)   96          dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 24, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 6, 24, 24)    1296        activation_62[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 29, 24, 24)   0           concatenate_24[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 29, 24, 24)   116         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 29, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 24, 24, 24)   696         activation_63[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 24, 24, 24)   96          dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 24, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 6, 24, 24)    1296        activation_64[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 35, 24, 24)   0           concatenate_25[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 35, 24, 24)   140         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 35, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 35)           0           activation_65[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            36          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 20,153
Trainable params: 19,105
Non-trainable params: 1,048
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 35s - loss: 0.6546 - acc: 0.6897 - val_loss: 0.5767 - val_acc: 0.7620

Epoch 00001: val_loss improved from inf to 0.57670, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 30s - loss: 0.5433 - acc: 0.7737 - val_loss: 0.5130 - val_acc: 0.7869

Epoch 00002: val_loss improved from 0.57670 to 0.51300, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 30s - loss: 0.5036 - acc: 0.7909 - val_loss: 0.4814 - val_acc: 0.7932

Epoch 00003: val_loss improved from 0.51300 to 0.48143, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 30s - loss: 0.4788 - acc: 0.7991 - val_loss: 0.4799 - val_acc: 0.7973

Epoch 00004: val_loss improved from 0.48143 to 0.47993, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 30s - loss: 0.4616 - acc: 0.8048 - val_loss: 0.4660 - val_acc: 0.7967

Epoch 00005: val_loss improved from 0.47993 to 0.46598, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 30s - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4602 - val_acc: 0.8031

Epoch 00006: val_loss improved from 0.46598 to 0.46020, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 30s - loss: 0.4359 - acc: 0.8186 - val_loss: 0.4230 - val_acc: 0.8245

Epoch 00007: val_loss improved from 0.46020 to 0.42299, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 30s - loss: 0.4307 - acc: 0.8218 - val_loss: 0.4373 - val_acc: 0.8171

Epoch 00008: val_loss did not improve from 0.42299
Epoch 9/40
 - 30s - loss: 0.4204 - acc: 0.8268 - val_loss: 0.4411 - val_acc: 0.8204

Epoch 00009: val_loss did not improve from 0.42299
Epoch 10/40
 - 30s - loss: 0.4125 - acc: 0.8318 - val_loss: 0.4126 - val_acc: 0.8284

Epoch 00010: val_loss improved from 0.42299 to 0.41257, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 30s - loss: 0.4045 - acc: 0.8368 - val_loss: 0.4023 - val_acc: 0.8371

Epoch 00011: val_loss improved from 0.41257 to 0.40226, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 30s - loss: 0.3989 - acc: 0.8389 - val_loss: 0.3887 - val_acc: 0.8478

Epoch 00012: val_loss improved from 0.40226 to 0.38867, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 30s - loss: 0.3918 - acc: 0.8439 - val_loss: 0.4057 - val_acc: 0.8377

Epoch 00013: val_loss did not improve from 0.38867
Epoch 14/40
 - 30s - loss: 0.3884 - acc: 0.8467 - val_loss: 0.3797 - val_acc: 0.8523

Epoch 00014: val_loss improved from 0.38867 to 0.37968, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 30s - loss: 0.3806 - acc: 0.8491 - val_loss: 0.4124 - val_acc: 0.8396

Epoch 00015: val_loss did not improve from 0.37968
Epoch 16/40
 - 30s - loss: 0.3751 - acc: 0.8532 - val_loss: 0.3861 - val_acc: 0.8470

Epoch 00016: val_loss did not improve from 0.37968
Epoch 17/40
 - 30s - loss: 0.3737 - acc: 0.8531 - val_loss: 0.3911 - val_acc: 0.8476

Epoch 00017: val_loss did not improve from 0.37968
Epoch 18/40
 - 30s - loss: 0.3682 - acc: 0.8579 - val_loss: 0.3776 - val_acc: 0.8520

Epoch 00018: val_loss improved from 0.37968 to 0.37762, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 30s - loss: 0.3609 - acc: 0.8603 - val_loss: 0.3643 - val_acc: 0.8598

Epoch 00019: val_loss improved from 0.37762 to 0.36428, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 30s - loss: 0.3600 - acc: 0.8609 - val_loss: 0.3570 - val_acc: 0.8662

Epoch 00020: val_loss improved from 0.36428 to 0.35703, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 30s - loss: 0.3502 - acc: 0.8658 - val_loss: 0.3479 - val_acc: 0.8622

Epoch 00021: val_loss improved from 0.35703 to 0.34786, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 30s - loss: 0.3493 - acc: 0.8688 - val_loss: 0.4038 - val_acc: 0.8389

Epoch 00022: val_loss did not improve from 0.34786
Epoch 23/40
 - 30s - loss: 0.3448 - acc: 0.8695 - val_loss: 0.3622 - val_acc: 0.8586

Epoch 00023: val_loss did not improve from 0.34786
Epoch 24/40
 - 30s - loss: 0.3382 - acc: 0.8729 - val_loss: 0.3481 - val_acc: 0.8675

Epoch 00024: val_loss did not improve from 0.34786
Epoch 25/40
 - 30s - loss: 0.3375 - acc: 0.8721 - val_loss: 0.3436 - val_acc: 0.8734

Epoch 00025: val_loss improved from 0.34786 to 0.34360, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 30s - loss: 0.3351 - acc: 0.8745 - val_loss: 0.3419 - val_acc: 0.8666

Epoch 00026: val_loss improved from 0.34360 to 0.34192, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 30s - loss: 0.3333 - acc: 0.8761 - val_loss: 0.3337 - val_acc: 0.8735

Epoch 00027: val_loss improved from 0.34192 to 0.33368, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 30s - loss: 0.3279 - acc: 0.8770 - val_loss: 0.3351 - val_acc: 0.8739

Epoch 00028: val_loss did not improve from 0.33368
Epoch 29/40
 - 30s - loss: 0.3227 - acc: 0.8812 - val_loss: 0.3340 - val_acc: 0.8700

Epoch 00029: val_loss did not improve from 0.33368
Epoch 30/40
 - 30s - loss: 0.3215 - acc: 0.8810 - val_loss: 0.3549 - val_acc: 0.8658

Epoch 00030: val_loss did not improve from 0.33368
Epoch 31/40
 - 30s - loss: 0.3183 - acc: 0.8836 - val_loss: 0.3389 - val_acc: 0.8705

Epoch 00031: val_loss did not improve from 0.33368
Epoch 32/40
 - 30s - loss: 0.3155 - acc: 0.8848 - val_loss: 0.3170 - val_acc: 0.8853

Epoch 00032: val_loss improved from 0.33368 to 0.31698, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 30s - loss: 0.3147 - acc: 0.8855 - val_loss: 0.3134 - val_acc: 0.8877

Epoch 00033: val_loss improved from 0.31698 to 0.31343, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 30s - loss: 0.3091 - acc: 0.8886 - val_loss: 0.3245 - val_acc: 0.8784

Epoch 00034: val_loss did not improve from 0.31343
Epoch 35/40
 - 30s - loss: 0.3047 - acc: 0.8896 - val_loss: 0.3594 - val_acc: 0.8518

Epoch 00035: val_loss did not improve from 0.31343
Epoch 36/40
 - 30s - loss: 0.3045 - acc: 0.8889 - val_loss: 0.3156 - val_acc: 0.8848

Epoch 00036: val_loss did not improve from 0.31343
Epoch 37/40
 - 30s - loss: 0.2983 - acc: 0.8945 - val_loss: 0.3208 - val_acc: 0.8794

Epoch 00037: val_loss did not improve from 0.31343
Epoch 38/40
 - 30s - loss: 0.2983 - acc: 0.8934 - val_loss: 0.3159 - val_acc: 0.8840

Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00038: val_loss did not improve from 0.31343
Epoch 39/40
 - 30s - loss: 0.2840 - acc: 0.9014 - val_loss: 0.3036 - val_acc: 0.8881

Epoch 00039: val_loss improved from 0.31343 to 0.30361, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 29s - loss: 0.2850 - acc: 0.9009 - val_loss: 0.2929 - val_acc: 0.8950

Epoch 00040: val_loss improved from 0.30361 to 0.29293, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 285us/step
current Test accuracy: 0.8181451612903226
current auc_score ------------------>  0.8947200182101975

  32/7440 [..............................] - ETA: 4:41
 192/7440 [..............................] - ETA: 47s 
 384/7440 [>.............................] - ETA: 24s
 576/7440 [=>............................] - ETA: 16s
 768/7440 [==>...........................] - ETA: 12s
 960/7440 [==>...........................] - ETA: 10s
1152/7440 [===>..........................] - ETA: 8s 
1344/7440 [====>.........................] - ETA: 7s
1536/7440 [=====>........................] - ETA: 6s
1728/7440 [=====>........................] - ETA: 5s
1920/7440 [======>.......................] - ETA: 5s
2112/7440 [=======>......................] - ETA: 4s
2304/7440 [========>.....................] - ETA: 4s
2496/7440 [=========>....................] - ETA: 3s
2688/7440 [=========>....................] - ETA: 3s
2880/7440 [==========>...................] - ETA: 3s
3072/7440 [===========>..................] - ETA: 2s
3264/7440 [============>.................] - ETA: 2s
3456/7440 [============>.................] - ETA: 2s
3648/7440 [=============>................] - ETA: 2s
3840/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4224/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 1s
4608/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 453us/step
Best saved model Test accuracy: 0.8181451612903226
best saved model auc_score ------------------>  0.8947200182101975
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_66[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_67[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_68[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_69[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 52, 96, 96)   0           concatenate_27[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_70[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   1872        activation_71[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_72[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 44, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 44, 48, 48)   176         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 44, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3168        activation_73[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_74[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 62, 48, 48)   0           concatenate_29[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 62, 48, 48)   248         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 62, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 62)           0           activation_75[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            63          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 59,087
Trainable params: 58,043
Non-trainable params: 1,044
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 36s - loss: 0.6074 - acc: 0.7330 - val_loss: 0.5570 - val_acc: 0.7651

Epoch 00001: val_loss improved from inf to 0.55704, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 32s - loss: 0.5372 - acc: 0.7729 - val_loss: 0.5121 - val_acc: 0.7769

Epoch 00002: val_loss improved from 0.55704 to 0.51208, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 31s - loss: 0.5103 - acc: 0.7847 - val_loss: 0.5115 - val_acc: 0.7879

Epoch 00003: val_loss improved from 0.51208 to 0.51153, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 31s - loss: 0.4902 - acc: 0.7962 - val_loss: 0.4816 - val_acc: 0.7992

Epoch 00004: val_loss improved from 0.51153 to 0.48164, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 31s - loss: 0.4750 - acc: 0.8034 - val_loss: 0.5003 - val_acc: 0.7777

Epoch 00005: val_loss did not improve from 0.48164
Epoch 6/40
 - 32s - loss: 0.4607 - acc: 0.8104 - val_loss: 0.4476 - val_acc: 0.8160

Epoch 00006: val_loss improved from 0.48164 to 0.44765, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 32s - loss: 0.4501 - acc: 0.8169 - val_loss: 0.4536 - val_acc: 0.8202

Epoch 00007: val_loss did not improve from 0.44765
Epoch 8/40
 - 32s - loss: 0.4428 - acc: 0.8195 - val_loss: 0.4589 - val_acc: 0.8138

Epoch 00008: val_loss did not improve from 0.44765
Epoch 9/40
 - 32s - loss: 0.4325 - acc: 0.8262 - val_loss: 0.4274 - val_acc: 0.8243

Epoch 00009: val_loss improved from 0.44765 to 0.42744, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 32s - loss: 0.4274 - acc: 0.8283 - val_loss: 0.4326 - val_acc: 0.8239

Epoch 00010: val_loss did not improve from 0.42744
Epoch 11/40
 - 32s - loss: 0.4190 - acc: 0.8324 - val_loss: 0.4350 - val_acc: 0.8174

Epoch 00011: val_loss did not improve from 0.42744
Epoch 12/40
 - 32s - loss: 0.4151 - acc: 0.8352 - val_loss: 0.4100 - val_acc: 0.8410

Epoch 00012: val_loss improved from 0.42744 to 0.40998, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 32s - loss: 0.4068 - acc: 0.8396 - val_loss: 0.4533 - val_acc: 0.8096

Epoch 00013: val_loss did not improve from 0.40998
Epoch 14/40
 - 32s - loss: 0.4024 - acc: 0.8425 - val_loss: 0.4580 - val_acc: 0.8150

Epoch 00014: val_loss did not improve from 0.40998
Epoch 15/40
 - 31s - loss: 0.3977 - acc: 0.8439 - val_loss: 0.4107 - val_acc: 0.8387

Epoch 00015: val_loss did not improve from 0.40998
Epoch 16/40
 - 32s - loss: 0.3924 - acc: 0.8464 - val_loss: 0.4022 - val_acc: 0.8402

Epoch 00016: val_loss improved from 0.40998 to 0.40218, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 32s - loss: 0.3882 - acc: 0.8505 - val_loss: 0.3980 - val_acc: 0.8416

Epoch 00017: val_loss improved from 0.40218 to 0.39799, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 32s - loss: 0.3821 - acc: 0.8520 - val_loss: 0.5325 - val_acc: 0.7509

Epoch 00018: val_loss did not improve from 0.39799
Epoch 19/40
 - 32s - loss: 0.3801 - acc: 0.8528 - val_loss: 0.3854 - val_acc: 0.8484

Epoch 00019: val_loss improved from 0.39799 to 0.38540, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 32s - loss: 0.3728 - acc: 0.8572 - val_loss: 0.4000 - val_acc: 0.8430

Epoch 00020: val_loss did not improve from 0.38540
Epoch 21/40
 - 32s - loss: 0.3711 - acc: 0.8587 - val_loss: 0.3793 - val_acc: 0.8527

Epoch 00021: val_loss improved from 0.38540 to 0.37933, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 32s - loss: 0.3659 - acc: 0.8597 - val_loss: 0.3652 - val_acc: 0.8593

Epoch 00022: val_loss improved from 0.37933 to 0.36523, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 31s - loss: 0.3609 - acc: 0.8627 - val_loss: 0.3820 - val_acc: 0.8577

Epoch 00023: val_loss did not improve from 0.36523
Epoch 24/40
 - 31s - loss: 0.3613 - acc: 0.8648 - val_loss: 0.3687 - val_acc: 0.8618

Epoch 00024: val_loss did not improve from 0.36523
Epoch 25/40
 - 31s - loss: 0.3575 - acc: 0.8659 - val_loss: 0.3531 - val_acc: 0.8667

Epoch 00025: val_loss improved from 0.36523 to 0.35309, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 32s - loss: 0.3532 - acc: 0.8656 - val_loss: 0.3882 - val_acc: 0.8483

Epoch 00026: val_loss did not improve from 0.35309
Epoch 27/40
 - 31s - loss: 0.3496 - acc: 0.8696 - val_loss: 0.3817 - val_acc: 0.8569

Epoch 00027: val_loss did not improve from 0.35309
Epoch 28/40
 - 31s - loss: 0.3436 - acc: 0.8725 - val_loss: 0.3840 - val_acc: 0.8489

Epoch 00028: val_loss did not improve from 0.35309
Epoch 29/40
 - 32s - loss: 0.3425 - acc: 0.8698 - val_loss: 0.4157 - val_acc: 0.8350

Epoch 00029: val_loss did not improve from 0.35309
Epoch 30/40
 - 32s - loss: 0.3425 - acc: 0.8716 - val_loss: 0.3794 - val_acc: 0.8571

Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00030: val_loss did not improve from 0.35309
Epoch 31/40
 - 32s - loss: 0.3264 - acc: 0.8822 - val_loss: 0.3305 - val_acc: 0.8819

Epoch 00031: val_loss improved from 0.35309 to 0.33054, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 32s - loss: 0.3230 - acc: 0.8835 - val_loss: 0.3257 - val_acc: 0.8838

Epoch 00032: val_loss improved from 0.33054 to 0.32573, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 32s - loss: 0.3224 - acc: 0.8854 - val_loss: 0.3310 - val_acc: 0.8808

Epoch 00033: val_loss did not improve from 0.32573
Epoch 34/40
 - 32s - loss: 0.3224 - acc: 0.8853 - val_loss: 0.3310 - val_acc: 0.8790

Epoch 00034: val_loss did not improve from 0.32573
Epoch 35/40
 - 32s - loss: 0.3180 - acc: 0.8855 - val_loss: 0.3314 - val_acc: 0.8794

Epoch 00035: val_loss did not improve from 0.32573
Epoch 36/40
 - 32s - loss: 0.3169 - acc: 0.8872 - val_loss: 0.3241 - val_acc: 0.8803

Epoch 00036: val_loss improved from 0.32573 to 0.32409, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 32s - loss: 0.3149 - acc: 0.8876 - val_loss: 0.3257 - val_acc: 0.8879

Epoch 00037: val_loss did not improve from 0.32409
Epoch 38/40
 - 32s - loss: 0.3166 - acc: 0.8880 - val_loss: 0.3301 - val_acc: 0.8779

Epoch 00038: val_loss did not improve from 0.32409
Epoch 39/40
 - 32s - loss: 0.3122 - acc: 0.8910 - val_loss: 0.3334 - val_acc: 0.8761

Epoch 00039: val_loss did not improve from 0.32409
Epoch 40/40
 - 32s - loss: 0.3132 - acc: 0.8886 - val_loss: 0.3360 - val_acc: 0.8722

Epoch 00040: val_loss did not improve from 0.32409

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 345us/step
current Test accuracy: 0.7974462365591398
current auc_score ------------------>  0.8822415163602729

  32/7440 [..............................] - ETA: 5:11
 192/7440 [..............................] - ETA: 53s 
 352/7440 [>.............................] - ETA: 29s
 512/7440 [=>............................] - ETA: 20s
 672/7440 [=>............................] - ETA: 15s
 832/7440 [==>...........................] - ETA: 12s
 992/7440 [===>..........................] - ETA: 10s
1152/7440 [===>..........................] - ETA: 9s 
1312/7440 [====>.........................] - ETA: 8s
1472/7440 [====>.........................] - ETA: 7s
1632/7440 [=====>........................] - ETA: 6s
1792/7440 [======>.......................] - ETA: 6s
1952/7440 [======>.......................] - ETA: 5s
2112/7440 [=======>......................] - ETA: 5s
2272/7440 [========>.....................] - ETA: 4s
2432/7440 [========>.....................] - ETA: 4s
2592/7440 [=========>....................] - ETA: 4s
2752/7440 [==========>...................] - ETA: 3s
2912/7440 [==========>...................] - ETA: 3s
3072/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3552/7440 [=============>................] - ETA: 2s
3712/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 528us/step
Best saved model Test accuracy: 0.814247311827957
best saved model auc_score ------------------>  0.8915936090877558
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_76 (Activation)   (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_10 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_77 (Activation)   (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_11 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_78 (Activation)   (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_7 ( (None, 4)                 0         
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 12s - loss: 0.6588 - acc: 0.6487 - val_loss: 0.6429 - val_acc: 0.6732

Epoch 00001: val_loss improved from inf to 0.64286, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 8s - loss: 0.6374 - acc: 0.6819 - val_loss: 0.6306 - val_acc: 0.6920

Epoch 00002: val_loss improved from 0.64286 to 0.63064, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 8s - loss: 0.6279 - acc: 0.6895 - val_loss: 0.6223 - val_acc: 0.6983

Epoch 00003: val_loss improved from 0.63064 to 0.62231, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 8s - loss: 0.6208 - acc: 0.6970 - val_loss: 0.6152 - val_acc: 0.7048

Epoch 00004: val_loss improved from 0.62231 to 0.61518, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 8s - loss: 0.6138 - acc: 0.7029 - val_loss: 0.6096 - val_acc: 0.7093

Epoch 00005: val_loss improved from 0.61518 to 0.60962, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 8s - loss: 0.6082 - acc: 0.7079 - val_loss: 0.6059 - val_acc: 0.7108

Epoch 00006: val_loss improved from 0.60962 to 0.60586, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 8s - loss: 0.6037 - acc: 0.7121 - val_loss: 0.6005 - val_acc: 0.7130

Epoch 00007: val_loss improved from 0.60586 to 0.60047, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 8s - loss: 0.5984 - acc: 0.7145 - val_loss: 0.5955 - val_acc: 0.7182

Epoch 00008: val_loss improved from 0.60047 to 0.59552, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 8s - loss: 0.5956 - acc: 0.7179 - val_loss: 0.5918 - val_acc: 0.7203

Epoch 00009: val_loss improved from 0.59552 to 0.59182, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 8s - loss: 0.5919 - acc: 0.7198 - val_loss: 0.5878 - val_acc: 0.7233

Epoch 00010: val_loss improved from 0.59182 to 0.58776, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 8s - loss: 0.5873 - acc: 0.7230 - val_loss: 0.5863 - val_acc: 0.7204

Epoch 00011: val_loss improved from 0.58776 to 0.58634, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 8s - loss: 0.5829 - acc: 0.7245 - val_loss: 0.5795 - val_acc: 0.7293

Epoch 00012: val_loss improved from 0.58634 to 0.57947, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 8s - loss: 0.5803 - acc: 0.7259 - val_loss: 0.5774 - val_acc: 0.7248

Epoch 00013: val_loss improved from 0.57947 to 0.57740, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 8s - loss: 0.5756 - acc: 0.7298 - val_loss: 0.5708 - val_acc: 0.7322

Epoch 00014: val_loss improved from 0.57740 to 0.57079, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 8s - loss: 0.5727 - acc: 0.7319 - val_loss: 0.5680 - val_acc: 0.7307

Epoch 00015: val_loss improved from 0.57079 to 0.56801, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 8s - loss: 0.5686 - acc: 0.7327 - val_loss: 0.5639 - val_acc: 0.7351

Epoch 00016: val_loss improved from 0.56801 to 0.56395, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 8s - loss: 0.5643 - acc: 0.7359 - val_loss: 0.5600 - val_acc: 0.7316

Epoch 00017: val_loss improved from 0.56395 to 0.56004, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 8s - loss: 0.5610 - acc: 0.7370 - val_loss: 0.5591 - val_acc: 0.7333

Epoch 00018: val_loss improved from 0.56004 to 0.55905, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 8s - loss: 0.5588 - acc: 0.7382 - val_loss: 0.5539 - val_acc: 0.7324

Epoch 00019: val_loss improved from 0.55905 to 0.55386, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 8s - loss: 0.5545 - acc: 0.7390 - val_loss: 0.5536 - val_acc: 0.7374

Epoch 00020: val_loss improved from 0.55386 to 0.55358, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 8s - loss: 0.5524 - acc: 0.7420 - val_loss: 0.5503 - val_acc: 0.7457

Epoch 00021: val_loss improved from 0.55358 to 0.55027, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 8s - loss: 0.5503 - acc: 0.7423 - val_loss: 0.5471 - val_acc: 0.7425

Epoch 00022: val_loss improved from 0.55027 to 0.54711, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 8s - loss: 0.5469 - acc: 0.7450 - val_loss: 0.5540 - val_acc: 0.7378

Epoch 00023: val_loss did not improve from 0.54711
Epoch 24/40
 - 8s - loss: 0.5449 - acc: 0.7435 - val_loss: 0.5574 - val_acc: 0.7354

Epoch 00024: val_loss did not improve from 0.54711
Epoch 25/40
 - 8s - loss: 0.5424 - acc: 0.7460 - val_loss: 0.5427 - val_acc: 0.7439

Epoch 00025: val_loss improved from 0.54711 to 0.54267, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 8s - loss: 0.5399 - acc: 0.7467 - val_loss: 0.5367 - val_acc: 0.7452

Epoch 00026: val_loss improved from 0.54267 to 0.53667, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 8s - loss: 0.5382 - acc: 0.7490 - val_loss: 0.5391 - val_acc: 0.7441

Epoch 00027: val_loss did not improve from 0.53667
Epoch 28/40
 - 8s - loss: 0.5357 - acc: 0.7492 - val_loss: 0.5760 - val_acc: 0.7313

Epoch 00028: val_loss did not improve from 0.53667
Epoch 29/40
 - 8s - loss: 0.5331 - acc: 0.7530 - val_loss: 0.5319 - val_acc: 0.7465

Epoch 00029: val_loss improved from 0.53667 to 0.53186, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 30/40
 - 8s - loss: 0.5311 - acc: 0.7516 - val_loss: 0.5282 - val_acc: 0.7469

Epoch 00030: val_loss improved from 0.53186 to 0.52822, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 8s - loss: 0.5292 - acc: 0.7516 - val_loss: 0.5303 - val_acc: 0.7477

Epoch 00031: val_loss did not improve from 0.52822
Epoch 32/40
 - 8s - loss: 0.5296 - acc: 0.7490 - val_loss: 0.5331 - val_acc: 0.7386

Epoch 00032: val_loss did not improve from 0.52822
Epoch 33/40
 - 8s - loss: 0.5271 - acc: 0.7530 - val_loss: 0.5220 - val_acc: 0.7499

Epoch 00033: val_loss improved from 0.52822 to 0.52196, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 8s - loss: 0.5272 - acc: 0.7502 - val_loss: 0.5223 - val_acc: 0.7467

Epoch 00034: val_loss did not improve from 0.52196
Epoch 35/40
 - 8s - loss: 0.5239 - acc: 0.7519 - val_loss: 0.5196 - val_acc: 0.7496

Epoch 00035: val_loss improved from 0.52196 to 0.51958, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 8s - loss: 0.5239 - acc: 0.7517 - val_loss: 0.5430 - val_acc: 0.7444

Epoch 00036: val_loss did not improve from 0.51958
Epoch 37/40
 - 8s - loss: 0.5226 - acc: 0.7529 - val_loss: 0.5336 - val_acc: 0.7354

Epoch 00037: val_loss did not improve from 0.51958
Epoch 38/40
 - 8s - loss: 0.5210 - acc: 0.7536 - val_loss: 0.5163 - val_acc: 0.7520

Epoch 00038: val_loss improved from 0.51958 to 0.51629, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 8s - loss: 0.5193 - acc: 0.7533 - val_loss: 0.5147 - val_acc: 0.7529

Epoch 00039: val_loss improved from 0.51629 to 0.51469, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 8s - loss: 0.5184 - acc: 0.7542 - val_loss: 0.5190 - val_acc: 0.7459

Epoch 00040: val_loss did not improve from 0.51469

  32/7440 [..............................] - ETA: 0s
 448/7440 [>.............................] - ETA: 0s
 864/7440 [==>...........................] - ETA: 0s
1280/7440 [====>.........................] - ETA: 0s
1728/7440 [=====>........................] - ETA: 0s
2176/7440 [=======>......................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
3040/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4320/7440 [================>.............] - ETA: 0s
4768/7440 [==================>...........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 120us/step
current Test accuracy: 0.8244623655913978
current auc_score ------------------>  0.8679551826800787

  32/7440 [..............................] - ETA: 5:10
 480/7440 [>.............................] - ETA: 20s 
 960/7440 [==>...........................] - ETA: 9s 
1472/7440 [====>.........................] - ETA: 6s
1984/7440 [=======>......................] - ETA: 4s
2496/7440 [=========>....................] - ETA: 3s
3008/7440 [===========>..................] - ETA: 2s
3520/7440 [=============>................] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4544/7440 [=================>............] - ETA: 1s
5024/7440 [===================>..........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 284us/step
Best saved model Test accuracy: 0.8247311827956989
best saved model auc_score ------------------>  0.8734719693028095
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_79[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_80[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_81[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_82[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 36, 96, 96)   0           concatenate_31[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_83[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_84[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 46, 96, 96)   0           concatenate_32[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_85[0][0]              
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_86[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_87[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 33, 48, 48)   0           average_pooling2d_12[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_88[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_89[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 43, 48, 48)   0           concatenate_34[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_90[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_91[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 53, 48, 48)   0           concatenate_35[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 53, 48, 48)   212         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 53, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 53)           0           activation_92[0][0]              
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            54          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 32,144
Trainable params: 31,112
Non-trainable params: 1,032
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 38s - loss: 0.5896 - acc: 0.7458 - val_loss: 0.5440 - val_acc: 0.7647

Epoch 00001: val_loss improved from inf to 0.54405, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 32s - loss: 0.5320 - acc: 0.7736 - val_loss: 0.5115 - val_acc: 0.7836

Epoch 00002: val_loss improved from 0.54405 to 0.51150, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 32s - loss: 0.5082 - acc: 0.7836 - val_loss: 0.4930 - val_acc: 0.7910

Epoch 00003: val_loss improved from 0.51150 to 0.49302, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 32s - loss: 0.4937 - acc: 0.7882 - val_loss: 0.4750 - val_acc: 0.7942

Epoch 00004: val_loss improved from 0.49302 to 0.47504, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 32s - loss: 0.4803 - acc: 0.7946 - val_loss: 0.4673 - val_acc: 0.8045

Epoch 00005: val_loss improved from 0.47504 to 0.46727, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 32s - loss: 0.4694 - acc: 0.7996 - val_loss: 0.4535 - val_acc: 0.8077

Epoch 00006: val_loss improved from 0.46727 to 0.45353, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 32s - loss: 0.4605 - acc: 0.8030 - val_loss: 0.4526 - val_acc: 0.8071

Epoch 00007: val_loss improved from 0.45353 to 0.45258, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 32s - loss: 0.4521 - acc: 0.8070 - val_loss: 0.4573 - val_acc: 0.8041

Epoch 00008: val_loss did not improve from 0.45258
Epoch 9/40
 - 32s - loss: 0.4440 - acc: 0.8115 - val_loss: 0.4387 - val_acc: 0.8168

Epoch 00009: val_loss improved from 0.45258 to 0.43871, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 32s - loss: 0.4377 - acc: 0.8157 - val_loss: 0.4201 - val_acc: 0.8268

Epoch 00010: val_loss improved from 0.43871 to 0.42008, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 32s - loss: 0.4326 - acc: 0.8201 - val_loss: 0.4548 - val_acc: 0.7963

Epoch 00011: val_loss did not improve from 0.42008
Epoch 12/40
 - 32s - loss: 0.4242 - acc: 0.8241 - val_loss: 0.4104 - val_acc: 0.8337

Epoch 00012: val_loss improved from 0.42008 to 0.41039, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 32s - loss: 0.4200 - acc: 0.8273 - val_loss: 0.4287 - val_acc: 0.8245

Epoch 00013: val_loss did not improve from 0.41039
Epoch 14/40
 - 32s - loss: 0.4134 - acc: 0.8303 - val_loss: 0.4656 - val_acc: 0.7941

Epoch 00014: val_loss did not improve from 0.41039
Epoch 15/40
 - 32s - loss: 0.4113 - acc: 0.8294 - val_loss: 0.4545 - val_acc: 0.8081

Epoch 00015: val_loss did not improve from 0.41039
Epoch 16/40
 - 32s - loss: 0.4069 - acc: 0.8345 - val_loss: 0.4244 - val_acc: 0.8272

Epoch 00016: val_loss did not improve from 0.41039
Epoch 17/40
 - 32s - loss: 0.3982 - acc: 0.8377 - val_loss: 0.4543 - val_acc: 0.8159

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00017: val_loss did not improve from 0.41039
Epoch 18/40
 - 32s - loss: 0.3900 - acc: 0.8436 - val_loss: 0.3875 - val_acc: 0.8430

Epoch 00018: val_loss improved from 0.41039 to 0.38751, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 32s - loss: 0.3874 - acc: 0.8450 - val_loss: 0.3848 - val_acc: 0.8478

Epoch 00019: val_loss improved from 0.38751 to 0.38475, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 32s - loss: 0.3854 - acc: 0.8459 - val_loss: 0.3896 - val_acc: 0.8441

Epoch 00020: val_loss did not improve from 0.38475
Epoch 21/40
 - 32s - loss: 0.3866 - acc: 0.8438 - val_loss: 0.3768 - val_acc: 0.8505

Epoch 00021: val_loss improved from 0.38475 to 0.37677, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 32s - loss: 0.3858 - acc: 0.8467 - val_loss: 0.3785 - val_acc: 0.8494

Epoch 00022: val_loss did not improve from 0.37677
Epoch 23/40
 - 32s - loss: 0.3826 - acc: 0.8479 - val_loss: 0.3733 - val_acc: 0.8535

Epoch 00023: val_loss improved from 0.37677 to 0.37328, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 32s - loss: 0.3805 - acc: 0.8493 - val_loss: 0.3762 - val_acc: 0.8507

Epoch 00024: val_loss did not improve from 0.37328
Epoch 25/40
 - 32s - loss: 0.3791 - acc: 0.8481 - val_loss: 0.3987 - val_acc: 0.8400

Epoch 00025: val_loss did not improve from 0.37328
Epoch 26/40
 - 32s - loss: 0.3772 - acc: 0.8521 - val_loss: 0.3825 - val_acc: 0.8480

Epoch 00026: val_loss did not improve from 0.37328
Epoch 27/40
 - 32s - loss: 0.3764 - acc: 0.8521 - val_loss: 0.3842 - val_acc: 0.8464

Epoch 00027: val_loss did not improve from 0.37328
Epoch 28/40
 - 32s - loss: 0.3758 - acc: 0.8510 - val_loss: 0.3691 - val_acc: 0.8535

Epoch 00028: val_loss improved from 0.37328 to 0.36911, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 32s - loss: 0.3732 - acc: 0.8531 - val_loss: 0.3732 - val_acc: 0.8484

Epoch 00029: val_loss did not improve from 0.36911
Epoch 30/40
 - 33s - loss: 0.3718 - acc: 0.8558 - val_loss: 0.3699 - val_acc: 0.8593

Epoch 00030: val_loss did not improve from 0.36911
Epoch 31/40
 - 33s - loss: 0.3700 - acc: 0.8554 - val_loss: 0.3624 - val_acc: 0.8598

Epoch 00031: val_loss improved from 0.36911 to 0.36237, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 32s - loss: 0.3685 - acc: 0.8541 - val_loss: 0.3739 - val_acc: 0.8530

Epoch 00032: val_loss did not improve from 0.36237
Epoch 33/40
 - 32s - loss: 0.3676 - acc: 0.8551 - val_loss: 0.3606 - val_acc: 0.8563

Epoch 00033: val_loss improved from 0.36237 to 0.36061, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 32s - loss: 0.3660 - acc: 0.8566 - val_loss: 0.3614 - val_acc: 0.8587

Epoch 00034: val_loss did not improve from 0.36061
Epoch 35/40
 - 32s - loss: 0.3642 - acc: 0.8584 - val_loss: 0.3704 - val_acc: 0.8572

Epoch 00035: val_loss did not improve from 0.36061
Epoch 36/40
 - 32s - loss: 0.3644 - acc: 0.8582 - val_loss: 0.3736 - val_acc: 0.8520

Epoch 00036: val_loss did not improve from 0.36061
Epoch 37/40
 - 32s - loss: 0.3600 - acc: 0.8600 - val_loss: 0.3820 - val_acc: 0.8465

Epoch 00037: val_loss did not improve from 0.36061
Epoch 38/40
 - 32s - loss: 0.3624 - acc: 0.8593 - val_loss: 0.3595 - val_acc: 0.8592

Epoch 00038: val_loss improved from 0.36061 to 0.35947, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 32s - loss: 0.3605 - acc: 0.8605 - val_loss: 0.3543 - val_acc: 0.8597

Epoch 00039: val_loss improved from 0.35947 to 0.35426, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 32s - loss: 0.3586 - acc: 0.8609 - val_loss: 0.3549 - val_acc: 0.8619

Epoch 00040: val_loss did not improve from 0.35426

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 339us/step
current Test accuracy: 0.7794354838709677
current auc_score ------------------>  0.860533659960689

  32/7440 [..............................] - ETA: 6:41
 192/7440 [..............................] - ETA: 1:07
 352/7440 [>.............................] - ETA: 37s 
 512/7440 [=>............................] - ETA: 25s
 672/7440 [=>............................] - ETA: 19s
 832/7440 [==>...........................] - ETA: 16s
 992/7440 [===>..........................] - ETA: 13s
1152/7440 [===>..........................] - ETA: 11s
1312/7440 [====>.........................] - ETA: 10s
1472/7440 [====>.........................] - ETA: 9s 
1632/7440 [=====>........................] - ETA: 8s
1792/7440 [======>.......................] - ETA: 7s
1952/7440 [======>.......................] - ETA: 6s
2112/7440 [=======>......................] - ETA: 6s
2272/7440 [========>.....................] - ETA: 5s
2432/7440 [========>.....................] - ETA: 5s
2592/7440 [=========>....................] - ETA: 4s
2752/7440 [==========>...................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3552/7440 [=============>................] - ETA: 3s
3712/7440 [=============>................] - ETA: 3s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 2s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 580us/step
Best saved model Test accuracy: 0.7692204301075268
best saved model auc_score ------------------>  0.8576685165915134
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_93[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_94[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_95[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_96[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 52, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_97[0][0]              
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   1872        activation_98[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_99[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 44, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 44, 48, 48)   176         concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 44, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3168        activation_100[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_101[0][0]             
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 62, 48, 48)   0           concatenate_39[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 62, 48, 48)   248         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 62, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 62)           0           activation_102[0][0]             
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            63          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 59,087
Trainable params: 58,043
Non-trainable params: 1,044
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 37s - loss: 0.6395 - acc: 0.7233 - val_loss: 0.5777 - val_acc: 0.7629

Epoch 00001: val_loss improved from inf to 0.57767, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 32s - loss: 0.5547 - acc: 0.7730 - val_loss: 0.6630 - val_acc: 0.7061

Epoch 00002: val_loss did not improve from 0.57767
Epoch 3/40
 - 32s - loss: 0.5268 - acc: 0.7815 - val_loss: 0.5253 - val_acc: 0.7861

Epoch 00003: val_loss improved from 0.57767 to 0.52533, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 32s - loss: 0.5062 - acc: 0.7880 - val_loss: 0.5359 - val_acc: 0.7654

Epoch 00004: val_loss did not improve from 0.52533
Epoch 5/40
 - 32s - loss: 0.4935 - acc: 0.7936 - val_loss: 0.5017 - val_acc: 0.7905

Epoch 00005: val_loss improved from 0.52533 to 0.50168, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 32s - loss: 0.4808 - acc: 0.7981 - val_loss: 0.5388 - val_acc: 0.7806

Epoch 00006: val_loss did not improve from 0.50168
Epoch 7/40
 - 32s - loss: 0.4720 - acc: 0.8038 - val_loss: 0.5347 - val_acc: 0.7647

Epoch 00007: val_loss did not improve from 0.50168
Epoch 8/40
 - 32s - loss: 0.4630 - acc: 0.8080 - val_loss: 0.5248 - val_acc: 0.7855

Epoch 00008: val_loss did not improve from 0.50168
Epoch 9/40
 - 32s - loss: 0.4554 - acc: 0.8112 - val_loss: 0.4713 - val_acc: 0.8021

Epoch 00009: val_loss improved from 0.50168 to 0.47135, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 32s - loss: 0.4463 - acc: 0.8195 - val_loss: 0.4399 - val_acc: 0.8262

Epoch 00010: val_loss improved from 0.47135 to 0.43994, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 32s - loss: 0.4397 - acc: 0.8194 - val_loss: 0.4375 - val_acc: 0.8202

Epoch 00011: val_loss improved from 0.43994 to 0.43751, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 32s - loss: 0.4317 - acc: 0.8255 - val_loss: 0.5641 - val_acc: 0.7642

Epoch 00012: val_loss did not improve from 0.43751
Epoch 13/40
 - 32s - loss: 0.4271 - acc: 0.8283 - val_loss: 0.4504 - val_acc: 0.8114

Epoch 00013: val_loss did not improve from 0.43751
Epoch 14/40
 - 32s - loss: 0.4211 - acc: 0.8305 - val_loss: 0.4904 - val_acc: 0.8058

Epoch 00014: val_loss did not improve from 0.43751
Epoch 15/40
 - 32s - loss: 0.4158 - acc: 0.8302 - val_loss: 0.4249 - val_acc: 0.8264

Epoch 00015: val_loss improved from 0.43751 to 0.42491, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 32s - loss: 0.4126 - acc: 0.8348 - val_loss: 0.4738 - val_acc: 0.8107

Epoch 00016: val_loss did not improve from 0.42491
Epoch 17/40
 - 32s - loss: 0.4045 - acc: 0.8397 - val_loss: 0.3951 - val_acc: 0.8434

Epoch 00017: val_loss improved from 0.42491 to 0.39509, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 32s - loss: 0.3996 - acc: 0.8404 - val_loss: 0.3906 - val_acc: 0.8523

Epoch 00018: val_loss improved from 0.39509 to 0.39058, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 32s - loss: 0.3948 - acc: 0.8447 - val_loss: 0.3953 - val_acc: 0.8503

Epoch 00019: val_loss did not improve from 0.39058
Epoch 20/40
 - 32s - loss: 0.3903 - acc: 0.8479 - val_loss: 0.5509 - val_acc: 0.7764

Epoch 00020: val_loss did not improve from 0.39058
Epoch 21/40
 - 32s - loss: 0.3867 - acc: 0.8482 - val_loss: 0.3987 - val_acc: 0.8429

Epoch 00021: val_loss did not improve from 0.39058
Epoch 22/40
 - 32s - loss: 0.3849 - acc: 0.8508 - val_loss: 0.3899 - val_acc: 0.8469

Epoch 00022: val_loss improved from 0.39058 to 0.38989, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 32s - loss: 0.3821 - acc: 0.8509 - val_loss: 0.3804 - val_acc: 0.8612

Epoch 00023: val_loss improved from 0.38989 to 0.38040, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 32s - loss: 0.3792 - acc: 0.8521 - val_loss: 0.3778 - val_acc: 0.8630

Epoch 00024: val_loss improved from 0.38040 to 0.37784, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 32s - loss: 0.3747 - acc: 0.8538 - val_loss: 0.3922 - val_acc: 0.8473

Epoch 00025: val_loss did not improve from 0.37784
Epoch 26/40
 - 32s - loss: 0.3714 - acc: 0.8572 - val_loss: 0.3676 - val_acc: 0.8614

Epoch 00026: val_loss improved from 0.37784 to 0.36760, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 32s - loss: 0.3690 - acc: 0.8582 - val_loss: 0.3710 - val_acc: 0.8606

Epoch 00027: val_loss did not improve from 0.36760
Epoch 28/40
 - 32s - loss: 0.3621 - acc: 0.8617 - val_loss: 0.4791 - val_acc: 0.8159

Epoch 00028: val_loss did not improve from 0.36760
Epoch 29/40
 - 32s - loss: 0.3633 - acc: 0.8599 - val_loss: 0.3675 - val_acc: 0.8665

Epoch 00029: val_loss improved from 0.36760 to 0.36753, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 30/40
 - 32s - loss: 0.3559 - acc: 0.8653 - val_loss: 0.4249 - val_acc: 0.8399

Epoch 00030: val_loss did not improve from 0.36753
Epoch 31/40
 - 32s - loss: 0.3571 - acc: 0.8635 - val_loss: 0.4245 - val_acc: 0.8419

Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00031: val_loss did not improve from 0.36753
Epoch 32/40
 - 32s - loss: 0.3409 - acc: 0.8735 - val_loss: 0.3524 - val_acc: 0.8696

Epoch 00032: val_loss improved from 0.36753 to 0.35236, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 32s - loss: 0.3370 - acc: 0.8754 - val_loss: 0.3392 - val_acc: 0.8742

Epoch 00033: val_loss improved from 0.35236 to 0.33925, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 32s - loss: 0.3367 - acc: 0.8758 - val_loss: 0.3488 - val_acc: 0.8658

Epoch 00034: val_loss did not improve from 0.33925
Epoch 35/40
 - 32s - loss: 0.3349 - acc: 0.8772 - val_loss: 0.3363 - val_acc: 0.8769

Epoch 00035: val_loss improved from 0.33925 to 0.33632, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 32s - loss: 0.3331 - acc: 0.8773 - val_loss: 0.3541 - val_acc: 0.8712

Epoch 00036: val_loss did not improve from 0.33632
Epoch 37/40
 - 32s - loss: 0.3313 - acc: 0.8784 - val_loss: 0.3674 - val_acc: 0.8583

Epoch 00037: val_loss did not improve from 0.33632
Epoch 38/40
 - 32s - loss: 0.3324 - acc: 0.8800 - val_loss: 0.3342 - val_acc: 0.8773

Epoch 00038: val_loss improved from 0.33632 to 0.33423, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 32s - loss: 0.3292 - acc: 0.8809 - val_loss: 0.3430 - val_acc: 0.8749

Epoch 00039: val_loss did not improve from 0.33423
Epoch 40/40
 - 32s - loss: 0.3288 - acc: 0.8784 - val_loss: 0.3332 - val_acc: 0.8780

Epoch 00040: val_loss improved from 0.33423 to 0.33320, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 336us/step
current Test accuracy: 0.8018817204301075
current auc_score ------------------>  0.8801108148340848

  32/7440 [..............................] - ETA: 7:18
 192/7440 [..............................] - ETA: 1:13
 352/7440 [>.............................] - ETA: 40s 
 512/7440 [=>............................] - ETA: 27s
 672/7440 [=>............................] - ETA: 21s
 832/7440 [==>...........................] - ETA: 17s
 992/7440 [===>..........................] - ETA: 14s
1152/7440 [===>..........................] - ETA: 12s
1312/7440 [====>.........................] - ETA: 10s
1472/7440 [====>.........................] - ETA: 9s 
1632/7440 [=====>........................] - ETA: 8s
1792/7440 [======>.......................] - ETA: 7s
1952/7440 [======>.......................] - ETA: 7s
2112/7440 [=======>......................] - ETA: 6s
2272/7440 [========>.....................] - ETA: 6s
2432/7440 [========>.....................] - ETA: 5s
2592/7440 [=========>....................] - ETA: 5s
2752/7440 [==========>...................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 4s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3552/7440 [=============>................] - ETA: 3s
3712/7440 [=============>................] - ETA: 3s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 2s
4672/7440 [=================>............] - ETA: 2s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 604us/step
Best saved model Test accuracy: 0.8018817204301075
best saved model auc_score ------------------>  0.8801108148340848
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_103[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_104[0][0]             
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_105[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_106[0][0]             
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 28, 96, 96)   0           concatenate_41[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_107[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_108[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 34, 96, 96)   0           concatenate_42[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 34, 96, 96)   136         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 34, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 17, 96, 96)   578         activation_109[0][0]             
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 17, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 17, 48, 48)   68          average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 17, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   408         activation_110[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_111[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_14[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 23, 48, 48)   92          concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 23, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   552         activation_112[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_113[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 29, 48, 48)   0           concatenate_44[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 29, 48, 48)   116         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 29, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 24, 48, 48)   696         activation_114[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 24, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_115[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 35, 48, 48)   0           concatenate_45[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 35, 48, 48)   140         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 35, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 17, 48, 48)   595         activation_116[0][0]             
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 17, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 17, 24, 24)   68          average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 17, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 24, 24, 24)   408         activation_117[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 24, 24, 24)   96          dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 24, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 6, 24, 24)    1296        activation_118[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 23, 24, 24)   0           average_pooling2d_15[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 23, 24, 24)   92          concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 23, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 24, 24, 24)   552         activation_119[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 24, 24, 24)   96          dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 24, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 6, 24, 24)    1296        activation_120[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 29, 24, 24)   0           concatenate_47[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 29, 24, 24)   116         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 29, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 24, 24, 24)   696         activation_121[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 24, 24, 24)   96          dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 24, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 6, 24, 24)    1296        activation_122[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 35, 24, 24)   0           concatenate_48[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 35, 24, 24)   140         concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 35, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 35)           0           activation_123[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            36          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 20,153
Trainable params: 19,105
Non-trainable params: 1,048
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 38s - loss: 0.6173 - acc: 0.7192 - val_loss: 0.5451 - val_acc: 0.7656

Epoch 00001: val_loss improved from inf to 0.54513, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 30s - loss: 0.5343 - acc: 0.7706 - val_loss: 0.5137 - val_acc: 0.7774

Epoch 00002: val_loss improved from 0.54513 to 0.51372, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 30s - loss: 0.4981 - acc: 0.7883 - val_loss: 0.4789 - val_acc: 0.7908

Epoch 00003: val_loss improved from 0.51372 to 0.47895, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 31s - loss: 0.4741 - acc: 0.7980 - val_loss: 0.4620 - val_acc: 0.7976

Epoch 00004: val_loss improved from 0.47895 to 0.46199, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 31s - loss: 0.4576 - acc: 0.8050 - val_loss: 0.4401 - val_acc: 0.8161

Epoch 00005: val_loss improved from 0.46199 to 0.44013, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 31s - loss: 0.4465 - acc: 0.8096 - val_loss: 0.4300 - val_acc: 0.8218

Epoch 00006: val_loss improved from 0.44013 to 0.43002, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 31s - loss: 0.4333 - acc: 0.8176 - val_loss: 0.4177 - val_acc: 0.8268

Epoch 00007: val_loss improved from 0.43002 to 0.41766, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 31s - loss: 0.4217 - acc: 0.8254 - val_loss: 0.4186 - val_acc: 0.8306

Epoch 00008: val_loss did not improve from 0.41766
Epoch 9/40
 - 30s - loss: 0.4121 - acc: 0.8318 - val_loss: 0.3976 - val_acc: 0.8384

Epoch 00009: val_loss improved from 0.41766 to 0.39762, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 30s - loss: 0.4056 - acc: 0.8339 - val_loss: 0.3884 - val_acc: 0.8475

Epoch 00010: val_loss improved from 0.39762 to 0.38835, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 31s - loss: 0.3990 - acc: 0.8370 - val_loss: 0.3983 - val_acc: 0.8426

Epoch 00011: val_loss did not improve from 0.38835
Epoch 12/40
 - 30s - loss: 0.3910 - acc: 0.8411 - val_loss: 0.4083 - val_acc: 0.8370

Epoch 00012: val_loss did not improve from 0.38835
Epoch 13/40
 - 31s - loss: 0.3850 - acc: 0.8454 - val_loss: 0.4229 - val_acc: 0.8332

Epoch 00013: val_loss did not improve from 0.38835
Epoch 14/40
 - 30s - loss: 0.3813 - acc: 0.8468 - val_loss: 0.3981 - val_acc: 0.8444

Epoch 00014: val_loss did not improve from 0.38835
Epoch 15/40
 - 30s - loss: 0.3779 - acc: 0.8471 - val_loss: 0.4024 - val_acc: 0.8417

Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00015: val_loss did not improve from 0.38835
Epoch 16/40
 - 30s - loss: 0.3640 - acc: 0.8591 - val_loss: 0.3577 - val_acc: 0.8642

Epoch 00016: val_loss improved from 0.38835 to 0.35774, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 31s - loss: 0.3621 - acc: 0.8579 - val_loss: 0.3534 - val_acc: 0.8638

Epoch 00017: val_loss improved from 0.35774 to 0.35341, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 31s - loss: 0.3587 - acc: 0.8586 - val_loss: 0.3549 - val_acc: 0.8655

Epoch 00018: val_loss did not improve from 0.35341
Epoch 19/40
 - 31s - loss: 0.3589 - acc: 0.8597 - val_loss: 0.3570 - val_acc: 0.8675

Epoch 00019: val_loss did not improve from 0.35341
Epoch 20/40
 - 31s - loss: 0.3537 - acc: 0.8635 - val_loss: 0.3646 - val_acc: 0.8658

Epoch 00020: val_loss did not improve from 0.35341
Epoch 21/40
 - 30s - loss: 0.3545 - acc: 0.8618 - val_loss: 0.3493 - val_acc: 0.8741

Epoch 00021: val_loss improved from 0.35341 to 0.34933, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 31s - loss: 0.3519 - acc: 0.8626 - val_loss: 0.3493 - val_acc: 0.8706

Epoch 00022: val_loss improved from 0.34933 to 0.34927, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 31s - loss: 0.3513 - acc: 0.8628 - val_loss: 0.3467 - val_acc: 0.8700

Epoch 00023: val_loss improved from 0.34927 to 0.34670, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 30s - loss: 0.3510 - acc: 0.8646 - val_loss: 0.3462 - val_acc: 0.8732

Epoch 00024: val_loss improved from 0.34670 to 0.34621, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 31s - loss: 0.3471 - acc: 0.8665 - val_loss: 0.3439 - val_acc: 0.8724

Epoch 00025: val_loss improved from 0.34621 to 0.34391, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 30s - loss: 0.3448 - acc: 0.8667 - val_loss: 0.3418 - val_acc: 0.8716

Epoch 00026: val_loss improved from 0.34391 to 0.34181, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 30s - loss: 0.3442 - acc: 0.8674 - val_loss: 0.3369 - val_acc: 0.8763

Epoch 00027: val_loss improved from 0.34181 to 0.33685, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 31s - loss: 0.3447 - acc: 0.8684 - val_loss: 0.3368 - val_acc: 0.8754

Epoch 00028: val_loss improved from 0.33685 to 0.33676, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 30s - loss: 0.3396 - acc: 0.8707 - val_loss: 0.3402 - val_acc: 0.8745

Epoch 00029: val_loss did not improve from 0.33676
Epoch 30/40
 - 30s - loss: 0.3373 - acc: 0.8688 - val_loss: 0.3388 - val_acc: 0.8745

Epoch 00030: val_loss did not improve from 0.33676
Epoch 31/40
 - 30s - loss: 0.3395 - acc: 0.8693 - val_loss: 0.3356 - val_acc: 0.8760

Epoch 00031: val_loss improved from 0.33676 to 0.33563, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 30s - loss: 0.3377 - acc: 0.8706 - val_loss: 0.3315 - val_acc: 0.8770

Epoch 00032: val_loss improved from 0.33563 to 0.33149, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 30s - loss: 0.3361 - acc: 0.8722 - val_loss: 0.3305 - val_acc: 0.8794

Epoch 00033: val_loss improved from 0.33149 to 0.33053, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 31s - loss: 0.3332 - acc: 0.8727 - val_loss: 0.3324 - val_acc: 0.8780

Epoch 00034: val_loss did not improve from 0.33053
Epoch 35/40
 - 31s - loss: 0.3343 - acc: 0.8726 - val_loss: 0.3309 - val_acc: 0.8770

Epoch 00035: val_loss did not improve from 0.33053
Epoch 36/40
 - 31s - loss: 0.3331 - acc: 0.8742 - val_loss: 0.3484 - val_acc: 0.8755

Epoch 00036: val_loss did not improve from 0.33053
Epoch 37/40
 - 31s - loss: 0.3295 - acc: 0.8747 - val_loss: 0.3326 - val_acc: 0.8742

Epoch 00037: val_loss did not improve from 0.33053
Epoch 38/40
 - 31s - loss: 0.3285 - acc: 0.8755 - val_loss: 0.3287 - val_acc: 0.8804

Epoch 00038: val_loss improved from 0.33053 to 0.32866, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 31s - loss: 0.3267 - acc: 0.8775 - val_loss: 0.3380 - val_acc: 0.8749

Epoch 00039: val_loss did not improve from 0.32866
Epoch 40/40
 - 30s - loss: 0.3240 - acc: 0.8794 - val_loss: 0.3234 - val_acc: 0.8824

Epoch 00040: val_loss improved from 0.32866 to 0.32340, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 306us/step
current Test accuracy: 0.8341397849462365
current auc_score ------------------>  0.9143824791883456

  32/7440 [..............................] - ETA: 10:10
 192/7440 [..............................] - ETA: 1:41 
 352/7440 [>.............................] - ETA: 55s 
 512/7440 [=>............................] - ETA: 37s
 672/7440 [=>............................] - ETA: 28s
 832/7440 [==>...........................] - ETA: 22s
1024/7440 [===>..........................] - ETA: 18s
1216/7440 [===>..........................] - ETA: 15s
1408/7440 [====>.........................] - ETA: 13s
1600/7440 [=====>........................] - ETA: 11s
1792/7440 [======>.......................] - ETA: 10s
1984/7440 [=======>......................] - ETA: 8s 
2176/7440 [=======>......................] - ETA: 8s
2368/7440 [========>.....................] - ETA: 7s
2560/7440 [=========>....................] - ETA: 6s
2752/7440 [==========>...................] - ETA: 5s
2912/7440 [==========>...................] - ETA: 5s
3104/7440 [===========>..................] - ETA: 5s
3296/7440 [============>.................] - ETA: 4s
3456/7440 [============>.................] - ETA: 4s
3648/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 3s
4224/7440 [================>.............] - ETA: 3s
4416/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4800/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 665us/step
Best saved model Test accuracy: 0.8341397849462365
best saved model auc_score ------------------>  0.9143824791883456
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_11[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_124[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_125[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_126[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_127[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 36, 96, 96)   0           concatenate_50[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_128[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_129[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 46, 96, 96)   0           concatenate_51[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_130[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_131[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_132[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 33, 48, 48)   0           average_pooling2d_16[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_133[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_134[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 43, 48, 48)   0           concatenate_53[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_135[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_136[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 53, 48, 48)   0           concatenate_54[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 53, 48, 48)   212         concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 53, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 26, 48, 48)   1378        activation_137[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 26, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 26, 24, 24)   104         average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 26, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   1040        activation_138[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_139[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 36, 24, 24)   0           average_pooling2d_17[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 36, 24, 24)   144         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 36, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 40, 24, 24)   1440        activation_140[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 40, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_141[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 46, 24, 24)   0           concatenate_56[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 46, 24, 24)   184         concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 46, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 40, 24, 24)   1840        activation_142[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 40, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_143[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 56, 24, 24)   0           concatenate_57[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 56, 24, 24)   224         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 56, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_11 (Gl (None, 56)           0           activation_144[0][0]             
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            57          global_average_pooling2d_11[0][0]
==================================================================================================
Total params: 49,781
Trainable params: 48,181
Non-trainable params: 1,600
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 47s - loss: 0.5993 - acc: 0.7517 - val_loss: 0.5405 - val_acc: 0.7894

Epoch 00001: val_loss improved from inf to 0.54049, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 38s - loss: 0.5232 - acc: 0.7916 - val_loss: 0.5031 - val_acc: 0.8026

Epoch 00002: val_loss improved from 0.54049 to 0.50305, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 38s - loss: 0.4886 - acc: 0.8103 - val_loss: 0.5070 - val_acc: 0.8016

Epoch 00003: val_loss did not improve from 0.50305
Epoch 4/40
 - 38s - loss: 0.4645 - acc: 0.8206 - val_loss: 0.4680 - val_acc: 0.8179

Epoch 00004: val_loss improved from 0.50305 to 0.46801, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 38s - loss: 0.4429 - acc: 0.8318 - val_loss: 0.4508 - val_acc: 0.8287

Epoch 00005: val_loss improved from 0.46801 to 0.45082, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 38s - loss: 0.4254 - acc: 0.8432 - val_loss: 0.4216 - val_acc: 0.8382

Epoch 00006: val_loss improved from 0.45082 to 0.42156, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 38s - loss: 0.4086 - acc: 0.8505 - val_loss: 0.3998 - val_acc: 0.8529

Epoch 00007: val_loss improved from 0.42156 to 0.39977, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 38s - loss: 0.3950 - acc: 0.8584 - val_loss: 0.4079 - val_acc: 0.8445

Epoch 00008: val_loss did not improve from 0.39977
Epoch 9/40
 - 38s - loss: 0.3847 - acc: 0.8644 - val_loss: 0.4681 - val_acc: 0.8360

Epoch 00009: val_loss did not improve from 0.39977
Epoch 10/40
 - 38s - loss: 0.3740 - acc: 0.8723 - val_loss: 0.3675 - val_acc: 0.8711

Epoch 00010: val_loss improved from 0.39977 to 0.36748, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 38s - loss: 0.3637 - acc: 0.8747 - val_loss: 0.3662 - val_acc: 0.8722

Epoch 00011: val_loss improved from 0.36748 to 0.36620, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 38s - loss: 0.3533 - acc: 0.8800 - val_loss: 0.3481 - val_acc: 0.8829

Epoch 00012: val_loss improved from 0.36620 to 0.34807, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 38s - loss: 0.3430 - acc: 0.8858 - val_loss: 0.5565 - val_acc: 0.8307

Epoch 00013: val_loss did not improve from 0.34807
Epoch 14/40
 - 38s - loss: 0.3379 - acc: 0.8870 - val_loss: 0.3452 - val_acc: 0.8876

Epoch 00014: val_loss improved from 0.34807 to 0.34524, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 38s - loss: 0.3278 - acc: 0.8918 - val_loss: 0.3476 - val_acc: 0.8809

Epoch 00015: val_loss did not improve from 0.34524
Epoch 16/40
 - 38s - loss: 0.3200 - acc: 0.8975 - val_loss: 0.3265 - val_acc: 0.8903

Epoch 00016: val_loss improved from 0.34524 to 0.32649, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 38s - loss: 0.3135 - acc: 0.8994 - val_loss: 0.3371 - val_acc: 0.8908

Epoch 00017: val_loss did not improve from 0.32649
Epoch 18/40
 - 38s - loss: 0.3077 - acc: 0.9006 - val_loss: 0.3710 - val_acc: 0.8740

Epoch 00018: val_loss did not improve from 0.32649
Epoch 19/40
 - 37s - loss: 0.2992 - acc: 0.9046 - val_loss: 0.3158 - val_acc: 0.8987

Epoch 00019: val_loss improved from 0.32649 to 0.31578, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 38s - loss: 0.2932 - acc: 0.9071 - val_loss: 0.3220 - val_acc: 0.8878

Epoch 00020: val_loss did not improve from 0.31578
Epoch 21/40
 - 37s - loss: 0.2868 - acc: 0.9123 - val_loss: 0.3372 - val_acc: 0.8827

Epoch 00021: val_loss did not improve from 0.31578
Epoch 22/40
 - 38s - loss: 0.2865 - acc: 0.9103 - val_loss: 0.3161 - val_acc: 0.8946

Epoch 00022: val_loss did not improve from 0.31578
Epoch 23/40
 - 38s - loss: 0.2772 - acc: 0.9160 - val_loss: 0.3460 - val_acc: 0.8872

Epoch 00023: val_loss did not improve from 0.31578
Epoch 24/40
 - 37s - loss: 0.2712 - acc: 0.9187 - val_loss: 0.3433 - val_acc: 0.8758

Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00024: val_loss did not improve from 0.31578
Epoch 25/40
 - 38s - loss: 0.2525 - acc: 0.9284 - val_loss: 0.2830 - val_acc: 0.9110

Epoch 00025: val_loss improved from 0.31578 to 0.28304, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 38s - loss: 0.2481 - acc: 0.9287 - val_loss: 0.2824 - val_acc: 0.9123

Epoch 00026: val_loss improved from 0.28304 to 0.28241, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 37s - loss: 0.2458 - acc: 0.9298 - val_loss: 0.2906 - val_acc: 0.9057

Epoch 00027: val_loss did not improve from 0.28241
Epoch 28/40
 - 37s - loss: 0.2461 - acc: 0.9302 - val_loss: 0.2835 - val_acc: 0.9116

Epoch 00028: val_loss did not improve from 0.28241
Epoch 29/40
 - 37s - loss: 0.2424 - acc: 0.9322 - val_loss: 0.2786 - val_acc: 0.9114

Epoch 00029: val_loss improved from 0.28241 to 0.27863, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 30/40
 - 37s - loss: 0.2423 - acc: 0.9319 - val_loss: 0.2876 - val_acc: 0.9051

Epoch 00030: val_loss did not improve from 0.27863
Epoch 31/40
 - 38s - loss: 0.2381 - acc: 0.9357 - val_loss: 0.2785 - val_acc: 0.9132

Epoch 00031: val_loss improved from 0.27863 to 0.27853, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 38s - loss: 0.2386 - acc: 0.9347 - val_loss: 0.2750 - val_acc: 0.9162

Epoch 00032: val_loss improved from 0.27853 to 0.27495, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 38s - loss: 0.2338 - acc: 0.9355 - val_loss: 0.3043 - val_acc: 0.8972

Epoch 00033: val_loss did not improve from 0.27495
Epoch 34/40
 - 39s - loss: 0.2328 - acc: 0.9366 - val_loss: 0.2781 - val_acc: 0.9124

Epoch 00034: val_loss did not improve from 0.27495
Epoch 35/40
 - 38s - loss: 0.2315 - acc: 0.9372 - val_loss: 0.2781 - val_acc: 0.9115

Epoch 00035: val_loss did not improve from 0.27495
Epoch 36/40
 - 38s - loss: 0.2316 - acc: 0.9380 - val_loss: 0.2770 - val_acc: 0.9152

Epoch 00036: val_loss did not improve from 0.27495
Epoch 37/40
 - 37s - loss: 0.2292 - acc: 0.9383 - val_loss: 0.2840 - val_acc: 0.9100

Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.

Epoch 00037: val_loss did not improve from 0.27495
Epoch 38/40
 - 38s - loss: 0.2224 - acc: 0.9414 - val_loss: 0.2676 - val_acc: 0.9180

Epoch 00038: val_loss improved from 0.27495 to 0.26762, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 37s - loss: 0.2231 - acc: 0.9408 - val_loss: 0.2692 - val_acc: 0.9162

Epoch 00039: val_loss did not improve from 0.26762
Epoch 40/40
 - 38s - loss: 0.2203 - acc: 0.9423 - val_loss: 0.2687 - val_acc: 0.9189

Epoch 00040: val_loss did not improve from 0.26762

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 960/7440 [==>...........................] - ETA: 2s
1120/7440 [===>..........................] - ETA: 2s
1280/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1600/7440 [=====>........................] - ETA: 2s
1760/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2240/7440 [========>.....................] - ETA: 2s
2400/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3040/7440 [===========>..................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 1s
4576/7440 [=================>............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4864/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 388us/step
current Test accuracy: 0.8143817204301075
current auc_score ------------------>  0.9030581570123714

  32/7440 [..............................] - ETA: 12:10
 160/7440 [..............................] - ETA: 2:26 
 320/7440 [>.............................] - ETA: 1:12
 480/7440 [>.............................] - ETA: 48s 
 640/7440 [=>............................] - ETA: 36s
 800/7440 [==>...........................] - ETA: 28s
 960/7440 [==>...........................] - ETA: 23s
1120/7440 [===>..........................] - ETA: 20s
1280/7440 [====>.........................] - ETA: 17s
1440/7440 [====>.........................] - ETA: 15s
1600/7440 [=====>........................] - ETA: 13s
1760/7440 [======>.......................] - ETA: 12s
1920/7440 [======>.......................] - ETA: 11s
2080/7440 [=======>......................] - ETA: 10s
2240/7440 [========>.....................] - ETA: 9s 
2400/7440 [========>.....................] - ETA: 8s
2560/7440 [=========>....................] - ETA: 7s
2720/7440 [=========>....................] - ETA: 7s
2880/7440 [==========>...................] - ETA: 6s
3040/7440 [===========>..................] - ETA: 6s
3200/7440 [===========>..................] - ETA: 5s
3360/7440 [============>.................] - ETA: 5s
3520/7440 [=============>................] - ETA: 4s
3680/7440 [=============>................] - ETA: 4s
3840/7440 [==============>...............] - ETA: 4s
4000/7440 [===============>..............] - ETA: 4s
4128/7440 [===============>..............] - ETA: 3s
4288/7440 [================>.............] - ETA: 3s
4448/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 3s
4768/7440 [==================>...........] - ETA: 2s
4928/7440 [==================>...........] - ETA: 2s
5088/7440 [===================>..........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5728/7440 [======================>.......] - ETA: 1s
5888/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6208/7440 [========================>.....] - ETA: 1s
6368/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 803us/step
Best saved model Test accuracy: 0.8102150537634408
best saved model auc_score ------------------>  0.9030094519597642
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_145 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_18 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_146 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_19 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_147 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_12  (None, 4)                 0         
_________________________________________________________________
dense_12 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 17s - loss: 0.6937 - acc: 0.5214 - val_loss: 0.6822 - val_acc: 0.5415

Epoch 00001: val_loss improved from inf to 0.68224, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 9s - loss: 0.6691 - acc: 0.6212 - val_loss: 0.6531 - val_acc: 0.6453

Epoch 00002: val_loss improved from 0.68224 to 0.65309, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 9s - loss: 0.6429 - acc: 0.7014 - val_loss: 0.6329 - val_acc: 0.7053

Epoch 00003: val_loss improved from 0.65309 to 0.63294, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 9s - loss: 0.6275 - acc: 0.7134 - val_loss: 0.6211 - val_acc: 0.7141

Epoch 00004: val_loss improved from 0.63294 to 0.62105, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 9s - loss: 0.6169 - acc: 0.7152 - val_loss: 0.6113 - val_acc: 0.7177

Epoch 00005: val_loss improved from 0.62105 to 0.61131, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 9s - loss: 0.6069 - acc: 0.7168 - val_loss: 0.6038 - val_acc: 0.7191

Epoch 00006: val_loss improved from 0.61131 to 0.60377, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 9s - loss: 0.5991 - acc: 0.7194 - val_loss: 0.5941 - val_acc: 0.7239

Epoch 00007: val_loss improved from 0.60377 to 0.59407, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 9s - loss: 0.5905 - acc: 0.7231 - val_loss: 0.5862 - val_acc: 0.7265

Epoch 00008: val_loss improved from 0.59407 to 0.58619, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 9s - loss: 0.5836 - acc: 0.7265 - val_loss: 0.5822 - val_acc: 0.7302

Epoch 00009: val_loss improved from 0.58619 to 0.58218, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 9s - loss: 0.5773 - acc: 0.7304 - val_loss: 0.5746 - val_acc: 0.7351

Epoch 00010: val_loss improved from 0.58218 to 0.57457, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 9s - loss: 0.5720 - acc: 0.7321 - val_loss: 0.5698 - val_acc: 0.7357

Epoch 00011: val_loss improved from 0.57457 to 0.56978, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 9s - loss: 0.5661 - acc: 0.7367 - val_loss: 0.5646 - val_acc: 0.7358

Epoch 00012: val_loss improved from 0.56978 to 0.56463, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 9s - loss: 0.5612 - acc: 0.7374 - val_loss: 0.5593 - val_acc: 0.7342

Epoch 00013: val_loss improved from 0.56463 to 0.55930, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 9s - loss: 0.5571 - acc: 0.7368 - val_loss: 0.5558 - val_acc: 0.7356

Epoch 00014: val_loss improved from 0.55930 to 0.55576, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 9s - loss: 0.5541 - acc: 0.7380 - val_loss: 0.5530 - val_acc: 0.7373

Epoch 00015: val_loss improved from 0.55576 to 0.55304, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 9s - loss: 0.5506 - acc: 0.7401 - val_loss: 0.5486 - val_acc: 0.7439

Epoch 00016: val_loss improved from 0.55304 to 0.54863, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 9s - loss: 0.5468 - acc: 0.7428 - val_loss: 0.5452 - val_acc: 0.7402

Epoch 00017: val_loss improved from 0.54863 to 0.54518, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 9s - loss: 0.5429 - acc: 0.7452 - val_loss: 0.5448 - val_acc: 0.7464

Epoch 00018: val_loss improved from 0.54518 to 0.54476, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 9s - loss: 0.5412 - acc: 0.7441 - val_loss: 0.5389 - val_acc: 0.7403

Epoch 00019: val_loss improved from 0.54476 to 0.53890, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 9s - loss: 0.5374 - acc: 0.7469 - val_loss: 0.5362 - val_acc: 0.7491

Epoch 00020: val_loss improved from 0.53890 to 0.53625, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 9s - loss: 0.5354 - acc: 0.7481 - val_loss: 0.5335 - val_acc: 0.7447

Epoch 00021: val_loss improved from 0.53625 to 0.53348, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 9s - loss: 0.5333 - acc: 0.7506 - val_loss: 0.5306 - val_acc: 0.7510

Epoch 00022: val_loss improved from 0.53348 to 0.53056, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 9s - loss: 0.5297 - acc: 0.7499 - val_loss: 0.5302 - val_acc: 0.7521

Epoch 00023: val_loss improved from 0.53056 to 0.53020, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 9s - loss: 0.5270 - acc: 0.7512 - val_loss: 0.5293 - val_acc: 0.7456

Epoch 00024: val_loss improved from 0.53020 to 0.52932, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 9s - loss: 0.5273 - acc: 0.7507 - val_loss: 0.5254 - val_acc: 0.7485

Epoch 00025: val_loss improved from 0.52932 to 0.52542, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 9s - loss: 0.5242 - acc: 0.7523 - val_loss: 0.5227 - val_acc: 0.7539

Epoch 00026: val_loss improved from 0.52542 to 0.52272, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 9s - loss: 0.5234 - acc: 0.7514 - val_loss: 0.5231 - val_acc: 0.7543

Epoch 00027: val_loss did not improve from 0.52272
Epoch 28/40
 - 9s - loss: 0.5216 - acc: 0.7529 - val_loss: 0.5209 - val_acc: 0.7543

Epoch 00028: val_loss improved from 0.52272 to 0.52090, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 9s - loss: 0.5201 - acc: 0.7525 - val_loss: 0.5477 - val_acc: 0.7209

Epoch 00029: val_loss did not improve from 0.52090
Epoch 30/40
 - 9s - loss: 0.5201 - acc: 0.7540 - val_loss: 0.5163 - val_acc: 0.7551

Epoch 00030: val_loss improved from 0.52090 to 0.51629, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 9s - loss: 0.5171 - acc: 0.7558 - val_loss: 0.5249 - val_acc: 0.7408

Epoch 00031: val_loss did not improve from 0.51629
Epoch 32/40
 - 9s - loss: 0.5164 - acc: 0.7567 - val_loss: 0.5141 - val_acc: 0.7580

Epoch 00032: val_loss improved from 0.51629 to 0.51411, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 9s - loss: 0.5167 - acc: 0.7540 - val_loss: 0.5134 - val_acc: 0.7568

Epoch 00033: val_loss improved from 0.51411 to 0.51341, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 9s - loss: 0.5149 - acc: 0.7540 - val_loss: 0.5149 - val_acc: 0.7590

Epoch 00034: val_loss did not improve from 0.51341
Epoch 35/40
 - 9s - loss: 0.5133 - acc: 0.7539 - val_loss: 0.5108 - val_acc: 0.7584

Epoch 00035: val_loss improved from 0.51341 to 0.51076, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 9s - loss: 0.5123 - acc: 0.7554 - val_loss: 0.5106 - val_acc: 0.7539

Epoch 00036: val_loss improved from 0.51076 to 0.51065, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 9s - loss: 0.5116 - acc: 0.7554 - val_loss: 0.5115 - val_acc: 0.7496

Epoch 00037: val_loss did not improve from 0.51065
Epoch 38/40
 - 9s - loss: 0.5109 - acc: 0.7558 - val_loss: 0.5072 - val_acc: 0.7604

Epoch 00038: val_loss improved from 0.51065 to 0.50717, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 9s - loss: 0.5103 - acc: 0.7579 - val_loss: 0.5242 - val_acc: 0.7341

Epoch 00039: val_loss did not improve from 0.50717
Epoch 40/40
 - 9s - loss: 0.5104 - acc: 0.7549 - val_loss: 0.5056 - val_acc: 0.7574

Epoch 00040: val_loss improved from 0.50717 to 0.50560, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 1s
 384/7440 [>.............................] - ETA: 1s
 736/7440 [=>............................] - ETA: 0s
1088/7440 [===>..........................] - ETA: 0s
1440/7440 [====>.........................] - ETA: 0s
1792/7440 [======>.......................] - ETA: 0s
2144/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2880/7440 [==========>...................] - ETA: 0s
3264/7440 [============>.................] - ETA: 0s
3648/7440 [=============>................] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 142us/step
current Test accuracy: 0.8326612903225806
current auc_score ------------------>  0.8665751098392878

  32/7440 [..............................] - ETA: 11:39
 352/7440 [>.............................] - ETA: 1:01 
 704/7440 [=>............................] - ETA: 29s 
1056/7440 [===>..........................] - ETA: 19s
1408/7440 [====>.........................] - ETA: 13s
1760/7440 [======>.......................] - ETA: 10s
2112/7440 [=======>......................] - ETA: 8s 
2464/7440 [========>.....................] - ETA: 6s
2816/7440 [==========>...................] - ETA: 5s
3168/7440 [===========>..................] - ETA: 4s
3520/7440 [=============>................] - ETA: 3s
3904/7440 [==============>...............] - ETA: 3s
4288/7440 [================>.............] - ETA: 2s
4672/7440 [=================>............] - ETA: 2s
5056/7440 [===================>..........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 1s
6208/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 550us/step
Best saved model Test accuracy: 0.8326612903225806
best saved model auc_score ------------------>  0.8665751098392878
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_13[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_148[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_149[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_150[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_151[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 36, 96, 96)   0           concatenate_59[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_152[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_153[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 46, 96, 96)   0           concatenate_60[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_154[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_155[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_156[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 33, 48, 48)   0           average_pooling2d_20[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_157[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_158[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 43, 48, 48)   0           concatenate_62[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_159[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 53, 48, 48)   0           concatenate_63[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 53, 48, 48)   212         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 53, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 26, 48, 48)   1378        activation_161[0][0]             
__________________________________________________________________________________________________
average_pooling2d_21 (AveragePo (None, 26, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 26, 24, 24)   104         average_pooling2d_21[0][0]       
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 26, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   1040        activation_162[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_163[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 36, 24, 24)   0           average_pooling2d_21[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 36, 24, 24)   144         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 36, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 40, 24, 24)   1440        activation_164[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 40, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 46, 24, 24)   0           concatenate_65[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 46, 24, 24)   184         concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 46, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 40, 24, 24)   1840        activation_166[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 40, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 56, 24, 24)   0           concatenate_66[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 56, 24, 24)   224         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 56, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_13 (Gl (None, 56)           0           activation_168[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            57          global_average_pooling2d_13[0][0]
==================================================================================================
Total params: 49,781
Trainable params: 48,181
Non-trainable params: 1,600
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 49s - loss: 0.6015 - acc: 0.7477 - val_loss: 0.5588 - val_acc: 0.7711

Epoch 00001: val_loss improved from inf to 0.55881, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 38s - loss: 0.5204 - acc: 0.7904 - val_loss: 0.4874 - val_acc: 0.8035

Epoch 00002: val_loss improved from 0.55881 to 0.48738, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 37s - loss: 0.4872 - acc: 0.8070 - val_loss: 0.4721 - val_acc: 0.8136

Epoch 00003: val_loss improved from 0.48738 to 0.47206, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 37s - loss: 0.4665 - acc: 0.8192 - val_loss: 0.4606 - val_acc: 0.8228

Epoch 00004: val_loss improved from 0.47206 to 0.46064, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 38s - loss: 0.4478 - acc: 0.8292 - val_loss: 0.4308 - val_acc: 0.8338

Epoch 00005: val_loss improved from 0.46064 to 0.43075, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 37s - loss: 0.4360 - acc: 0.8368 - val_loss: 0.4274 - val_acc: 0.8389

Epoch 00006: val_loss improved from 0.43075 to 0.42741, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 38s - loss: 0.4243 - acc: 0.8413 - val_loss: 0.4305 - val_acc: 0.8392

Epoch 00007: val_loss did not improve from 0.42741
Epoch 8/40
 - 38s - loss: 0.4110 - acc: 0.8489 - val_loss: 0.4028 - val_acc: 0.8622

Epoch 00008: val_loss improved from 0.42741 to 0.40280, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 37s - loss: 0.3983 - acc: 0.8541 - val_loss: 0.4279 - val_acc: 0.8466

Epoch 00009: val_loss did not improve from 0.40280
Epoch 10/40
 - 37s - loss: 0.3900 - acc: 0.8615 - val_loss: 0.3787 - val_acc: 0.8621

Epoch 00010: val_loss improved from 0.40280 to 0.37869, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 38s - loss: 0.3786 - acc: 0.8665 - val_loss: 0.3873 - val_acc: 0.8714

Epoch 00011: val_loss did not improve from 0.37869
Epoch 12/40
 - 37s - loss: 0.3694 - acc: 0.8698 - val_loss: 0.4069 - val_acc: 0.8554

Epoch 00012: val_loss did not improve from 0.37869
Epoch 13/40
 - 38s - loss: 0.3617 - acc: 0.8753 - val_loss: 0.3602 - val_acc: 0.8770

Epoch 00013: val_loss improved from 0.37869 to 0.36022, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 37s - loss: 0.3498 - acc: 0.8799 - val_loss: 0.3468 - val_acc: 0.8827

Epoch 00014: val_loss improved from 0.36022 to 0.34684, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 38s - loss: 0.3447 - acc: 0.8846 - val_loss: 0.3401 - val_acc: 0.8830

Epoch 00015: val_loss improved from 0.34684 to 0.34012, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 38s - loss: 0.3306 - acc: 0.8918 - val_loss: 0.3858 - val_acc: 0.8744

Epoch 00016: val_loss did not improve from 0.34012
Epoch 17/40
 - 38s - loss: 0.3290 - acc: 0.8888 - val_loss: 0.3641 - val_acc: 0.8707

Epoch 00017: val_loss did not improve from 0.34012
Epoch 18/40
 - 38s - loss: 0.3229 - acc: 0.8936 - val_loss: 0.4120 - val_acc: 0.8436

Epoch 00018: val_loss did not improve from 0.34012
Epoch 19/40
 - 38s - loss: 0.3135 - acc: 0.9007 - val_loss: 0.3542 - val_acc: 0.8796

Epoch 00019: val_loss did not improve from 0.34012
Epoch 20/40
 - 38s - loss: 0.3073 - acc: 0.9027 - val_loss: 0.3300 - val_acc: 0.8981

Epoch 00020: val_loss improved from 0.34012 to 0.33003, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 37s - loss: 0.3025 - acc: 0.9042 - val_loss: 0.3180 - val_acc: 0.8963

Epoch 00021: val_loss improved from 0.33003 to 0.31802, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 37s - loss: 0.2955 - acc: 0.9072 - val_loss: 0.3117 - val_acc: 0.8961

Epoch 00022: val_loss improved from 0.31802 to 0.31166, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 38s - loss: 0.2897 - acc: 0.9093 - val_loss: 0.3140 - val_acc: 0.8992

Epoch 00023: val_loss did not improve from 0.31166
Epoch 24/40
 - 38s - loss: 0.2839 - acc: 0.9135 - val_loss: 0.3260 - val_acc: 0.8921

Epoch 00024: val_loss did not improve from 0.31166
Epoch 25/40
 - 38s - loss: 0.2794 - acc: 0.9153 - val_loss: 0.3138 - val_acc: 0.8996

Epoch 00025: val_loss did not improve from 0.31166
Epoch 26/40
 - 38s - loss: 0.2748 - acc: 0.9161 - val_loss: 0.2971 - val_acc: 0.9056

Epoch 00026: val_loss improved from 0.31166 to 0.29707, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 38s - loss: 0.2739 - acc: 0.9172 - val_loss: 0.3100 - val_acc: 0.9022

Epoch 00027: val_loss did not improve from 0.29707
Epoch 28/40
 - 37s - loss: 0.2641 - acc: 0.9220 - val_loss: 0.2891 - val_acc: 0.9123

Epoch 00028: val_loss improved from 0.29707 to 0.28915, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 38s - loss: 0.2617 - acc: 0.9228 - val_loss: 0.3577 - val_acc: 0.8824

Epoch 00029: val_loss did not improve from 0.28915
Epoch 30/40
 - 37s - loss: 0.2556 - acc: 0.9257 - val_loss: 0.3498 - val_acc: 0.8811

Epoch 00030: val_loss did not improve from 0.28915
Epoch 31/40
 - 37s - loss: 0.2521 - acc: 0.9262 - val_loss: 0.2965 - val_acc: 0.9041

Epoch 00031: val_loss did not improve from 0.28915
Epoch 32/40
 - 37s - loss: 0.2499 - acc: 0.9281 - val_loss: 0.2826 - val_acc: 0.9104

Epoch 00032: val_loss improved from 0.28915 to 0.28260, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 38s - loss: 0.2453 - acc: 0.9299 - val_loss: 0.3297 - val_acc: 0.8972

Epoch 00033: val_loss did not improve from 0.28260
Epoch 34/40
 - 38s - loss: 0.2410 - acc: 0.9311 - val_loss: 0.3397 - val_acc: 0.8818

Epoch 00034: val_loss did not improve from 0.28260
Epoch 35/40
 - 38s - loss: 0.2353 - acc: 0.9339 - val_loss: 0.2956 - val_acc: 0.9090

Epoch 00035: val_loss did not improve from 0.28260
Epoch 36/40
 - 38s - loss: 0.2326 - acc: 0.9350 - val_loss: 0.3063 - val_acc: 0.9060

Epoch 00036: val_loss did not improve from 0.28260
Epoch 37/40
 - 38s - loss: 0.2305 - acc: 0.9361 - val_loss: 0.2904 - val_acc: 0.9081

Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00037: val_loss did not improve from 0.28260
Epoch 38/40
 - 37s - loss: 0.2099 - acc: 0.9470 - val_loss: 0.2672 - val_acc: 0.9213

Epoch 00038: val_loss improved from 0.28260 to 0.26716, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 38s - loss: 0.2063 - acc: 0.9479 - val_loss: 0.2560 - val_acc: 0.9226

Epoch 00039: val_loss improved from 0.26716 to 0.25596, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 37s - loss: 0.2046 - acc: 0.9490 - val_loss: 0.2611 - val_acc: 0.9204

Epoch 00040: val_loss did not improve from 0.25596

  32/7440 [..............................] - ETA: 2s
 160/7440 [..............................] - ETA: 2s
 320/7440 [>.............................] - ETA: 2s
 480/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 736/7440 [=>............................] - ETA: 2s
 864/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1280/7440 [====>.........................] - ETA: 2s
1408/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1856/7440 [======>.......................] - ETA: 2s
2016/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3648/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 1s
4576/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 388us/step
current Test accuracy: 0.7881720430107527
current auc_score ------------------>  0.9068573679038039

  32/7440 [..............................] - ETA: 15:06
 160/7440 [..............................] - ETA: 3:00 
 288/7440 [>.............................] - ETA: 1:39
 416/7440 [>.............................] - ETA: 1:08
 544/7440 [=>............................] - ETA: 52s 
 672/7440 [=>............................] - ETA: 42s
 800/7440 [==>...........................] - ETA: 35s
 928/7440 [==>...........................] - ETA: 30s
1056/7440 [===>..........................] - ETA: 26s
1184/7440 [===>..........................] - ETA: 23s
1312/7440 [====>.........................] - ETA: 20s
1440/7440 [====>.........................] - ETA: 18s
1568/7440 [=====>........................] - ETA: 16s
1696/7440 [=====>........................] - ETA: 15s
1824/7440 [======>.......................] - ETA: 14s
1952/7440 [======>.......................] - ETA: 13s
2080/7440 [=======>......................] - ETA: 12s
2208/7440 [=======>......................] - ETA: 11s
2336/7440 [========>.....................] - ETA: 10s
2464/7440 [========>.....................] - ETA: 9s 
2592/7440 [=========>....................] - ETA: 9s
2720/7440 [=========>....................] - ETA: 8s
2848/7440 [==========>...................] - ETA: 8s
2976/7440 [===========>..................] - ETA: 7s
3104/7440 [===========>..................] - ETA: 7s
3232/7440 [============>.................] - ETA: 6s
3360/7440 [============>.................] - ETA: 6s
3488/7440 [=============>................] - ETA: 6s
3616/7440 [=============>................] - ETA: 5s
3744/7440 [==============>...............] - ETA: 5s
3872/7440 [==============>...............] - ETA: 5s
4000/7440 [===============>..............] - ETA: 4s
4128/7440 [===============>..............] - ETA: 4s
4256/7440 [================>.............] - ETA: 4s
4384/7440 [================>.............] - ETA: 3s
4512/7440 [=================>............] - ETA: 3s
4640/7440 [=================>............] - ETA: 3s
4768/7440 [==================>...........] - ETA: 3s
4896/7440 [==================>...........] - ETA: 3s
5024/7440 [===================>..........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5280/7440 [====================>.........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 2s
5536/7440 [=====================>........] - ETA: 2s
5664/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5920/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 927us/step
Best saved model Test accuracy: 0.7971774193548387
best saved model auc_score ------------------>  0.9030150884495316
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_14[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_169[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_170[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 36, 96, 96)   0           concatenate_68[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_173[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 46, 96, 96)   0           concatenate_69[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_3_bn (BatchNormalizatio (None, 46, 96, 96)   184         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 46, 96, 96)   0           dense_0_3_bn[0][0]               
__________________________________________________________________________________________________
dense_0_3_bottleneck_conv2D (Co (None, 40, 96, 96)   1840        activation_175[0][0]             
__________________________________________________________________________________________________
dense_0_3_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_3_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 40, 96, 96)   0           dense_0_3_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_3_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_176[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 56, 96, 96)   0           concatenate_70[0][0]             
                                                                 dense_0_3_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 56, 96, 96)   224         concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 56, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 28, 96, 96)   1568        activation_177[0][0]             
__________________________________________________________________________________________________
average_pooling2d_22 (AveragePo (None, 28, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 28, 48, 48)   112         average_pooling2d_22[0][0]       
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 28, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   1120        activation_178[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_179[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 38, 48, 48)   0           average_pooling2d_22[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1520        activation_180[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_181[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 48, 48, 48)   0           concatenate_72[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1920        activation_182[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_183[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 58, 48, 48)   0           concatenate_73[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_3_bn (BatchNormalizatio (None, 58, 48, 48)   232         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 58, 48, 48)   0           dense_1_3_bn[0][0]               
__________________________________________________________________________________________________
dense_1_3_bottleneck_conv2D (Co (None, 40, 48, 48)   2320        activation_184[0][0]             
__________________________________________________________________________________________________
dense_1_3_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_3_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 40, 48, 48)   0           dense_1_3_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_3_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_185[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 68, 48, 48)   0           concatenate_74[0][0]             
                                                                 dense_1_3_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 68, 48, 48)   272         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 68, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_14 (Gl (None, 68)           0           activation_186[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            69          global_average_pooling2d_14[0][0]
==================================================================================================
Total params: 45,525
Trainable params: 44,045
Non-trainable params: 1,480
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 55s - loss: 0.6061 - acc: 0.7439 - val_loss: 0.5494 - val_acc: 0.7679

Epoch 00001: val_loss improved from inf to 0.54941, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 43s - loss: 0.5345 - acc: 0.7778 - val_loss: 0.5497 - val_acc: 0.7526

Epoch 00002: val_loss did not improve from 0.54941
Epoch 3/40
 - 44s - loss: 0.5052 - acc: 0.7891 - val_loss: 0.4968 - val_acc: 0.7846

Epoch 00003: val_loss improved from 0.54941 to 0.49683, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 43s - loss: 0.4892 - acc: 0.7970 - val_loss: 0.4740 - val_acc: 0.8032

Epoch 00004: val_loss improved from 0.49683 to 0.47400, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 43s - loss: 0.4755 - acc: 0.8008 - val_loss: 0.4598 - val_acc: 0.8079

Epoch 00005: val_loss improved from 0.47400 to 0.45984, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 43s - loss: 0.4609 - acc: 0.8086 - val_loss: 0.4615 - val_acc: 0.8159

Epoch 00006: val_loss did not improve from 0.45984
Epoch 7/40
 - 43s - loss: 0.4475 - acc: 0.8182 - val_loss: 0.4350 - val_acc: 0.8213

Epoch 00007: val_loss improved from 0.45984 to 0.43505, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 43s - loss: 0.4375 - acc: 0.8228 - val_loss: 0.4256 - val_acc: 0.8312

Epoch 00008: val_loss improved from 0.43505 to 0.42565, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 43s - loss: 0.4297 - acc: 0.8262 - val_loss: 0.4231 - val_acc: 0.8406

Epoch 00009: val_loss improved from 0.42565 to 0.42311, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 43s - loss: 0.4210 - acc: 0.8329 - val_loss: 0.4229 - val_acc: 0.8304

Epoch 00010: val_loss improved from 0.42311 to 0.42287, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 43s - loss: 0.4170 - acc: 0.8363 - val_loss: 0.4641 - val_acc: 0.8109

Epoch 00011: val_loss did not improve from 0.42287
Epoch 12/40
 - 43s - loss: 0.4071 - acc: 0.8391 - val_loss: 0.4299 - val_acc: 0.8253

Epoch 00012: val_loss did not improve from 0.42287
Epoch 13/40
 - 43s - loss: 0.3999 - acc: 0.8464 - val_loss: 0.3934 - val_acc: 0.8538

Epoch 00013: val_loss improved from 0.42287 to 0.39339, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 43s - loss: 0.3936 - acc: 0.8496 - val_loss: 0.3817 - val_acc: 0.8503

Epoch 00014: val_loss improved from 0.39339 to 0.38174, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 43s - loss: 0.3900 - acc: 0.8518 - val_loss: 0.3883 - val_acc: 0.8574

Epoch 00015: val_loss did not improve from 0.38174
Epoch 16/40
 - 43s - loss: 0.3844 - acc: 0.8537 - val_loss: 0.3846 - val_acc: 0.8588

Epoch 00016: val_loss did not improve from 0.38174
Epoch 17/40
 - 43s - loss: 0.3781 - acc: 0.8557 - val_loss: 0.3842 - val_acc: 0.8587

Epoch 00017: val_loss did not improve from 0.38174
Epoch 18/40
 - 43s - loss: 0.3726 - acc: 0.8604 - val_loss: 0.3571 - val_acc: 0.8696

Epoch 00018: val_loss improved from 0.38174 to 0.35712, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 43s - loss: 0.3695 - acc: 0.8617 - val_loss: 0.3541 - val_acc: 0.8704

Epoch 00019: val_loss improved from 0.35712 to 0.35407, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 43s - loss: 0.3645 - acc: 0.8646 - val_loss: 0.3860 - val_acc: 0.8652

Epoch 00020: val_loss did not improve from 0.35407
Epoch 21/40
 - 43s - loss: 0.3592 - acc: 0.8675 - val_loss: 0.3912 - val_acc: 0.8566

Epoch 00021: val_loss did not improve from 0.35407
Epoch 22/40
 - 43s - loss: 0.3568 - acc: 0.8684 - val_loss: 0.3614 - val_acc: 0.8719

Epoch 00022: val_loss did not improve from 0.35407
Epoch 23/40
 - 43s - loss: 0.3500 - acc: 0.8687 - val_loss: 0.5479 - val_acc: 0.7492

Epoch 00023: val_loss did not improve from 0.35407
Epoch 24/40
 - 43s - loss: 0.3459 - acc: 0.8718 - val_loss: 0.3908 - val_acc: 0.8520

Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00024: val_loss did not improve from 0.35407
Epoch 25/40
 - 43s - loss: 0.3319 - acc: 0.8813 - val_loss: 0.3360 - val_acc: 0.8817

Epoch 00025: val_loss improved from 0.35407 to 0.33599, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 43s - loss: 0.3282 - acc: 0.8839 - val_loss: 0.3428 - val_acc: 0.8706

Epoch 00026: val_loss did not improve from 0.33599
Epoch 27/40
 - 43s - loss: 0.3279 - acc: 0.8839 - val_loss: 0.3363 - val_acc: 0.8780

Epoch 00027: val_loss did not improve from 0.33599
Epoch 28/40
 - 43s - loss: 0.3267 - acc: 0.8849 - val_loss: 0.3313 - val_acc: 0.8805

Epoch 00028: val_loss improved from 0.33599 to 0.33134, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 43s - loss: 0.3265 - acc: 0.8837 - val_loss: 0.3384 - val_acc: 0.8740

Epoch 00029: val_loss did not improve from 0.33134
Epoch 30/40
 - 43s - loss: 0.3219 - acc: 0.8855 - val_loss: 0.3263 - val_acc: 0.8889

Epoch 00030: val_loss improved from 0.33134 to 0.32631, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 43s - loss: 0.3211 - acc: 0.8868 - val_loss: 0.3292 - val_acc: 0.8869

Epoch 00031: val_loss did not improve from 0.32631
Epoch 32/40
 - 43s - loss: 0.3193 - acc: 0.8872 - val_loss: 0.3280 - val_acc: 0.8834

Epoch 00032: val_loss did not improve from 0.32631
Epoch 33/40
 - 43s - loss: 0.3190 - acc: 0.8887 - val_loss: 0.3417 - val_acc: 0.8820

Epoch 00033: val_loss did not improve from 0.32631
Epoch 34/40
 - 43s - loss: 0.3152 - acc: 0.8892 - val_loss: 0.3309 - val_acc: 0.8830

Epoch 00034: val_loss did not improve from 0.32631
Epoch 35/40
 - 43s - loss: 0.3171 - acc: 0.8884 - val_loss: 0.3403 - val_acc: 0.8735

Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999259090306e-06.

Epoch 00035: val_loss did not improve from 0.32631
Epoch 36/40
 - 43s - loss: 0.3105 - acc: 0.8919 - val_loss: 0.3252 - val_acc: 0.8878

Epoch 00036: val_loss improved from 0.32631 to 0.32520, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 43s - loss: 0.3107 - acc: 0.8923 - val_loss: 0.3200 - val_acc: 0.8918

Epoch 00037: val_loss improved from 0.32520 to 0.31998, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 38/40
 - 43s - loss: 0.3101 - acc: 0.8941 - val_loss: 0.3190 - val_acc: 0.8879

Epoch 00038: val_loss improved from 0.31998 to 0.31901, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 43s - loss: 0.3085 - acc: 0.8943 - val_loss: 0.3191 - val_acc: 0.8876

Epoch 00039: val_loss did not improve from 0.31901
Epoch 40/40
 - 43s - loss: 0.3077 - acc: 0.8939 - val_loss: 0.3177 - val_acc: 0.8907

Epoch 00040: val_loss improved from 0.31901 to 0.31770, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 471us/step
current Test accuracy: 0.7987903225806452
current auc_score ------------------>  0.8876029020696035

  32/7440 [..............................] - ETA: 17:27
 160/7440 [..............................] - ETA: 3:28 
 288/7440 [>.............................] - ETA: 1:55
 416/7440 [>.............................] - ETA: 1:19
 544/7440 [=>............................] - ETA: 1:00
 672/7440 [=>............................] - ETA: 48s 
 800/7440 [==>...........................] - ETA: 40s
 928/7440 [==>...........................] - ETA: 34s
1056/7440 [===>..........................] - ETA: 30s
1184/7440 [===>..........................] - ETA: 26s
1312/7440 [====>.........................] - ETA: 23s
1440/7440 [====>.........................] - ETA: 21s
1568/7440 [=====>........................] - ETA: 19s
1696/7440 [=====>........................] - ETA: 17s
1824/7440 [======>.......................] - ETA: 16s
1952/7440 [======>.......................] - ETA: 15s
2080/7440 [=======>......................] - ETA: 14s
2208/7440 [=======>......................] - ETA: 13s
2336/7440 [========>.....................] - ETA: 12s
2464/7440 [========>.....................] - ETA: 11s
2592/7440 [=========>....................] - ETA: 10s
2720/7440 [=========>....................] - ETA: 10s
2848/7440 [==========>...................] - ETA: 9s 
2976/7440 [===========>..................] - ETA: 8s
3104/7440 [===========>..................] - ETA: 8s
3232/7440 [============>.................] - ETA: 7s
3360/7440 [============>.................] - ETA: 7s
3488/7440 [=============>................] - ETA: 6s
3616/7440 [=============>................] - ETA: 6s
3744/7440 [==============>...............] - ETA: 6s
3872/7440 [==============>...............] - ETA: 5s
4000/7440 [===============>..............] - ETA: 5s
4128/7440 [===============>..............] - ETA: 5s
4256/7440 [================>.............] - ETA: 4s
4384/7440 [================>.............] - ETA: 4s
4512/7440 [=================>............] - ETA: 4s
4640/7440 [=================>............] - ETA: 4s
4768/7440 [==================>...........] - ETA: 3s
4896/7440 [==================>...........] - ETA: 3s
5024/7440 [===================>..........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 3s
5280/7440 [====================>.........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 2s
5536/7440 [=====================>........] - ETA: 2s
5664/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.7987903225806452
best saved model auc_score ------------------>  0.8876029020696035
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_15 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_187 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_23 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_188 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_24 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_189 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_15  (None, 4)                 0         
_________________________________________________________________
dense_15 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 20s - loss: 0.7143 - acc: 0.5173 - val_loss: 0.6680 - val_acc: 0.5764

Epoch 00001: val_loss improved from inf to 0.66796, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 10s - loss: 0.6574 - acc: 0.6184 - val_loss: 0.6435 - val_acc: 0.6472

Epoch 00002: val_loss improved from 0.66796 to 0.64355, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 10s - loss: 0.6360 - acc: 0.6737 - val_loss: 0.6202 - val_acc: 0.6984

Epoch 00003: val_loss improved from 0.64355 to 0.62020, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 10s - loss: 0.6172 - acc: 0.7068 - val_loss: 0.6091 - val_acc: 0.7144

Epoch 00004: val_loss improved from 0.62020 to 0.60906, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 10s - loss: 0.6081 - acc: 0.7154 - val_loss: 0.6111 - val_acc: 0.7053

Epoch 00005: val_loss did not improve from 0.60906
Epoch 6/40
 - 10s - loss: 0.6001 - acc: 0.7192 - val_loss: 0.6034 - val_acc: 0.7154

Epoch 00006: val_loss improved from 0.60906 to 0.60335, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 10s - loss: 0.5917 - acc: 0.7247 - val_loss: 0.5865 - val_acc: 0.7278

Epoch 00007: val_loss improved from 0.60335 to 0.58654, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 10s - loss: 0.5841 - acc: 0.7297 - val_loss: 0.5787 - val_acc: 0.7391

Epoch 00008: val_loss improved from 0.58654 to 0.57872, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 10s - loss: 0.5775 - acc: 0.7316 - val_loss: 0.5746 - val_acc: 0.7368

Epoch 00009: val_loss improved from 0.57872 to 0.57462, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 10s - loss: 0.5725 - acc: 0.7367 - val_loss: 0.5736 - val_acc: 0.7356

Epoch 00010: val_loss improved from 0.57462 to 0.57359, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 10s - loss: 0.5676 - acc: 0.7368 - val_loss: 0.5738 - val_acc: 0.7265

Epoch 00011: val_loss did not improve from 0.57359
Epoch 12/40
 - 10s - loss: 0.5628 - acc: 0.7418 - val_loss: 0.5683 - val_acc: 0.7376

Epoch 00012: val_loss improved from 0.57359 to 0.56833, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 9s - loss: 0.5597 - acc: 0.7407 - val_loss: 0.5687 - val_acc: 0.7332

Epoch 00013: val_loss did not improve from 0.56833
Epoch 14/40
 - 9s - loss: 0.5563 - acc: 0.7419 - val_loss: 0.5669 - val_acc: 0.7349

Epoch 00014: val_loss improved from 0.56833 to 0.56694, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 10s - loss: 0.5527 - acc: 0.7440 - val_loss: 0.5549 - val_acc: 0.7393

Epoch 00015: val_loss improved from 0.56694 to 0.55486, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 10s - loss: 0.5488 - acc: 0.7471 - val_loss: 0.5575 - val_acc: 0.7354

Epoch 00016: val_loss did not improve from 0.55486
Epoch 17/40
 - 9s - loss: 0.5455 - acc: 0.7471 - val_loss: 0.6180 - val_acc: 0.6969

Epoch 00017: val_loss did not improve from 0.55486
Epoch 18/40
 - 10s - loss: 0.5439 - acc: 0.7478 - val_loss: 0.5497 - val_acc: 0.7403

Epoch 00018: val_loss improved from 0.55486 to 0.54968, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 10s - loss: 0.5425 - acc: 0.7460 - val_loss: 0.5549 - val_acc: 0.7407

Epoch 00019: val_loss did not improve from 0.54968
Epoch 20/40
 - 10s - loss: 0.5395 - acc: 0.7483 - val_loss: 0.5389 - val_acc: 0.7454

Epoch 00020: val_loss improved from 0.54968 to 0.53891, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 10s - loss: 0.5370 - acc: 0.7485 - val_loss: 0.5993 - val_acc: 0.6803

Epoch 00021: val_loss did not improve from 0.53891
Epoch 22/40
 - 10s - loss: 0.5365 - acc: 0.7500 - val_loss: 0.5480 - val_acc: 0.7383

Epoch 00022: val_loss did not improve from 0.53891
Epoch 23/40
 - 10s - loss: 0.5350 - acc: 0.7514 - val_loss: 0.5333 - val_acc: 0.7495

Epoch 00023: val_loss improved from 0.53891 to 0.53333, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 10s - loss: 0.5316 - acc: 0.7508 - val_loss: 0.5859 - val_acc: 0.7277

Epoch 00024: val_loss did not improve from 0.53333
Epoch 25/40
 - 10s - loss: 0.5311 - acc: 0.7521 - val_loss: 0.5282 - val_acc: 0.7519

Epoch 00025: val_loss improved from 0.53333 to 0.52824, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 10s - loss: 0.5288 - acc: 0.7535 - val_loss: 0.5272 - val_acc: 0.7515

Epoch 00026: val_loss improved from 0.52824 to 0.52719, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 10s - loss: 0.5276 - acc: 0.7541 - val_loss: 0.5302 - val_acc: 0.7549

Epoch 00027: val_loss did not improve from 0.52719
Epoch 28/40
 - 10s - loss: 0.5270 - acc: 0.7530 - val_loss: 0.5273 - val_acc: 0.7530

Epoch 00028: val_loss did not improve from 0.52719
Epoch 29/40
 - 10s - loss: 0.5268 - acc: 0.7540 - val_loss: 0.5303 - val_acc: 0.7485

Epoch 00029: val_loss did not improve from 0.52719
Epoch 30/40
 - 10s - loss: 0.5243 - acc: 0.7536 - val_loss: 0.5356 - val_acc: 0.7412

Epoch 00030: val_loss did not improve from 0.52719
Epoch 31/40
 - 10s - loss: 0.5244 - acc: 0.7532 - val_loss: 0.5383 - val_acc: 0.7487

Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.1622775802825264e-05.

Epoch 00031: val_loss did not improve from 0.52719
Epoch 32/40
 - 10s - loss: 0.5216 - acc: 0.7541 - val_loss: 0.5210 - val_acc: 0.7564

Epoch 00032: val_loss improved from 0.52719 to 0.52102, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 10s - loss: 0.5205 - acc: 0.7561 - val_loss: 0.5303 - val_acc: 0.7447

Epoch 00033: val_loss did not improve from 0.52102
Epoch 34/40
 - 10s - loss: 0.5211 - acc: 0.7553 - val_loss: 0.5181 - val_acc: 0.7546

Epoch 00034: val_loss improved from 0.52102 to 0.51812, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 35/40
 - 10s - loss: 0.5199 - acc: 0.7563 - val_loss: 0.5257 - val_acc: 0.7474

Epoch 00035: val_loss did not improve from 0.51812
Epoch 36/40
 - 10s - loss: 0.5215 - acc: 0.7551 - val_loss: 0.5181 - val_acc: 0.7554

Epoch 00036: val_loss improved from 0.51812 to 0.51805, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 10s - loss: 0.5188 - acc: 0.7559 - val_loss: 0.5242 - val_acc: 0.7469

Epoch 00037: val_loss did not improve from 0.51805
Epoch 38/40
 - 10s - loss: 0.5181 - acc: 0.7566 - val_loss: 0.5194 - val_acc: 0.7518

Epoch 00038: val_loss did not improve from 0.51805
Epoch 39/40
 - 9s - loss: 0.5200 - acc: 0.7555 - val_loss: 0.5159 - val_acc: 0.7556

Epoch 00039: val_loss improved from 0.51805 to 0.51588, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 10s - loss: 0.5186 - acc: 0.7574 - val_loss: 0.5157 - val_acc: 0.7544

Epoch 00040: val_loss improved from 0.51588 to 0.51566, saving model to keras_densenet_simple_wt_29Sept_2200.h5

  32/7440 [..............................] - ETA: 1s
 384/7440 [>.............................] - ETA: 1s
 736/7440 [=>............................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 0s
1440/7440 [====>.........................] - ETA: 0s
1792/7440 [======>.......................] - ETA: 0s
2144/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2848/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 147us/step
current Test accuracy: 0.8366935483870968
current auc_score ------------------>  0.8636792548271477

  32/7440 [..............................] - ETA: 16:42
 352/7440 [>.............................] - ETA: 1:28 
 704/7440 [=>............................] - ETA: 42s 
1024/7440 [===>..........................] - ETA: 28s
1376/7440 [====>.........................] - ETA: 20s
1728/7440 [=====>........................] - ETA: 15s
2080/7440 [=======>......................] - ETA: 11s
2432/7440 [========>.....................] - ETA: 9s 
2784/7440 [==========>...................] - ETA: 7s
3136/7440 [===========>..................] - ETA: 6s
3488/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 4s
4192/7440 [===============>..............] - ETA: 3s
4544/7440 [=================>............] - ETA: 3s
4896/7440 [==================>...........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 735us/step
Best saved model Test accuracy: 0.8366935483870968
best saved model auc_score ------------------>  0.8636792548271477
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_16 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_190 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_25 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_191 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_26 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_192 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_16  (None, 4)                 0         
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 21s - loss: 0.7043 - acc: 0.5023 - val_loss: 0.6888 - val_acc: 0.5255

Epoch 00001: val_loss improved from inf to 0.68882, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 10s - loss: 0.6755 - acc: 0.5995 - val_loss: 0.6546 - val_acc: 0.6647

Epoch 00002: val_loss improved from 0.68882 to 0.65461, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 10s - loss: 0.6416 - acc: 0.6728 - val_loss: 0.6279 - val_acc: 0.6947

Epoch 00003: val_loss improved from 0.65461 to 0.62791, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 10s - loss: 0.6224 - acc: 0.7005 - val_loss: 0.6146 - val_acc: 0.7101

Epoch 00004: val_loss improved from 0.62791 to 0.61456, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 10s - loss: 0.6116 - acc: 0.7115 - val_loss: 0.6086 - val_acc: 0.7169

Epoch 00005: val_loss improved from 0.61456 to 0.60856, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 10s - loss: 0.6034 - acc: 0.7181 - val_loss: 0.5981 - val_acc: 0.7239

Epoch 00006: val_loss improved from 0.60856 to 0.59812, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 10s - loss: 0.5959 - acc: 0.7235 - val_loss: 0.5924 - val_acc: 0.7248

Epoch 00007: val_loss improved from 0.59812 to 0.59236, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 9s - loss: 0.5912 - acc: 0.7263 - val_loss: 0.5855 - val_acc: 0.7317

Epoch 00008: val_loss improved from 0.59236 to 0.58549, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 10s - loss: 0.5843 - acc: 0.7291 - val_loss: 0.5809 - val_acc: 0.7316

Epoch 00009: val_loss improved from 0.58549 to 0.58088, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 9s - loss: 0.5797 - acc: 0.7309 - val_loss: 0.5766 - val_acc: 0.7292

Epoch 00010: val_loss improved from 0.58088 to 0.57658, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 10s - loss: 0.5748 - acc: 0.7345 - val_loss: 0.5721 - val_acc: 0.7351

Epoch 00011: val_loss improved from 0.57658 to 0.57214, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 9s - loss: 0.5707 - acc: 0.7361 - val_loss: 0.5660 - val_acc: 0.7357

Epoch 00012: val_loss improved from 0.57214 to 0.56602, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 9s - loss: 0.5650 - acc: 0.7384 - val_loss: 0.5622 - val_acc: 0.7369

Epoch 00013: val_loss improved from 0.56602 to 0.56225, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 10s - loss: 0.5612 - acc: 0.7411 - val_loss: 0.5579 - val_acc: 0.7372

Epoch 00014: val_loss improved from 0.56225 to 0.55792, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 10s - loss: 0.5567 - acc: 0.7422 - val_loss: 0.5543 - val_acc: 0.7402

Epoch 00015: val_loss improved from 0.55792 to 0.55435, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 10s - loss: 0.5539 - acc: 0.7430 - val_loss: 0.5509 - val_acc: 0.7400

Epoch 00016: val_loss improved from 0.55435 to 0.55089, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 10s - loss: 0.5496 - acc: 0.7439 - val_loss: 0.5488 - val_acc: 0.7432

Epoch 00017: val_loss improved from 0.55089 to 0.54877, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 10s - loss: 0.5473 - acc: 0.7456 - val_loss: 0.5476 - val_acc: 0.7426

Epoch 00018: val_loss improved from 0.54877 to 0.54764, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 10s - loss: 0.5431 - acc: 0.7472 - val_loss: 0.5406 - val_acc: 0.7446

Epoch 00019: val_loss improved from 0.54764 to 0.54062, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 10s - loss: 0.5406 - acc: 0.7481 - val_loss: 0.5402 - val_acc: 0.7491

Epoch 00020: val_loss improved from 0.54062 to 0.54017, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 10s - loss: 0.5382 - acc: 0.7492 - val_loss: 0.5463 - val_acc: 0.7356

Epoch 00021: val_loss did not improve from 0.54017
Epoch 22/40
 - 10s - loss: 0.5359 - acc: 0.7479 - val_loss: 0.5326 - val_acc: 0.7481

Epoch 00022: val_loss improved from 0.54017 to 0.53263, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 10s - loss: 0.5322 - acc: 0.7501 - val_loss: 0.5374 - val_acc: 0.7400

Epoch 00023: val_loss did not improve from 0.53263
Epoch 24/40
 - 10s - loss: 0.5309 - acc: 0.7516 - val_loss: 0.5273 - val_acc: 0.7521

Epoch 00024: val_loss improved from 0.53263 to 0.52727, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 10s - loss: 0.5282 - acc: 0.7535 - val_loss: 0.5373 - val_acc: 0.7406

Epoch 00025: val_loss did not improve from 0.52727
Epoch 26/40
 - 10s - loss: 0.5255 - acc: 0.7538 - val_loss: 0.5295 - val_acc: 0.7435

Epoch 00026: val_loss did not improve from 0.52727
Epoch 27/40
 - 10s - loss: 0.5247 - acc: 0.7521 - val_loss: 0.5206 - val_acc: 0.7533

Epoch 00027: val_loss improved from 0.52727 to 0.52065, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 10s - loss: 0.5213 - acc: 0.7576 - val_loss: 0.5264 - val_acc: 0.7541

Epoch 00028: val_loss did not improve from 0.52065
Epoch 29/40
 - 10s - loss: 0.5198 - acc: 0.7598 - val_loss: 0.5209 - val_acc: 0.7479

Epoch 00029: val_loss did not improve from 0.52065
Epoch 30/40
 - 10s - loss: 0.5190 - acc: 0.7564 - val_loss: 0.5169 - val_acc: 0.7559

Epoch 00030: val_loss improved from 0.52065 to 0.51694, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 10s - loss: 0.5170 - acc: 0.7574 - val_loss: 0.5135 - val_acc: 0.7533

Epoch 00031: val_loss improved from 0.51694 to 0.51350, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 10s - loss: 0.5157 - acc: 0.7579 - val_loss: 0.5204 - val_acc: 0.7461

Epoch 00032: val_loss did not improve from 0.51350
Epoch 33/40
 - 10s - loss: 0.5148 - acc: 0.7567 - val_loss: 0.5113 - val_acc: 0.7594

Epoch 00033: val_loss improved from 0.51350 to 0.51133, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 10s - loss: 0.5116 - acc: 0.7604 - val_loss: 0.5078 - val_acc: 0.7620

Epoch 00034: val_loss improved from 0.51133 to 0.50779, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 35/40
 - 10s - loss: 0.5111 - acc: 0.7614 - val_loss: 0.5168 - val_acc: 0.7466

Epoch 00035: val_loss did not improve from 0.50779
Epoch 36/40
 - 10s - loss: 0.5095 - acc: 0.7602 - val_loss: 0.5052 - val_acc: 0.7561

Epoch 00036: val_loss improved from 0.50779 to 0.50516, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 10s - loss: 0.5085 - acc: 0.7602 - val_loss: 0.5032 - val_acc: 0.7600

Epoch 00037: val_loss improved from 0.50516 to 0.50317, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 38/40
 - 10s - loss: 0.5057 - acc: 0.7622 - val_loss: 0.5055 - val_acc: 0.7587

Epoch 00038: val_loss did not improve from 0.50317
Epoch 39/40
 - 10s - loss: 0.5046 - acc: 0.7604 - val_loss: 0.5011 - val_acc: 0.7604

Epoch 00039: val_loss improved from 0.50317 to 0.50114, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 10s - loss: 0.5050 - acc: 0.7615 - val_loss: 0.5135 - val_acc: 0.7433

Epoch 00040: val_loss did not improve from 0.50114

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 0s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 158us/step
current Test accuracy: 0.812231182795699
current auc_score ------------------>  0.8647746849346745

  32/7440 [..............................] - ETA: 17:45
 320/7440 [>.............................] - ETA: 1:43 
 672/7440 [=>............................] - ETA: 47s 
1024/7440 [===>..........................] - ETA: 29s
1376/7440 [====>.........................] - ETA: 21s
1728/7440 [=====>........................] - ETA: 16s
2080/7440 [=======>......................] - ETA: 12s
2432/7440 [========>.....................] - ETA: 10s
2784/7440 [==========>...................] - ETA: 8s 
3136/7440 [===========>..................] - ETA: 6s
3488/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 4s
4192/7440 [===============>..............] - ETA: 4s
4544/7440 [=================>............] - ETA: 3s
4896/7440 [==================>...........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 770us/step
Best saved model Test accuracy: 0.8310483870967742
best saved model auc_score ------------------>  0.8732145696034224
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_17[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_193[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_194[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 26, 96, 96)   104         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 26, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 13, 96, 96)   338         activation_195[0][0]             
__________________________________________________________________________________________________
average_pooling2d_27 (AveragePo (None, 13, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 13, 48, 48)   52          average_pooling2d_27[0][0]       
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 13, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   520         activation_196[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_197[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_27[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 23, 48, 48)   92          concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 23, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 11, 48, 48)   253         activation_198[0][0]             
__________________________________________________________________________________________________
average_pooling2d_28 (AveragePo (None, 11, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 11, 24, 24)   44          average_pooling2d_28[0][0]       
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 11, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   440         activation_199[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_200[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 21, 24, 24)   0           average_pooling2d_28[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 21, 24, 24)   84          concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 21, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_17 (Gl (None, 21)           0           activation_201[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            22          global_average_pooling2d_17[0][0]
==================================================================================================
Total params: 14,221
Trainable params: 13,761
Non-trainable params: 460
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 30s - loss: 0.6184 - acc: 0.7146 - val_loss: 0.5764 - val_acc: 0.7470

Epoch 00001: val_loss improved from inf to 0.57636, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 18s - loss: 0.5489 - acc: 0.7644 - val_loss: 0.5409 - val_acc: 0.7569

Epoch 00002: val_loss improved from 0.57636 to 0.54095, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 18s - loss: 0.5153 - acc: 0.7742 - val_loss: 0.5085 - val_acc: 0.7718

Epoch 00003: val_loss improved from 0.54095 to 0.50853, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 18s - loss: 0.4940 - acc: 0.7817 - val_loss: 0.4799 - val_acc: 0.7861

Epoch 00004: val_loss improved from 0.50853 to 0.47994, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 18s - loss: 0.4769 - acc: 0.7885 - val_loss: 0.4665 - val_acc: 0.7917

Epoch 00005: val_loss improved from 0.47994 to 0.46652, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 18s - loss: 0.4653 - acc: 0.7927 - val_loss: 0.4638 - val_acc: 0.7903

Epoch 00006: val_loss improved from 0.46652 to 0.46379, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 18s - loss: 0.4538 - acc: 0.7965 - val_loss: 0.4503 - val_acc: 0.7934

Epoch 00007: val_loss improved from 0.46379 to 0.45035, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 18s - loss: 0.4446 - acc: 0.8007 - val_loss: 0.4475 - val_acc: 0.7938

Epoch 00008: val_loss improved from 0.45035 to 0.44746, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 18s - loss: 0.4384 - acc: 0.8046 - val_loss: 0.4343 - val_acc: 0.8012

Epoch 00009: val_loss improved from 0.44746 to 0.43431, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 18s - loss: 0.4328 - acc: 0.8059 - val_loss: 0.4285 - val_acc: 0.8095

Epoch 00010: val_loss improved from 0.43431 to 0.42848, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 18s - loss: 0.4269 - acc: 0.8101 - val_loss: 0.4259 - val_acc: 0.8107

Epoch 00011: val_loss improved from 0.42848 to 0.42591, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 18s - loss: 0.4237 - acc: 0.8141 - val_loss: 0.4084 - val_acc: 0.8181

Epoch 00012: val_loss improved from 0.42591 to 0.40839, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 18s - loss: 0.4183 - acc: 0.8158 - val_loss: 0.4147 - val_acc: 0.8150

Epoch 00013: val_loss did not improve from 0.40839
Epoch 14/40
 - 18s - loss: 0.4118 - acc: 0.8199 - val_loss: 0.4043 - val_acc: 0.8253

Epoch 00014: val_loss improved from 0.40839 to 0.40429, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 18s - loss: 0.4083 - acc: 0.8200 - val_loss: 0.3989 - val_acc: 0.8271

Epoch 00015: val_loss improved from 0.40429 to 0.39886, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 18s - loss: 0.4068 - acc: 0.8237 - val_loss: 0.3965 - val_acc: 0.8307

Epoch 00016: val_loss improved from 0.39886 to 0.39654, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 18s - loss: 0.4020 - acc: 0.8268 - val_loss: 0.3991 - val_acc: 0.8248

Epoch 00017: val_loss did not improve from 0.39654
Epoch 18/40
 - 18s - loss: 0.4014 - acc: 0.8274 - val_loss: 0.4149 - val_acc: 0.8173

Epoch 00018: val_loss did not improve from 0.39654
Epoch 19/40
 - 18s - loss: 0.3974 - acc: 0.8265 - val_loss: 0.3882 - val_acc: 0.8333

Epoch 00019: val_loss improved from 0.39654 to 0.38817, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 18s - loss: 0.3936 - acc: 0.8294 - val_loss: 0.3857 - val_acc: 0.8347

Epoch 00020: val_loss improved from 0.38817 to 0.38567, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 18s - loss: 0.3901 - acc: 0.8329 - val_loss: 0.4094 - val_acc: 0.8188

Epoch 00021: val_loss did not improve from 0.38567
Epoch 22/40
 - 18s - loss: 0.3892 - acc: 0.8317 - val_loss: 0.3835 - val_acc: 0.8409

Epoch 00022: val_loss improved from 0.38567 to 0.38353, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 18s - loss: 0.3859 - acc: 0.8335 - val_loss: 0.3771 - val_acc: 0.8387

Epoch 00023: val_loss improved from 0.38353 to 0.37711, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 18s - loss: 0.3804 - acc: 0.8392 - val_loss: 0.3775 - val_acc: 0.8417

Epoch 00024: val_loss did not improve from 0.37711
Epoch 25/40
 - 18s - loss: 0.3785 - acc: 0.8389 - val_loss: 0.3806 - val_acc: 0.8446

Epoch 00025: val_loss did not improve from 0.37711
Epoch 26/40
 - 18s - loss: 0.3784 - acc: 0.8411 - val_loss: 0.3771 - val_acc: 0.8400

Epoch 00026: val_loss did not improve from 0.37711
Epoch 27/40
 - 18s - loss: 0.3755 - acc: 0.8419 - val_loss: 0.3723 - val_acc: 0.8474

Epoch 00027: val_loss improved from 0.37711 to 0.37233, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 18s - loss: 0.3730 - acc: 0.8421 - val_loss: 0.3715 - val_acc: 0.8471

Epoch 00028: val_loss improved from 0.37233 to 0.37153, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 18s - loss: 0.3719 - acc: 0.8429 - val_loss: 0.4929 - val_acc: 0.7998

Epoch 00029: val_loss did not improve from 0.37153
Epoch 30/40
 - 18s - loss: 0.3720 - acc: 0.8427 - val_loss: 0.3893 - val_acc: 0.8362

Epoch 00030: val_loss did not improve from 0.37153
Epoch 31/40
 - 18s - loss: 0.3665 - acc: 0.8450 - val_loss: 0.3854 - val_acc: 0.8358

Epoch 00031: val_loss did not improve from 0.37153
Epoch 32/40
 - 18s - loss: 0.3648 - acc: 0.8469 - val_loss: 0.3924 - val_acc: 0.8386

Epoch 00032: val_loss did not improve from 0.37153
Epoch 33/40
 - 18s - loss: 0.3628 - acc: 0.8484 - val_loss: 0.3701 - val_acc: 0.8401

Epoch 00033: val_loss improved from 0.37153 to 0.37005, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 18s - loss: 0.3617 - acc: 0.8482 - val_loss: 0.3711 - val_acc: 0.8430

Epoch 00034: val_loss did not improve from 0.37005
Epoch 35/40
 - 18s - loss: 0.3624 - acc: 0.8496 - val_loss: 0.3630 - val_acc: 0.8505

Epoch 00035: val_loss improved from 0.37005 to 0.36304, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 18s - loss: 0.3591 - acc: 0.8506 - val_loss: 0.3694 - val_acc: 0.8461

Epoch 00036: val_loss did not improve from 0.36304
Epoch 37/40
 - 18s - loss: 0.3559 - acc: 0.8529 - val_loss: 0.3669 - val_acc: 0.8484

Epoch 00037: val_loss did not improve from 0.36304
Epoch 38/40
 - 18s - loss: 0.3562 - acc: 0.8510 - val_loss: 0.3515 - val_acc: 0.8563

Epoch 00038: val_loss improved from 0.36304 to 0.35150, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 18s - loss: 0.3489 - acc: 0.8567 - val_loss: 0.3508 - val_acc: 0.8576

Epoch 00039: val_loss improved from 0.35150 to 0.35083, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 18s - loss: 0.3526 - acc: 0.8531 - val_loss: 0.3718 - val_acc: 0.8475

Epoch 00040: val_loss did not improve from 0.35083

  32/7440 [..............................] - ETA: 1s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 0s
3424/7440 [============>.................] - ETA: 0s
3680/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 230us/step
current Test accuracy: 0.8236559139784946
current auc_score ------------------>  0.9053800656145219

  32/7440 [..............................] - ETA: 19:01
 256/7440 [>.............................] - ETA: 2:19 
 480/7440 [>.............................] - ETA: 1:13
 704/7440 [=>............................] - ETA: 48s 
 928/7440 [==>...........................] - ETA: 36s
1152/7440 [===>..........................] - ETA: 28s
1376/7440 [====>.........................] - ETA: 23s
1600/7440 [=====>........................] - ETA: 19s
1824/7440 [======>.......................] - ETA: 16s
2048/7440 [=======>......................] - ETA: 14s
2272/7440 [========>.....................] - ETA: 12s
2496/7440 [=========>....................] - ETA: 10s
2720/7440 [=========>....................] - ETA: 9s 
2944/7440 [==========>...................] - ETA: 8s
3168/7440 [===========>..................] - ETA: 7s
3392/7440 [============>.................] - ETA: 6s
3616/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
4064/7440 [===============>..............] - ETA: 4s
4288/7440 [================>.............] - ETA: 4s
4512/7440 [=================>............] - ETA: 3s
4736/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 1s
5856/7440 [======================>.......] - ETA: 1s
6080/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 894us/step
Best saved model Test accuracy: 0.7997311827956989
best saved model auc_score ------------------>  0.8853298791767834
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_202 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_29 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_203 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_30 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_204 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_18  (None, 4)                 0         
_________________________________________________________________
dense_18 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 22s - loss: 0.6766 - acc: 0.5732 - val_loss: 0.6583 - val_acc: 0.6491

Epoch 00001: val_loss improved from inf to 0.65830, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 10s - loss: 0.6491 - acc: 0.6806 - val_loss: 0.6405 - val_acc: 0.6993

Epoch 00002: val_loss improved from 0.65830 to 0.64053, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 10s - loss: 0.6366 - acc: 0.6974 - val_loss: 0.6293 - val_acc: 0.7072

Epoch 00003: val_loss improved from 0.64053 to 0.62931, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 10s - loss: 0.6263 - acc: 0.7038 - val_loss: 0.6194 - val_acc: 0.7096

Epoch 00004: val_loss improved from 0.62931 to 0.61940, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 10s - loss: 0.6163 - acc: 0.7083 - val_loss: 0.6112 - val_acc: 0.7125

Epoch 00005: val_loss improved from 0.61940 to 0.61117, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 10s - loss: 0.6070 - acc: 0.7146 - val_loss: 0.6006 - val_acc: 0.7241

Epoch 00006: val_loss improved from 0.61117 to 0.60057, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 10s - loss: 0.5986 - acc: 0.7193 - val_loss: 0.5918 - val_acc: 0.7235

Epoch 00007: val_loss improved from 0.60057 to 0.59176, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 10s - loss: 0.5890 - acc: 0.7248 - val_loss: 0.5837 - val_acc: 0.7258

Epoch 00008: val_loss improved from 0.59176 to 0.58369, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 10s - loss: 0.5823 - acc: 0.7293 - val_loss: 0.5790 - val_acc: 0.7290

Epoch 00009: val_loss improved from 0.58369 to 0.57899, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 10s - loss: 0.5733 - acc: 0.7348 - val_loss: 0.5683 - val_acc: 0.7373

Epoch 00010: val_loss improved from 0.57899 to 0.56832, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 10s - loss: 0.5696 - acc: 0.7367 - val_loss: 0.5647 - val_acc: 0.7449

Epoch 00011: val_loss improved from 0.56832 to 0.56468, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 10s - loss: 0.5624 - acc: 0.7390 - val_loss: 0.5582 - val_acc: 0.7446

Epoch 00012: val_loss improved from 0.56468 to 0.55824, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 10s - loss: 0.5560 - acc: 0.7424 - val_loss: 0.5519 - val_acc: 0.7449

Epoch 00013: val_loss improved from 0.55824 to 0.55191, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 10s - loss: 0.5518 - acc: 0.7476 - val_loss: 0.5475 - val_acc: 0.7524

Epoch 00014: val_loss improved from 0.55191 to 0.54750, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 10s - loss: 0.5470 - acc: 0.7518 - val_loss: 0.5446 - val_acc: 0.7545

Epoch 00015: val_loss improved from 0.54750 to 0.54458, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 10s - loss: 0.5419 - acc: 0.7503 - val_loss: 0.5390 - val_acc: 0.7460

Epoch 00016: val_loss improved from 0.54458 to 0.53899, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 10s - loss: 0.5385 - acc: 0.7525 - val_loss: 0.5372 - val_acc: 0.7459

Epoch 00017: val_loss improved from 0.53899 to 0.53724, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 10s - loss: 0.5355 - acc: 0.7566 - val_loss: 0.5330 - val_acc: 0.7535

Epoch 00018: val_loss improved from 0.53724 to 0.53295, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 10s - loss: 0.5319 - acc: 0.7560 - val_loss: 0.5297 - val_acc: 0.7588

Epoch 00019: val_loss improved from 0.53295 to 0.52972, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 10s - loss: 0.5282 - acc: 0.7563 - val_loss: 0.5272 - val_acc: 0.7510

Epoch 00020: val_loss improved from 0.52972 to 0.52723, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 10s - loss: 0.5254 - acc: 0.7594 - val_loss: 0.5248 - val_acc: 0.7524

Epoch 00021: val_loss improved from 0.52723 to 0.52484, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 22/40
 - 10s - loss: 0.5237 - acc: 0.7575 - val_loss: 0.5261 - val_acc: 0.7577

Epoch 00022: val_loss did not improve from 0.52484
Epoch 23/40
 - 10s - loss: 0.5216 - acc: 0.7599 - val_loss: 0.5199 - val_acc: 0.7587

Epoch 00023: val_loss improved from 0.52484 to 0.51992, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 10s - loss: 0.5192 - acc: 0.7584 - val_loss: 0.5169 - val_acc: 0.7592

Epoch 00024: val_loss improved from 0.51992 to 0.51688, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 10s - loss: 0.5177 - acc: 0.7605 - val_loss: 0.5170 - val_acc: 0.7579

Epoch 00025: val_loss did not improve from 0.51688
Epoch 26/40
 - 10s - loss: 0.5149 - acc: 0.7607 - val_loss: 0.5116 - val_acc: 0.7584

Epoch 00026: val_loss improved from 0.51688 to 0.51159, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 10s - loss: 0.5132 - acc: 0.7625 - val_loss: 0.5150 - val_acc: 0.7626

Epoch 00027: val_loss did not improve from 0.51159
Epoch 28/40
 - 10s - loss: 0.5121 - acc: 0.7624 - val_loss: 0.5108 - val_acc: 0.7554

Epoch 00028: val_loss improved from 0.51159 to 0.51077, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 10s - loss: 0.5107 - acc: 0.7629 - val_loss: 0.5120 - val_acc: 0.7615

Epoch 00029: val_loss did not improve from 0.51077
Epoch 30/40
 - 10s - loss: 0.5093 - acc: 0.7644 - val_loss: 0.5082 - val_acc: 0.7624

Epoch 00030: val_loss improved from 0.51077 to 0.50815, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 10s - loss: 0.5082 - acc: 0.7631 - val_loss: 0.5064 - val_acc: 0.7582

Epoch 00031: val_loss improved from 0.50815 to 0.50642, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 10s - loss: 0.5070 - acc: 0.7626 - val_loss: 0.5057 - val_acc: 0.7634

Epoch 00032: val_loss improved from 0.50642 to 0.50575, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 10s - loss: 0.5055 - acc: 0.7652 - val_loss: 0.5042 - val_acc: 0.7590

Epoch 00033: val_loss improved from 0.50575 to 0.50419, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 10s - loss: 0.5046 - acc: 0.7638 - val_loss: 0.5064 - val_acc: 0.7529

Epoch 00034: val_loss did not improve from 0.50419
Epoch 35/40
 - 10s - loss: 0.5039 - acc: 0.7635 - val_loss: 0.5033 - val_acc: 0.7643

Epoch 00035: val_loss improved from 0.50419 to 0.50330, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 10s - loss: 0.5040 - acc: 0.7635 - val_loss: 0.5070 - val_acc: 0.7642

Epoch 00036: val_loss did not improve from 0.50330
Epoch 37/40
 - 10s - loss: 0.5032 - acc: 0.7635 - val_loss: 0.5140 - val_acc: 0.7628

Epoch 00037: val_loss did not improve from 0.50330
Epoch 38/40
 - 10s - loss: 0.5024 - acc: 0.7627 - val_loss: 0.5008 - val_acc: 0.7638

Epoch 00038: val_loss improved from 0.50330 to 0.50081, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 10s - loss: 0.5023 - acc: 0.7646 - val_loss: 0.5017 - val_acc: 0.7677

Epoch 00039: val_loss did not improve from 0.50081
Epoch 40/40
 - 10s - loss: 0.4984 - acc: 0.7682 - val_loss: 0.5054 - val_acc: 0.7487

Epoch 00040: val_loss did not improve from 0.50081

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 0s
1632/7440 [=====>........................] - ETA: 0s
1984/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2656/7440 [=========>....................] - ETA: 0s
2976/7440 [===========>..................] - ETA: 0s
3296/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3936/7440 [==============>...............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 158us/step
current Test accuracy: 0.8048387096774193
current auc_score ------------------>  0.8545902705515088

  32/7440 [..............................] - ETA: 19:16
 320/7440 [>.............................] - ETA: 1:52 
 640/7440 [=>............................] - ETA: 54s 
 960/7440 [==>...........................] - ETA: 34s
1280/7440 [====>.........................] - ETA: 25s
1600/7440 [=====>........................] - ETA: 19s
1920/7440 [======>.......................] - ETA: 15s
2240/7440 [========>.....................] - ETA: 12s
2592/7440 [=========>....................] - ETA: 10s
2944/7440 [==========>...................] - ETA: 8s 
3264/7440 [============>.................] - ETA: 7s
3584/7440 [=============>................] - ETA: 5s
3904/7440 [==============>...............] - ETA: 5s
4224/7440 [================>.............] - ETA: 4s
4544/7440 [=================>............] - ETA: 3s
4864/7440 [==================>...........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 2s
5504/7440 [=====================>........] - ETA: 2s
5824/7440 [======================>.......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6464/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 6s 830us/step
Best saved model Test accuracy: 0.8240591397849463
best saved model auc_score ------------------>  0.866558200369985
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_19 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_205 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_31 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_206 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_19  (None, 8)                 0         
_________________________________________________________________
dense_19 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 21s - loss: 0.6948 - acc: 0.5377 - val_loss: 0.6676 - val_acc: 0.6128

Epoch 00001: val_loss improved from inf to 0.66755, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 9s - loss: 0.6605 - acc: 0.6494 - val_loss: 0.6527 - val_acc: 0.6716

Epoch 00002: val_loss improved from 0.66755 to 0.65274, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 9s - loss: 0.6487 - acc: 0.6648 - val_loss: 0.6422 - val_acc: 0.6771

Epoch 00003: val_loss improved from 0.65274 to 0.64221, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 9s - loss: 0.6391 - acc: 0.6729 - val_loss: 0.6352 - val_acc: 0.6805

Epoch 00004: val_loss improved from 0.64221 to 0.63520, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 9s - loss: 0.6330 - acc: 0.6751 - val_loss: 0.6292 - val_acc: 0.6836

Epoch 00005: val_loss improved from 0.63520 to 0.62921, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 9s - loss: 0.6270 - acc: 0.6798 - val_loss: 0.6249 - val_acc: 0.6851

Epoch 00006: val_loss improved from 0.62921 to 0.62488, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 9s - loss: 0.6220 - acc: 0.6844 - val_loss: 0.6193 - val_acc: 0.6935

Epoch 00007: val_loss improved from 0.62488 to 0.61929, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 9s - loss: 0.6180 - acc: 0.6883 - val_loss: 0.6157 - val_acc: 0.6935

Epoch 00008: val_loss improved from 0.61929 to 0.61569, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 9s - loss: 0.6142 - acc: 0.6915 - val_loss: 0.6118 - val_acc: 0.6984

Epoch 00009: val_loss improved from 0.61569 to 0.61176, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 9s - loss: 0.6107 - acc: 0.6950 - val_loss: 0.6088 - val_acc: 0.7007

Epoch 00010: val_loss improved from 0.61176 to 0.60879, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 9s - loss: 0.6083 - acc: 0.6975 - val_loss: 0.6054 - val_acc: 0.7046

Epoch 00011: val_loss improved from 0.60879 to 0.60536, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 12/40
 - 9s - loss: 0.6048 - acc: 0.7033 - val_loss: 0.6025 - val_acc: 0.7085

Epoch 00012: val_loss improved from 0.60536 to 0.60250, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 9s - loss: 0.6015 - acc: 0.7052 - val_loss: 0.5996 - val_acc: 0.7093

Epoch 00013: val_loss improved from 0.60250 to 0.59960, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 9s - loss: 0.5976 - acc: 0.7064 - val_loss: 0.5957 - val_acc: 0.7107

Epoch 00014: val_loss improved from 0.59960 to 0.59571, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 9s - loss: 0.5950 - acc: 0.7106 - val_loss: 0.5922 - val_acc: 0.7126

Epoch 00015: val_loss improved from 0.59571 to 0.59221, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 9s - loss: 0.5910 - acc: 0.7111 - val_loss: 0.5892 - val_acc: 0.7165

Epoch 00016: val_loss improved from 0.59221 to 0.58919, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 17/40
 - 9s - loss: 0.5873 - acc: 0.7161 - val_loss: 0.5891 - val_acc: 0.7155

Epoch 00017: val_loss improved from 0.58919 to 0.58905, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 9s - loss: 0.5847 - acc: 0.7168 - val_loss: 0.5820 - val_acc: 0.7225

Epoch 00018: val_loss improved from 0.58905 to 0.58202, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 9s - loss: 0.5805 - acc: 0.7182 - val_loss: 0.5780 - val_acc: 0.7243

Epoch 00019: val_loss improved from 0.58202 to 0.57799, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 9s - loss: 0.5774 - acc: 0.7226 - val_loss: 0.5754 - val_acc: 0.7284

Epoch 00020: val_loss improved from 0.57799 to 0.57536, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 9s - loss: 0.5744 - acc: 0.7240 - val_loss: 0.5794 - val_acc: 0.7161

Epoch 00021: val_loss did not improve from 0.57536
Epoch 22/40
 - 9s - loss: 0.5717 - acc: 0.7238 - val_loss: 0.5701 - val_acc: 0.7274

Epoch 00022: val_loss improved from 0.57536 to 0.57010, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 9s - loss: 0.5686 - acc: 0.7277 - val_loss: 0.5670 - val_acc: 0.7324

Epoch 00023: val_loss improved from 0.57010 to 0.56699, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 9s - loss: 0.5655 - acc: 0.7320 - val_loss: 0.5652 - val_acc: 0.7336

Epoch 00024: val_loss improved from 0.56699 to 0.56520, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 25/40
 - 9s - loss: 0.5650 - acc: 0.7322 - val_loss: 0.5637 - val_acc: 0.7329

Epoch 00025: val_loss improved from 0.56520 to 0.56366, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 9s - loss: 0.5618 - acc: 0.7323 - val_loss: 0.5635 - val_acc: 0.7268

Epoch 00026: val_loss improved from 0.56366 to 0.56346, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 9s - loss: 0.5601 - acc: 0.7342 - val_loss: 0.5599 - val_acc: 0.7341

Epoch 00027: val_loss improved from 0.56346 to 0.55991, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 28/40
 - 9s - loss: 0.5577 - acc: 0.7361 - val_loss: 0.5565 - val_acc: 0.7356

Epoch 00028: val_loss improved from 0.55991 to 0.55651, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 9s - loss: 0.5560 - acc: 0.7373 - val_loss: 0.5571 - val_acc: 0.7348

Epoch 00029: val_loss did not improve from 0.55651
Epoch 30/40
 - 9s - loss: 0.5536 - acc: 0.7383 - val_loss: 0.5549 - val_acc: 0.7378

Epoch 00030: val_loss improved from 0.55651 to 0.55494, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 31/40
 - 9s - loss: 0.5524 - acc: 0.7384 - val_loss: 0.5510 - val_acc: 0.7392

Epoch 00031: val_loss improved from 0.55494 to 0.55104, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 32/40
 - 9s - loss: 0.5498 - acc: 0.7401 - val_loss: 0.5487 - val_acc: 0.7410

Epoch 00032: val_loss improved from 0.55104 to 0.54870, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 9s - loss: 0.5488 - acc: 0.7421 - val_loss: 0.5537 - val_acc: 0.7352

Epoch 00033: val_loss did not improve from 0.54870
Epoch 34/40
 - 9s - loss: 0.5481 - acc: 0.7407 - val_loss: 0.5535 - val_acc: 0.7303

Epoch 00034: val_loss did not improve from 0.54870
Epoch 35/40
 - 9s - loss: 0.5472 - acc: 0.7401 - val_loss: 0.5467 - val_acc: 0.7388

Epoch 00035: val_loss improved from 0.54870 to 0.54670, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 36/40
 - 9s - loss: 0.5448 - acc: 0.7433 - val_loss: 0.5424 - val_acc: 0.7462

Epoch 00036: val_loss improved from 0.54670 to 0.54241, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 9s - loss: 0.5437 - acc: 0.7454 - val_loss: 0.5500 - val_acc: 0.7398

Epoch 00037: val_loss did not improve from 0.54241
Epoch 38/40
 - 9s - loss: 0.5419 - acc: 0.7460 - val_loss: 0.5478 - val_acc: 0.7362

Epoch 00038: val_loss did not improve from 0.54241
Epoch 39/40
 - 9s - loss: 0.5418 - acc: 0.7460 - val_loss: 0.5415 - val_acc: 0.7456

Epoch 00039: val_loss improved from 0.54241 to 0.54152, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 9s - loss: 0.5413 - acc: 0.7461 - val_loss: 0.5424 - val_acc: 0.7430

Epoch 00040: val_loss did not improve from 0.54152

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 0s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 161us/step
current Test accuracy: 0.8602150537634409
current auc_score ------------------>  0.8772909079084286

  32/7440 [..............................] - ETA: 19:35
 320/7440 [>.............................] - ETA: 1:54 
 640/7440 [=>............................] - ETA: 55s 
 960/7440 [==>...........................] - ETA: 35s
1280/7440 [====>.........................] - ETA: 25s
1600/7440 [=====>........................] - ETA: 19s
1920/7440 [======>.......................] - ETA: 15s
2240/7440 [========>.....................] - ETA: 12s
2560/7440 [=========>....................] - ETA: 10s
2880/7440 [==========>...................] - ETA: 8s 
3232/7440 [============>.................] - ETA: 7s
3552/7440 [=============>................] - ETA: 6s
3872/7440 [==============>...............] - ETA: 5s
4192/7440 [===============>..............] - ETA: 4s
4512/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 843us/step
Best saved model Test accuracy: 0.8529569892473118
best saved model auc_score ------------------>  0.8781057781246386
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_20 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_207 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_32 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_208 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_33 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_209 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_20  (None, 4)                 0         
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/40
 - 23s - loss: 0.7087 - acc: 0.4992 - val_loss: 0.6774 - val_acc: 0.5041

Epoch 00001: val_loss improved from inf to 0.67744, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 2/40
 - 10s - loss: 0.6677 - acc: 0.5793 - val_loss: 0.6614 - val_acc: 0.6473

Epoch 00002: val_loss improved from 0.67744 to 0.66141, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 3/40
 - 10s - loss: 0.6581 - acc: 0.6847 - val_loss: 0.6543 - val_acc: 0.6920

Epoch 00003: val_loss improved from 0.66141 to 0.65426, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 4/40
 - 10s - loss: 0.6516 - acc: 0.7069 - val_loss: 0.6492 - val_acc: 0.7022

Epoch 00004: val_loss improved from 0.65426 to 0.64922, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 5/40
 - 10s - loss: 0.6438 - acc: 0.7118 - val_loss: 0.6391 - val_acc: 0.7027

Epoch 00005: val_loss improved from 0.64922 to 0.63912, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 6/40
 - 10s - loss: 0.6311 - acc: 0.7128 - val_loss: 0.6235 - val_acc: 0.7090

Epoch 00006: val_loss improved from 0.63912 to 0.62346, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 7/40
 - 10s - loss: 0.6195 - acc: 0.7140 - val_loss: 0.6134 - val_acc: 0.7126

Epoch 00007: val_loss improved from 0.62346 to 0.61339, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 8/40
 - 10s - loss: 0.6108 - acc: 0.7177 - val_loss: 0.6056 - val_acc: 0.7214

Epoch 00008: val_loss improved from 0.61339 to 0.60563, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 9/40
 - 10s - loss: 0.6046 - acc: 0.7197 - val_loss: 0.5994 - val_acc: 0.7214

Epoch 00009: val_loss improved from 0.60563 to 0.59944, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 10/40
 - 10s - loss: 0.5981 - acc: 0.7225 - val_loss: 0.5936 - val_acc: 0.7208

Epoch 00010: val_loss improved from 0.59944 to 0.59363, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 11/40
 - 10s - loss: 0.5924 - acc: 0.7246 - val_loss: 0.5942 - val_acc: 0.7150

Epoch 00011: val_loss did not improve from 0.59363
Epoch 12/40
 - 10s - loss: 0.5868 - acc: 0.7252 - val_loss: 0.5814 - val_acc: 0.7279

Epoch 00012: val_loss improved from 0.59363 to 0.58138, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 13/40
 - 10s - loss: 0.5813 - acc: 0.7287 - val_loss: 0.5768 - val_acc: 0.7314

Epoch 00013: val_loss improved from 0.58138 to 0.57675, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 14/40
 - 10s - loss: 0.5760 - acc: 0.7325 - val_loss: 0.5698 - val_acc: 0.7308

Epoch 00014: val_loss improved from 0.57675 to 0.56981, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 15/40
 - 10s - loss: 0.5703 - acc: 0.7358 - val_loss: 0.5627 - val_acc: 0.7343

Epoch 00015: val_loss improved from 0.56981 to 0.56268, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 16/40
 - 10s - loss: 0.5644 - acc: 0.7362 - val_loss: 0.5643 - val_acc: 0.7420

Epoch 00016: val_loss did not improve from 0.56268
Epoch 17/40
 - 10s - loss: 0.5583 - acc: 0.7429 - val_loss: 0.5551 - val_acc: 0.7410

Epoch 00017: val_loss improved from 0.56268 to 0.55515, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 18/40
 - 10s - loss: 0.5532 - acc: 0.7464 - val_loss: 0.5488 - val_acc: 0.7477

Epoch 00018: val_loss improved from 0.55515 to 0.54877, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 19/40
 - 10s - loss: 0.5496 - acc: 0.7470 - val_loss: 0.5416 - val_acc: 0.7515

Epoch 00019: val_loss improved from 0.54877 to 0.54156, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 20/40
 - 10s - loss: 0.5447 - acc: 0.7504 - val_loss: 0.5375 - val_acc: 0.7499

Epoch 00020: val_loss improved from 0.54156 to 0.53752, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 21/40
 - 10s - loss: 0.5412 - acc: 0.7496 - val_loss: 0.5514 - val_acc: 0.7431

Epoch 00021: val_loss did not improve from 0.53752
Epoch 22/40
 - 10s - loss: 0.5369 - acc: 0.7520 - val_loss: 0.5316 - val_acc: 0.7535

Epoch 00022: val_loss improved from 0.53752 to 0.53164, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 23/40
 - 10s - loss: 0.5343 - acc: 0.7536 - val_loss: 0.5300 - val_acc: 0.7526

Epoch 00023: val_loss improved from 0.53164 to 0.53005, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 24/40
 - 10s - loss: 0.5321 - acc: 0.7541 - val_loss: 0.5476 - val_acc: 0.7482

Epoch 00024: val_loss did not improve from 0.53005
Epoch 25/40
 - 10s - loss: 0.5298 - acc: 0.7547 - val_loss: 0.5239 - val_acc: 0.7570

Epoch 00025: val_loss improved from 0.53005 to 0.52388, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 26/40
 - 10s - loss: 0.5265 - acc: 0.7566 - val_loss: 0.5208 - val_acc: 0.7573

Epoch 00026: val_loss improved from 0.52388 to 0.52077, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 27/40
 - 10s - loss: 0.5242 - acc: 0.7555 - val_loss: 0.5214 - val_acc: 0.7530

Epoch 00027: val_loss did not improve from 0.52077
Epoch 28/40
 - 10s - loss: 0.5216 - acc: 0.7580 - val_loss: 0.5195 - val_acc: 0.7531

Epoch 00028: val_loss improved from 0.52077 to 0.51950, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 29/40
 - 10s - loss: 0.5218 - acc: 0.7552 - val_loss: 0.5362 - val_acc: 0.7393

Epoch 00029: val_loss did not improve from 0.51950
Epoch 30/40
 - 10s - loss: 0.5191 - acc: 0.7560 - val_loss: 0.5441 - val_acc: 0.7524

Epoch 00030: val_loss did not improve from 0.51950
Epoch 31/40
 - 10s - loss: 0.5176 - acc: 0.7578 - val_loss: 0.5679 - val_acc: 0.7095

Epoch 00031: val_loss did not improve from 0.51950
Epoch 32/40
 - 10s - loss: 0.5168 - acc: 0.7588 - val_loss: 0.5164 - val_acc: 0.7572

Epoch 00032: val_loss improved from 0.51950 to 0.51643, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 33/40
 - 10s - loss: 0.5143 - acc: 0.7602 - val_loss: 0.5163 - val_acc: 0.7578

Epoch 00033: val_loss improved from 0.51643 to 0.51629, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 34/40
 - 10s - loss: 0.5123 - acc: 0.7577 - val_loss: 0.5228 - val_acc: 0.7545

Epoch 00034: val_loss did not improve from 0.51629
Epoch 35/40
 - 10s - loss: 0.5117 - acc: 0.7593 - val_loss: 0.5951 - val_acc: 0.6862

Epoch 00035: val_loss did not improve from 0.51629
Epoch 36/40
 - 10s - loss: 0.5099 - acc: 0.7591 - val_loss: 0.5108 - val_acc: 0.7572

Epoch 00036: val_loss improved from 0.51629 to 0.51075, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 37/40
 - 10s - loss: 0.5104 - acc: 0.7581 - val_loss: 0.5053 - val_acc: 0.7559

Epoch 00037: val_loss improved from 0.51075 to 0.50534, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 38/40
 - 10s - loss: 0.5087 - acc: 0.7583 - val_loss: 0.5049 - val_acc: 0.7559

Epoch 00038: val_loss improved from 0.50534 to 0.50491, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 39/40
 - 10s - loss: 0.5072 - acc: 0.7602 - val_loss: 0.5024 - val_acc: 0.7558

Epoch 00039: val_loss improved from 0.50491 to 0.50236, saving model to keras_densenet_simple_wt_29Sept_2200.h5
Epoch 40/40
 - 10s - loss: 0.5070 - acc: 0.7595 - val_loss: 0.5164 - val_acc: 0.7580

Epoch 00040: val_loss did not improve from 0.50236

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
1024/7440 [===>..........................] - ETA: 1s
1344/7440 [====>.........................] - ETA: 0s
1664/7440 [=====>........................] - ETA: 0s
2016/7440 [=======>......................] - ETA: 0s
2368/7440 [========>.....................] - ETA: 0s
2688/7440 [=========>....................] - ETA: 0s
3008/7440 [===========>..................] - ETA: 0s
3328/7440 [============>.................] - ETA: 0s
3680/7440 [=============>................] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 158us/step
current Test accuracy: 0.819758064516129
current auc_score ------------------>  0.8644084937565036

  32/7440 [..............................] - ETA: 19:32
 320/7440 [>.............................] - ETA: 1:53 
 672/7440 [=>............................] - ETA: 52s 
 992/7440 [===>..........................] - ETA: 33s
1312/7440 [====>.........................] - ETA: 24s
1632/7440 [=====>........................] - ETA: 18s
1952/7440 [======>.......................] - ETA: 15s
2272/7440 [========>.....................] - ETA: 12s
2624/7440 [=========>....................] - ETA: 10s
2976/7440 [===========>..................] - ETA: 8s 
3296/7440 [============>.................] - ETA: 7s
3648/7440 [=============>................] - ETA: 5s
3968/7440 [===============>..............] - ETA: 4s
4288/7440 [================>.............] - ETA: 4s
4608/7440 [=================>............] - ETA: 3s
4928/7440 [==================>...........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 1s
5888/7440 [======================>.......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 839us/step
Best saved model Test accuracy: 0.8186827956989248
best saved model auc_score ------------------>  0.8590297650017343
best model <keras.engine.training.Model object at 0x7f4868f096a0>
best run {'depth': 3, 'growth_rate': 0, 'nb_dense_block': 1}
Evalutation of best performing model:
[0.42677464158304274, 0.8341397849462365]
val roc_auc_score 0.914
----------trials-------------
{'depth': [0], 'growth_rate': [1], 'nb_dense_block': [0]} -0.8631193273788879
{'depth': [1], 'growth_rate': [2], 'nb_dense_block': [0]} -0.898872954965892
{'depth': [4], 'growth_rate': [2], 'nb_dense_block': [1]} -0.8955412836744132
{'depth': [1], 'growth_rate': [1], 'nb_dense_block': [1]} -0.9000390218522373
{'depth': [3], 'growth_rate': [0], 'nb_dense_block': [1]} -0.8947200182101975
{'depth': [2], 'growth_rate': [3], 'nb_dense_block': [0]} -0.8915936090877558
{'depth': [0], 'growth_rate': [3], 'nb_dense_block': [1]} -0.8734719693028095
{'depth': [3], 'growth_rate': [1], 'nb_dense_block': [0]} -0.8576685165915134
{'depth': [2], 'growth_rate': [3], 'nb_dense_block': [0]} -0.8801108148340848
{'depth': [3], 'growth_rate': [0], 'nb_dense_block': [1]} -0.9143824791883456
{'depth': [3], 'growth_rate': [1], 'nb_dense_block': [1]} -0.9030094519597642
{'depth': [0], 'growth_rate': [0], 'nb_dense_block': [1]} -0.8665751098392878
{'depth': [3], 'growth_rate': [1], 'nb_dense_block': [1]} -0.9030150884495316
{'depth': [4], 'growth_rate': [1], 'nb_dense_block': [0]} -0.8876029020696035
{'depth': [0], 'growth_rate': [3], 'nb_dense_block': [1]} -0.8636792548271477
{'depth': [0], 'growth_rate': [1], 'nb_dense_block': [1]} -0.8732145696034224
{'depth': [1], 'growth_rate': [1], 'nb_dense_block': [1]} -0.8853298791767834
{'depth': [0], 'growth_rate': [0], 'nb_dense_block': [1]} -0.866558200369985
{'depth': [0], 'growth_rate': [3], 'nb_dense_block': [0]} -0.8781057781246386
{'depth': [0], 'growth_rate': [2], 'nb_dense_block': [1]} -0.8590297650017343
python evaluate_saved_model_simple.py
