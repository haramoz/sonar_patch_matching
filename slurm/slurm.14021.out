python hello-world.py
python hyperas_simple.py
python hyperas_contrastive_loss.py
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
python keras_densenet_simple.py
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import pickle
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'depth': hp.choice('depth', [7,10,13,16,19,22,25]),
        'nb_dense_block': hp.choice('nb_dense_block', [1,2,3]),
        'growth_rate': hp.choice('growth_rate', [6,8,10,12,14,16]),
    }

>>> Functions
  1: def process_data():
  2:     f = h5py.File('matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  3:     ln_training = 39840
  4:     X_train = f['X_train'].value
  5:     X_train_resize = X_train[0:ln_training,:,:,:]
  6:     #print(X_train_resize.shape)
  7:     y_train = f['y_train'].value
  8:     y_train_resize = y_train[0:ln_training,]
  9:     y_train_categorical = np_utils.to_categorical(y_train_resize, 2)
 10:     #X_reshaped = X_train_resize.reshape(*X_train_resize.shape[:1], -2)
 11:     #print(X_reshaped.shape)
 12:     ln_validation = 7440
 13:     X_val = f['X_val'].value
 14:     X_val_resize = X_val[0:ln_validation,:,:,:]
 15: 
 16:     #X_val_reshaped = X_val
 17:     #X_val_reshaped = X_val_resize.reshape(*X_val_resize.shape[:1], -2)
 18: 
 19:     y_val = f['y_val'].value
 20:     y_val_reshaped = y_val[0:ln_validation]
 21:     y_val_categorical = np_utils.to_categorical(y_val_reshaped, 2)
 22:     return X_train,y_train,X_val,y_val
 23: 
 24: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 30
   4:     #input_shape = (1,96,96)
   5:     es_patience = 5
   6:     lr_patience = 5
   7:     dropout = None
   8:     depth = space['depth']
   9:     nb_dense_block = space['nb_dense_block']
  10:     nb_filter = 16
  11:     growth_rate = space['growth_rate']
  12:     weight_decay = 1E-4
  13:     lr = 3E-4
  14:     weight_file = 'keras_densenet_simple_wt_28Sept_1352.h5'
  15:     
  16:     nb_classes = 1
  17:     img_dim = (2,96,96) 
  18:     n_channels = 2 
  19: 
  20:     
  21:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  22:                  growth_rate=growth_rate, nb_filter=nb_filter,
  23:                  dropout_rate=dropout,activation='sigmoid',
  24:                  input_shape=img_dim,include_top=True,
  25:                  bottleneck=True,reduction=0.5,
  26:                  classes=nb_classes,pooling='avg',
  27:                  weights=None)
  28:     
  29: 
  30:     model.summary()
  31:     opt = Adam(lr=lr)
  32:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  33: 
  34:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  35:     #es = EarlyStopping(monitor='val_acc', patience=es_patience,verbose=1,restore_best_weights=True)
  36:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  37: 
  38:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  39: 
  40:     model.fit(X_train,y_train,
  41:           batch_size=64,
  42:           epochs=epochs,
  43:           callbacks=[es,lr_reducer,checkpointer],
  44:           validation_data=(X_val,y_val),
  45:           verbose=2)
  46:     
  47:     score, acc = model.evaluate(X_val, y_val)
  48:     print('current Test accuracy:', acc)
  49:     pred = model.predict(X_val)
  50:     auc_score = roc_auc_score(y_val,pred)
  51:     print("current auc_score ------------------> ",auc_score)
  52: 
  53:     model = load_model(weight_file) #This is the best model
  54:     score, acc = model.evaluate(X_val, y_val)
  55:     print('Best saved model Test accuracy:', acc)
  56:     pred = model.predict(X_val)
  57:     auc_score = roc_auc_score(y_val,pred)
  58:     print("best saved model auc_score ------------------> ",auc_score)
  59: 
  60:     
  61:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  62: 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_1 (Activation)    (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_1 (Average (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_2 (Activation)    (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_2 (Average (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_3 (Activation)    (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_1 ( (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 7s - loss: 0.6763 - acc: 0.6085 - val_loss: 0.6219 - val_acc: 0.7895

Epoch 00001: val_loss improved from inf to 0.62190, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 6s - loss: 0.6476 - acc: 0.6755 - val_loss: 0.5864 - val_acc: 0.7965

Epoch 00002: val_loss improved from 0.62190 to 0.58636, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 6s - loss: 0.6275 - acc: 0.6856 - val_loss: 0.5571 - val_acc: 0.7840

Epoch 00003: val_loss improved from 0.58636 to 0.55710, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 6s - loss: 0.6106 - acc: 0.6967 - val_loss: 0.5333 - val_acc: 0.7965

Epoch 00004: val_loss improved from 0.55710 to 0.53332, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 6s - loss: 0.5949 - acc: 0.7116 - val_loss: 0.5253 - val_acc: 0.8188

Epoch 00005: val_loss improved from 0.53332 to 0.52525, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 6s - loss: 0.5800 - acc: 0.7216 - val_loss: 0.5251 - val_acc: 0.7995

Epoch 00006: val_loss improved from 0.52525 to 0.52512, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 6s - loss: 0.5686 - acc: 0.7310 - val_loss: 0.5224 - val_acc: 0.8078

Epoch 00007: val_loss improved from 0.52512 to 0.52240, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 6s - loss: 0.5575 - acc: 0.7365 - val_loss: 0.5264 - val_acc: 0.8339

Epoch 00008: val_loss did not improve from 0.52240
Epoch 9/30
 - 6s - loss: 0.5479 - acc: 0.7431 - val_loss: 0.5262 - val_acc: 0.7786

Epoch 00009: val_loss did not improve from 0.52240
Epoch 10/30
 - 6s - loss: 0.5401 - acc: 0.7474 - val_loss: 0.5038 - val_acc: 0.8539

Epoch 00010: val_loss improved from 0.52240 to 0.50378, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 6s - loss: 0.5331 - acc: 0.7508 - val_loss: 0.5204 - val_acc: 0.8051

Epoch 00011: val_loss did not improve from 0.50378
Epoch 12/30
 - 6s - loss: 0.5265 - acc: 0.7535 - val_loss: 0.5461 - val_acc: 0.7876

Epoch 00012: val_loss did not improve from 0.50378
Epoch 13/30
 - 6s - loss: 0.5205 - acc: 0.7562 - val_loss: 0.7133 - val_acc: 0.7345

Epoch 00013: val_loss did not improve from 0.50378
Epoch 14/30
 - 6s - loss: 0.5164 - acc: 0.7575 - val_loss: 0.5342 - val_acc: 0.8054

Epoch 00014: val_loss did not improve from 0.50378
Epoch 15/30
 - 6s - loss: 0.5125 - acc: 0.7576 - val_loss: 0.5440 - val_acc: 0.8144

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.50378
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 0s
 704/7440 [=>............................] - ETA: 0s
1408/7440 [====>.........................] - ETA: 0s
2080/7440 [=======>......................] - ETA: 0s
2784/7440 [==========>...................] - ETA: 0s
3456/7440 [============>.................] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7440/7440 [==============================] - 1s 76us/step
current Test accuracy: 0.8143817204301075
current auc_score ------------------>  0.857387812174818

  32/7440 [..............................] - ETA: 21s
 608/7440 [=>............................] - ETA: 1s 
1216/7440 [===>..........................] - ETA: 0s
1824/7440 [======>.......................] - ETA: 0s
2432/7440 [========>.....................] - ETA: 0s
3040/7440 [===========>..................] - ETA: 0s
3648/7440 [=============>................] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 97us/step
Best saved model Test accuracy: 0.8538978494623656
best saved model auc_score ------------------>  0.8753092481789803
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_4[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_6[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 32, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 96, 96)   128         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 32)           0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            33          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 6,753
Trainable params: 6,481
Non-trainable params: 272
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 18s - loss: 0.6211 - acc: 0.6996 - val_loss: 0.5684 - val_acc: 0.7610

Epoch 00001: val_loss improved from inf to 0.56845, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 17s - loss: 0.5747 - acc: 0.7353 - val_loss: 0.5473 - val_acc: 0.8320

Epoch 00002: val_loss improved from 0.56845 to 0.54733, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 17s - loss: 0.5441 - acc: 0.7528 - val_loss: 0.5213 - val_acc: 0.8378

Epoch 00003: val_loss improved from 0.54733 to 0.52133, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 17s - loss: 0.5258 - acc: 0.7583 - val_loss: 0.4955 - val_acc: 0.8405

Epoch 00004: val_loss improved from 0.52133 to 0.49554, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 16s - loss: 0.5136 - acc: 0.7610 - val_loss: 0.4921 - val_acc: 0.8352

Epoch 00005: val_loss improved from 0.49554 to 0.49207, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 17s - loss: 0.5029 - acc: 0.7652 - val_loss: 0.5485 - val_acc: 0.8109

Epoch 00006: val_loss did not improve from 0.49207
Epoch 7/30
 - 17s - loss: 0.4938 - acc: 0.7672 - val_loss: 0.4867 - val_acc: 0.8380

Epoch 00007: val_loss improved from 0.49207 to 0.48665, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 17s - loss: 0.4866 - acc: 0.7722 - val_loss: 0.5264 - val_acc: 0.8070

Epoch 00008: val_loss did not improve from 0.48665
Epoch 9/30
 - 16s - loss: 0.4804 - acc: 0.7749 - val_loss: 0.4842 - val_acc: 0.8335

Epoch 00009: val_loss improved from 0.48665 to 0.48420, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 16s - loss: 0.4754 - acc: 0.7783 - val_loss: 0.4791 - val_acc: 0.8192

Epoch 00010: val_loss improved from 0.48420 to 0.47908, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 17s - loss: 0.4707 - acc: 0.7794 - val_loss: 0.5562 - val_acc: 0.7630

Epoch 00011: val_loss did not improve from 0.47908
Epoch 12/30
 - 17s - loss: 0.4656 - acc: 0.7835 - val_loss: 0.4718 - val_acc: 0.8278

Epoch 00012: val_loss improved from 0.47908 to 0.47177, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 13/30
 - 17s - loss: 0.4619 - acc: 0.7850 - val_loss: 0.4823 - val_acc: 0.8126

Epoch 00013: val_loss did not improve from 0.47177
Epoch 14/30
 - 17s - loss: 0.4591 - acc: 0.7840 - val_loss: 0.4829 - val_acc: 0.7954

Epoch 00014: val_loss did not improve from 0.47177
Epoch 15/30
 - 17s - loss: 0.4554 - acc: 0.7868 - val_loss: 0.5536 - val_acc: 0.7798

Epoch 00015: val_loss did not improve from 0.47177
Epoch 16/30
 - 17s - loss: 0.4501 - acc: 0.7907 - val_loss: 0.5802 - val_acc: 0.7437

Epoch 00016: val_loss did not improve from 0.47177
Epoch 17/30
 - 17s - loss: 0.4477 - acc: 0.7911 - val_loss: 0.5051 - val_acc: 0.7702

Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00017: val_loss did not improve from 0.47177
Epoch 00017: early stopping

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2240/7440 [========>.....................] - ETA: 0s
2560/7440 [=========>....................] - ETA: 0s
2880/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 167us/step
current Test accuracy: 0.7701612903225806
current auc_score ------------------>  0.8380677679500521

  32/7440 [..............................] - ETA: 42s
 320/7440 [>.............................] - ETA: 5s 
 640/7440 [=>............................] - ETA: 3s
 960/7440 [==>...........................] - ETA: 2s
1280/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2240/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 193us/step
Best saved model Test accuracy: 0.8278225806451613
best saved model auc_score ------------------>  0.8750587495664238
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_9[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_11[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_12[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 36, 96, 96)   0           concatenate_3[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_13[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 46, 96, 96)   0           concatenate_4[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_15[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_16[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 33, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_18[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 43, 48, 48)   0           concatenate_6[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_20[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 53, 48, 48)   0           concatenate_7[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 53, 48, 48)   212         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 53, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 53)           0           activation_22[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            54          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 32,144
Trainable params: 31,112
Non-trainable params: 1,032
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 37s - loss: 0.5697 - acc: 0.7521 - val_loss: 0.5086 - val_acc: 0.8192

Epoch 00001: val_loss improved from inf to 0.50859, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 34s - loss: 0.4984 - acc: 0.7840 - val_loss: 0.5314 - val_acc: 0.8177

Epoch 00002: val_loss did not improve from 0.50859
Epoch 3/30
 - 34s - loss: 0.4645 - acc: 0.8016 - val_loss: 0.5924 - val_acc: 0.7513

Epoch 00003: val_loss did not improve from 0.50859
Epoch 4/30
 - 34s - loss: 0.4425 - acc: 0.8139 - val_loss: 0.5368 - val_acc: 0.7691

Epoch 00004: val_loss did not improve from 0.50859
Epoch 5/30
 - 34s - loss: 0.4250 - acc: 0.8224 - val_loss: 0.5129 - val_acc: 0.8001

Epoch 00005: val_loss did not improve from 0.50859
Epoch 6/30
 - 34s - loss: 0.4074 - acc: 0.8315 - val_loss: 0.5771 - val_acc: 0.7316

Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00006: val_loss did not improve from 0.50859
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 704/7440 [=>............................] - ETA: 2s
 864/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1216/7440 [===>..........................] - ETA: 1s
1408/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2464/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3136/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 314us/step
current Test accuracy: 0.7315860215053763
current auc_score ------------------>  0.846084879754885

  32/7440 [..............................] - ETA: 1:51
 192/7440 [..............................] - ETA: 20s 
 352/7440 [>.............................] - ETA: 11s
 512/7440 [=>............................] - ETA: 8s 
 672/7440 [=>............................] - ETA: 6s
 864/7440 [==>...........................] - ETA: 5s
1056/7440 [===>..........................] - ETA: 4s
1248/7440 [====>.........................] - ETA: 4s
1440/7440 [====>.........................] - ETA: 3s
1632/7440 [=====>........................] - ETA: 3s
1824/7440 [======>.......................] - ETA: 3s
2016/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2400/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2784/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3168/7440 [===========>..................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3936/7440 [==============>...............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4320/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5088/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 373us/step
Best saved model Test accuracy: 0.8192204301075269
best saved model auc_score ------------------>  0.8799525956758006
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_23[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_24[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_25[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 48, 96, 96)   0           concatenate_9[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_27[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 64, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_29[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_30[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 48, 48, 48)   0           average_pooling2d_4[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_32[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 64, 48, 48)   0           concatenate_12[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_34[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 80, 48, 48)   0           concatenate_13[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 80, 48, 48)   320         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 80, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 80)           0           activation_36[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            81          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 76,145
Trainable params: 74,609
Non-trainable params: 1,536
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 48s - loss: 0.5777 - acc: 0.7619 - val_loss: 0.5325 - val_acc: 0.8454

Epoch 00001: val_loss improved from inf to 0.53251, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 44s - loss: 0.4973 - acc: 0.7974 - val_loss: 0.4953 - val_acc: 0.8105

Epoch 00002: val_loss improved from 0.53251 to 0.49533, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 44s - loss: 0.4569 - acc: 0.8157 - val_loss: 0.5822 - val_acc: 0.7793

Epoch 00003: val_loss did not improve from 0.49533
Epoch 4/30
 - 44s - loss: 0.4325 - acc: 0.8276 - val_loss: 0.8801 - val_acc: 0.6923

Epoch 00004: val_loss did not improve from 0.49533
Epoch 5/30
 - 44s - loss: 0.4124 - acc: 0.8410 - val_loss: 0.5112 - val_acc: 0.8022

Epoch 00005: val_loss did not improve from 0.49533
Epoch 6/30
 - 44s - loss: 0.3980 - acc: 0.8464 - val_loss: 0.4838 - val_acc: 0.7972

Epoch 00006: val_loss improved from 0.49533 to 0.48380, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 44s - loss: 0.3815 - acc: 0.8560 - val_loss: 0.6025 - val_acc: 0.7773

Epoch 00007: val_loss did not improve from 0.48380
Epoch 8/30
 - 44s - loss: 0.3690 - acc: 0.8632 - val_loss: 0.5076 - val_acc: 0.8081

Epoch 00008: val_loss did not improve from 0.48380
Epoch 9/30
 - 44s - loss: 0.3595 - acc: 0.8660 - val_loss: 0.8201 - val_acc: 0.6894

Epoch 00009: val_loss did not improve from 0.48380
Epoch 10/30
 - 44s - loss: 0.3481 - acc: 0.8731 - val_loss: 0.4957 - val_acc: 0.8164

Epoch 00010: val_loss did not improve from 0.48380
Epoch 11/30
 - 44s - loss: 0.3363 - acc: 0.8785 - val_loss: 0.6646 - val_acc: 0.7551

Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00011: val_loss did not improve from 0.48380
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 544/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 408us/step
current Test accuracy: 0.7551075268817204
current auc_score ------------------>  0.8539374494161175

  32/7440 [..............................] - ETA: 2:47
 160/7440 [..............................] - ETA: 35s 
 288/7440 [>.............................] - ETA: 20s
 416/7440 [>.............................] - ETA: 14s
 544/7440 [=>............................] - ETA: 11s
 672/7440 [=>............................] - ETA: 9s 
 800/7440 [==>...........................] - ETA: 8s
 928/7440 [==>...........................] - ETA: 7s
1056/7440 [===>..........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 6s
1312/7440 [====>.........................] - ETA: 5s
1440/7440 [====>.........................] - ETA: 5s
1568/7440 [=====>........................] - ETA: 5s
1696/7440 [=====>........................] - ETA: 4s
1824/7440 [======>.......................] - ETA: 4s
1952/7440 [======>.......................] - ETA: 4s
2080/7440 [=======>......................] - ETA: 3s
2208/7440 [=======>......................] - ETA: 3s
2336/7440 [========>.....................] - ETA: 3s
2464/7440 [========>.....................] - ETA: 3s
2592/7440 [=========>....................] - ETA: 3s
2720/7440 [=========>....................] - ETA: 3s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 2s
3744/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 494us/step
Best saved model Test accuracy: 0.7971774193548387
best saved model auc_score ------------------>  0.8951674326511735
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_37[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_39[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_40[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 40, 96, 96)   0           concatenate_15[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_41[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_42[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 52, 96, 96)   0           concatenate_16[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 52, 96, 96)   208         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 52, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 52)           0           activation_43[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            53          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 21,045
Trainable params: 20,485
Non-trainable params: 560
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.5817 - acc: 0.7406 - val_loss: 0.5243 - val_acc: 0.8371

Epoch 00001: val_loss improved from inf to 0.52433, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 29s - loss: 0.5209 - acc: 0.7646 - val_loss: 0.4781 - val_acc: 0.8103

Epoch 00002: val_loss improved from 0.52433 to 0.47812, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 29s - loss: 0.4937 - acc: 0.7729 - val_loss: 0.4732 - val_acc: 0.8219

Epoch 00003: val_loss improved from 0.47812 to 0.47316, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 29s - loss: 0.4766 - acc: 0.7838 - val_loss: 0.4570 - val_acc: 0.8327

Epoch 00004: val_loss improved from 0.47316 to 0.45701, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 29s - loss: 0.4613 - acc: 0.7911 - val_loss: 0.4706 - val_acc: 0.8269

Epoch 00005: val_loss did not improve from 0.45701
Epoch 6/30
 - 29s - loss: 0.4503 - acc: 0.7964 - val_loss: 0.5738 - val_acc: 0.7460

Epoch 00006: val_loss did not improve from 0.45701
Epoch 7/30
 - 29s - loss: 0.4401 - acc: 0.8012 - val_loss: 0.4326 - val_acc: 0.8254

Epoch 00007: val_loss improved from 0.45701 to 0.43258, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 29s - loss: 0.4315 - acc: 0.8078 - val_loss: 0.4195 - val_acc: 0.8378

Epoch 00008: val_loss improved from 0.43258 to 0.41953, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 29s - loss: 0.4245 - acc: 0.8118 - val_loss: 0.4586 - val_acc: 0.8089

Epoch 00009: val_loss did not improve from 0.41953
Epoch 10/30
 - 29s - loss: 0.4186 - acc: 0.8126 - val_loss: 0.4653 - val_acc: 0.7919

Epoch 00010: val_loss did not improve from 0.41953
Epoch 11/30
 - 29s - loss: 0.4118 - acc: 0.8176 - val_loss: 0.4200 - val_acc: 0.8257

Epoch 00011: val_loss did not improve from 0.41953
Epoch 12/30
 - 29s - loss: 0.4067 - acc: 0.8197 - val_loss: 0.4368 - val_acc: 0.8159

Epoch 00012: val_loss did not improve from 0.41953
Epoch 13/30
 - 29s - loss: 0.4012 - acc: 0.8224 - val_loss: 0.4101 - val_acc: 0.8224

Epoch 00013: val_loss improved from 0.41953 to 0.41007, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 29s - loss: 0.3949 - acc: 0.8270 - val_loss: 0.4311 - val_acc: 0.8218

Epoch 00014: val_loss did not improve from 0.41007
Epoch 15/30
 - 29s - loss: 0.3906 - acc: 0.8282 - val_loss: 0.4341 - val_acc: 0.8233

Epoch 00015: val_loss did not improve from 0.41007
Epoch 16/30
 - 29s - loss: 0.3865 - acc: 0.8312 - val_loss: 0.3819 - val_acc: 0.8531

Epoch 00016: val_loss improved from 0.41007 to 0.38186, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 17/30
 - 29s - loss: 0.3828 - acc: 0.8301 - val_loss: 0.4564 - val_acc: 0.8120

Epoch 00017: val_loss did not improve from 0.38186
Epoch 18/30
 - 29s - loss: 0.3790 - acc: 0.8355 - val_loss: 0.4946 - val_acc: 0.8056

Epoch 00018: val_loss did not improve from 0.38186
Epoch 19/30
 - 29s - loss: 0.3761 - acc: 0.8392 - val_loss: 0.4167 - val_acc: 0.8259

Epoch 00019: val_loss did not improve from 0.38186
Epoch 20/30
 - 29s - loss: 0.3709 - acc: 0.8407 - val_loss: 0.4550 - val_acc: 0.8177

Epoch 00020: val_loss did not improve from 0.38186
Epoch 21/30
 - 29s - loss: 0.3692 - acc: 0.8416 - val_loss: 0.4220 - val_acc: 0.8238

Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00021: val_loss did not improve from 0.38186
Epoch 00021: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 276us/step
current Test accuracy: 0.8237903225806451
current auc_score ------------------>  0.9103701656260839

  32/7440 [..............................] - ETA: 2:57
 224/7440 [..............................] - ETA: 26s 
 416/7440 [>.............................] - ETA: 14s
 608/7440 [=>............................] - ETA: 10s
 800/7440 [==>...........................] - ETA: 8s 
 992/7440 [===>..........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 5s
1376/7440 [====>.........................] - ETA: 5s
1568/7440 [=====>........................] - ETA: 4s
1760/7440 [======>.......................] - ETA: 4s
1952/7440 [======>.......................] - ETA: 3s
2144/7440 [=======>......................] - ETA: 3s
2336/7440 [========>.....................] - ETA: 3s
2528/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2912/7440 [==========>...................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3296/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 385us/step
Best saved model Test accuracy: 0.8530913978494624
best saved model auc_score ------------------>  0.9266702436697885
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_44[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_45[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_46[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_47[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 36, 96, 96)   0           concatenate_18[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_48[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_49[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 46, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_50[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_51[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_52[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 33, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_53[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_54[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 43, 48, 48)   0           concatenate_21[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_55[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_56[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 53, 48, 48)   0           concatenate_22[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 53, 48, 48)   212         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 53, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 26, 48, 48)   1378        activation_57[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 26, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 26, 24, 24)   104         average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 26, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   1040        activation_58[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_59[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 36, 24, 24)   0           average_pooling2d_6[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 36, 24, 24)   144         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 36, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 40, 24, 24)   1440        activation_60[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 40, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_61[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 46, 24, 24)   0           concatenate_24[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 46, 24, 24)   184         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 46, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 40, 24, 24)   1840        activation_62[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 40, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_63[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 56, 24, 24)   0           concatenate_25[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 56, 24, 24)   224         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 56, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 56)           0           activation_64[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            57          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 49,781
Trainable params: 48,181
Non-trainable params: 1,600
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 42s - loss: 0.5617 - acc: 0.7701 - val_loss: 0.6273 - val_acc: 0.7234

Epoch 00001: val_loss improved from inf to 0.62726, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 37s - loss: 0.4690 - acc: 0.8167 - val_loss: 0.5617 - val_acc: 0.7645

Epoch 00002: val_loss improved from 0.62726 to 0.56172, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 37s - loss: 0.4246 - acc: 0.8385 - val_loss: 0.4968 - val_acc: 0.8255

Epoch 00003: val_loss improved from 0.56172 to 0.49685, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 37s - loss: 0.3931 - acc: 0.8563 - val_loss: 0.5661 - val_acc: 0.8040

Epoch 00004: val_loss did not improve from 0.49685
Epoch 5/30
 - 37s - loss: 0.3662 - acc: 0.8687 - val_loss: 0.4503 - val_acc: 0.8198

Epoch 00005: val_loss improved from 0.49685 to 0.45035, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 37s - loss: 0.3441 - acc: 0.8824 - val_loss: 0.6220 - val_acc: 0.7810

Epoch 00006: val_loss did not improve from 0.45035
Epoch 7/30
 - 37s - loss: 0.3268 - acc: 0.8890 - val_loss: 0.6310 - val_acc: 0.7663

Epoch 00007: val_loss did not improve from 0.45035
Epoch 8/30
 - 37s - loss: 0.3077 - acc: 0.8985 - val_loss: 0.5224 - val_acc: 0.8216

Epoch 00008: val_loss did not improve from 0.45035
Epoch 9/30
 - 37s - loss: 0.2956 - acc: 0.9044 - val_loss: 0.6157 - val_acc: 0.7626

Epoch 00009: val_loss did not improve from 0.45035
Epoch 10/30
 - 37s - loss: 0.2794 - acc: 0.9117 - val_loss: 0.5654 - val_acc: 0.8026

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.45035
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 351us/step
current Test accuracy: 0.8025537634408603
current auc_score ------------------>  0.8808181581685743

  32/7440 [..............................] - ETA: 4:55
 160/7440 [..............................] - ETA: 1:00
 320/7440 [>.............................] - ETA: 30s 
 480/7440 [>.............................] - ETA: 20s
 640/7440 [=>............................] - ETA: 15s
 800/7440 [==>...........................] - ETA: 12s
 960/7440 [==>...........................] - ETA: 10s
1120/7440 [===>..........................] - ETA: 9s 
1280/7440 [====>.........................] - ETA: 8s
1440/7440 [====>.........................] - ETA: 7s
1600/7440 [=====>........................] - ETA: 6s
1760/7440 [======>.......................] - ETA: 6s
1920/7440 [======>.......................] - ETA: 5s
2080/7440 [=======>......................] - ETA: 5s
2240/7440 [========>.....................] - ETA: 4s
2400/7440 [========>.....................] - ETA: 4s
2560/7440 [=========>....................] - ETA: 4s
2720/7440 [=========>....................] - ETA: 3s
2880/7440 [==========>...................] - ETA: 3s
3040/7440 [===========>..................] - ETA: 3s
3200/7440 [===========>..................] - ETA: 3s
3360/7440 [============>.................] - ETA: 2s
3520/7440 [=============>................] - ETA: 2s
3680/7440 [=============>................] - ETA: 2s
3840/7440 [==============>...............] - ETA: 2s
4000/7440 [===============>..............] - ETA: 2s
4160/7440 [===============>..............] - ETA: 2s
4320/7440 [================>.............] - ETA: 2s
4480/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 522us/step
Best saved model Test accuracy: 0.819758064516129
best saved model auc_score ------------------>  0.9096786478205574
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_65[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_66[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_67[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_68[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 28, 96, 96)   0           concatenate_27[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_69[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_70[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 34, 96, 96)   0           concatenate_28[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 34, 96, 96)   136         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 34, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 34)           0           activation_71[0][0]              
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            35          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 6,483
Trainable params: 6,139
Non-trainable params: 344
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 25s - loss: 0.6297 - acc: 0.6845 - val_loss: 0.5413 - val_acc: 0.7606

Epoch 00001: val_loss improved from inf to 0.54132, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 21s - loss: 0.5612 - acc: 0.7511 - val_loss: 0.4895 - val_acc: 0.8391

Epoch 00002: val_loss improved from 0.54132 to 0.48953, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 21s - loss: 0.5259 - acc: 0.7607 - val_loss: 0.5063 - val_acc: 0.8246

Epoch 00003: val_loss did not improve from 0.48953
Epoch 4/30
 - 21s - loss: 0.5062 - acc: 0.7660 - val_loss: 0.5070 - val_acc: 0.7894

Epoch 00004: val_loss did not improve from 0.48953
Epoch 5/30
 - 21s - loss: 0.4956 - acc: 0.7656 - val_loss: 0.5275 - val_acc: 0.8079

Epoch 00005: val_loss did not improve from 0.48953
Epoch 6/30
 - 21s - loss: 0.4852 - acc: 0.7729 - val_loss: 0.6023 - val_acc: 0.7910

Epoch 00006: val_loss did not improve from 0.48953
Epoch 7/30
 - 21s - loss: 0.4774 - acc: 0.7754 - val_loss: 0.4706 - val_acc: 0.7934

Epoch 00007: val_loss improved from 0.48953 to 0.47062, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 21s - loss: 0.4709 - acc: 0.7777 - val_loss: 0.4802 - val_acc: 0.8031

Epoch 00008: val_loss did not improve from 0.47062
Epoch 9/30
 - 21s - loss: 0.4636 - acc: 0.7831 - val_loss: 0.4450 - val_acc: 0.8230

Epoch 00009: val_loss improved from 0.47062 to 0.44499, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 21s - loss: 0.4598 - acc: 0.7840 - val_loss: 0.4724 - val_acc: 0.8023

Epoch 00010: val_loss did not improve from 0.44499
Epoch 11/30
 - 21s - loss: 0.4523 - acc: 0.7900 - val_loss: 0.4635 - val_acc: 0.8231

Epoch 00011: val_loss did not improve from 0.44499
Epoch 12/30
 - 21s - loss: 0.4491 - acc: 0.7902 - val_loss: 0.4516 - val_acc: 0.7950

Epoch 00012: val_loss did not improve from 0.44499
Epoch 13/30
 - 21s - loss: 0.4435 - acc: 0.7925 - val_loss: 0.4421 - val_acc: 0.7989

Epoch 00013: val_loss improved from 0.44499 to 0.44207, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 21s - loss: 0.4393 - acc: 0.7952 - val_loss: 0.4862 - val_acc: 0.7907

Epoch 00014: val_loss did not improve from 0.44207
Epoch 15/30
 - 21s - loss: 0.4358 - acc: 0.7986 - val_loss: 0.5518 - val_acc: 0.7801

Epoch 00015: val_loss did not improve from 0.44207
Epoch 16/30
 - 21s - loss: 0.4294 - acc: 0.8004 - val_loss: 0.4405 - val_acc: 0.8074

Epoch 00016: val_loss improved from 0.44207 to 0.44049, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 17/30
 - 21s - loss: 0.4253 - acc: 0.8029 - val_loss: 0.4195 - val_acc: 0.8155

Epoch 00017: val_loss improved from 0.44049 to 0.41952, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 18/30
 - 21s - loss: 0.4230 - acc: 0.8064 - val_loss: 0.4014 - val_acc: 0.8315

Epoch 00018: val_loss improved from 0.41952 to 0.40141, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 19/30
 - 21s - loss: 0.4205 - acc: 0.8066 - val_loss: 0.4236 - val_acc: 0.8019

Epoch 00019: val_loss did not improve from 0.40141
Epoch 20/30
 - 21s - loss: 0.4163 - acc: 0.8080 - val_loss: 0.4338 - val_acc: 0.8102

Epoch 00020: val_loss did not improve from 0.40141
Epoch 21/30
 - 21s - loss: 0.4119 - acc: 0.8108 - val_loss: 0.5122 - val_acc: 0.7902

Epoch 00021: val_loss did not improve from 0.40141
Epoch 22/30
 - 21s - loss: 0.4109 - acc: 0.8113 - val_loss: 0.6359 - val_acc: 0.7540

Epoch 00022: val_loss did not improve from 0.40141
Epoch 23/30
 - 21s - loss: 0.4077 - acc: 0.8133 - val_loss: 0.4401 - val_acc: 0.8038

Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00023: val_loss did not improve from 0.40141
Epoch 00023: early stopping

  32/7440 [..............................] - ETA: 1s
 256/7440 [>.............................] - ETA: 1s
 512/7440 [=>............................] - ETA: 1s
 768/7440 [==>...........................] - ETA: 1s
1024/7440 [===>..........................] - ETA: 1s
1280/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 0s
3328/7440 [============>.................] - ETA: 0s
3584/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4096/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 215us/step
current Test accuracy: 0.803763440860215
current auc_score ------------------>  0.8909401375881605

  32/7440 [..............................] - ETA: 4:52
 256/7440 [>.............................] - ETA: 37s 
 480/7440 [>.............................] - ETA: 19s
 704/7440 [=>............................] - ETA: 13s
 928/7440 [==>...........................] - ETA: 10s
1152/7440 [===>..........................] - ETA: 8s 
1376/7440 [====>.........................] - ETA: 6s
1632/7440 [=====>........................] - ETA: 5s
1888/7440 [======>.......................] - ETA: 4s
2144/7440 [=======>......................] - ETA: 4s
2400/7440 [========>.....................] - ETA: 3s
2656/7440 [=========>....................] - ETA: 3s
2912/7440 [==========>...................] - ETA: 2s
3168/7440 [===========>..................] - ETA: 2s
3424/7440 [============>.................] - ETA: 2s
3680/7440 [=============>................] - ETA: 2s
3936/7440 [==============>...............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 391us/step
Best saved model Test accuracy: 0.8314516129032258
best saved model auc_score ------------------>  0.9067802636142906
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_72[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_73[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_74[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_75[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 48, 96, 96)   0           concatenate_30[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 48, 96, 96)   192         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 48, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 24, 96, 96)   1152        activation_76[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 24, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 24, 48, 48)   96          average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 24, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1536        activation_77[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_78[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 40, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 40, 48, 48)   160         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_79[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_80[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 56, 48, 48)   0           concatenate_32[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 56, 48, 48)   224         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 56, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 28, 48, 48)   1568        activation_81[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 28, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 28, 24, 24)   112         average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 28, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   1792        activation_82[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_83[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 44, 24, 24)   0           average_pooling2d_8[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 44, 24, 24)   176         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 44, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_84[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_85[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 60, 24, 24)   0           concatenate_34[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 60, 24, 24)   240         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 60, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 60)           0           activation_86[0][0]              
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            61          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 73,069
Trainable params: 71,605
Non-trainable params: 1,464
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 38s - loss: 0.5525 - acc: 0.7784 - val_loss: 0.5971 - val_acc: 0.7792

Epoch 00001: val_loss improved from inf to 0.59707, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 33s - loss: 0.4664 - acc: 0.8181 - val_loss: 0.7829 - val_acc: 0.7214

Epoch 00002: val_loss did not improve from 0.59707
Epoch 3/30
 - 33s - loss: 0.4217 - acc: 0.8413 - val_loss: 0.5800 - val_acc: 0.7794

Epoch 00003: val_loss improved from 0.59707 to 0.57999, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 33s - loss: 0.3896 - acc: 0.8568 - val_loss: 0.5106 - val_acc: 0.8017

Epoch 00004: val_loss improved from 0.57999 to 0.51061, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 33s - loss: 0.3662 - acc: 0.8712 - val_loss: 0.6716 - val_acc: 0.7593

Epoch 00005: val_loss did not improve from 0.51061
Epoch 6/30
 - 33s - loss: 0.3418 - acc: 0.8835 - val_loss: 0.6653 - val_acc: 0.7788

Epoch 00006: val_loss did not improve from 0.51061
Epoch 7/30
 - 33s - loss: 0.3208 - acc: 0.8928 - val_loss: 0.8696 - val_acc: 0.6974

Epoch 00007: val_loss did not improve from 0.51061
Epoch 8/30
 - 33s - loss: 0.3024 - acc: 0.9021 - val_loss: 0.5434 - val_acc: 0.8171

Epoch 00008: val_loss did not improve from 0.51061
Epoch 9/30
 - 33s - loss: 0.2913 - acc: 0.9068 - val_loss: 0.5287 - val_acc: 0.8223

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.51061
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 326us/step
current Test accuracy: 0.8223118279569892
current auc_score ------------------>  0.9036680179789572

  32/7440 [..............................] - ETA: 6:17
 192/7440 [..............................] - ETA: 1:03
 352/7440 [>.............................] - ETA: 35s 
 512/7440 [=>............................] - ETA: 24s
 672/7440 [=>............................] - ETA: 18s
 832/7440 [==>...........................] - ETA: 15s
 992/7440 [===>..........................] - ETA: 12s
1152/7440 [===>..........................] - ETA: 10s
1312/7440 [====>.........................] - ETA: 9s 
1472/7440 [====>.........................] - ETA: 8s
1632/7440 [=====>........................] - ETA: 7s
1792/7440 [======>.......................] - ETA: 7s
1952/7440 [======>.......................] - ETA: 6s
2112/7440 [=======>......................] - ETA: 5s
2272/7440 [========>.....................] - ETA: 5s
2432/7440 [========>.....................] - ETA: 5s
2592/7440 [=========>....................] - ETA: 4s
2752/7440 [==========>...................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3552/7440 [=============>................] - ETA: 3s
3712/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 2s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 549us/step
Best saved model Test accuracy: 0.801747311827957
best saved model auc_score ------------------>  0.8927562073650134
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_87[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_89[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_90[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 36, 96, 96)   0           concatenate_36[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_91[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_92[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 46, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 96, 96)   184         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 46, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 46)           0           activation_93[0][0]              
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            47          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 15,231
Trainable params: 14,743
Non-trainable params: 488
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.5896 - acc: 0.7320 - val_loss: 0.6041 - val_acc: 0.7569

Epoch 00001: val_loss improved from inf to 0.60406, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 27s - loss: 0.5362 - acc: 0.7603 - val_loss: 0.5315 - val_acc: 0.8203

Epoch 00002: val_loss improved from 0.60406 to 0.53149, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 27s - loss: 0.5089 - acc: 0.7669 - val_loss: 0.5132 - val_acc: 0.7966

Epoch 00003: val_loss improved from 0.53149 to 0.51315, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 27s - loss: 0.4922 - acc: 0.7725 - val_loss: 0.5472 - val_acc: 0.7776

Epoch 00004: val_loss did not improve from 0.51315
Epoch 5/30
 - 27s - loss: 0.4776 - acc: 0.7802 - val_loss: 0.5574 - val_acc: 0.7649

Epoch 00005: val_loss did not improve from 0.51315
Epoch 6/30
 - 27s - loss: 0.4674 - acc: 0.7853 - val_loss: 0.4922 - val_acc: 0.8056

Epoch 00006: val_loss improved from 0.51315 to 0.49217, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 27s - loss: 0.4583 - acc: 0.7893 - val_loss: 0.4979 - val_acc: 0.7969

Epoch 00007: val_loss did not improve from 0.49217
Epoch 8/30
 - 27s - loss: 0.4485 - acc: 0.7948 - val_loss: 0.5869 - val_acc: 0.7839

Epoch 00008: val_loss did not improve from 0.49217
Epoch 9/30
 - 27s - loss: 0.4402 - acc: 0.7996 - val_loss: 0.4750 - val_acc: 0.7820

Epoch 00009: val_loss improved from 0.49217 to 0.47503, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 27s - loss: 0.4327 - acc: 0.8036 - val_loss: 0.4777 - val_acc: 0.7690

Epoch 00010: val_loss did not improve from 0.47503
Epoch 11/30
 - 27s - loss: 0.4267 - acc: 0.8067 - val_loss: 0.4976 - val_acc: 0.7940

Epoch 00011: val_loss did not improve from 0.47503
Epoch 12/30
 - 27s - loss: 0.4205 - acc: 0.8095 - val_loss: 0.4537 - val_acc: 0.8066

Epoch 00012: val_loss improved from 0.47503 to 0.45370, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 13/30
 - 27s - loss: 0.4146 - acc: 0.8154 - val_loss: 0.4347 - val_acc: 0.8167

Epoch 00013: val_loss improved from 0.45370 to 0.43472, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 27s - loss: 0.4087 - acc: 0.8172 - val_loss: 0.4882 - val_acc: 0.7747

Epoch 00014: val_loss did not improve from 0.43472
Epoch 15/30
 - 27s - loss: 0.4051 - acc: 0.8178 - val_loss: 0.7416 - val_acc: 0.7169

Epoch 00015: val_loss did not improve from 0.43472
Epoch 16/30
 - 27s - loss: 0.4022 - acc: 0.8208 - val_loss: 0.4732 - val_acc: 0.7829

Epoch 00016: val_loss did not improve from 0.43472
Epoch 17/30
 - 27s - loss: 0.3960 - acc: 0.8244 - val_loss: 0.4876 - val_acc: 0.8128

Epoch 00017: val_loss did not improve from 0.43472
Epoch 18/30
 - 27s - loss: 0.3922 - acc: 0.8266 - val_loss: 0.4320 - val_acc: 0.8098

Epoch 00018: val_loss improved from 0.43472 to 0.43202, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 19/30
 - 27s - loss: 0.3898 - acc: 0.8267 - val_loss: 0.4869 - val_acc: 0.7901

Epoch 00019: val_loss did not improve from 0.43202
Epoch 20/30
 - 27s - loss: 0.3857 - acc: 0.8307 - val_loss: 0.4332 - val_acc: 0.8194

Epoch 00020: val_loss did not improve from 0.43202
Epoch 21/30
 - 27s - loss: 0.3819 - acc: 0.8308 - val_loss: 0.4762 - val_acc: 0.8008

Epoch 00021: val_loss did not improve from 0.43202
Epoch 22/30
 - 27s - loss: 0.3793 - acc: 0.8342 - val_loss: 0.4178 - val_acc: 0.8117

Epoch 00022: val_loss improved from 0.43202 to 0.41777, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 23/30
 - 27s - loss: 0.3773 - acc: 0.8350 - val_loss: 0.4253 - val_acc: 0.8103

Epoch 00023: val_loss did not improve from 0.41777
Epoch 24/30
 - 27s - loss: 0.3720 - acc: 0.8390 - val_loss: 0.5023 - val_acc: 0.7739

Epoch 00024: val_loss did not improve from 0.41777
Epoch 25/30
 - 27s - loss: 0.3724 - acc: 0.8387 - val_loss: 0.7370 - val_acc: 0.7512

Epoch 00025: val_loss did not improve from 0.41777
Epoch 26/30
 - 27s - loss: 0.3672 - acc: 0.8423 - val_loss: 0.4498 - val_acc: 0.7961

Epoch 00026: val_loss did not improve from 0.41777
Epoch 27/30
 - 27s - loss: 0.3690 - acc: 0.8396 - val_loss: 0.4712 - val_acc: 0.7970

Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00027: val_loss did not improve from 0.41777
Epoch 00027: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 292us/step
current Test accuracy: 0.7970430107526881
current auc_score ------------------>  0.8825957841947046

  32/7440 [..............................] - ETA: 6:31
 192/7440 [..............................] - ETA: 1:05
 384/7440 [>.............................] - ETA: 33s 
 576/7440 [=>............................] - ETA: 22s
 768/7440 [==>...........................] - ETA: 16s
 960/7440 [==>...........................] - ETA: 13s
1152/7440 [===>..........................] - ETA: 11s
1344/7440 [====>.........................] - ETA: 9s 
1536/7440 [=====>........................] - ETA: 8s
1728/7440 [=====>........................] - ETA: 7s
1920/7440 [======>.......................] - ETA: 6s
2112/7440 [=======>......................] - ETA: 5s
2304/7440 [========>.....................] - ETA: 5s
2496/7440 [=========>....................] - ETA: 4s
2688/7440 [=========>....................] - ETA: 4s
2880/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 3s
3264/7440 [============>.................] - ETA: 3s
3456/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 2s
3840/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4224/7440 [================>.............] - ETA: 2s
4416/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 523us/step
Best saved model Test accuracy: 0.8116935483870967
best saved model auc_score ------------------>  0.911146953405018
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_94[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_95[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_96[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_97[0][0]              
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 28, 96, 96)   0           concatenate_39[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_98[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   336         activation_99[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_100[0][0]             
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 20, 48, 48)   80          concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 20, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   480         activation_101[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_102[0][0]             
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 26, 48, 48)   0           concatenate_41[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 26)           0           activation_103[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            27          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 8,507
Trainable params: 8,063
Non-trainable params: 444
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 26s - loss: 0.5985 - acc: 0.7078 - val_loss: 0.5168 - val_acc: 0.8177

Epoch 00001: val_loss improved from inf to 0.51679, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 20s - loss: 0.5177 - acc: 0.7693 - val_loss: 0.4912 - val_acc: 0.8223

Epoch 00002: val_loss improved from 0.51679 to 0.49123, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 20s - loss: 0.4856 - acc: 0.7800 - val_loss: 0.4701 - val_acc: 0.8204

Epoch 00003: val_loss improved from 0.49123 to 0.47015, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 20s - loss: 0.4642 - acc: 0.7880 - val_loss: 0.4917 - val_acc: 0.8152

Epoch 00004: val_loss did not improve from 0.47015
Epoch 5/30
 - 20s - loss: 0.4482 - acc: 0.7965 - val_loss: 0.4648 - val_acc: 0.8335

Epoch 00005: val_loss improved from 0.47015 to 0.46481, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 20s - loss: 0.4342 - acc: 0.8042 - val_loss: 0.4689 - val_acc: 0.8044

Epoch 00006: val_loss did not improve from 0.46481
Epoch 7/30
 - 20s - loss: 0.4248 - acc: 0.8087 - val_loss: 0.5445 - val_acc: 0.7935

Epoch 00007: val_loss did not improve from 0.46481
Epoch 8/30
 - 20s - loss: 0.4184 - acc: 0.8111 - val_loss: 0.4382 - val_acc: 0.8474

Epoch 00008: val_loss improved from 0.46481 to 0.43816, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 20s - loss: 0.4097 - acc: 0.8165 - val_loss: 0.4462 - val_acc: 0.8238

Epoch 00009: val_loss did not improve from 0.43816
Epoch 10/30
 - 20s - loss: 0.4054 - acc: 0.8202 - val_loss: 0.4658 - val_acc: 0.8228

Epoch 00010: val_loss did not improve from 0.43816
Epoch 11/30
 - 20s - loss: 0.4007 - acc: 0.8217 - val_loss: 0.4182 - val_acc: 0.8297

Epoch 00011: val_loss improved from 0.43816 to 0.41821, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 12/30
 - 20s - loss: 0.3934 - acc: 0.8271 - val_loss: 0.8300 - val_acc: 0.7433

Epoch 00012: val_loss did not improve from 0.41821
Epoch 13/30
 - 20s - loss: 0.3907 - acc: 0.8282 - val_loss: 0.4147 - val_acc: 0.8108

Epoch 00013: val_loss improved from 0.41821 to 0.41466, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 20s - loss: 0.3876 - acc: 0.8292 - val_loss: 0.4738 - val_acc: 0.7956

Epoch 00014: val_loss did not improve from 0.41466
Epoch 15/30
 - 20s - loss: 0.3841 - acc: 0.8303 - val_loss: 0.4636 - val_acc: 0.8430

Epoch 00015: val_loss did not improve from 0.41466
Epoch 16/30
 - 20s - loss: 0.3791 - acc: 0.8335 - val_loss: 0.4667 - val_acc: 0.8230

Epoch 00016: val_loss did not improve from 0.41466
Epoch 17/30
 - 20s - loss: 0.3770 - acc: 0.8363 - val_loss: 0.4966 - val_acc: 0.7685

Epoch 00017: val_loss did not improve from 0.41466
Epoch 18/30
 - 20s - loss: 0.3715 - acc: 0.8392 - val_loss: 0.6514 - val_acc: 0.7259

Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00018: val_loss did not improve from 0.41466
Epoch 00018: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1888/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
3008/7440 [===========>..................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3456/7440 [============>.................] - ETA: 0s
3712/7440 [=============>................] - ETA: 0s
3936/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 227us/step
current Test accuracy: 0.7259408602150538
current auc_score ------------------>  0.9008750289050758

  32/7440 [..............................] - ETA: 7:38
 256/7440 [>.............................] - ETA: 57s 
 480/7440 [>.............................] - ETA: 30s
 704/7440 [=>............................] - ETA: 20s
 928/7440 [==>...........................] - ETA: 15s
1152/7440 [===>..........................] - ETA: 12s
1376/7440 [====>.........................] - ETA: 10s
1600/7440 [=====>........................] - ETA: 8s 
1824/7440 [======>.......................] - ETA: 7s
2048/7440 [=======>......................] - ETA: 6s
2272/7440 [========>.....................] - ETA: 5s
2496/7440 [=========>....................] - ETA: 5s
2720/7440 [=========>....................] - ETA: 4s
2944/7440 [==========>...................] - ETA: 4s
3168/7440 [===========>..................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3616/7440 [=============>................] - ETA: 2s
3840/7440 [==============>...............] - ETA: 2s
4064/7440 [===============>..............] - ETA: 2s
4288/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 498us/step
Best saved model Test accuracy: 0.810752688172043
best saved model auc_score ------------------>  0.9042452884726557
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_11[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_104[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_105[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_106[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_107[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 40, 96, 96)   0           concatenate_43[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_108[0][0]             
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_109[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_110[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_10[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_111[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_112[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 44, 48, 48)   0           concatenate_45[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_11 (Gl (None, 44)           0           activation_113[0][0]             
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            45          global_average_pooling2d_11[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 33s - loss: 0.5539 - acc: 0.7584 - val_loss: 0.5064 - val_acc: 0.8024

Epoch 00001: val_loss improved from inf to 0.50642, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 26s - loss: 0.4876 - acc: 0.7873 - val_loss: 0.4829 - val_acc: 0.7882

Epoch 00002: val_loss improved from 0.50642 to 0.48288, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 26s - loss: 0.4586 - acc: 0.7978 - val_loss: 0.5195 - val_acc: 0.7644

Epoch 00003: val_loss did not improve from 0.48288
Epoch 4/30
 - 26s - loss: 0.4408 - acc: 0.8094 - val_loss: 0.5084 - val_acc: 0.7719

Epoch 00004: val_loss did not improve from 0.48288
Epoch 5/30
 - 26s - loss: 0.4262 - acc: 0.8165 - val_loss: 0.4315 - val_acc: 0.8569

Epoch 00005: val_loss improved from 0.48288 to 0.43148, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 26s - loss: 0.4133 - acc: 0.8233 - val_loss: 0.4934 - val_acc: 0.8125

Epoch 00006: val_loss did not improve from 0.43148
Epoch 7/30
 - 26s - loss: 0.4047 - acc: 0.8278 - val_loss: 0.5651 - val_acc: 0.7820

Epoch 00007: val_loss did not improve from 0.43148
Epoch 8/30
 - 26s - loss: 0.3946 - acc: 0.8338 - val_loss: 0.4889 - val_acc: 0.8199

Epoch 00008: val_loss did not improve from 0.43148
Epoch 9/30
 - 26s - loss: 0.3862 - acc: 0.8391 - val_loss: 0.5563 - val_acc: 0.7835

Epoch 00009: val_loss did not improve from 0.43148
Epoch 10/30
 - 26s - loss: 0.3778 - acc: 0.8447 - val_loss: 0.6002 - val_acc: 0.7712

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.43148
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 288us/step
current Test accuracy: 0.771236559139785
current auc_score ------------------>  0.8717926205341657

  32/7440 [..............................] - ETA: 8:38
 192/7440 [..............................] - ETA: 1:26
 384/7440 [>.............................] - ETA: 43s 
 576/7440 [=>............................] - ETA: 28s
 768/7440 [==>...........................] - ETA: 21s
 960/7440 [==>...........................] - ETA: 16s
1152/7440 [===>..........................] - ETA: 14s
1344/7440 [====>.........................] - ETA: 11s
1536/7440 [=====>........................] - ETA: 10s
1728/7440 [=====>........................] - ETA: 9s 
1920/7440 [======>.......................] - ETA: 8s
2112/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2496/7440 [=========>....................] - ETA: 5s
2688/7440 [=========>....................] - ETA: 5s
2880/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 4s
3264/7440 [============>.................] - ETA: 4s
3456/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 2s
4224/7440 [================>.............] - ETA: 2s
4416/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4800/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 585us/step
Best saved model Test accuracy: 0.8568548387096774
best saved model auc_score ------------------>  0.9026763932246501
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_114 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_11 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_115 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_12  (None, 8)                 0         
_________________________________________________________________
dense_12 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 12s - loss: 0.6561 - acc: 0.6264 - val_loss: 0.5817 - val_acc: 0.8052

Epoch 00001: val_loss improved from inf to 0.58169, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 6s - loss: 0.6204 - acc: 0.7000 - val_loss: 0.5475 - val_acc: 0.8305

Epoch 00002: val_loss improved from 0.58169 to 0.54748, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 6s - loss: 0.6081 - acc: 0.7070 - val_loss: 0.5281 - val_acc: 0.8187

Epoch 00003: val_loss improved from 0.54748 to 0.52815, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 6s - loss: 0.5987 - acc: 0.7147 - val_loss: 0.5289 - val_acc: 0.8087

Epoch 00004: val_loss did not improve from 0.52815
Epoch 5/30
 - 6s - loss: 0.5879 - acc: 0.7212 - val_loss: 0.5216 - val_acc: 0.8161

Epoch 00005: val_loss improved from 0.52815 to 0.52156, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 6s - loss: 0.5780 - acc: 0.7283 - val_loss: 0.5382 - val_acc: 0.7944

Epoch 00006: val_loss did not improve from 0.52156
Epoch 7/30
 - 6s - loss: 0.5687 - acc: 0.7318 - val_loss: 0.5022 - val_acc: 0.8233

Epoch 00007: val_loss improved from 0.52156 to 0.50220, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 6s - loss: 0.5612 - acc: 0.7342 - val_loss: 0.4969 - val_acc: 0.8384

Epoch 00008: val_loss improved from 0.50220 to 0.49692, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 6s - loss: 0.5551 - acc: 0.7373 - val_loss: 0.5181 - val_acc: 0.7958

Epoch 00009: val_loss did not improve from 0.49692
Epoch 10/30
 - 6s - loss: 0.5514 - acc: 0.7370 - val_loss: 0.5255 - val_acc: 0.8113

Epoch 00010: val_loss did not improve from 0.49692
Epoch 11/30
 - 6s - loss: 0.5466 - acc: 0.7397 - val_loss: 0.5820 - val_acc: 0.7785

Epoch 00011: val_loss did not improve from 0.49692
Epoch 12/30
 - 6s - loss: 0.5437 - acc: 0.7403 - val_loss: 0.5824 - val_acc: 0.7640

Epoch 00012: val_loss did not improve from 0.49692
Epoch 13/30
 - 6s - loss: 0.5403 - acc: 0.7422 - val_loss: 0.5017 - val_acc: 0.8570

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.49692
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 1s
 448/7440 [>.............................] - ETA: 0s
 864/7440 [==>...........................] - ETA: 0s
1280/7440 [====>.........................] - ETA: 0s
1696/7440 [=====>........................] - ETA: 0s
2112/7440 [=======>......................] - ETA: 0s
2528/7440 [=========>....................] - ETA: 0s
2944/7440 [==========>...................] - ETA: 0s
3392/7440 [============>.................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 121us/step
current Test accuracy: 0.8569892473118279
current auc_score ------------------>  0.8705042851774771

  32/7440 [..............................] - ETA: 8:24
 416/7440 [>.............................] - ETA: 37s 
 832/7440 [==>...........................] - ETA: 18s
1248/7440 [====>.........................] - ETA: 11s
1664/7440 [=====>........................] - ETA: 8s 
2080/7440 [=======>......................] - ETA: 6s
2496/7440 [=========>....................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 3s
3328/7440 [============>.................] - ETA: 3s
3744/7440 [==============>...............] - ETA: 2s
4160/7440 [===============>..............] - ETA: 2s
4576/7440 [=================>............] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 3s 418us/step
Best saved model Test accuracy: 0.8384408602150538
best saved model auc_score ------------------>  0.8729434761244074
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_13[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_116[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_117[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_118[0][0]             
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_119[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_120[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 26, 48, 48)   0           average_pooling2d_12[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_13 (Gl (None, 26)           0           activation_121[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            27          global_average_pooling2d_13[0][0]
==================================================================================================
Total params: 13,235
Trainable params: 12,875
Non-trainable params: 360
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 23s - loss: 0.5803 - acc: 0.7407 - val_loss: 0.5287 - val_acc: 0.8280

Epoch 00001: val_loss improved from inf to 0.52865, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 16s - loss: 0.5182 - acc: 0.7673 - val_loss: 0.4877 - val_acc: 0.8305

Epoch 00002: val_loss improved from 0.52865 to 0.48775, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 16s - loss: 0.4924 - acc: 0.7732 - val_loss: 0.5038 - val_acc: 0.8051

Epoch 00003: val_loss did not improve from 0.48775
Epoch 4/30
 - 16s - loss: 0.4759 - acc: 0.7821 - val_loss: 0.5258 - val_acc: 0.8157

Epoch 00004: val_loss did not improve from 0.48775
Epoch 5/30
 - 16s - loss: 0.4657 - acc: 0.7864 - val_loss: 0.5048 - val_acc: 0.7969

Epoch 00005: val_loss did not improve from 0.48775
Epoch 6/30
 - 16s - loss: 0.4547 - acc: 0.7925 - val_loss: 0.5011 - val_acc: 0.7962

Epoch 00006: val_loss did not improve from 0.48775
Epoch 7/30
 - 16s - loss: 0.4470 - acc: 0.7947 - val_loss: 0.4887 - val_acc: 0.7974

Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00007: val_loss did not improve from 0.48775
Epoch 00007: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1664/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2208/7440 [=======>......................] - ETA: 1s
2464/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3008/7440 [===========>..................] - ETA: 0s
3296/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 196us/step
current Test accuracy: 0.7974462365591398
current auc_score ------------------>  0.8794867903803907

  32/7440 [..............................] - ETA: 9:41
 256/7440 [>.............................] - ETA: 1:11
 512/7440 [=>............................] - ETA: 35s 
 768/7440 [==>...........................] - ETA: 23s
1024/7440 [===>..........................] - ETA: 17s
1280/7440 [====>.........................] - ETA: 13s
1536/7440 [=====>........................] - ETA: 10s
1792/7440 [======>.......................] - ETA: 9s 
2048/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2560/7440 [=========>....................] - ETA: 5s
2816/7440 [==========>...................] - ETA: 5s
3072/7440 [===========>..................] - ETA: 4s
3328/7440 [============>.................] - ETA: 3s
3584/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
4096/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4864/7440 [==================>...........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5888/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 537us/step
Best saved model Test accuracy: 0.8305107526881721
best saved model auc_score ------------------>  0.8807843392299687
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_14[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_122[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_123[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_124[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_125[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 32, 96, 96)   0           concatenate_49[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 96, 96)   128         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 32, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_14 (Gl (None, 32)           0           activation_126[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            33          global_average_pooling2d_14[0][0]
==================================================================================================
Total params: 6,753
Trainable params: 6,481
Non-trainable params: 272
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 24s - loss: 0.6029 - acc: 0.7137 - val_loss: 0.5379 - val_acc: 0.8124

Epoch 00001: val_loss improved from inf to 0.53794, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 18s - loss: 0.5490 - acc: 0.7509 - val_loss: 0.5948 - val_acc: 0.7988

Epoch 00002: val_loss did not improve from 0.53794
Epoch 3/30
 - 18s - loss: 0.5224 - acc: 0.7600 - val_loss: 0.5293 - val_acc: 0.7855

Epoch 00003: val_loss improved from 0.53794 to 0.52926, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 18s - loss: 0.5085 - acc: 0.7605 - val_loss: 0.4868 - val_acc: 0.8199

Epoch 00004: val_loss improved from 0.52926 to 0.48679, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 19s - loss: 0.4990 - acc: 0.7658 - val_loss: 0.4855 - val_acc: 0.8075

Epoch 00005: val_loss improved from 0.48679 to 0.48548, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 18s - loss: 0.4898 - acc: 0.7677 - val_loss: 0.4717 - val_acc: 0.8133

Epoch 00006: val_loss improved from 0.48548 to 0.47174, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 18s - loss: 0.4817 - acc: 0.7704 - val_loss: 0.4726 - val_acc: 0.8050

Epoch 00007: val_loss did not improve from 0.47174
Epoch 8/30
 - 18s - loss: 0.4750 - acc: 0.7736 - val_loss: 0.4731 - val_acc: 0.8016

Epoch 00008: val_loss did not improve from 0.47174
Epoch 9/30
 - 18s - loss: 0.4677 - acc: 0.7785 - val_loss: 0.5650 - val_acc: 0.8207

Epoch 00009: val_loss did not improve from 0.47174
Epoch 10/30
 - 18s - loss: 0.4624 - acc: 0.7827 - val_loss: 0.4573 - val_acc: 0.8069

Epoch 00010: val_loss improved from 0.47174 to 0.45726, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 18s - loss: 0.4568 - acc: 0.7865 - val_loss: 0.4587 - val_acc: 0.8044

Epoch 00011: val_loss did not improve from 0.45726
Epoch 12/30
 - 18s - loss: 0.4494 - acc: 0.7901 - val_loss: 0.4458 - val_acc: 0.8157

Epoch 00012: val_loss improved from 0.45726 to 0.44578, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 13/30
 - 18s - loss: 0.4458 - acc: 0.7904 - val_loss: 0.4742 - val_acc: 0.7863

Epoch 00013: val_loss did not improve from 0.44578
Epoch 14/30
 - 18s - loss: 0.4411 - acc: 0.7960 - val_loss: 0.4397 - val_acc: 0.8319

Epoch 00014: val_loss improved from 0.44578 to 0.43969, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 15/30
 - 18s - loss: 0.4364 - acc: 0.7979 - val_loss: 0.4658 - val_acc: 0.7909

Epoch 00015: val_loss did not improve from 0.43969
Epoch 16/30
 - 18s - loss: 0.4330 - acc: 0.7990 - val_loss: 0.4567 - val_acc: 0.8163

Epoch 00016: val_loss did not improve from 0.43969
Epoch 17/30
 - 18s - loss: 0.4299 - acc: 0.7997 - val_loss: 0.4392 - val_acc: 0.8052

Epoch 00017: val_loss improved from 0.43969 to 0.43922, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 18/30
 - 18s - loss: 0.4256 - acc: 0.8029 - val_loss: 0.4345 - val_acc: 0.8156

Epoch 00018: val_loss improved from 0.43922 to 0.43447, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 19/30
 - 18s - loss: 0.4215 - acc: 0.8055 - val_loss: 0.4617 - val_acc: 0.8059

Epoch 00019: val_loss did not improve from 0.43447
Epoch 20/30
 - 18s - loss: 0.4195 - acc: 0.8074 - val_loss: 0.4238 - val_acc: 0.8136

Epoch 00020: val_loss improved from 0.43447 to 0.42381, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 21/30
 - 18s - loss: 0.4169 - acc: 0.8087 - val_loss: 0.6043 - val_acc: 0.7796

Epoch 00021: val_loss did not improve from 0.42381
Epoch 22/30
 - 18s - loss: 0.4144 - acc: 0.8103 - val_loss: 0.4267 - val_acc: 0.8383

Epoch 00022: val_loss did not improve from 0.42381
Epoch 23/30
 - 18s - loss: 0.4126 - acc: 0.8114 - val_loss: 0.4364 - val_acc: 0.8089

Epoch 00023: val_loss did not improve from 0.42381
Epoch 24/30
 - 18s - loss: 0.4096 - acc: 0.8145 - val_loss: 0.4302 - val_acc: 0.8360

Epoch 00024: val_loss did not improve from 0.42381
Epoch 25/30
 - 18s - loss: 0.4055 - acc: 0.8154 - val_loss: 0.4353 - val_acc: 0.8238

Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00025: val_loss did not improve from 0.42381
Epoch 00025: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 0s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 230us/step
current Test accuracy: 0.8237903225806451
current auc_score ------------------>  0.8900254364666436

  32/7440 [..............................] - ETA: 11:16
 224/7440 [..............................] - ETA: 1:35 
 448/7440 [>.............................] - ETA: 47s 
 672/7440 [=>............................] - ETA: 31s
 896/7440 [==>...........................] - ETA: 22s
1120/7440 [===>..........................] - ETA: 17s
1344/7440 [====>.........................] - ETA: 14s
1568/7440 [=====>........................] - ETA: 12s
1792/7440 [======>.......................] - ETA: 10s
2016/7440 [=======>......................] - ETA: 9s 
2240/7440 [========>.....................] - ETA: 8s
2464/7440 [========>.....................] - ETA: 7s
2688/7440 [=========>....................] - ETA: 6s
2912/7440 [==========>...................] - ETA: 5s
3136/7440 [===========>..................] - ETA: 5s
3360/7440 [============>.................] - ETA: 4s
3584/7440 [=============>................] - ETA: 4s
3808/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 3s
4256/7440 [================>.............] - ETA: 2s
4480/7440 [=================>............] - ETA: 2s
4704/7440 [=================>............] - ETA: 2s
4928/7440 [==================>...........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 1s
6048/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 0s
6496/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 633us/step
Best saved model Test accuracy: 0.8135752688172043
best saved model auc_score ------------------>  0.8940853711411725
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_15[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_127[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_128[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_129[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_130[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 28, 96, 96)   0           concatenate_51[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_131[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_132[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 34, 96, 96)   0           concatenate_52[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 34, 96, 96)   136         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 34, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 17, 96, 96)   578         activation_133[0][0]             
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 17, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 17, 48, 48)   68          average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 17, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   408         activation_134[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_135[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 23, 48, 48)   92          concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 23, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   552         activation_136[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_137[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 29, 48, 48)   0           concatenate_54[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 29, 48, 48)   116         concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 29, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 24, 48, 48)   696         activation_138[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 24, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_139[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 35, 48, 48)   0           concatenate_55[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 35, 48, 48)   140         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 35, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_15 (Gl (None, 35)           0           activation_140[0][0]             
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 1)            36          global_average_pooling2d_15[0][0]
==================================================================================================
Total params: 13,310
Trainable params: 12,614
Non-trainable params: 696
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 37s - loss: 0.6382 - acc: 0.6977 - val_loss: 0.5582 - val_acc: 0.8152

Epoch 00001: val_loss improved from inf to 0.55819, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 28s - loss: 0.5363 - acc: 0.7650 - val_loss: 0.4998 - val_acc: 0.8103

Epoch 00002: val_loss improved from 0.55819 to 0.49982, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 28s - loss: 0.4959 - acc: 0.7762 - val_loss: 0.4930 - val_acc: 0.8078

Epoch 00003: val_loss improved from 0.49982 to 0.49300, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 28s - loss: 0.4722 - acc: 0.7880 - val_loss: 0.4677 - val_acc: 0.8348

Epoch 00004: val_loss improved from 0.49300 to 0.46769, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 28s - loss: 0.4557 - acc: 0.7945 - val_loss: 0.5381 - val_acc: 0.8184

Epoch 00005: val_loss did not improve from 0.46769
Epoch 6/30
 - 28s - loss: 0.4411 - acc: 0.8033 - val_loss: 0.5048 - val_acc: 0.8333

Epoch 00006: val_loss did not improve from 0.46769
Epoch 7/30
 - 28s - loss: 0.4291 - acc: 0.8089 - val_loss: 0.4894 - val_acc: 0.7887

Epoch 00007: val_loss did not improve from 0.46769
Epoch 8/30
 - 28s - loss: 0.4179 - acc: 0.8166 - val_loss: 0.4533 - val_acc: 0.8332

Epoch 00008: val_loss improved from 0.46769 to 0.45332, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 28s - loss: 0.4101 - acc: 0.8198 - val_loss: 0.4887 - val_acc: 0.7859

Epoch 00009: val_loss did not improve from 0.45332
Epoch 10/30
 - 28s - loss: 0.3965 - acc: 0.8273 - val_loss: 0.4876 - val_acc: 0.7695

Epoch 00010: val_loss did not improve from 0.45332
Epoch 11/30
 - 28s - loss: 0.3899 - acc: 0.8316 - val_loss: 0.5409 - val_acc: 0.7878

Epoch 00011: val_loss did not improve from 0.45332
Epoch 12/30
 - 28s - loss: 0.3865 - acc: 0.8339 - val_loss: 0.5060 - val_acc: 0.7954

Epoch 00012: val_loss did not improve from 0.45332
Epoch 13/30
 - 28s - loss: 0.3774 - acc: 0.8402 - val_loss: 0.5163 - val_acc: 0.7813

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.45332
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1344/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1728/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2688/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3456/7440 [============>.................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
3936/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 313us/step
current Test accuracy: 0.7813172043010753
current auc_score ------------------>  0.8626331078737426

  32/7440 [..............................] - ETA: 12:31
 192/7440 [..............................] - ETA: 2:04 
 384/7440 [>.............................] - ETA: 1:01
 576/7440 [=>............................] - ETA: 40s 
 768/7440 [==>...........................] - ETA: 30s
 928/7440 [==>...........................] - ETA: 24s
1120/7440 [===>..........................] - ETA: 20s
1312/7440 [====>.........................] - ETA: 17s
1504/7440 [=====>........................] - ETA: 14s
1664/7440 [=====>........................] - ETA: 13s
1856/7440 [======>.......................] - ETA: 11s
2048/7440 [=======>......................] - ETA: 10s
2240/7440 [========>.....................] - ETA: 9s 
2432/7440 [========>.....................] - ETA: 8s
2624/7440 [=========>....................] - ETA: 7s
2816/7440 [==========>...................] - ETA: 6s
3008/7440 [===========>..................] - ETA: 6s
3200/7440 [===========>..................] - ETA: 5s
3392/7440 [============>.................] - ETA: 5s
3552/7440 [=============>................] - ETA: 4s
3712/7440 [=============>................] - ETA: 4s
3904/7440 [==============>...............] - ETA: 4s
4096/7440 [===============>..............] - ETA: 3s
4256/7440 [================>.............] - ETA: 3s
4416/7440 [================>.............] - ETA: 3s
4576/7440 [=================>............] - ETA: 2s
4768/7440 [==================>...........] - ETA: 2s
4928/7440 [==================>...........] - ETA: 2s
5120/7440 [===================>..........] - ETA: 2s
5312/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 1s
5664/7440 [=====================>........] - ETA: 1s
5824/7440 [======================>.......] - ETA: 1s
6016/7440 [=======================>......] - ETA: 1s
6208/7440 [========================>.....] - ETA: 1s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 747us/step
Best saved model Test accuracy: 0.8331989247311828
best saved model auc_score ------------------>  0.9006508700427794
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_16 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_141 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_14 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_142 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_16  (None, 8)                 0         
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 14s - loss: 0.6493 - acc: 0.6581 - val_loss: 0.5857 - val_acc: 0.8121

Epoch 00001: val_loss improved from inf to 0.58572, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 7s - loss: 0.6095 - acc: 0.7069 - val_loss: 0.5284 - val_acc: 0.8284

Epoch 00002: val_loss improved from 0.58572 to 0.52836, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 7s - loss: 0.5899 - acc: 0.7180 - val_loss: 0.5202 - val_acc: 0.8122

Epoch 00003: val_loss improved from 0.52836 to 0.52016, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 7s - loss: 0.5777 - acc: 0.7238 - val_loss: 0.5060 - val_acc: 0.8487

Epoch 00004: val_loss improved from 0.52016 to 0.50603, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 7s - loss: 0.5686 - acc: 0.7274 - val_loss: 0.4963 - val_acc: 0.8199

Epoch 00005: val_loss improved from 0.50603 to 0.49633, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 7s - loss: 0.5606 - acc: 0.7311 - val_loss: 0.5120 - val_acc: 0.7911

Epoch 00006: val_loss did not improve from 0.49633
Epoch 7/30
 - 7s - loss: 0.5561 - acc: 0.7307 - val_loss: 0.4901 - val_acc: 0.8569

Epoch 00007: val_loss improved from 0.49633 to 0.49014, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 7s - loss: 0.5501 - acc: 0.7343 - val_loss: 0.4999 - val_acc: 0.8616

Epoch 00008: val_loss did not improve from 0.49014
Epoch 9/30
 - 7s - loss: 0.5462 - acc: 0.7368 - val_loss: 0.5002 - val_acc: 0.8556

Epoch 00009: val_loss did not improve from 0.49014
Epoch 10/30
 - 7s - loss: 0.5417 - acc: 0.7394 - val_loss: 0.4973 - val_acc: 0.8642

Epoch 00010: val_loss did not improve from 0.49014
Epoch 11/30
 - 7s - loss: 0.5388 - acc: 0.7428 - val_loss: 0.5092 - val_acc: 0.8665

Epoch 00011: val_loss did not improve from 0.49014
Epoch 12/30
 - 7s - loss: 0.5361 - acc: 0.7431 - val_loss: 0.5298 - val_acc: 0.8300

Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00012: val_loss did not improve from 0.49014
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 1s
 384/7440 [>.............................] - ETA: 1s
 736/7440 [=>............................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 0s
1440/7440 [====>.........................] - ETA: 0s
1792/7440 [======>.......................] - ETA: 0s
2144/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2848/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3904/7440 [==============>...............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 146us/step
current Test accuracy: 0.8299731182795699
current auc_score ------------------>  0.8685284425945197

  32/7440 [..............................] - ETA: 11:40
 352/7440 [>.............................] - ETA: 1:01 
 704/7440 [=>............................] - ETA: 29s 
1056/7440 [===>..........................] - ETA: 19s
1408/7440 [====>.........................] - ETA: 13s
1760/7440 [======>.......................] - ETA: 10s
2112/7440 [=======>......................] - ETA: 8s 
2464/7440 [========>.....................] - ETA: 6s
2816/7440 [==========>...................] - ETA: 5s
3168/7440 [===========>..................] - ETA: 4s
3520/7440 [=============>................] - ETA: 3s
3872/7440 [==============>...............] - ETA: 3s
4224/7440 [================>.............] - ETA: 2s
4576/7440 [=================>............] - ETA: 2s
4928/7440 [==================>...........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
6016/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 555us/step
Best saved model Test accuracy: 0.8568548387096774
best saved model auc_score ------------------>  0.8726360345126605
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_17[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_143[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_144[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_145[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_146[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 32, 96, 96)   0           concatenate_57[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_147[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_148[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 40, 96, 96)   0           concatenate_58[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_149[0][0]             
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_150[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_151[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 28, 48, 48)   0           average_pooling2d_15[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_152[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_153[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 36, 48, 48)   0           concatenate_60[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_154[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_155[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 44, 48, 48)   0           concatenate_61[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_17 (Gl (None, 44)           0           activation_156[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            45          global_average_pooling2d_17[0][0]
==================================================================================================
Total params: 21,677
Trainable params: 20,813
Non-trainable params: 864
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 42s - loss: 0.5606 - acc: 0.7553 - val_loss: 0.4734 - val_acc: 0.8195

Epoch 00001: val_loss improved from inf to 0.47344, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 32s - loss: 0.4837 - acc: 0.7872 - val_loss: 0.5083 - val_acc: 0.8165

Epoch 00002: val_loss did not improve from 0.47344
Epoch 3/30
 - 32s - loss: 0.4548 - acc: 0.7985 - val_loss: 0.4487 - val_acc: 0.8234

Epoch 00003: val_loss improved from 0.47344 to 0.44869, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 32s - loss: 0.4345 - acc: 0.8106 - val_loss: 0.4764 - val_acc: 0.8246

Epoch 00004: val_loss did not improve from 0.44869
Epoch 5/30
 - 32s - loss: 0.4210 - acc: 0.8164 - val_loss: 0.4891 - val_acc: 0.8099

Epoch 00005: val_loss did not improve from 0.44869
Epoch 6/30
 - 32s - loss: 0.4088 - acc: 0.8258 - val_loss: 0.6242 - val_acc: 0.8070

Epoch 00006: val_loss did not improve from 0.44869
Epoch 7/30
 - 32s - loss: 0.3987 - acc: 0.8311 - val_loss: 0.5957 - val_acc: 0.7970

Epoch 00007: val_loss did not improve from 0.44869
Epoch 8/30
 - 32s - loss: 0.3911 - acc: 0.8346 - val_loss: 0.5063 - val_acc: 0.7999

Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00008: val_loss did not improve from 0.44869
Epoch 00008: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 354us/step
current Test accuracy: 0.7998655913978494
current auc_score ------------------>  0.8768084821944735

  32/7440 [..............................] - ETA: 13:44
 192/7440 [..............................] - ETA: 2:16 
 352/7440 [>.............................] - ETA: 1:13
 512/7440 [=>............................] - ETA: 50s 
 672/7440 [=>............................] - ETA: 38s
 832/7440 [==>...........................] - ETA: 30s
 992/7440 [===>..........................] - ETA: 25s
1152/7440 [===>..........................] - ETA: 21s
1312/7440 [====>.........................] - ETA: 18s
1472/7440 [====>.........................] - ETA: 16s
1632/7440 [=====>........................] - ETA: 14s
1792/7440 [======>.......................] - ETA: 13s
1952/7440 [======>.......................] - ETA: 11s
2112/7440 [=======>......................] - ETA: 10s
2272/7440 [========>.....................] - ETA: 9s 
2432/7440 [========>.....................] - ETA: 9s
2592/7440 [=========>....................] - ETA: 8s
2752/7440 [==========>...................] - ETA: 7s
2912/7440 [==========>...................] - ETA: 7s
3072/7440 [===========>..................] - ETA: 6s
3232/7440 [============>.................] - ETA: 6s
3392/7440 [============>.................] - ETA: 5s
3552/7440 [=============>................] - ETA: 5s
3712/7440 [=============>................] - ETA: 4s
3872/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 4s
4192/7440 [===============>..............] - ETA: 3s
4352/7440 [================>.............] - ETA: 3s
4512/7440 [=================>............] - ETA: 3s
4672/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5312/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 824us/step
Best saved model Test accuracy: 0.8233870967741935
best saved model auc_score ------------------>  0.8943705197132616
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_18 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_18[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_157[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_158[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_159[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 28, 96, 96)   0           concatenate_63[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_161[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_162[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 34, 96, 96)   0           concatenate_64[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 34, 96, 96)   136         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 34, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_18 (Gl (None, 34)           0           activation_163[0][0]             
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 1)            35          global_average_pooling2d_18[0][0]
==================================================================================================
Total params: 6,483
Trainable params: 6,139
Non-trainable params: 344
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 31s - loss: 0.6098 - acc: 0.7070 - val_loss: 0.5750 - val_acc: 0.7679

Epoch 00001: val_loss improved from inf to 0.57498, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 22s - loss: 0.5529 - acc: 0.7463 - val_loss: 0.5049 - val_acc: 0.8312

Epoch 00002: val_loss improved from 0.57498 to 0.50491, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 22s - loss: 0.5217 - acc: 0.7624 - val_loss: 0.5162 - val_acc: 0.8345

Epoch 00003: val_loss did not improve from 0.50491
Epoch 4/30
 - 22s - loss: 0.5009 - acc: 0.7697 - val_loss: 0.5092 - val_acc: 0.8199

Epoch 00004: val_loss did not improve from 0.50491
Epoch 5/30
 - 22s - loss: 0.4859 - acc: 0.7760 - val_loss: 0.4750 - val_acc: 0.8294

Epoch 00005: val_loss improved from 0.50491 to 0.47501, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 22s - loss: 0.4769 - acc: 0.7777 - val_loss: 0.5022 - val_acc: 0.8055

Epoch 00006: val_loss did not improve from 0.47501
Epoch 7/30
 - 22s - loss: 0.4666 - acc: 0.7827 - val_loss: 0.5067 - val_acc: 0.8090

Epoch 00007: val_loss did not improve from 0.47501
Epoch 8/30
 - 22s - loss: 0.4605 - acc: 0.7867 - val_loss: 0.5034 - val_acc: 0.7833

Epoch 00008: val_loss did not improve from 0.47501
Epoch 9/30
 - 22s - loss: 0.4555 - acc: 0.7874 - val_loss: 0.4800 - val_acc: 0.7922

Epoch 00009: val_loss did not improve from 0.47501
Epoch 10/30
 - 22s - loss: 0.4503 - acc: 0.7913 - val_loss: 0.5223 - val_acc: 0.7578

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.47501
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 273us/step
current Test accuracy: 0.7577956989247312
current auc_score ------------------>  0.8355337322233785

  32/7440 [..............................] - ETA: 14:18
 192/7440 [..............................] - ETA: 2:21 
 384/7440 [>.............................] - ETA: 1:10
 576/7440 [=>............................] - ETA: 46s 
 768/7440 [==>...........................] - ETA: 34s
 960/7440 [==>...........................] - ETA: 26s
1152/7440 [===>..........................] - ETA: 21s
1344/7440 [====>.........................] - ETA: 18s
1536/7440 [=====>........................] - ETA: 15s
1728/7440 [=====>........................] - ETA: 13s
1920/7440 [======>.......................] - ETA: 12s
2112/7440 [=======>......................] - ETA: 10s
2304/7440 [========>.....................] - ETA: 9s 
2496/7440 [=========>....................] - ETA: 8s
2688/7440 [=========>....................] - ETA: 7s
2880/7440 [==========>...................] - ETA: 7s
3072/7440 [===========>..................] - ETA: 6s
3264/7440 [============>.................] - ETA: 5s
3456/7440 [============>.................] - ETA: 5s
3648/7440 [=============>................] - ETA: 4s
3840/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 4s
4224/7440 [================>.............] - ETA: 3s
4416/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 778us/step
Best saved model Test accuracy: 0.8294354838709678
best saved model auc_score ------------------>  0.8684681755116199
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_19[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_164[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_166[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 28, 96, 96)   0           concatenate_66[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_168[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_169[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 34, 96, 96)   0           concatenate_67[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 34, 96, 96)   136         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 34, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_19 (Gl (None, 34)           0           activation_170[0][0]             
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 1)            35          global_average_pooling2d_19[0][0]
==================================================================================================
Total params: 6,483
Trainable params: 6,139
Non-trainable params: 344
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.6340 - acc: 0.6787 - val_loss: 0.5573 - val_acc: 0.7772

Epoch 00001: val_loss improved from inf to 0.55730, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 22s - loss: 0.5640 - acc: 0.7461 - val_loss: 0.5226 - val_acc: 0.8210

Epoch 00002: val_loss improved from 0.55730 to 0.52263, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 22s - loss: 0.5312 - acc: 0.7606 - val_loss: 0.4981 - val_acc: 0.7923

Epoch 00003: val_loss improved from 0.52263 to 0.49810, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 22s - loss: 0.5077 - acc: 0.7695 - val_loss: 0.4995 - val_acc: 0.7985

Epoch 00004: val_loss did not improve from 0.49810
Epoch 5/30
 - 22s - loss: 0.4915 - acc: 0.7756 - val_loss: 0.4695 - val_acc: 0.8040

Epoch 00005: val_loss improved from 0.49810 to 0.46946, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 22s - loss: 0.4798 - acc: 0.7795 - val_loss: 0.4949 - val_acc: 0.8066

Epoch 00006: val_loss did not improve from 0.46946
Epoch 7/30
 - 22s - loss: 0.4701 - acc: 0.7820 - val_loss: 0.4976 - val_acc: 0.7633

Epoch 00007: val_loss did not improve from 0.46946
Epoch 8/30
 - 22s - loss: 0.4633 - acc: 0.7845 - val_loss: 0.4708 - val_acc: 0.8048

Epoch 00008: val_loss did not improve from 0.46946
Epoch 9/30
 - 22s - loss: 0.4547 - acc: 0.7888 - val_loss: 0.4397 - val_acc: 0.8306

Epoch 00009: val_loss improved from 0.46946 to 0.43975, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 22s - loss: 0.4485 - acc: 0.7920 - val_loss: 0.5656 - val_acc: 0.7976

Epoch 00010: val_loss did not improve from 0.43975
Epoch 11/30
 - 22s - loss: 0.4430 - acc: 0.7955 - val_loss: 0.4843 - val_acc: 0.8046

Epoch 00011: val_loss did not improve from 0.43975
Epoch 12/30
 - 22s - loss: 0.4368 - acc: 0.7986 - val_loss: 0.4960 - val_acc: 0.8019

Epoch 00012: val_loss did not improve from 0.43975
Epoch 13/30
 - 22s - loss: 0.4323 - acc: 0.8026 - val_loss: 0.4525 - val_acc: 0.8043

Epoch 00013: val_loss did not improve from 0.43975
Epoch 14/30
 - 22s - loss: 0.4280 - acc: 0.8028 - val_loss: 0.4642 - val_acc: 0.8282

Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00014: val_loss did not improve from 0.43975
Epoch 00014: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 283us/step
current Test accuracy: 0.8282258064516129
current auc_score ------------------>  0.8808928055266505

  32/7440 [..............................] - ETA: 15:17
 192/7440 [..............................] - ETA: 2:31 
 384/7440 [>.............................] - ETA: 1:14
 576/7440 [=>............................] - ETA: 49s 
 768/7440 [==>...........................] - ETA: 36s
 960/7440 [==>...........................] - ETA: 28s
1152/7440 [===>..........................] - ETA: 23s
1344/7440 [====>.........................] - ETA: 19s
1536/7440 [=====>........................] - ETA: 16s
1728/7440 [=====>........................] - ETA: 14s
1920/7440 [======>.......................] - ETA: 12s
2112/7440 [=======>......................] - ETA: 11s
2304/7440 [========>.....................] - ETA: 10s
2496/7440 [=========>....................] - ETA: 9s 
2688/7440 [=========>....................] - ETA: 8s
2880/7440 [==========>...................] - ETA: 7s
3072/7440 [===========>..................] - ETA: 6s
3264/7440 [============>.................] - ETA: 6s
3456/7440 [============>.................] - ETA: 5s
3648/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 4s
4224/7440 [================>.............] - ETA: 3s
4416/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 812us/step
Best saved model Test accuracy: 0.8306451612903226
best saved model auc_score ------------------>  0.8943966065441091
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_20 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_20[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_173[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 28, 96, 96)   0           concatenate_69[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_175[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   336         activation_176[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_177[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_16[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 20, 48, 48)   80          concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 20, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   480         activation_178[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_179[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 26, 48, 48)   0           concatenate_71[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_20 (Gl (None, 26)           0           activation_180[0][0]             
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 1)            27          global_average_pooling2d_20[0][0]
==================================================================================================
Total params: 8,507
Trainable params: 8,063
Non-trainable params: 444
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.6331 - acc: 0.6871 - val_loss: 0.6117 - val_acc: 0.6624

Epoch 00001: val_loss improved from inf to 0.61167, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 21s - loss: 0.5430 - acc: 0.7582 - val_loss: 0.5312 - val_acc: 0.8118

Epoch 00002: val_loss improved from 0.61167 to 0.53120, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 21s - loss: 0.5050 - acc: 0.7693 - val_loss: 0.4570 - val_acc: 0.8215

Epoch 00003: val_loss improved from 0.53120 to 0.45695, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 21s - loss: 0.4820 - acc: 0.7791 - val_loss: 0.4989 - val_acc: 0.7988

Epoch 00004: val_loss did not improve from 0.45695
Epoch 5/30
 - 21s - loss: 0.4669 - acc: 0.7847 - val_loss: 0.4768 - val_acc: 0.7878

Epoch 00005: val_loss did not improve from 0.45695
Epoch 6/30
 - 21s - loss: 0.4550 - acc: 0.7916 - val_loss: 0.5379 - val_acc: 0.7840

Epoch 00006: val_loss did not improve from 0.45695
Epoch 7/30
 - 21s - loss: 0.4439 - acc: 0.7974 - val_loss: 0.4549 - val_acc: 0.7898

Epoch 00007: val_loss improved from 0.45695 to 0.45488, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 21s - loss: 0.4358 - acc: 0.7998 - val_loss: 0.4910 - val_acc: 0.7733

Epoch 00008: val_loss did not improve from 0.45488
Epoch 9/30
 - 21s - loss: 0.4282 - acc: 0.8042 - val_loss: 0.6489 - val_acc: 0.7397

Epoch 00009: val_loss did not improve from 0.45488
Epoch 10/30
 - 21s - loss: 0.4208 - acc: 0.8068 - val_loss: 0.4314 - val_acc: 0.8161

Epoch 00010: val_loss improved from 0.45488 to 0.43138, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 21s - loss: 0.4157 - acc: 0.8120 - val_loss: 0.4227 - val_acc: 0.8356

Epoch 00011: val_loss improved from 0.43138 to 0.42269, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 12/30
 - 21s - loss: 0.4103 - acc: 0.8156 - val_loss: 0.4357 - val_acc: 0.8128

Epoch 00012: val_loss did not improve from 0.42269
Epoch 13/30
 - 21s - loss: 0.4070 - acc: 0.8173 - val_loss: 0.4794 - val_acc: 0.7874

Epoch 00013: val_loss did not improve from 0.42269
Epoch 14/30
 - 21s - loss: 0.4035 - acc: 0.8190 - val_loss: 0.5715 - val_acc: 0.7878

Epoch 00014: val_loss did not improve from 0.42269
Epoch 15/30
 - 21s - loss: 0.3999 - acc: 0.8223 - val_loss: 0.4975 - val_acc: 0.7901

Epoch 00015: val_loss did not improve from 0.42269
Epoch 16/30
 - 21s - loss: 0.3926 - acc: 0.8252 - val_loss: 0.4603 - val_acc: 0.8095

Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00016: val_loss did not improve from 0.42269
Epoch 00016: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 272us/step
current Test accuracy: 0.8095430107526882
current auc_score ------------------>  0.9034262631518093

  32/7440 [..............................] - ETA: 16:53
 192/7440 [..............................] - ETA: 2:47 
 384/7440 [>.............................] - ETA: 1:22
 576/7440 [=>............................] - ETA: 54s 
 768/7440 [==>...........................] - ETA: 39s
 960/7440 [==>...........................] - ETA: 31s
1152/7440 [===>..........................] - ETA: 25s
1344/7440 [====>.........................] - ETA: 21s
1536/7440 [=====>........................] - ETA: 18s
1728/7440 [=====>........................] - ETA: 16s
1920/7440 [======>.......................] - ETA: 14s
2112/7440 [=======>......................] - ETA: 12s
2304/7440 [========>.....................] - ETA: 11s
2496/7440 [=========>....................] - ETA: 10s
2688/7440 [=========>....................] - ETA: 9s 
2880/7440 [==========>...................] - ETA: 8s
3072/7440 [===========>..................] - ETA: 7s
3264/7440 [============>.................] - ETA: 6s
3456/7440 [============>.................] - ETA: 6s
3648/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 5s
4032/7440 [===============>..............] - ETA: 4s
4224/7440 [================>.............] - ETA: 4s
4416/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 863us/step
Best saved model Test accuracy: 0.8356182795698924
best saved model auc_score ------------------>  0.9038768571511158
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_21[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_181[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_182[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_183[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_184[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 36, 96, 96)   0           concatenate_73[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_185[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_186[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 46, 96, 96)   0           concatenate_74[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 96, 96)   184         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 46, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_21 (Gl (None, 46)           0           activation_187[0][0]             
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 1)            47          global_average_pooling2d_21[0][0]
==================================================================================================
Total params: 15,231
Trainable params: 14,743
Non-trainable params: 488
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 40s - loss: 0.6103 - acc: 0.7016 - val_loss: 0.6929 - val_acc: 0.7081

Epoch 00001: val_loss improved from inf to 0.69286, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 28s - loss: 0.5438 - acc: 0.7534 - val_loss: 0.5776 - val_acc: 0.8046

Epoch 00002: val_loss improved from 0.69286 to 0.57761, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 28s - loss: 0.5133 - acc: 0.7691 - val_loss: 0.5646 - val_acc: 0.7910

Epoch 00003: val_loss improved from 0.57761 to 0.56455, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 28s - loss: 0.4914 - acc: 0.7765 - val_loss: 0.4740 - val_acc: 0.8120

Epoch 00004: val_loss improved from 0.56455 to 0.47405, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 28s - loss: 0.4778 - acc: 0.7811 - val_loss: 0.4883 - val_acc: 0.8296

Epoch 00005: val_loss did not improve from 0.47405
Epoch 6/30
 - 28s - loss: 0.4632 - acc: 0.7889 - val_loss: 0.5514 - val_acc: 0.7676

Epoch 00006: val_loss did not improve from 0.47405
Epoch 7/30
 - 28s - loss: 0.4541 - acc: 0.7922 - val_loss: 0.5087 - val_acc: 0.7620

Epoch 00007: val_loss did not improve from 0.47405
Epoch 8/30
 - 28s - loss: 0.4451 - acc: 0.7984 - val_loss: 0.4504 - val_acc: 0.8055

Epoch 00008: val_loss improved from 0.47405 to 0.45038, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 28s - loss: 0.4370 - acc: 0.8025 - val_loss: 0.5228 - val_acc: 0.7870

Epoch 00009: val_loss did not improve from 0.45038
Epoch 10/30
 - 28s - loss: 0.4305 - acc: 0.8057 - val_loss: 0.4586 - val_acc: 0.7883

Epoch 00010: val_loss did not improve from 0.45038
Epoch 11/30
 - 28s - loss: 0.4236 - acc: 0.8096 - val_loss: 0.4719 - val_acc: 0.7788

Epoch 00011: val_loss did not improve from 0.45038
Epoch 12/30
 - 28s - loss: 0.4168 - acc: 0.8117 - val_loss: 0.5158 - val_acc: 0.8077

Epoch 00012: val_loss did not improve from 0.45038
Epoch 13/30
 - 28s - loss: 0.4122 - acc: 0.8153 - val_loss: 0.6671 - val_acc: 0.7316

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.45038
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 335us/step
current Test accuracy: 0.7315860215053763
current auc_score ------------------>  0.8849689270435889

  32/7440 [..............................] - ETA: 17:50
 160/7440 [..............................] - ETA: 3:32 
 320/7440 [>.............................] - ETA: 1:45
 480/7440 [>.............................] - ETA: 1:09
 640/7440 [=>............................] - ETA: 51s 
 800/7440 [==>...........................] - ETA: 40s
 960/7440 [==>...........................] - ETA: 33s
1120/7440 [===>..........................] - ETA: 28s
1280/7440 [====>.........................] - ETA: 24s
1440/7440 [====>.........................] - ETA: 21s
1600/7440 [=====>........................] - ETA: 18s
1760/7440 [======>.......................] - ETA: 16s
1920/7440 [======>.......................] - ETA: 15s
2080/7440 [=======>......................] - ETA: 13s
2240/7440 [========>.....................] - ETA: 12s
2400/7440 [========>.....................] - ETA: 11s
2560/7440 [=========>....................] - ETA: 10s
2720/7440 [=========>....................] - ETA: 9s 
2880/7440 [==========>...................] - ETA: 8s
3040/7440 [===========>..................] - ETA: 8s
3200/7440 [===========>..................] - ETA: 7s
3360/7440 [============>.................] - ETA: 6s
3520/7440 [=============>................] - ETA: 6s
3680/7440 [=============>................] - ETA: 5s
3840/7440 [==============>...............] - ETA: 5s
4000/7440 [===============>..............] - ETA: 5s
4160/7440 [===============>..............] - ETA: 4s
4320/7440 [================>.............] - ETA: 4s
4480/7440 [=================>............] - ETA: 4s
4640/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 2s
5280/7440 [====================>.........] - ETA: 2s
5440/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 1s
5920/7440 [======================>.......] - ETA: 1s
6080/7440 [=======================>......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 959us/step
Best saved model Test accuracy: 0.8055107526881721
best saved model auc_score ------------------>  0.8861611168921264
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_22 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_22[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_188[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_189 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_189[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 28, 96, 96)   112         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_190 (Activation)     (None, 28, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_22 (Gl (None, 28)           0           activation_190[0][0]             
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 1)            29          global_average_pooling2d_22[0][0]
==================================================================================================
Total params: 6,637
Trainable params: 6,453
Non-trainable params: 184
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 26s - loss: 0.6436 - acc: 0.6681 - val_loss: 0.6258 - val_acc: 0.7047

Epoch 00001: val_loss improved from inf to 0.62579, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 14s - loss: 0.5757 - acc: 0.7369 - val_loss: 0.5060 - val_acc: 0.8579

Epoch 00002: val_loss improved from 0.62579 to 0.50597, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 14s - loss: 0.5472 - acc: 0.7496 - val_loss: 0.4913 - val_acc: 0.8386

Epoch 00003: val_loss improved from 0.50597 to 0.49132, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 14s - loss: 0.5308 - acc: 0.7524 - val_loss: 0.5113 - val_acc: 0.8195

Epoch 00004: val_loss did not improve from 0.49132
Epoch 5/30
 - 14s - loss: 0.5199 - acc: 0.7548 - val_loss: 0.5452 - val_acc: 0.8235

Epoch 00005: val_loss did not improve from 0.49132
Epoch 6/30
 - 14s - loss: 0.5131 - acc: 0.7550 - val_loss: 0.4977 - val_acc: 0.8241

Epoch 00006: val_loss did not improve from 0.49132
Epoch 7/30
 - 14s - loss: 0.5079 - acc: 0.7591 - val_loss: 0.4855 - val_acc: 0.8288

Epoch 00007: val_loss improved from 0.49132 to 0.48554, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 14s - loss: 0.5027 - acc: 0.7598 - val_loss: 0.5371 - val_acc: 0.7972

Epoch 00008: val_loss did not improve from 0.48554
Epoch 9/30
 - 14s - loss: 0.4993 - acc: 0.7601 - val_loss: 0.4937 - val_acc: 0.8040

Epoch 00009: val_loss did not improve from 0.48554
Epoch 10/30
 - 14s - loss: 0.4945 - acc: 0.7630 - val_loss: 0.4958 - val_acc: 0.8187

Epoch 00010: val_loss did not improve from 0.48554
Epoch 11/30
 - 14s - loss: 0.4931 - acc: 0.7639 - val_loss: 0.4999 - val_acc: 0.8069

Epoch 00011: val_loss did not improve from 0.48554
Epoch 12/30
 - 14s - loss: 0.4893 - acc: 0.7651 - val_loss: 0.5416 - val_acc: 0.7511

Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00012: val_loss did not improve from 0.48554
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 736/7440 [=>............................] - ETA: 1s
 960/7440 [==>...........................] - ETA: 1s
1216/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1728/7440 [=====>........................] - ETA: 1s
1984/7440 [=======>......................] - ETA: 1s
2240/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
3008/7440 [===========>..................] - ETA: 0s
3264/7440 [============>.................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4544/7440 [=================>............] - ETA: 0s
4768/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 221us/step
current Test accuracy: 0.7510752688172043
current auc_score ------------------>  0.812752052260377

  32/7440 [..............................] - ETA: 17:56
 256/7440 [>.............................] - ETA: 2:12 
 512/7440 [=>............................] - ETA: 1:04
 768/7440 [==>...........................] - ETA: 41s 
1024/7440 [===>..........................] - ETA: 30s
1248/7440 [====>.........................] - ETA: 24s
1472/7440 [====>.........................] - ETA: 20s
1696/7440 [=====>........................] - ETA: 17s
1952/7440 [======>.......................] - ETA: 14s
2176/7440 [=======>......................] - ETA: 12s
2400/7440 [========>.....................] - ETA: 10s
2656/7440 [=========>....................] - ETA: 9s 
2912/7440 [==========>...................] - ETA: 8s
3136/7440 [===========>..................] - ETA: 7s
3360/7440 [============>.................] - ETA: 6s
3616/7440 [=============>................] - ETA: 5s
3872/7440 [==============>...............] - ETA: 5s
4128/7440 [===============>..............] - ETA: 4s
4352/7440 [================>.............] - ETA: 3s
4576/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 3s
5024/7440 [===================>..........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 2s
5504/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 1s
5984/7440 [=======================>......] - ETA: 1s
6208/7440 [========================>.....] - ETA: 1s
6464/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 845us/step
Best saved model Test accuracy: 0.828763440860215
best saved model auc_score ------------------>  0.8636320672910163
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_23[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_191 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_191[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_192 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_192[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 56, 96, 96)   1680        activation_193[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 56, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_194[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 44, 96, 96)   0           concatenate_77[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 44, 96, 96)   176         concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 44, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 56, 96, 96)   2464        activation_195[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 56, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_196[0][0]             
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 58, 96, 96)   0           concatenate_78[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 58, 96, 96)   232         concatenate_79[0][0]             
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 58, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_23 (Gl (None, 58)           0           activation_197[0][0]             
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 1)            59          global_average_pooling2d_23[0][0]
==================================================================================================
Total params: 27,819
Trainable params: 27,187
Non-trainable params: 632
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 46s - loss: 0.5861 - acc: 0.7323 - val_loss: 0.5423 - val_acc: 0.7501

Epoch 00001: val_loss improved from inf to 0.54230, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 33s - loss: 0.5264 - acc: 0.7650 - val_loss: 0.5263 - val_acc: 0.8000

Epoch 00002: val_loss improved from 0.54230 to 0.52626, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 33s - loss: 0.5006 - acc: 0.7717 - val_loss: 0.5091 - val_acc: 0.8004

Epoch 00003: val_loss improved from 0.52626 to 0.50914, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 33s - loss: 0.4802 - acc: 0.7831 - val_loss: 0.4855 - val_acc: 0.8145

Epoch 00004: val_loss improved from 0.50914 to 0.48547, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 33s - loss: 0.4663 - acc: 0.7888 - val_loss: 0.4750 - val_acc: 0.7930

Epoch 00005: val_loss improved from 0.48547 to 0.47499, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 33s - loss: 0.4516 - acc: 0.7969 - val_loss: 0.4756 - val_acc: 0.8026

Epoch 00006: val_loss did not improve from 0.47499
Epoch 7/30
 - 33s - loss: 0.4412 - acc: 0.8041 - val_loss: 0.6867 - val_acc: 0.7638

Epoch 00007: val_loss did not improve from 0.47499
Epoch 8/30
 - 33s - loss: 0.4320 - acc: 0.8082 - val_loss: 0.4468 - val_acc: 0.8148

Epoch 00008: val_loss improved from 0.47499 to 0.44683, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 33s - loss: 0.4212 - acc: 0.8155 - val_loss: 0.4646 - val_acc: 0.8013

Epoch 00009: val_loss did not improve from 0.44683
Epoch 10/30
 - 33s - loss: 0.4169 - acc: 0.8154 - val_loss: 0.4244 - val_acc: 0.8348

Epoch 00010: val_loss improved from 0.44683 to 0.42440, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 33s - loss: 0.4100 - acc: 0.8204 - val_loss: 0.5137 - val_acc: 0.8005

Epoch 00011: val_loss did not improve from 0.42440
Epoch 12/30
 - 33s - loss: 0.4038 - acc: 0.8228 - val_loss: 0.4638 - val_acc: 0.8171

Epoch 00012: val_loss did not improve from 0.42440
Epoch 13/30
 - 33s - loss: 0.3972 - acc: 0.8245 - val_loss: 0.4772 - val_acc: 0.8055

Epoch 00013: val_loss did not improve from 0.42440
Epoch 14/30
 - 33s - loss: 0.3915 - acc: 0.8309 - val_loss: 0.5859 - val_acc: 0.7724

Epoch 00014: val_loss did not improve from 0.42440
Epoch 15/30
 - 33s - loss: 0.3891 - acc: 0.8333 - val_loss: 0.4631 - val_acc: 0.8110

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.42440
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 320/7440 [>.............................] - ETA: 2s
 480/7440 [>.............................] - ETA: 2s
 640/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 960/7440 [==>...........................] - ETA: 2s
1120/7440 [===>..........................] - ETA: 2s
1280/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1600/7440 [=====>........................] - ETA: 2s
1760/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2240/7440 [========>.....................] - ETA: 1s
2400/7440 [========>.....................] - ETA: 1s
2560/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3040/7440 [===========>..................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4160/7440 [===============>..............] - ETA: 1s
4320/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 378us/step
current Test accuracy: 0.8110215053763441
current auc_score ------------------>  0.8856470401202452

  32/7440 [..............................] - ETA: 18:39
 160/7440 [..............................] - ETA: 3:42 
 320/7440 [>.............................] - ETA: 1:50
 480/7440 [>.............................] - ETA: 1:12
 640/7440 [=>............................] - ETA: 53s 
 800/7440 [==>...........................] - ETA: 42s
 960/7440 [==>...........................] - ETA: 35s
1120/7440 [===>..........................] - ETA: 29s
1280/7440 [====>.........................] - ETA: 25s
1440/7440 [====>.........................] - ETA: 22s
1600/7440 [=====>........................] - ETA: 19s
1760/7440 [======>.......................] - ETA: 17s
1920/7440 [======>.......................] - ETA: 15s
2080/7440 [=======>......................] - ETA: 14s
2240/7440 [========>.....................] - ETA: 13s
2400/7440 [========>.....................] - ETA: 12s
2560/7440 [=========>....................] - ETA: 11s
2720/7440 [=========>....................] - ETA: 10s
2880/7440 [==========>...................] - ETA: 9s 
3040/7440 [===========>..................] - ETA: 8s
3200/7440 [===========>..................] - ETA: 8s
3360/7440 [============>.................] - ETA: 7s
3520/7440 [=============>................] - ETA: 6s
3680/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
4000/7440 [===============>..............] - ETA: 5s
4160/7440 [===============>..............] - ETA: 5s
4320/7440 [================>.............] - ETA: 4s
4480/7440 [=================>............] - ETA: 4s
4640/7440 [=================>............] - ETA: 3s
4800/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 3s
5280/7440 [====================>.........] - ETA: 2s
5440/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 1s
6080/7440 [=======================>......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8348118279569893
best saved model auc_score ------------------>  0.8983452928084172
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_24 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_24[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_198[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_199[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 28, 96, 96)   112         concatenate_80[0][0]             
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 28, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_24 (Gl (None, 28)           0           activation_200[0][0]             
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 1)            29          global_average_pooling2d_24[0][0]
==================================================================================================
Total params: 6,637
Trainable params: 6,453
Non-trainable params: 184
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 26s - loss: 0.6306 - acc: 0.6849 - val_loss: 0.5411 - val_acc: 0.8020

Epoch 00001: val_loss improved from inf to 0.54107, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 14s - loss: 0.5913 - acc: 0.7293 - val_loss: 0.5095 - val_acc: 0.8339

Epoch 00002: val_loss improved from 0.54107 to 0.50955, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 14s - loss: 0.5618 - acc: 0.7461 - val_loss: 0.5404 - val_acc: 0.8312

Epoch 00003: val_loss did not improve from 0.50955
Epoch 4/30
 - 14s - loss: 0.5426 - acc: 0.7542 - val_loss: 0.5180 - val_acc: 0.8427

Epoch 00004: val_loss did not improve from 0.50955
Epoch 5/30
 - 14s - loss: 0.5309 - acc: 0.7554 - val_loss: 0.5195 - val_acc: 0.8210

Epoch 00005: val_loss did not improve from 0.50955
Epoch 6/30
 - 14s - loss: 0.5226 - acc: 0.7603 - val_loss: 0.4983 - val_acc: 0.8043

Epoch 00006: val_loss improved from 0.50955 to 0.49828, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 14s - loss: 0.5164 - acc: 0.7610 - val_loss: 0.5323 - val_acc: 0.7946

Epoch 00007: val_loss did not improve from 0.49828
Epoch 8/30
 - 14s - loss: 0.5127 - acc: 0.7593 - val_loss: 0.5528 - val_acc: 0.8027

Epoch 00008: val_loss did not improve from 0.49828
Epoch 9/30
 - 14s - loss: 0.5087 - acc: 0.7622 - val_loss: 0.5257 - val_acc: 0.8022

Epoch 00009: val_loss did not improve from 0.49828
Epoch 10/30
 - 14s - loss: 0.5047 - acc: 0.7608 - val_loss: 0.5316 - val_acc: 0.8203

Epoch 00010: val_loss did not improve from 0.49828
Epoch 11/30
 - 14s - loss: 0.5013 - acc: 0.7624 - val_loss: 0.5611 - val_acc: 0.7673

Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00011: val_loss did not improve from 0.49828
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 0s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 229us/step
current Test accuracy: 0.7673387096774194
current auc_score ------------------>  0.8229085732454618

  32/7440 [..............................] - ETA: 20:13
 256/7440 [>.............................] - ETA: 2:28 
 480/7440 [>.............................] - ETA: 1:17
 704/7440 [=>............................] - ETA: 51s 
 928/7440 [==>...........................] - ETA: 38s
1152/7440 [===>..........................] - ETA: 30s
1376/7440 [====>.........................] - ETA: 24s
1600/7440 [=====>........................] - ETA: 20s
1824/7440 [======>.......................] - ETA: 17s
2048/7440 [=======>......................] - ETA: 15s
2272/7440 [========>.....................] - ETA: 13s
2496/7440 [=========>....................] - ETA: 11s
2720/7440 [=========>....................] - ETA: 10s
2944/7440 [==========>...................] - ETA: 9s 
3168/7440 [===========>..................] - ETA: 8s
3392/7440 [============>.................] - ETA: 7s
3616/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
4064/7440 [===============>..............] - ETA: 5s
4288/7440 [================>.............] - ETA: 4s
4512/7440 [=================>............] - ETA: 4s
4736/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5856/7440 [======================>.......] - ETA: 1s
6080/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 933us/step
Best saved model Test accuracy: 0.8043010752688172
best saved model auc_score ------------------>  0.863280870620881
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_25[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_201[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_202 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_202[0][0]             
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 96, 96)   104         concatenate_81[0][0]             
__________________________________________________________________________________________________
activation_203 (Activation)     (None, 26, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_25 (Gl (None, 26)           0           activation_203[0][0]             
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 1)            27          global_average_pooling2d_25[0][0]
==================================================================================================
Total params: 4,883
Trainable params: 4,719
Non-trainable params: 164
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 30s - loss: 0.6284 - acc: 0.6854 - val_loss: 0.5770 - val_acc: 0.7750

Epoch 00001: val_loss improved from inf to 0.57697, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 14s - loss: 0.5816 - acc: 0.7298 - val_loss: 0.5060 - val_acc: 0.8595

Epoch 00002: val_loss improved from 0.57697 to 0.50596, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 14s - loss: 0.5569 - acc: 0.7435 - val_loss: 0.5090 - val_acc: 0.8620

Epoch 00003: val_loss did not improve from 0.50596
Epoch 4/30
 - 14s - loss: 0.5416 - acc: 0.7471 - val_loss: 0.5266 - val_acc: 0.8523

Epoch 00004: val_loss did not improve from 0.50596
Epoch 5/30
 - 14s - loss: 0.5310 - acc: 0.7508 - val_loss: 0.5035 - val_acc: 0.8473

Epoch 00005: val_loss improved from 0.50596 to 0.50348, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 13s - loss: 0.5251 - acc: 0.7516 - val_loss: 0.5015 - val_acc: 0.8491

Epoch 00006: val_loss improved from 0.50348 to 0.50153, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 13s - loss: 0.5186 - acc: 0.7538 - val_loss: 0.4965 - val_acc: 0.8352

Epoch 00007: val_loss improved from 0.50153 to 0.49649, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 13s - loss: 0.5133 - acc: 0.7531 - val_loss: 0.4985 - val_acc: 0.8324

Epoch 00008: val_loss did not improve from 0.49649
Epoch 9/30
 - 13s - loss: 0.5088 - acc: 0.7575 - val_loss: 0.5050 - val_acc: 0.8191

Epoch 00009: val_loss did not improve from 0.49649
Epoch 10/30
 - 13s - loss: 0.5038 - acc: 0.7583 - val_loss: 0.4837 - val_acc: 0.8347

Epoch 00010: val_loss improved from 0.49649 to 0.48372, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 13s - loss: 0.5022 - acc: 0.7585 - val_loss: 0.5038 - val_acc: 0.8284

Epoch 00011: val_loss did not improve from 0.48372
Epoch 12/30
 - 13s - loss: 0.4980 - acc: 0.7610 - val_loss: 0.5118 - val_acc: 0.8246

Epoch 00012: val_loss did not improve from 0.48372
Epoch 13/30
 - 13s - loss: 0.4956 - acc: 0.7632 - val_loss: 0.4865 - val_acc: 0.8109

Epoch 00013: val_loss did not improve from 0.48372
Epoch 14/30
 - 14s - loss: 0.4912 - acc: 0.7633 - val_loss: 0.4773 - val_acc: 0.8098

Epoch 00014: val_loss improved from 0.48372 to 0.47728, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 15/30
 - 13s - loss: 0.4881 - acc: 0.7664 - val_loss: 0.4788 - val_acc: 0.8144

Epoch 00015: val_loss did not improve from 0.47728
Epoch 16/30
 - 13s - loss: 0.4873 - acc: 0.7653 - val_loss: 0.5113 - val_acc: 0.8048

Epoch 00016: val_loss did not improve from 0.47728
Epoch 17/30
 - 13s - loss: 0.4832 - acc: 0.7682 - val_loss: 0.4788 - val_acc: 0.8215

Epoch 00017: val_loss did not improve from 0.47728
Epoch 18/30
 - 13s - loss: 0.4807 - acc: 0.7701 - val_loss: 0.4666 - val_acc: 0.8169

Epoch 00018: val_loss improved from 0.47728 to 0.46657, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 19/30
 - 13s - loss: 0.4776 - acc: 0.7720 - val_loss: 0.5065 - val_acc: 0.8388

Epoch 00019: val_loss did not improve from 0.46657
Epoch 20/30
 - 13s - loss: 0.4745 - acc: 0.7735 - val_loss: 0.4905 - val_acc: 0.8172

Epoch 00020: val_loss did not improve from 0.46657
Epoch 21/30
 - 13s - loss: 0.4737 - acc: 0.7750 - val_loss: 0.4664 - val_acc: 0.8188

Epoch 00021: val_loss improved from 0.46657 to 0.46642, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 22/30
 - 13s - loss: 0.4722 - acc: 0.7740 - val_loss: 0.4630 - val_acc: 0.8288

Epoch 00022: val_loss improved from 0.46642 to 0.46304, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 23/30
 - 13s - loss: 0.4688 - acc: 0.7744 - val_loss: 0.4653 - val_acc: 0.8284

Epoch 00023: val_loss did not improve from 0.46304
Epoch 24/30
 - 13s - loss: 0.4661 - acc: 0.7773 - val_loss: 0.4885 - val_acc: 0.8194

Epoch 00024: val_loss did not improve from 0.46304
Epoch 25/30
 - 13s - loss: 0.4658 - acc: 0.7773 - val_loss: 0.4734 - val_acc: 0.8300

Epoch 00025: val_loss did not improve from 0.46304
Epoch 26/30
 - 13s - loss: 0.4618 - acc: 0.7806 - val_loss: 0.5021 - val_acc: 0.7957

Epoch 00026: val_loss did not improve from 0.46304
Epoch 27/30
 - 13s - loss: 0.4625 - acc: 0.7789 - val_loss: 0.5008 - val_acc: 0.8052

Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00027: val_loss did not improve from 0.46304
Epoch 00027: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1056/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2080/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4096/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 215us/step
current Test accuracy: 0.805241935483871
current auc_score ------------------>  0.8655378511966703

  32/7440 [..............................] - ETA: 20:28
 256/7440 [>.............................] - ETA: 2:30 
 512/7440 [=>............................] - ETA: 1:13
 768/7440 [==>...........................] - ETA: 47s 
1024/7440 [===>..........................] - ETA: 34s
1280/7440 [====>.........................] - ETA: 26s
1536/7440 [=====>........................] - ETA: 21s
1792/7440 [======>.......................] - ETA: 17s
2048/7440 [=======>......................] - ETA: 15s
2304/7440 [========>.....................] - ETA: 12s
2560/7440 [=========>....................] - ETA: 11s
2816/7440 [==========>...................] - ETA: 9s 
3072/7440 [===========>..................] - ETA: 8s
3328/7440 [============>.................] - ETA: 7s
3584/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
4096/7440 [===============>..............] - ETA: 5s
4352/7440 [================>.............] - ETA: 4s
4608/7440 [=================>............] - ETA: 3s
4864/7440 [==================>...........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5888/7440 [======================>.......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 925us/step
Best saved model Test accuracy: 0.828763440860215
best saved model auc_score ------------------>  0.8823210775812234
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_26 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_26[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_204 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_204[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_205 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_205[0][0]             
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_82[0][0]             
__________________________________________________________________________________________________
activation_206 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_206[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_207 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_207[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 36, 96, 96)   0           concatenate_82[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_83[0][0]             
__________________________________________________________________________________________________
activation_208 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_208[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_209 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_209[0][0]             
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 46, 96, 96)   0           concatenate_83[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 96, 96)   184         concatenate_84[0][0]             
__________________________________________________________________________________________________
activation_210 (Activation)     (None, 46, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_26 (Gl (None, 46)           0           activation_210[0][0]             
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 1)            47          global_average_pooling2d_26[0][0]
==================================================================================================
Total params: 15,231
Trainable params: 14,743
Non-trainable params: 488
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 41s - loss: 0.5969 - acc: 0.7257 - val_loss: 0.5122 - val_acc: 0.8454

Epoch 00001: val_loss improved from inf to 0.51220, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 27s - loss: 0.5307 - acc: 0.7638 - val_loss: 0.5048 - val_acc: 0.7922

Epoch 00002: val_loss improved from 0.51220 to 0.50479, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 27s - loss: 0.5053 - acc: 0.7706 - val_loss: 0.5208 - val_acc: 0.7941

Epoch 00003: val_loss did not improve from 0.50479
Epoch 4/30
 - 27s - loss: 0.4879 - acc: 0.7784 - val_loss: 0.4943 - val_acc: 0.8153

Epoch 00004: val_loss improved from 0.50479 to 0.49434, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 27s - loss: 0.4751 - acc: 0.7844 - val_loss: 0.4924 - val_acc: 0.8384

Epoch 00005: val_loss improved from 0.49434 to 0.49242, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 27s - loss: 0.4631 - acc: 0.7892 - val_loss: 0.5528 - val_acc: 0.8008

Epoch 00006: val_loss did not improve from 0.49242
Epoch 7/30
 - 27s - loss: 0.4528 - acc: 0.7943 - val_loss: 0.4501 - val_acc: 0.8032

Epoch 00007: val_loss improved from 0.49242 to 0.45011, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 27s - loss: 0.4430 - acc: 0.8011 - val_loss: 0.5669 - val_acc: 0.7765

Epoch 00008: val_loss did not improve from 0.45011
Epoch 9/30
 - 27s - loss: 0.4358 - acc: 0.8028 - val_loss: 0.4354 - val_acc: 0.8151

Epoch 00009: val_loss improved from 0.45011 to 0.43543, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 27s - loss: 0.4282 - acc: 0.8058 - val_loss: 0.4757 - val_acc: 0.8230

Epoch 00010: val_loss did not improve from 0.43543
Epoch 11/30
 - 27s - loss: 0.4224 - acc: 0.8104 - val_loss: 0.4413 - val_acc: 0.8097

Epoch 00011: val_loss did not improve from 0.43543
Epoch 12/30
 - 27s - loss: 0.4168 - acc: 0.8125 - val_loss: 0.4489 - val_acc: 0.8219

Epoch 00012: val_loss did not improve from 0.43543
Epoch 13/30
 - 27s - loss: 0.4122 - acc: 0.8162 - val_loss: 0.5749 - val_acc: 0.7949

Epoch 00013: val_loss did not improve from 0.43543
Epoch 14/30
 - 27s - loss: 0.4078 - acc: 0.8186 - val_loss: 0.5816 - val_acc: 0.7634

Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00014: val_loss did not improve from 0.43543
Epoch 00014: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 325us/step
current Test accuracy: 0.7634408602150538
current auc_score ------------------>  0.8978492455775234

  32/7440 [..............................] - ETA: 21:00
 192/7440 [..............................] - ETA: 3:27 
 352/7440 [>.............................] - ETA: 1:51
 512/7440 [=>............................] - ETA: 1:15
 672/7440 [=>............................] - ETA: 57s 
 832/7440 [==>...........................] - ETA: 45s
 992/7440 [===>..........................] - ETA: 37s
1152/7440 [===>..........................] - ETA: 31s
1312/7440 [====>.........................] - ETA: 27s
1472/7440 [====>.........................] - ETA: 24s
1632/7440 [=====>........................] - ETA: 21s
1792/7440 [======>.......................] - ETA: 19s
1952/7440 [======>.......................] - ETA: 17s
2112/7440 [=======>......................] - ETA: 15s
2272/7440 [========>.....................] - ETA: 14s
2432/7440 [========>.....................] - ETA: 12s
2592/7440 [=========>....................] - ETA: 11s
2752/7440 [==========>...................] - ETA: 10s
2912/7440 [==========>...................] - ETA: 9s 
3072/7440 [===========>..................] - ETA: 9s
3232/7440 [============>.................] - ETA: 8s
3392/7440 [============>.................] - ETA: 7s
3552/7440 [=============>................] - ETA: 7s
3712/7440 [=============>................] - ETA: 6s
3872/7440 [==============>...............] - ETA: 6s
4032/7440 [===============>..............] - ETA: 5s
4192/7440 [===============>..............] - ETA: 5s
4352/7440 [================>.............] - ETA: 4s
4512/7440 [=================>............] - ETA: 4s
4672/7440 [=================>............] - ETA: 4s
4832/7440 [==================>...........] - ETA: 3s
4992/7440 [===================>..........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 3s
5312/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8150537634408602
best saved model auc_score ------------------>  0.8954483538559371
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_27[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_211 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_211[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_212 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_212[0][0]             
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_85[0][0]             
__________________________________________________________________________________________________
activation_213 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_213[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_214 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_214[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 40, 96, 96)   0           concatenate_85[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_86[0][0]             
__________________________________________________________________________________________________
activation_215 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_215[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_216 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_216[0][0]             
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 52, 96, 96)   0           concatenate_86[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_87[0][0]             
__________________________________________________________________________________________________
activation_217 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_217[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_218 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_218[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_219 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_219[0][0]             
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 38, 48, 48)   0           average_pooling2d_17[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_88[0][0]             
__________________________________________________________________________________________________
activation_220 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_220[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_221 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_221[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 50, 48, 48)   0           concatenate_88[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_89[0][0]             
__________________________________________________________________________________________________
activation_222 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_222[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_223 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_223[0][0]             
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 62, 48, 48)   0           concatenate_89[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 62, 48, 48)   248         concatenate_90[0][0]             
__________________________________________________________________________________________________
activation_224 (Activation)     (None, 62, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 31, 48, 48)   1922        activation_224[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 31, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 31, 24, 24)   124         average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
activation_225 (Activation)     (None, 31, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   1488        activation_225[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_226 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_226[0][0]             
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 43, 24, 24)   0           average_pooling2d_18[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 43, 24, 24)   172         concatenate_91[0][0]             
__________________________________________________________________________________________________
activation_227 (Activation)     (None, 43, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 48, 24, 24)   2064        activation_227[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_228 (Activation)     (None, 48, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_228[0][0]             
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 55, 24, 24)   0           concatenate_91[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 55, 24, 24)   220         concatenate_92[0][0]             
__________________________________________________________________________________________________
activation_229 (Activation)     (None, 55, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 48, 24, 24)   2640        activation_229[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_230 (Activation)     (None, 48, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_230[0][0]             
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 67, 24, 24)   0           concatenate_92[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 67, 24, 24)   268         concatenate_93[0][0]             
__________________________________________________________________________________________________
activation_231 (Activation)     (None, 67, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_27 (Gl (None, 67)           0           activation_231[0][0]             
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 1)            68          global_average_pooling2d_27[0][0]
==================================================================================================
Total params: 69,742
Trainable params: 67,862
Non-trainable params: 1,880
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 60s - loss: 0.5698 - acc: 0.7704 - val_loss: 0.5141 - val_acc: 0.8176

Epoch 00001: val_loss improved from inf to 0.51408, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 42s - loss: 0.4692 - acc: 0.8212 - val_loss: 0.5410 - val_acc: 0.8468

Epoch 00002: val_loss did not improve from 0.51408
Epoch 3/30
 - 42s - loss: 0.4217 - acc: 0.8497 - val_loss: 0.4829 - val_acc: 0.8184

Epoch 00003: val_loss improved from 0.51408 to 0.48290, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 42s - loss: 0.3837 - acc: 0.8669 - val_loss: 0.5014 - val_acc: 0.8120

Epoch 00004: val_loss did not improve from 0.48290
Epoch 5/30
 - 42s - loss: 0.3590 - acc: 0.8801 - val_loss: 0.4440 - val_acc: 0.8527

Epoch 00005: val_loss improved from 0.48290 to 0.44401, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 42s - loss: 0.3317 - acc: 0.8942 - val_loss: 0.9705 - val_acc: 0.7000

Epoch 00006: val_loss did not improve from 0.44401
Epoch 7/30
 - 42s - loss: 0.3098 - acc: 0.9045 - val_loss: 0.8070 - val_acc: 0.7444

Epoch 00007: val_loss did not improve from 0.44401
Epoch 8/30
 - 42s - loss: 0.2943 - acc: 0.9097 - val_loss: 0.8969 - val_acc: 0.7214

Epoch 00008: val_loss did not improve from 0.44401
Epoch 9/30
 - 42s - loss: 0.2796 - acc: 0.9170 - val_loss: 0.6658 - val_acc: 0.7974

Epoch 00009: val_loss did not improve from 0.44401
Epoch 10/30
 - 42s - loss: 0.2644 - acc: 0.9229 - val_loss: 0.7757 - val_acc: 0.7909

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.44401
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 453us/step
current Test accuracy: 0.7908602150537635
current auc_score ------------------>  0.8926464042085791

  32/7440 [..............................] - ETA: 25:04
 128/7440 [..............................] - ETA: 6:14 
 256/7440 [>.............................] - ETA: 3:05
 384/7440 [>.............................] - ETA: 2:02
 512/7440 [=>............................] - ETA: 1:30
 640/7440 [=>............................] - ETA: 1:12
 768/7440 [==>...........................] - ETA: 59s 
 896/7440 [==>...........................] - ETA: 50s
1024/7440 [===>..........................] - ETA: 43s
1152/7440 [===>..........................] - ETA: 38s
1280/7440 [====>.........................] - ETA: 34s
1408/7440 [====>.........................] - ETA: 30s
1536/7440 [=====>........................] - ETA: 27s
1664/7440 [=====>........................] - ETA: 25s
1792/7440 [======>.......................] - ETA: 23s
1920/7440 [======>.......................] - ETA: 21s
2048/7440 [=======>......................] - ETA: 19s
2176/7440 [=======>......................] - ETA: 18s
2304/7440 [========>.....................] - ETA: 16s
2432/7440 [========>.....................] - ETA: 15s
2560/7440 [=========>....................] - ETA: 14s
2688/7440 [=========>....................] - ETA: 13s
2816/7440 [==========>...................] - ETA: 12s
2944/7440 [==========>...................] - ETA: 11s
3072/7440 [===========>..................] - ETA: 11s
3200/7440 [===========>..................] - ETA: 10s
3328/7440 [============>.................] - ETA: 9s 
3456/7440 [============>.................] - ETA: 9s
3584/7440 [=============>................] - ETA: 8s
3712/7440 [=============>................] - ETA: 8s
3840/7440 [==============>...............] - ETA: 7s
3968/7440 [===============>..............] - ETA: 7s
4096/7440 [===============>..............] - ETA: 6s
4224/7440 [================>.............] - ETA: 6s
4352/7440 [================>.............] - ETA: 6s
4480/7440 [=================>............] - ETA: 5s
4608/7440 [=================>............] - ETA: 5s
4736/7440 [==================>...........] - ETA: 4s
4864/7440 [==================>...........] - ETA: 4s
4992/7440 [===================>..........] - ETA: 4s
5120/7440 [===================>..........] - ETA: 3s
5248/7440 [====================>.........] - ETA: 3s
5376/7440 [====================>.........] - ETA: 3s
5504/7440 [=====================>........] - ETA: 3s
5632/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5888/7440 [======================>.......] - ETA: 2s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 10s 1ms/step
Best saved model Test accuracy: 0.8526881720430107
best saved model auc_score ------------------>  0.9256039354260608
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_28 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_28[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_232 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_232[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_233 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_233[0][0]             
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_94[0][0]             
__________________________________________________________________________________________________
activation_234 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_234[0][0]             
__________________________________________________________________________________________________
average_pooling2d_19 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_19[0][0]       
__________________________________________________________________________________________________
activation_235 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_235[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_236 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_236[0][0]             
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 26, 48, 48)   0           average_pooling2d_19[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 26, 48, 48)   104         concatenate_95[0][0]             
__________________________________________________________________________________________________
activation_237 (Activation)     (None, 26, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 13, 48, 48)   338         activation_237[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 13, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 13, 24, 24)   52          average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_238 (Activation)     (None, 13, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   624         activation_238[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_239 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_239[0][0]             
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 25, 24, 24)   0           average_pooling2d_20[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 25, 24, 24)   100         concatenate_96[0][0]             
__________________________________________________________________________________________________
activation_240 (Activation)     (None, 25, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_28 (Gl (None, 25)           0           activation_240[0][0]             
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 1)            26          global_average_pooling2d_28[0][0]
==================================================================================================
Total params: 19,724
Trainable params: 19,192
Non-trainable params: 532
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 34s - loss: 0.6048 - acc: 0.7200 - val_loss: 0.4985 - val_acc: 0.8133

Epoch 00001: val_loss improved from inf to 0.49854, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 18s - loss: 0.5069 - acc: 0.7739 - val_loss: 0.4584 - val_acc: 0.8199

Epoch 00002: val_loss improved from 0.49854 to 0.45845, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 18s - loss: 0.4701 - acc: 0.7904 - val_loss: 0.5677 - val_acc: 0.7741

Epoch 00003: val_loss did not improve from 0.45845
Epoch 4/30
 - 18s - loss: 0.4491 - acc: 0.7994 - val_loss: 0.4583 - val_acc: 0.8043

Epoch 00004: val_loss improved from 0.45845 to 0.45834, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 18s - loss: 0.4316 - acc: 0.8087 - val_loss: 0.4814 - val_acc: 0.7819

Epoch 00005: val_loss did not improve from 0.45834
Epoch 6/30
 - 18s - loss: 0.4149 - acc: 0.8171 - val_loss: 0.5266 - val_acc: 0.7952

Epoch 00006: val_loss did not improve from 0.45834
Epoch 7/30
 - 18s - loss: 0.4058 - acc: 0.8250 - val_loss: 0.4988 - val_acc: 0.7942

Epoch 00007: val_loss did not improve from 0.45834
Epoch 8/30
 - 18s - loss: 0.3963 - acc: 0.8289 - val_loss: 0.4387 - val_acc: 0.7984

Epoch 00008: val_loss improved from 0.45834 to 0.43868, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 18s - loss: 0.3851 - acc: 0.8359 - val_loss: 0.5055 - val_acc: 0.7727

Epoch 00009: val_loss did not improve from 0.43868
Epoch 10/30
 - 18s - loss: 0.3806 - acc: 0.8396 - val_loss: 0.4307 - val_acc: 0.8523

Epoch 00010: val_loss improved from 0.43868 to 0.43070, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 18s - loss: 0.3727 - acc: 0.8445 - val_loss: 0.5109 - val_acc: 0.8106

Epoch 00011: val_loss did not improve from 0.43070
Epoch 12/30
 - 18s - loss: 0.3674 - acc: 0.8478 - val_loss: 0.4749 - val_acc: 0.8273

Epoch 00012: val_loss did not improve from 0.43070
Epoch 13/30
 - 18s - loss: 0.3583 - acc: 0.8513 - val_loss: 0.4113 - val_acc: 0.8230

Epoch 00013: val_loss improved from 0.43070 to 0.41134, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 18s - loss: 0.3530 - acc: 0.8538 - val_loss: 0.4489 - val_acc: 0.8316

Epoch 00014: val_loss did not improve from 0.41134
Epoch 15/30
 - 18s - loss: 0.3526 - acc: 0.8550 - val_loss: 0.4040 - val_acc: 0.8273

Epoch 00015: val_loss improved from 0.41134 to 0.40400, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 16/30
 - 18s - loss: 0.3460 - acc: 0.8578 - val_loss: 0.4425 - val_acc: 0.8349

Epoch 00016: val_loss did not improve from 0.40400
Epoch 17/30
 - 18s - loss: 0.3381 - acc: 0.8629 - val_loss: 0.4650 - val_acc: 0.8027

Epoch 00017: val_loss did not improve from 0.40400
Epoch 18/30
 - 18s - loss: 0.3362 - acc: 0.8623 - val_loss: 0.6882 - val_acc: 0.7659

Epoch 00018: val_loss did not improve from 0.40400
Epoch 19/30
 - 18s - loss: 0.3300 - acc: 0.8664 - val_loss: 0.4713 - val_acc: 0.8125

Epoch 00019: val_loss did not improve from 0.40400
Epoch 20/30
 - 18s - loss: 0.3289 - acc: 0.8675 - val_loss: 0.5177 - val_acc: 0.8134

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00020: val_loss did not improve from 0.40400
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 1s
 640/7440 [=>............................] - ETA: 1s
 864/7440 [==>...........................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1984/7440 [=======>......................] - ETA: 1s
2208/7440 [=======>......................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2656/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
3968/7440 [===============>..............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 255us/step
current Test accuracy: 0.8134408602150538
current auc_score ------------------>  0.9095572465024858

  32/7440 [..............................] - ETA: 25:55
 224/7440 [..............................] - ETA: 3:38 
 416/7440 [>.............................] - ETA: 1:55
 608/7440 [=>............................] - ETA: 1:17
 800/7440 [==>...........................] - ETA: 57s 
 992/7440 [===>..........................] - ETA: 45s
1184/7440 [===>..........................] - ETA: 37s
1408/7440 [====>.........................] - ETA: 30s
1600/7440 [=====>........................] - ETA: 26s
1792/7440 [======>.......................] - ETA: 22s
1984/7440 [=======>......................] - ETA: 19s
2208/7440 [=======>......................] - ETA: 17s
2400/7440 [========>.....................] - ETA: 15s
2592/7440 [=========>....................] - ETA: 13s
2816/7440 [==========>...................] - ETA: 12s
3040/7440 [===========>..................] - ETA: 10s
3264/7440 [============>.................] - ETA: 9s 
3488/7440 [=============>................] - ETA: 8s
3712/7440 [=============>................] - ETA: 7s
3936/7440 [==============>...............] - ETA: 6s
4160/7440 [===============>..............] - ETA: 6s
4384/7440 [================>.............] - ETA: 5s
4608/7440 [=================>............] - ETA: 4s
4832/7440 [==================>...........] - ETA: 4s
5056/7440 [===================>..........] - ETA: 3s
5280/7440 [====================>.........] - ETA: 3s
5504/7440 [=====================>........] - ETA: 2s
5728/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 2s
6176/7440 [=======================>......] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 9s 1ms/step
Best saved model Test accuracy: 0.8272849462365591
best saved model auc_score ------------------>  0.9153822334952018
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_29[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_241 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_241[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_242 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_242[0][0]             
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_97[0][0]             
__________________________________________________________________________________________________
activation_243 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_243[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_244 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_244[0][0]             
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 40, 96, 96)   0           concatenate_97[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_98[0][0]             
__________________________________________________________________________________________________
activation_245 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_245[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_246 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_246[0][0]             
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 52, 96, 96)   0           concatenate_98[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_99[0][0]             
__________________________________________________________________________________________________
activation_247 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_247[0][0]             
__________________________________________________________________________________________________
average_pooling2d_21 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_21[0][0]       
__________________________________________________________________________________________________
activation_248 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_248[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_249 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_249[0][0]             
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 38, 48, 48)   0           average_pooling2d_21[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_100[0][0]            
__________________________________________________________________________________________________
activation_250 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_250[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_251 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_251[0][0]             
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 50, 48, 48)   0           concatenate_100[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_101[0][0]            
__________________________________________________________________________________________________
activation_252 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_252[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_253 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_253[0][0]             
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 62, 48, 48)   0           concatenate_101[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 62, 48, 48)   248         concatenate_102[0][0]            
__________________________________________________________________________________________________
activation_254 (Activation)     (None, 62, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 31, 48, 48)   1922        activation_254[0][0]             
__________________________________________________________________________________________________
average_pooling2d_22 (AveragePo (None, 31, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 31, 24, 24)   124         average_pooling2d_22[0][0]       
__________________________________________________________________________________________________
activation_255 (Activation)     (None, 31, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   1488        activation_255[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_256 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_256[0][0]             
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 43, 24, 24)   0           average_pooling2d_22[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 43, 24, 24)   172         concatenate_103[0][0]            
__________________________________________________________________________________________________
activation_257 (Activation)     (None, 43, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 48, 24, 24)   2064        activation_257[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_258 (Activation)     (None, 48, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_258[0][0]             
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 55, 24, 24)   0           concatenate_103[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 55, 24, 24)   220         concatenate_104[0][0]            
__________________________________________________________________________________________________
activation_259 (Activation)     (None, 55, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 48, 24, 24)   2640        activation_259[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_260 (Activation)     (None, 48, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_260[0][0]             
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 67, 24, 24)   0           concatenate_104[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 67, 24, 24)   268         concatenate_105[0][0]            
__________________________________________________________________________________________________
activation_261 (Activation)     (None, 67, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_29 (Gl (None, 67)           0           activation_261[0][0]             
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 1)            68          global_average_pooling2d_29[0][0]
==================================================================================================
Total params: 69,742
Trainable params: 67,862
Non-trainable params: 1,880
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 62s - loss: 0.5634 - acc: 0.7754 - val_loss: 0.6869 - val_acc: 0.7341

Epoch 00001: val_loss improved from inf to 0.68692, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 43s - loss: 0.4709 - acc: 0.8221 - val_loss: 0.7200 - val_acc: 0.7645

Epoch 00002: val_loss did not improve from 0.68692
Epoch 3/30
 - 43s - loss: 0.4213 - acc: 0.8485 - val_loss: 0.4822 - val_acc: 0.8137

Epoch 00003: val_loss improved from 0.68692 to 0.48223, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 43s - loss: 0.3846 - acc: 0.8677 - val_loss: 0.5143 - val_acc: 0.7829

Epoch 00004: val_loss did not improve from 0.48223
Epoch 5/30
 - 43s - loss: 0.3565 - acc: 0.8814 - val_loss: 0.4499 - val_acc: 0.8380

Epoch 00005: val_loss improved from 0.48223 to 0.44992, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 43s - loss: 0.3315 - acc: 0.8923 - val_loss: 0.4829 - val_acc: 0.8261

Epoch 00006: val_loss did not improve from 0.44992
Epoch 7/30
 - 43s - loss: 0.3111 - acc: 0.9027 - val_loss: 0.7914 - val_acc: 0.7262

Epoch 00007: val_loss did not improve from 0.44992
Epoch 8/30
 - 43s - loss: 0.2978 - acc: 0.9065 - val_loss: 0.6346 - val_acc: 0.7700

Epoch 00008: val_loss did not improve from 0.44992
Epoch 9/30
 - 43s - loss: 0.2768 - acc: 0.9184 - val_loss: 0.4499 - val_acc: 0.8212

Epoch 00009: val_loss did not improve from 0.44992
Epoch 10/30
 - 43s - loss: 0.2636 - acc: 0.9233 - val_loss: 0.5030 - val_acc: 0.8130

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.44992
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 455us/step
current Test accuracy: 0.8130376344086021
current auc_score ------------------>  0.9043668343161059

  32/7440 [..............................] - ETA: 29:07
 128/7440 [..............................] - ETA: 7:14 
 256/7440 [>.............................] - ETA: 3:34
 384/7440 [>.............................] - ETA: 2:21
 512/7440 [=>............................] - ETA: 1:45
 640/7440 [=>............................] - ETA: 1:23
 768/7440 [==>...........................] - ETA: 1:08
 896/7440 [==>...........................] - ETA: 58s 
1024/7440 [===>..........................] - ETA: 50s
1152/7440 [===>..........................] - ETA: 44s
1280/7440 [====>.........................] - ETA: 39s
1408/7440 [====>.........................] - ETA: 35s
1536/7440 [=====>........................] - ETA: 31s
1664/7440 [=====>........................] - ETA: 28s
1792/7440 [======>.......................] - ETA: 26s
1920/7440 [======>.......................] - ETA: 24s
2048/7440 [=======>......................] - ETA: 22s
2176/7440 [=======>......................] - ETA: 20s
2304/7440 [========>.....................] - ETA: 19s
2432/7440 [========>.....................] - ETA: 17s
2560/7440 [=========>....................] - ETA: 16s
2688/7440 [=========>....................] - ETA: 15s
2816/7440 [==========>...................] - ETA: 14s
2944/7440 [==========>...................] - ETA: 13s
3072/7440 [===========>..................] - ETA: 12s
3200/7440 [===========>..................] - ETA: 11s
3328/7440 [============>.................] - ETA: 11s
3456/7440 [============>.................] - ETA: 10s
3584/7440 [=============>................] - ETA: 9s 
3712/7440 [=============>................] - ETA: 9s
3840/7440 [==============>...............] - ETA: 8s
3968/7440 [===============>..............] - ETA: 8s
4096/7440 [===============>..............] - ETA: 7s
4224/7440 [================>.............] - ETA: 7s
4352/7440 [================>.............] - ETA: 6s
4480/7440 [=================>............] - ETA: 6s
4608/7440 [=================>............] - ETA: 5s
4736/7440 [==================>...........] - ETA: 5s
4864/7440 [==================>...........] - ETA: 5s
4992/7440 [===================>..........] - ETA: 4s
5120/7440 [===================>..........] - ETA: 4s
5248/7440 [====================>.........] - ETA: 4s
5376/7440 [====================>.........] - ETA: 3s
5504/7440 [=====================>........] - ETA: 3s
5632/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 2s
5888/7440 [======================>.......] - ETA: 2s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 11s 1ms/step
Best saved model Test accuracy: 0.8380376344086021
best saved model auc_score ------------------>  0.9240139756041161
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_30 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_30[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_262 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_262[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_263 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_263[0][0]             
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_106[0][0]            
__________________________________________________________________________________________________
activation_264 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_264[0][0]             
__________________________________________________________________________________________________
average_pooling2d_23 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_23[0][0]       
__________________________________________________________________________________________________
activation_265 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_265[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_266 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_266[0][0]             
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 26, 48, 48)   0           average_pooling2d_23[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 26, 48, 48)   104         concatenate_107[0][0]            
__________________________________________________________________________________________________
activation_267 (Activation)     (None, 26, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 13, 48, 48)   338         activation_267[0][0]             
__________________________________________________________________________________________________
average_pooling2d_24 (AveragePo (None, 13, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 13, 24, 24)   52          average_pooling2d_24[0][0]       
__________________________________________________________________________________________________
activation_268 (Activation)     (None, 13, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   624         activation_268[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_269 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_269[0][0]             
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 25, 24, 24)   0           average_pooling2d_24[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 25, 24, 24)   100         concatenate_108[0][0]            
__________________________________________________________________________________________________
activation_270 (Activation)     (None, 25, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_30 (Gl (None, 25)           0           activation_270[0][0]             
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 1)            26          global_average_pooling2d_30[0][0]
==================================================================================================
Total params: 19,724
Trainable params: 19,192
Non-trainable params: 532
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 37s - loss: 0.5623 - acc: 0.7513 - val_loss: 0.5042 - val_acc: 0.8198

Epoch 00001: val_loss improved from inf to 0.50416, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 18s - loss: 0.4907 - acc: 0.7851 - val_loss: 0.5261 - val_acc: 0.8055

Epoch 00002: val_loss did not improve from 0.50416
Epoch 3/30
 - 18s - loss: 0.4550 - acc: 0.7999 - val_loss: 0.4464 - val_acc: 0.8312

Epoch 00003: val_loss improved from 0.50416 to 0.44645, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 18s - loss: 0.4335 - acc: 0.8110 - val_loss: 0.4409 - val_acc: 0.8419

Epoch 00004: val_loss improved from 0.44645 to 0.44095, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 18s - loss: 0.4177 - acc: 0.8192 - val_loss: 0.5198 - val_acc: 0.8055

Epoch 00005: val_loss did not improve from 0.44095
Epoch 6/30
 - 18s - loss: 0.4073 - acc: 0.8253 - val_loss: 0.6607 - val_acc: 0.7772

Epoch 00006: val_loss did not improve from 0.44095
Epoch 7/30
 - 18s - loss: 0.3965 - acc: 0.8337 - val_loss: 0.4495 - val_acc: 0.8220

Epoch 00007: val_loss did not improve from 0.44095
Epoch 8/30
 - 18s - loss: 0.3867 - acc: 0.8366 - val_loss: 0.6984 - val_acc: 0.7427

Epoch 00008: val_loss did not improve from 0.44095
Epoch 9/30
 - 18s - loss: 0.3778 - acc: 0.8411 - val_loss: 0.4242 - val_acc: 0.8315

Epoch 00009: val_loss improved from 0.44095 to 0.42420, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 18s - loss: 0.3712 - acc: 0.8457 - val_loss: 0.5496 - val_acc: 0.8117

Epoch 00010: val_loss did not improve from 0.42420
Epoch 11/30
 - 18s - loss: 0.3650 - acc: 0.8484 - val_loss: 0.5045 - val_acc: 0.8164

Epoch 00011: val_loss did not improve from 0.42420
Epoch 12/30
 - 18s - loss: 0.3602 - acc: 0.8506 - val_loss: 0.5104 - val_acc: 0.8083

Epoch 00012: val_loss did not improve from 0.42420
Epoch 13/30
 - 18s - loss: 0.3552 - acc: 0.8537 - val_loss: 0.4528 - val_acc: 0.8348

Epoch 00013: val_loss did not improve from 0.42420
Epoch 14/30
 - 18s - loss: 0.3514 - acc: 0.8565 - val_loss: 0.5838 - val_acc: 0.7517

Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00014: val_loss did not improve from 0.42420
Epoch 00014: early stopping

  32/7440 [..............................] - ETA: 4s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 275us/step
current Test accuracy: 0.751747311827957
current auc_score ------------------>  0.8974723956526766

  32/7440 [..............................] - ETA: 30:10
 192/7440 [..............................] - ETA: 4:57 
 384/7440 [>.............................] - ETA: 2:25
 576/7440 [=>............................] - ETA: 1:35
 768/7440 [==>...........................] - ETA: 1:09
 960/7440 [==>...........................] - ETA: 54s 
1152/7440 [===>..........................] - ETA: 44s
1344/7440 [====>.........................] - ETA: 37s
1536/7440 [=====>........................] - ETA: 31s
1728/7440 [=====>........................] - ETA: 27s
1920/7440 [======>.......................] - ETA: 24s
2112/7440 [=======>......................] - ETA: 21s
2304/7440 [========>.....................] - ETA: 18s
2496/7440 [=========>....................] - ETA: 16s
2688/7440 [=========>....................] - ETA: 15s
2880/7440 [==========>...................] - ETA: 13s
3072/7440 [===========>..................] - ETA: 12s
3264/7440 [============>.................] - ETA: 11s
3456/7440 [============>.................] - ETA: 10s
3648/7440 [=============>................] - ETA: 9s 
3840/7440 [==============>...............] - ETA: 8s
4032/7440 [===============>..............] - ETA: 7s
4224/7440 [================>.............] - ETA: 6s
4416/7440 [================>.............] - ETA: 6s
4608/7440 [=================>............] - ETA: 5s
4800/7440 [==================>...........] - ETA: 5s
4992/7440 [===================>..........] - ETA: 4s
5184/7440 [===================>..........] - ETA: 4s
5376/7440 [====================>.........] - ETA: 3s
5568/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6336/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6720/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 10s 1ms/step
Best saved model Test accuracy: 0.8314516129032258
best saved model auc_score ------------------>  0.9135655133541449
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_31 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_271 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_25 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_272 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_26 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_273 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_31  (None, 4)                 0         
_________________________________________________________________
dense_31 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 27s - loss: 0.6733 - acc: 0.6136 - val_loss: 0.6315 - val_acc: 0.7712

Epoch 00001: val_loss improved from inf to 0.63146, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 9s - loss: 0.6457 - acc: 0.6667 - val_loss: 0.5952 - val_acc: 0.8187

Epoch 00002: val_loss improved from 0.63146 to 0.59515, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 8s - loss: 0.6170 - acc: 0.7090 - val_loss: 0.5560 - val_acc: 0.8444

Epoch 00003: val_loss improved from 0.59515 to 0.55603, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 8s - loss: 0.5912 - acc: 0.7252 - val_loss: 0.5217 - val_acc: 0.8444

Epoch 00004: val_loss improved from 0.55603 to 0.52167, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 9s - loss: 0.5689 - acc: 0.7386 - val_loss: 0.4985 - val_acc: 0.8569

Epoch 00005: val_loss improved from 0.52167 to 0.49853, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 9s - loss: 0.5521 - acc: 0.7494 - val_loss: 0.4875 - val_acc: 0.8566

Epoch 00006: val_loss improved from 0.49853 to 0.48750, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 9s - loss: 0.5390 - acc: 0.7548 - val_loss: 0.4876 - val_acc: 0.8344

Epoch 00007: val_loss did not improve from 0.48750
Epoch 8/30
 - 9s - loss: 0.5296 - acc: 0.7571 - val_loss: 0.4822 - val_acc: 0.8419

Epoch 00008: val_loss improved from 0.48750 to 0.48218, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 9s - loss: 0.5217 - acc: 0.7593 - val_loss: 0.4837 - val_acc: 0.8356

Epoch 00009: val_loss did not improve from 0.48218
Epoch 10/30
 - 9s - loss: 0.5154 - acc: 0.7608 - val_loss: 0.4847 - val_acc: 0.8305

Epoch 00010: val_loss did not improve from 0.48218
Epoch 11/30
 - 9s - loss: 0.5088 - acc: 0.7589 - val_loss: 0.4825 - val_acc: 0.8220

Epoch 00011: val_loss did not improve from 0.48218
Epoch 12/30
 - 9s - loss: 0.5055 - acc: 0.7617 - val_loss: 0.5144 - val_acc: 0.8288

Epoch 00012: val_loss did not improve from 0.48218
Epoch 13/30
 - 9s - loss: 0.5010 - acc: 0.7647 - val_loss: 0.4746 - val_acc: 0.8228

Epoch 00013: val_loss improved from 0.48218 to 0.47457, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 9s - loss: 0.4984 - acc: 0.7639 - val_loss: 0.4950 - val_acc: 0.8099

Epoch 00014: val_loss did not improve from 0.47457
Epoch 15/30
 - 8s - loss: 0.4964 - acc: 0.7634 - val_loss: 0.4714 - val_acc: 0.8148

Epoch 00015: val_loss improved from 0.47457 to 0.47144, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 16/30
 - 9s - loss: 0.4931 - acc: 0.7658 - val_loss: 0.4856 - val_acc: 0.8129

Epoch 00016: val_loss did not improve from 0.47144
Epoch 17/30
 - 9s - loss: 0.4909 - acc: 0.7652 - val_loss: 0.4751 - val_acc: 0.8011

Epoch 00017: val_loss did not improve from 0.47144
Epoch 18/30
 - 9s - loss: 0.4878 - acc: 0.7652 - val_loss: 0.4919 - val_acc: 0.8050

Epoch 00018: val_loss did not improve from 0.47144
Epoch 19/30
 - 9s - loss: 0.4867 - acc: 0.7666 - val_loss: 0.4876 - val_acc: 0.8082

Epoch 00019: val_loss did not improve from 0.47144
Epoch 20/30
 - 9s - loss: 0.4851 - acc: 0.7665 - val_loss: 0.4854 - val_acc: 0.7931

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00020: val_loss did not improve from 0.47144
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1056/7440 [===>..........................] - ETA: 1s
1344/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1888/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2400/7440 [========>.....................] - ETA: 1s
2656/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3168/7440 [===========>..................] - ETA: 0s
3456/7440 [============>.................] - ETA: 0s
3712/7440 [=============>................] - ETA: 0s
3968/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 197us/step
current Test accuracy: 0.7931451612903225
current auc_score ------------------>  0.8558430887963927

  32/7440 [..............................] - ETA: 30:55
 256/7440 [>.............................] - ETA: 3:46 
 544/7440 [=>............................] - ETA: 1:42
 832/7440 [==>...........................] - ETA: 1:04
1120/7440 [===>..........................] - ETA: 46s 
1408/7440 [====>.........................] - ETA: 35s
1696/7440 [=====>........................] - ETA: 28s
1984/7440 [=======>......................] - ETA: 23s
2272/7440 [========>.....................] - ETA: 19s
2560/7440 [=========>....................] - ETA: 16s
2848/7440 [==========>...................] - ETA: 13s
3136/7440 [===========>..................] - ETA: 11s
3424/7440 [============>.................] - ETA: 10s
3712/7440 [=============>................] - ETA: 8s 
4000/7440 [===============>..............] - ETA: 7s
4288/7440 [================>.............] - ETA: 6s
4576/7440 [=================>............] - ETA: 5s
4864/7440 [==================>...........] - ETA: 4s
5152/7440 [===================>..........] - ETA: 3s
5440/7440 [====================>.........] - ETA: 3s
5728/7440 [======================>.......] - ETA: 2s
6016/7440 [=======================>......] - ETA: 2s
6304/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 1s
6880/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 9s 1ms/step
Best saved model Test accuracy: 0.8147849462365592
best saved model auc_score ------------------>  0.8703429948548965
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_32 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_32[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_274 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_274[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_275 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_275[0][0]             
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_109[0][0]            
__________________________________________________________________________________________________
activation_276 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_276[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_277 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_277[0][0]             
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 40, 96, 96)   0           concatenate_109[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_110[0][0]            
__________________________________________________________________________________________________
activation_278 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_278[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_279 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_279[0][0]             
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 52, 96, 96)   0           concatenate_110[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_111[0][0]            
__________________________________________________________________________________________________
activation_280 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_280[0][0]             
__________________________________________________________________________________________________
average_pooling2d_27 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_27[0][0]       
__________________________________________________________________________________________________
activation_281 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_281[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_282 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_282[0][0]             
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 38, 48, 48)   0           average_pooling2d_27[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_112[0][0]            
__________________________________________________________________________________________________
activation_283 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_283[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_284 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_284[0][0]             
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 50, 48, 48)   0           concatenate_112[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_113[0][0]            
__________________________________________________________________________________________________
activation_285 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_285[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_286 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_286[0][0]             
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 62, 48, 48)   0           concatenate_113[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 62, 48, 48)   248         concatenate_114[0][0]            
__________________________________________________________________________________________________
activation_287 (Activation)     (None, 62, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 31, 48, 48)   1922        activation_287[0][0]             
__________________________________________________________________________________________________
average_pooling2d_28 (AveragePo (None, 31, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 31, 24, 24)   124         average_pooling2d_28[0][0]       
__________________________________________________________________________________________________
activation_288 (Activation)     (None, 31, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   1488        activation_288[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_289 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_289[0][0]             
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 43, 24, 24)   0           average_pooling2d_28[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 43, 24, 24)   172         concatenate_115[0][0]            
__________________________________________________________________________________________________
activation_290 (Activation)     (None, 43, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 48, 24, 24)   2064        activation_290[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_291 (Activation)     (None, 48, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_291[0][0]             
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 55, 24, 24)   0           concatenate_115[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 55, 24, 24)   220         concatenate_116[0][0]            
__________________________________________________________________________________________________
activation_292 (Activation)     (None, 55, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 48, 24, 24)   2640        activation_292[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_293 (Activation)     (None, 48, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_293[0][0]             
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 67, 24, 24)   0           concatenate_116[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 67, 24, 24)   268         concatenate_117[0][0]            
__________________________________________________________________________________________________
activation_294 (Activation)     (None, 67, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_32 (Gl (None, 67)           0           activation_294[0][0]             
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 1)            68          global_average_pooling2d_32[0][0]
==================================================================================================
Total params: 69,742
Trainable params: 67,862
Non-trainable params: 1,880
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 66s - loss: 0.5589 - acc: 0.7764 - val_loss: 0.5327 - val_acc: 0.7731

Epoch 00001: val_loss improved from inf to 0.53271, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 44s - loss: 0.4659 - acc: 0.8225 - val_loss: 0.5550 - val_acc: 0.7849

Epoch 00002: val_loss did not improve from 0.53271
Epoch 3/30
 - 44s - loss: 0.4161 - acc: 0.8513 - val_loss: 0.5653 - val_acc: 0.7823

Epoch 00003: val_loss did not improve from 0.53271
Epoch 4/30
 - 44s - loss: 0.3733 - acc: 0.8734 - val_loss: 0.5216 - val_acc: 0.7828

Epoch 00004: val_loss improved from 0.53271 to 0.52157, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 44s - loss: 0.3423 - acc: 0.8878 - val_loss: 0.6198 - val_acc: 0.7774

Epoch 00005: val_loss did not improve from 0.52157
Epoch 6/30
 - 44s - loss: 0.3184 - acc: 0.8999 - val_loss: 0.6166 - val_acc: 0.7676

Epoch 00006: val_loss did not improve from 0.52157
Epoch 7/30
 - 44s - loss: 0.2965 - acc: 0.9103 - val_loss: 0.6217 - val_acc: 0.7882

Epoch 00007: val_loss did not improve from 0.52157
Epoch 8/30
 - 44s - loss: 0.2814 - acc: 0.9153 - val_loss: 0.7618 - val_acc: 0.7743

Epoch 00008: val_loss did not improve from 0.52157
Epoch 9/30
 - 44s - loss: 0.2641 - acc: 0.9239 - val_loss: 0.5647 - val_acc: 0.8177

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.52157
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 476us/step
current Test accuracy: 0.817741935483871
current auc_score ------------------>  0.8914908515435311

  32/7440 [..............................] - ETA: 36:03
 128/7440 [..............................] - ETA: 8:57 
 256/7440 [>.............................] - ETA: 4:25
 384/7440 [>.............................] - ETA: 2:55
 512/7440 [=>............................] - ETA: 2:09
 640/7440 [=>............................] - ETA: 1:42
 768/7440 [==>...........................] - ETA: 1:24
 896/7440 [==>...........................] - ETA: 1:11
1024/7440 [===>..........................] - ETA: 1:01
1152/7440 [===>..........................] - ETA: 54s 
1280/7440 [====>.........................] - ETA: 47s
1408/7440 [====>.........................] - ETA: 42s
1536/7440 [=====>........................] - ETA: 38s
1664/7440 [=====>........................] - ETA: 35s
1792/7440 [======>.......................] - ETA: 32s
1920/7440 [======>.......................] - ETA: 29s
2048/7440 [=======>......................] - ETA: 27s
2176/7440 [=======>......................] - ETA: 25s
2304/7440 [========>.....................] - ETA: 23s
2432/7440 [========>.....................] - ETA: 21s
2560/7440 [=========>....................] - ETA: 20s
2688/7440 [=========>....................] - ETA: 18s
2816/7440 [==========>...................] - ETA: 17s
2944/7440 [==========>...................] - ETA: 16s
3072/7440 [===========>..................] - ETA: 15s
3200/7440 [===========>..................] - ETA: 14s
3328/7440 [============>.................] - ETA: 13s
3456/7440 [============>.................] - ETA: 12s
3584/7440 [=============>................] - ETA: 11s
3712/7440 [=============>................] - ETA: 11s
3840/7440 [==============>...............] - ETA: 10s
3968/7440 [===============>..............] - ETA: 9s 
4096/7440 [===============>..............] - ETA: 9s
4224/7440 [================>.............] - ETA: 8s
4352/7440 [================>.............] - ETA: 8s
4480/7440 [=================>............] - ETA: 7s
4608/7440 [=================>............] - ETA: 7s
4736/7440 [==================>...........] - ETA: 6s
4864/7440 [==================>...........] - ETA: 6s
4992/7440 [===================>..........] - ETA: 5s
5120/7440 [===================>..........] - ETA: 5s
5248/7440 [====================>.........] - ETA: 4s
5376/7440 [====================>.........] - ETA: 4s
5504/7440 [=====================>........] - ETA: 4s
5632/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 3s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.7827956989247312
best saved model auc_score ------------------>  0.9093373872702046
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_33[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_295 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_295[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_296 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_296[0][0]             
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_118[0][0]            
__________________________________________________________________________________________________
activation_297 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_297[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_298 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_298[0][0]             
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 48, 96, 96)   0           concatenate_118[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 48, 96, 96)   192         concatenate_119[0][0]            
__________________________________________________________________________________________________
activation_299 (Activation)     (None, 48, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 24, 96, 96)   1152        activation_299[0][0]             
__________________________________________________________________________________________________
average_pooling2d_29 (AveragePo (None, 24, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 24, 48, 48)   96          average_pooling2d_29[0][0]       
__________________________________________________________________________________________________
activation_300 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1536        activation_300[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_301 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_301[0][0]             
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 40, 48, 48)   0           average_pooling2d_29[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 40, 48, 48)   160         concatenate_120[0][0]            
__________________________________________________________________________________________________
activation_302 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_302[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_303 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_303[0][0]             
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 56, 48, 48)   0           concatenate_120[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 56, 48, 48)   224         concatenate_121[0][0]            
__________________________________________________________________________________________________
activation_304 (Activation)     (None, 56, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 28, 48, 48)   1568        activation_304[0][0]             
__________________________________________________________________________________________________
average_pooling2d_30 (AveragePo (None, 28, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 28, 24, 24)   112         average_pooling2d_30[0][0]       
__________________________________________________________________________________________________
activation_305 (Activation)     (None, 28, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   1792        activation_305[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_306 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_306[0][0]             
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 44, 24, 24)   0           average_pooling2d_30[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 44, 24, 24)   176         concatenate_122[0][0]            
__________________________________________________________________________________________________
activation_307 (Activation)     (None, 44, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_307[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_308 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_308[0][0]             
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 60, 24, 24)   0           concatenate_122[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 60, 24, 24)   240         concatenate_123[0][0]            
__________________________________________________________________________________________________
activation_309 (Activation)     (None, 60, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_33 (Gl (None, 60)           0           activation_309[0][0]             
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 1)            61          global_average_pooling2d_33[0][0]
==================================================================================================
Total params: 73,069
Trainable params: 71,605
Non-trainable params: 1,464
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 59s - loss: 0.5580 - acc: 0.7672 - val_loss: 0.4731 - val_acc: 0.8363

Epoch 00001: val_loss improved from inf to 0.47307, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 36s - loss: 0.4677 - acc: 0.8161 - val_loss: 0.5108 - val_acc: 0.7996

Epoch 00002: val_loss did not improve from 0.47307
Epoch 3/30
 - 36s - loss: 0.4277 - acc: 0.8375 - val_loss: 0.4743 - val_acc: 0.8251

Epoch 00003: val_loss did not improve from 0.47307
Epoch 4/30
 - 35s - loss: 0.3963 - acc: 0.8546 - val_loss: 0.7369 - val_acc: 0.7438

Epoch 00004: val_loss did not improve from 0.47307
Epoch 5/30
 - 36s - loss: 0.3743 - acc: 0.8663 - val_loss: 0.7047 - val_acc: 0.7435

Epoch 00005: val_loss did not improve from 0.47307
Epoch 6/30
 - 36s - loss: 0.3524 - acc: 0.8764 - val_loss: 0.6843 - val_acc: 0.7554

Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00006: val_loss did not improve from 0.47307
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 418us/step
current Test accuracy: 0.7553763440860215
current auc_score ------------------>  0.8983157012371372

  32/7440 [..............................] - ETA: 37:38
 128/7440 [..............................] - ETA: 9:20 
 256/7440 [>.............................] - ETA: 4:36
 384/7440 [>.............................] - ETA: 3:02
 512/7440 [=>............................] - ETA: 2:14
 640/7440 [=>............................] - ETA: 1:46
 768/7440 [==>...........................] - ETA: 1:27
 896/7440 [==>...........................] - ETA: 1:14
1024/7440 [===>..........................] - ETA: 1:03
1152/7440 [===>..........................] - ETA: 55s 
1280/7440 [====>.........................] - ETA: 49s
1408/7440 [====>.........................] - ETA: 44s
1536/7440 [=====>........................] - ETA: 40s
1664/7440 [=====>........................] - ETA: 36s
1792/7440 [======>.......................] - ETA: 33s
1920/7440 [======>.......................] - ETA: 30s
2048/7440 [=======>......................] - ETA: 28s
2176/7440 [=======>......................] - ETA: 25s
2304/7440 [========>.....................] - ETA: 23s
2432/7440 [========>.....................] - ETA: 22s
2560/7440 [=========>....................] - ETA: 20s
2688/7440 [=========>....................] - ETA: 19s
2816/7440 [==========>...................] - ETA: 18s
2944/7440 [==========>...................] - ETA: 16s
3072/7440 [===========>..................] - ETA: 15s
3200/7440 [===========>..................] - ETA: 14s
3328/7440 [============>.................] - ETA: 13s
3456/7440 [============>.................] - ETA: 12s
3584/7440 [=============>................] - ETA: 12s
3712/7440 [=============>................] - ETA: 11s
3840/7440 [==============>...............] - ETA: 10s
3968/7440 [===============>..............] - ETA: 10s
4096/7440 [===============>..............] - ETA: 9s 
4224/7440 [================>.............] - ETA: 8s
4352/7440 [================>.............] - ETA: 8s
4480/7440 [=================>............] - ETA: 7s
4608/7440 [=================>............] - ETA: 7s
4736/7440 [==================>...........] - ETA: 6s
4864/7440 [==================>...........] - ETA: 6s
4992/7440 [===================>..........] - ETA: 5s
5120/7440 [===================>..........] - ETA: 5s
5248/7440 [====================>.........] - ETA: 5s
5376/7440 [====================>.........] - ETA: 4s
5504/7440 [=====================>........] - ETA: 4s
5632/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 3s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.8362903225806452
best saved model auc_score ------------------>  0.9110950687940802
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_34 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_34[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_310 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_310[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_311 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_311[0][0]             
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_124[0][0]            
__________________________________________________________________________________________________
activation_312 (Activation)     (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 56, 96, 96)   1680        activation_312[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_313 (Activation)     (None, 56, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_313[0][0]             
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 44, 96, 96)   0           concatenate_124[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 44, 96, 96)   176         concatenate_125[0][0]            
__________________________________________________________________________________________________
activation_314 (Activation)     (None, 44, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 56, 96, 96)   2464        activation_314[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_315 (Activation)     (None, 56, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_315[0][0]             
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 58, 96, 96)   0           concatenate_125[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 58, 96, 96)   232         concatenate_126[0][0]            
__________________________________________________________________________________________________
activation_316 (Activation)     (None, 58, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 29, 96, 96)   1682        activation_316[0][0]             
__________________________________________________________________________________________________
average_pooling2d_31 (AveragePo (None, 29, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 29, 48, 48)   116         average_pooling2d_31[0][0]       
__________________________________________________________________________________________________
activation_317 (Activation)     (None, 29, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   1624        activation_317[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_318 (Activation)     (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_318[0][0]             
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 43, 48, 48)   0           average_pooling2d_31[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_127[0][0]            
__________________________________________________________________________________________________
activation_319 (Activation)     (None, 43, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 56, 48, 48)   2408        activation_319[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_320 (Activation)     (None, 56, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_320[0][0]             
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 57, 48, 48)   0           concatenate_127[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 57, 48, 48)   228         concatenate_128[0][0]            
__________________________________________________________________________________________________
activation_321 (Activation)     (None, 57, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 56, 48, 48)   3192        activation_321[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_322 (Activation)     (None, 56, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_322[0][0]             
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 71, 48, 48)   0           concatenate_128[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 71, 48, 48)   284         concatenate_129[0][0]            
__________________________________________________________________________________________________
activation_323 (Activation)     (None, 71, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 35, 48, 48)   2485        activation_323[0][0]             
__________________________________________________________________________________________________
average_pooling2d_32 (AveragePo (None, 35, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 35, 24, 24)   140         average_pooling2d_32[0][0]       
__________________________________________________________________________________________________
activation_324 (Activation)     (None, 35, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 56, 24, 24)   1960        activation_324[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_325 (Activation)     (None, 56, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_325[0][0]             
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 49, 24, 24)   0           average_pooling2d_32[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 49, 24, 24)   196         concatenate_130[0][0]            
__________________________________________________________________________________________________
activation_326 (Activation)     (None, 49, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 56, 24, 24)   2744        activation_326[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_327 (Activation)     (None, 56, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_327[0][0]             
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 63, 24, 24)   0           concatenate_130[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 63, 24, 24)   252         concatenate_131[0][0]            
__________________________________________________________________________________________________
activation_328 (Activation)     (None, 63, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 56, 24, 24)   3528        activation_328[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_329 (Activation)     (None, 56, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_329[0][0]             
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 77, 24, 24)   0           concatenate_131[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 77, 24, 24)   308         concatenate_132[0][0]            
__________________________________________________________________________________________________
activation_330 (Activation)     (None, 77, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_34 (Gl (None, 77)           0           activation_330[0][0]             
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 1)            78          global_average_pooling2d_34[0][0]
==================================================================================================
Total params: 92,837
Trainable params: 90,685
Non-trainable params: 2,152
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 75s - loss: 0.5758 - acc: 0.7758 - val_loss: 0.5986 - val_acc: 0.7875

Epoch 00001: val_loss improved from inf to 0.59863, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 48s - loss: 0.4736 - acc: 0.8280 - val_loss: 0.6018 - val_acc: 0.7945

Epoch 00002: val_loss did not improve from 0.59863
Epoch 3/30
 - 48s - loss: 0.4216 - acc: 0.8580 - val_loss: 0.5109 - val_acc: 0.8105

Epoch 00003: val_loss improved from 0.59863 to 0.51086, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 48s - loss: 0.3809 - acc: 0.8788 - val_loss: 0.5047 - val_acc: 0.8227

Epoch 00004: val_loss improved from 0.51086 to 0.50465, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 48s - loss: 0.3510 - acc: 0.8917 - val_loss: 0.6377 - val_acc: 0.8313

Epoch 00005: val_loss did not improve from 0.50465
Epoch 6/30
 - 48s - loss: 0.3223 - acc: 0.9067 - val_loss: 0.5822 - val_acc: 0.8226

Epoch 00006: val_loss did not improve from 0.50465
Epoch 7/30
 - 48s - loss: 0.3007 - acc: 0.9155 - val_loss: 0.6041 - val_acc: 0.8112

Epoch 00007: val_loss did not improve from 0.50465
Epoch 8/30
 - 48s - loss: 0.2819 - acc: 0.9232 - val_loss: 0.7905 - val_acc: 0.7438

Epoch 00008: val_loss did not improve from 0.50465
Epoch 9/30
 - 48s - loss: 0.2669 - acc: 0.9293 - val_loss: 0.5280 - val_acc: 0.8138

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.50465
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 6s
 128/7440 [..............................] - ETA: 4s
 224/7440 [..............................] - ETA: 4s
 320/7440 [>.............................] - ETA: 4s
 416/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 608/7440 [=>............................] - ETA: 3s
 736/7440 [=>............................] - ETA: 3s
 864/7440 [==>...........................] - ETA: 3s
 992/7440 [===>..........................] - ETA: 3s
1120/7440 [===>..........................] - ETA: 3s
1248/7440 [====>.........................] - ETA: 3s
1376/7440 [====>.........................] - ETA: 3s
1504/7440 [=====>........................] - ETA: 3s
1632/7440 [=====>........................] - ETA: 3s
1760/7440 [======>.......................] - ETA: 3s
1888/7440 [======>.......................] - ETA: 2s
2016/7440 [=======>......................] - ETA: 2s
2144/7440 [=======>......................] - ETA: 2s
2272/7440 [========>.....................] - ETA: 2s
2368/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3040/7440 [===========>..................] - ETA: 2s
3168/7440 [===========>..................] - ETA: 2s
3296/7440 [============>.................] - ETA: 2s
3392/7440 [============>.................] - ETA: 2s
3520/7440 [=============>................] - ETA: 2s
3648/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4160/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4928/7440 [==================>...........] - ETA: 1s
5056/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 522us/step
current Test accuracy: 0.8138440860215054
current auc_score ------------------>  0.9125303141981731

  32/7440 [..............................] - ETA: 43:13
 128/7440 [..............................] - ETA: 10:43
 224/7440 [..............................] - ETA: 6:04 
 320/7440 [>.............................] - ETA: 4:12
 416/7440 [>.............................] - ETA: 3:12
 512/7440 [=>............................] - ETA: 2:35
 608/7440 [=>............................] - ETA: 2:09
 704/7440 [=>............................] - ETA: 1:50
 800/7440 [==>...........................] - ETA: 1:36
 896/7440 [==>...........................] - ETA: 1:25
 992/7440 [===>..........................] - ETA: 1:16
1088/7440 [===>..........................] - ETA: 1:08
1184/7440 [===>..........................] - ETA: 1:02
1280/7440 [====>.........................] - ETA: 57s 
1376/7440 [====>.........................] - ETA: 52s
1472/7440 [====>.........................] - ETA: 48s
1600/7440 [=====>........................] - ETA: 43s
1696/7440 [=====>........................] - ETA: 40s
1792/7440 [======>.......................] - ETA: 38s
1888/7440 [======>.......................] - ETA: 35s
1984/7440 [=======>......................] - ETA: 33s
2080/7440 [=======>......................] - ETA: 31s
2176/7440 [=======>......................] - ETA: 29s
2272/7440 [========>.....................] - ETA: 28s
2368/7440 [========>.....................] - ETA: 26s
2464/7440 [========>.....................] - ETA: 25s
2560/7440 [=========>....................] - ETA: 23s
2656/7440 [=========>....................] - ETA: 22s
2752/7440 [==========>...................] - ETA: 21s
2848/7440 [==========>...................] - ETA: 20s
2944/7440 [==========>...................] - ETA: 19s
3072/7440 [===========>..................] - ETA: 18s
3168/7440 [===========>..................] - ETA: 17s
3264/7440 [============>.................] - ETA: 16s
3360/7440 [============>.................] - ETA: 15s
3456/7440 [============>.................] - ETA: 15s
3584/7440 [=============>................] - ETA: 14s
3680/7440 [=============>................] - ETA: 13s
3776/7440 [==============>...............] - ETA: 12s
3872/7440 [==============>...............] - ETA: 12s
3968/7440 [===============>..............] - ETA: 11s
4064/7440 [===============>..............] - ETA: 11s
4160/7440 [===============>..............] - ETA: 10s
4256/7440 [================>.............] - ETA: 10s
4352/7440 [================>.............] - ETA: 9s 
4448/7440 [================>.............] - ETA: 9s
4544/7440 [=================>............] - ETA: 8s
4640/7440 [=================>............] - ETA: 8s
4736/7440 [==================>...........] - ETA: 7s
4864/7440 [==================>...........] - ETA: 7s
4992/7440 [===================>..........] - ETA: 6s
5088/7440 [===================>..........] - ETA: 6s
5184/7440 [===================>..........] - ETA: 6s
5280/7440 [====================>.........] - ETA: 5s
5376/7440 [====================>.........] - ETA: 5s
5472/7440 [=====================>........] - ETA: 5s
5568/7440 [=====================>........] - ETA: 4s
5664/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 4s
5856/7440 [======================>.......] - ETA: 3s
5952/7440 [=======================>......] - ETA: 3s
6048/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6240/7440 [========================>.....] - ETA: 2s
6336/7440 [========================>.....] - ETA: 2s
6432/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6624/7440 [=========================>....] - ETA: 1s
6720/7440 [==========================>...] - ETA: 1s
6848/7440 [==========================>...] - ETA: 1s
6944/7440 [===========================>..] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 15s 2ms/step
Best saved model Test accuracy: 0.8227150537634409
best saved model auc_score ------------------>  0.9206612758700428
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_35[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_331 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_331[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_332 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_332[0][0]             
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_133[0][0]            
__________________________________________________________________________________________________
activation_333 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_333[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_334 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_334[0][0]             
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 40, 96, 96)   0           concatenate_133[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_134[0][0]            
__________________________________________________________________________________________________
activation_335 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_335[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_336 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_336[0][0]             
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 52, 96, 96)   0           concatenate_134[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_135[0][0]            
__________________________________________________________________________________________________
activation_337 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_337[0][0]             
__________________________________________________________________________________________________
average_pooling2d_33 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_33[0][0]       
__________________________________________________________________________________________________
activation_338 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_338[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_339 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_339[0][0]             
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 38, 48, 48)   0           average_pooling2d_33[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_136[0][0]            
__________________________________________________________________________________________________
activation_340 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_340[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_341 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_341[0][0]             
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 50, 48, 48)   0           concatenate_136[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_137[0][0]            
__________________________________________________________________________________________________
activation_342 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_342[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_343 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_343[0][0]             
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 62, 48, 48)   0           concatenate_137[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 62, 48, 48)   248         concatenate_138[0][0]            
__________________________________________________________________________________________________
activation_344 (Activation)     (None, 62, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 31, 48, 48)   1922        activation_344[0][0]             
__________________________________________________________________________________________________
average_pooling2d_34 (AveragePo (None, 31, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 31, 24, 24)   124         average_pooling2d_34[0][0]       
__________________________________________________________________________________________________
activation_345 (Activation)     (None, 31, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   1488        activation_345[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_346 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_346[0][0]             
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 43, 24, 24)   0           average_pooling2d_34[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 43, 24, 24)   172         concatenate_139[0][0]            
__________________________________________________________________________________________________
activation_347 (Activation)     (None, 43, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 48, 24, 24)   2064        activation_347[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_348 (Activation)     (None, 48, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_348[0][0]             
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 55, 24, 24)   0           concatenate_139[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 55, 24, 24)   220         concatenate_140[0][0]            
__________________________________________________________________________________________________
activation_349 (Activation)     (None, 55, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 48, 24, 24)   2640        activation_349[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_350 (Activation)     (None, 48, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_350[0][0]             
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 67, 24, 24)   0           concatenate_140[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 67, 24, 24)   268         concatenate_141[0][0]            
__________________________________________________________________________________________________
activation_351 (Activation)     (None, 67, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_35 (Gl (None, 67)           0           activation_351[0][0]             
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 1)            68          global_average_pooling2d_35[0][0]
==================================================================================================
Total params: 69,742
Trainable params: 67,862
Non-trainable params: 1,880
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 72s - loss: 0.5660 - acc: 0.7662 - val_loss: 0.7549 - val_acc: 0.7480

Epoch 00001: val_loss improved from inf to 0.75486, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 44s - loss: 0.4665 - acc: 0.8227 - val_loss: 0.5769 - val_acc: 0.7937

Epoch 00002: val_loss improved from 0.75486 to 0.57695, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 44s - loss: 0.4195 - acc: 0.8481 - val_loss: 0.6174 - val_acc: 0.7956

Epoch 00003: val_loss did not improve from 0.57695
Epoch 4/30
 - 44s - loss: 0.3831 - acc: 0.8677 - val_loss: 0.4837 - val_acc: 0.8196

Epoch 00004: val_loss improved from 0.57695 to 0.48371, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 44s - loss: 0.3537 - acc: 0.8830 - val_loss: 0.7622 - val_acc: 0.7758

Epoch 00005: val_loss did not improve from 0.48371
Epoch 6/30
 - 44s - loss: 0.3324 - acc: 0.8922 - val_loss: 0.5040 - val_acc: 0.8199

Epoch 00006: val_loss did not improve from 0.48371
Epoch 7/30
 - 44s - loss: 0.3100 - acc: 0.9022 - val_loss: 0.5200 - val_acc: 0.7935

Epoch 00007: val_loss did not improve from 0.48371
Epoch 8/30
 - 44s - loss: 0.2905 - acc: 0.9116 - val_loss: 0.5509 - val_acc: 0.7859

Epoch 00008: val_loss did not improve from 0.48371
Epoch 9/30
 - 44s - loss: 0.2769 - acc: 0.9179 - val_loss: 0.5290 - val_acc: 0.8015

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.48371
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 5s
 128/7440 [..............................] - ETA: 4s
 256/7440 [>.............................] - ETA: 4s
 384/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 640/7440 [=>............................] - ETA: 3s
 768/7440 [==>...........................] - ETA: 3s
 896/7440 [==>...........................] - ETA: 3s
1024/7440 [===>..........................] - ETA: 3s
1152/7440 [===>..........................] - ETA: 3s
1280/7440 [====>.........................] - ETA: 3s
1408/7440 [====>.........................] - ETA: 3s
1536/7440 [=====>........................] - ETA: 3s
1664/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2048/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2304/7440 [========>.....................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 2s
2560/7440 [=========>....................] - ETA: 2s
2688/7440 [=========>....................] - ETA: 2s
2816/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3456/7440 [============>.................] - ETA: 2s
3584/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4608/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 507us/step
current Test accuracy: 0.801478494623656
current auc_score ------------------>  0.9063681494970517

  32/7440 [..............................] - ETA: 46:51
 128/7440 [..............................] - ETA: 11:37
 256/7440 [>.............................] - ETA: 5:44 
 384/7440 [>.............................] - ETA: 3:46
 512/7440 [=>............................] - ETA: 2:47
 640/7440 [=>............................] - ETA: 2:12
 768/7440 [==>...........................] - ETA: 1:48
 896/7440 [==>...........................] - ETA: 1:32
1024/7440 [===>..........................] - ETA: 1:19
1152/7440 [===>..........................] - ETA: 1:09
1280/7440 [====>.........................] - ETA: 1:01
1408/7440 [====>.........................] - ETA: 55s 
1536/7440 [=====>........................] - ETA: 49s
1664/7440 [=====>........................] - ETA: 45s
1792/7440 [======>.......................] - ETA: 41s
1920/7440 [======>.......................] - ETA: 37s
2048/7440 [=======>......................] - ETA: 34s
2176/7440 [=======>......................] - ETA: 32s
2304/7440 [========>.....................] - ETA: 29s
2432/7440 [========>.....................] - ETA: 27s
2560/7440 [=========>....................] - ETA: 25s
2688/7440 [=========>....................] - ETA: 23s
2816/7440 [==========>...................] - ETA: 22s
2944/7440 [==========>...................] - ETA: 20s
3072/7440 [===========>..................] - ETA: 19s
3200/7440 [===========>..................] - ETA: 18s
3328/7440 [============>.................] - ETA: 17s
3456/7440 [============>.................] - ETA: 16s
3584/7440 [=============>................] - ETA: 15s
3712/7440 [=============>................] - ETA: 14s
3840/7440 [==============>...............] - ETA: 13s
3968/7440 [===============>..............] - ETA: 12s
4096/7440 [===============>..............] - ETA: 11s
4224/7440 [================>.............] - ETA: 10s
4352/7440 [================>.............] - ETA: 10s
4480/7440 [=================>............] - ETA: 9s 
4608/7440 [=================>............] - ETA: 8s
4736/7440 [==================>...........] - ETA: 8s
4864/7440 [==================>...........] - ETA: 7s
4992/7440 [===================>..........] - ETA: 7s
5120/7440 [===================>..........] - ETA: 6s
5248/7440 [====================>.........] - ETA: 6s
5376/7440 [====================>.........] - ETA: 5s
5504/7440 [=====================>........] - ETA: 5s
5632/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 16s 2ms/step
Best saved model Test accuracy: 0.8196236559139785
best saved model auc_score ------------------>  0.9078117773731066
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_36 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_36[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_352 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_352[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_353 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_353[0][0]             
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_142[0][0]            
__________________________________________________________________________________________________
activation_354 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_354[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_355 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_355[0][0]             
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 48, 96, 96)   0           concatenate_142[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 48, 96, 96)   192         concatenate_143[0][0]            
__________________________________________________________________________________________________
activation_356 (Activation)     (None, 48, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 24, 96, 96)   1152        activation_356[0][0]             
__________________________________________________________________________________________________
average_pooling2d_35 (AveragePo (None, 24, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 24, 48, 48)   96          average_pooling2d_35[0][0]       
__________________________________________________________________________________________________
activation_357 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1536        activation_357[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_358 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_358[0][0]             
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 40, 48, 48)   0           average_pooling2d_35[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 40, 48, 48)   160         concatenate_144[0][0]            
__________________________________________________________________________________________________
activation_359 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_359[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_360 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_360[0][0]             
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 56, 48, 48)   0           concatenate_144[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 56, 48, 48)   224         concatenate_145[0][0]            
__________________________________________________________________________________________________
activation_361 (Activation)     (None, 56, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 28, 48, 48)   1568        activation_361[0][0]             
__________________________________________________________________________________________________
average_pooling2d_36 (AveragePo (None, 28, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 28, 24, 24)   112         average_pooling2d_36[0][0]       
__________________________________________________________________________________________________
activation_362 (Activation)     (None, 28, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   1792        activation_362[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_363 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_363[0][0]             
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 44, 24, 24)   0           average_pooling2d_36[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 44, 24, 24)   176         concatenate_146[0][0]            
__________________________________________________________________________________________________
activation_364 (Activation)     (None, 44, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_364[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_365 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_365[0][0]             
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 60, 24, 24)   0           concatenate_146[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 60, 24, 24)   240         concatenate_147[0][0]            
__________________________________________________________________________________________________
activation_366 (Activation)     (None, 60, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_36 (Gl (None, 60)           0           activation_366[0][0]             
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 1)            61          global_average_pooling2d_36[0][0]
==================================================================================================
Total params: 73,069
Trainable params: 71,605
Non-trainable params: 1,464
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 66s - loss: 0.5493 - acc: 0.7790 - val_loss: 0.5720 - val_acc: 0.7730

Epoch 00001: val_loss improved from inf to 0.57201, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 36s - loss: 0.4587 - acc: 0.8217 - val_loss: 0.5028 - val_acc: 0.8341

Epoch 00002: val_loss improved from 0.57201 to 0.50276, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 36s - loss: 0.4198 - acc: 0.8422 - val_loss: 0.5433 - val_acc: 0.8227

Epoch 00003: val_loss did not improve from 0.50276
Epoch 4/30
 - 36s - loss: 0.3861 - acc: 0.8598 - val_loss: 0.5256 - val_acc: 0.7984

Epoch 00004: val_loss did not improve from 0.50276
Epoch 5/30
 - 36s - loss: 0.3610 - acc: 0.8744 - val_loss: 0.5406 - val_acc: 0.8094

Epoch 00005: val_loss did not improve from 0.50276
Epoch 6/30
 - 36s - loss: 0.3373 - acc: 0.8856 - val_loss: 0.5879 - val_acc: 0.8167

Epoch 00006: val_loss did not improve from 0.50276
Epoch 7/30
 - 36s - loss: 0.3182 - acc: 0.8957 - val_loss: 0.6495 - val_acc: 0.8168

Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00007: val_loss did not improve from 0.50276
Epoch 00007: early stopping

  32/7440 [..............................] - ETA: 6s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 455us/step
current Test accuracy: 0.8168010752688172
current auc_score ------------------>  0.8724177650595444

  32/7440 [..............................] - ETA: 48:03
 128/7440 [..............................] - ETA: 11:55
 256/7440 [>.............................] - ETA: 5:52 
 384/7440 [>.............................] - ETA: 3:52
 512/7440 [=>............................] - ETA: 2:51
 640/7440 [=>............................] - ETA: 2:15
 768/7440 [==>...........................] - ETA: 1:51
 896/7440 [==>...........................] - ETA: 1:34
1024/7440 [===>..........................] - ETA: 1:21
1152/7440 [===>..........................] - ETA: 1:10
1280/7440 [====>.........................] - ETA: 1:02
1408/7440 [====>.........................] - ETA: 56s 
1536/7440 [=====>........................] - ETA: 50s
1664/7440 [=====>........................] - ETA: 45s
1792/7440 [======>.......................] - ETA: 41s
1920/7440 [======>.......................] - ETA: 38s
2048/7440 [=======>......................] - ETA: 35s
2176/7440 [=======>......................] - ETA: 32s
2304/7440 [========>.....................] - ETA: 30s
2432/7440 [========>.....................] - ETA: 27s
2560/7440 [=========>....................] - ETA: 25s
2688/7440 [=========>....................] - ETA: 24s
2816/7440 [==========>...................] - ETA: 22s
2944/7440 [==========>...................] - ETA: 21s
3072/7440 [===========>..................] - ETA: 19s
3200/7440 [===========>..................] - ETA: 18s
3328/7440 [============>.................] - ETA: 17s
3456/7440 [============>.................] - ETA: 16s
3584/7440 [=============>................] - ETA: 15s
3712/7440 [=============>................] - ETA: 14s
3840/7440 [==============>...............] - ETA: 13s
3968/7440 [===============>..............] - ETA: 12s
4096/7440 [===============>..............] - ETA: 11s
4224/7440 [================>.............] - ETA: 10s
4352/7440 [================>.............] - ETA: 10s
4480/7440 [=================>............] - ETA: 9s 
4608/7440 [=================>............] - ETA: 8s
4736/7440 [==================>...........] - ETA: 8s
4864/7440 [==================>...........] - ETA: 7s
4992/7440 [===================>..........] - ETA: 7s
5120/7440 [===================>..........] - ETA: 6s
5248/7440 [====================>.........] - ETA: 6s
5376/7440 [====================>.........] - ETA: 5s
5504/7440 [=====================>........] - ETA: 5s
5632/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 3s
6112/7440 [=======================>......] - ETA: 3s
6240/7440 [========================>.....] - ETA: 2s
6368/7440 [========================>.....] - ETA: 2s
6496/7440 [=========================>....] - ETA: 2s
6624/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7008/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 16s 2ms/step
Best saved model Test accuracy: 0.8341397849462365
best saved model auc_score ------------------>  0.8986666811192046
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_37[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_367 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_367[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_368 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_368[0][0]             
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_148[0][0]            
__________________________________________________________________________________________________
activation_369 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_369[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_370 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_370[0][0]             
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 40, 96, 96)   0           concatenate_148[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_149[0][0]            
__________________________________________________________________________________________________
activation_371 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_371[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_372 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_372[0][0]             
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 52, 96, 96)   0           concatenate_149[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_150[0][0]            
__________________________________________________________________________________________________
activation_373 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_373[0][0]             
__________________________________________________________________________________________________
average_pooling2d_37 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_37[0][0]       
__________________________________________________________________________________________________
activation_374 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_374[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_375 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_375[0][0]             
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 38, 48, 48)   0           average_pooling2d_37[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_151[0][0]            
__________________________________________________________________________________________________
activation_376 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_376[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_377 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_377[0][0]             
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 50, 48, 48)   0           concatenate_151[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_152[0][0]            
__________________________________________________________________________________________________
activation_378 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_378[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_379 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_379[0][0]             
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 62, 48, 48)   0           concatenate_152[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 62, 48, 48)   248         concatenate_153[0][0]            
__________________________________________________________________________________________________
activation_380 (Activation)     (None, 62, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 31, 48, 48)   1922        activation_380[0][0]             
__________________________________________________________________________________________________
average_pooling2d_38 (AveragePo (None, 31, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 31, 24, 24)   124         average_pooling2d_38[0][0]       
__________________________________________________________________________________________________
activation_381 (Activation)     (None, 31, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   1488        activation_381[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_382 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_382[0][0]             
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 43, 24, 24)   0           average_pooling2d_38[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 43, 24, 24)   172         concatenate_154[0][0]            
__________________________________________________________________________________________________
activation_383 (Activation)     (None, 43, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 48, 24, 24)   2064        activation_383[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_384 (Activation)     (None, 48, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_384[0][0]             
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 55, 24, 24)   0           concatenate_154[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 55, 24, 24)   220         concatenate_155[0][0]            
__________________________________________________________________________________________________
activation_385 (Activation)     (None, 55, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 48, 24, 24)   2640        activation_385[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_386 (Activation)     (None, 48, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_386[0][0]             
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 67, 24, 24)   0           concatenate_155[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 67, 24, 24)   268         concatenate_156[0][0]            
__________________________________________________________________________________________________
activation_387 (Activation)     (None, 67, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_37 (Gl (None, 67)           0           activation_387[0][0]             
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 1)            68          global_average_pooling2d_37[0][0]
==================================================================================================
Total params: 69,742
Trainable params: 67,862
Non-trainable params: 1,880
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 78s - loss: 0.5694 - acc: 0.7689 - val_loss: 0.5588 - val_acc: 0.7942

Epoch 00001: val_loss improved from inf to 0.55875, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 45s - loss: 0.4721 - acc: 0.8175 - val_loss: 0.4783 - val_acc: 0.8528

Epoch 00002: val_loss improved from 0.55875 to 0.47833, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 45s - loss: 0.4257 - acc: 0.8408 - val_loss: 0.4806 - val_acc: 0.8194

Epoch 00003: val_loss did not improve from 0.47833
Epoch 4/30
 - 45s - loss: 0.3951 - acc: 0.8582 - val_loss: 0.4445 - val_acc: 0.8406

Epoch 00004: val_loss improved from 0.47833 to 0.44452, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 45s - loss: 0.3675 - acc: 0.8720 - val_loss: 1.2777 - val_acc: 0.7367

Epoch 00005: val_loss did not improve from 0.44452
Epoch 6/30
 - 45s - loss: 0.3461 - acc: 0.8846 - val_loss: 0.5270 - val_acc: 0.8132

Epoch 00006: val_loss did not improve from 0.44452
Epoch 7/30
 - 45s - loss: 0.3244 - acc: 0.8946 - val_loss: 0.7050 - val_acc: 0.7948

Epoch 00007: val_loss did not improve from 0.44452
Epoch 8/30
 - 45s - loss: 0.3068 - acc: 0.9036 - val_loss: 0.4625 - val_acc: 0.8539

Epoch 00008: val_loss did not improve from 0.44452
Epoch 9/30
 - 45s - loss: 0.2931 - acc: 0.9099 - val_loss: 0.5226 - val_acc: 0.8450

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.44452
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 7s
 128/7440 [..............................] - ETA: 4s
 224/7440 [..............................] - ETA: 4s
 352/7440 [>.............................] - ETA: 4s
 480/7440 [>.............................] - ETA: 3s
 608/7440 [=>............................] - ETA: 3s
 704/7440 [=>............................] - ETA: 3s
 832/7440 [==>...........................] - ETA: 3s
 960/7440 [==>...........................] - ETA: 3s
1088/7440 [===>..........................] - ETA: 3s
1216/7440 [===>..........................] - ETA: 3s
1344/7440 [====>.........................] - ETA: 3s
1472/7440 [====>.........................] - ETA: 3s
1568/7440 [=====>........................] - ETA: 3s
1696/7440 [=====>........................] - ETA: 3s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 2s
3360/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4576/7440 [=================>............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5088/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 519us/step
current Test accuracy: 0.8450268817204301
current auc_score ------------------>  0.9129733928777893

  32/7440 [..............................] - ETA: 55:13
  96/7440 [..............................] - ETA: 18:18
 192/7440 [..............................] - ETA: 9:04 
 288/7440 [>.............................] - ETA: 5:59
 384/7440 [>.............................] - ETA: 4:26
 512/7440 [=>............................] - ETA: 3:17
 640/7440 [=>............................] - ETA: 2:35
 768/7440 [==>...........................] - ETA: 2:07
 896/7440 [==>...........................] - ETA: 1:47
 992/7440 [===>..........................] - ETA: 1:36
1120/7440 [===>..........................] - ETA: 1:24
1248/7440 [====>.........................] - ETA: 1:14
1376/7440 [====>.........................] - ETA: 1:06
1504/7440 [=====>........................] - ETA: 59s 
1632/7440 [=====>........................] - ETA: 53s
1760/7440 [======>.......................] - ETA: 49s
1888/7440 [======>.......................] - ETA: 44s
2016/7440 [=======>......................] - ETA: 41s
2144/7440 [=======>......................] - ETA: 38s
2272/7440 [========>.....................] - ETA: 35s
2400/7440 [========>.....................] - ETA: 32s
2528/7440 [=========>....................] - ETA: 30s
2656/7440 [=========>....................] - ETA: 28s
2784/7440 [==========>...................] - ETA: 26s
2912/7440 [==========>...................] - ETA: 24s
3040/7440 [===========>..................] - ETA: 22s
3168/7440 [===========>..................] - ETA: 21s
3296/7440 [============>.................] - ETA: 20s
3424/7440 [============>.................] - ETA: 18s
3552/7440 [=============>................] - ETA: 17s
3680/7440 [=============>................] - ETA: 16s
3808/7440 [==============>...............] - ETA: 15s
3904/7440 [==============>...............] - ETA: 14s
4032/7440 [===============>..............] - ETA: 13s
4160/7440 [===============>..............] - ETA: 12s
4288/7440 [================>.............] - ETA: 12s
4416/7440 [================>.............] - ETA: 11s
4544/7440 [=================>............] - ETA: 10s
4640/7440 [=================>............] - ETA: 10s
4768/7440 [==================>...........] - ETA: 9s 
4896/7440 [==================>...........] - ETA: 8s
5024/7440 [===================>..........] - ETA: 8s
5152/7440 [===================>..........] - ETA: 7s
5280/7440 [====================>.........] - ETA: 6s
5408/7440 [====================>.........] - ETA: 6s
5536/7440 [=====================>........] - ETA: 5s
5632/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 5s
5856/7440 [======================>.......] - ETA: 4s
5984/7440 [=======================>......] - ETA: 4s
6112/7440 [=======================>......] - ETA: 3s
6240/7440 [========================>.....] - ETA: 3s
6336/7440 [========================>.....] - ETA: 3s
6432/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 2s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 1s
7136/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 18s 2ms/step
Best saved model Test accuracy: 0.8405913978494624
best saved model auc_score ------------------>  0.9198371198982541
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_38 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_38[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_388 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_388[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_389 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_389[0][0]             
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 30, 96, 96)   120         concatenate_157[0][0]            
__________________________________________________________________________________________________
activation_390 (Activation)     (None, 30, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_38 (Gl (None, 30)           0           activation_390[0][0]             
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 1)            31          global_average_pooling2d_38[0][0]
==================================================================================================
Total params: 8,679
Trainable params: 8,475
Non-trainable params: 204
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 50s - loss: 0.6284 - acc: 0.6857 - val_loss: 0.5342 - val_acc: 0.7918

Epoch 00001: val_loss improved from inf to 0.53418, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 17s - loss: 0.5823 - acc: 0.7303 - val_loss: 0.5147 - val_acc: 0.8198

Epoch 00002: val_loss improved from 0.53418 to 0.51473, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 17s - loss: 0.5496 - acc: 0.7482 - val_loss: 0.4865 - val_acc: 0.8496

Epoch 00003: val_loss improved from 0.51473 to 0.48655, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 17s - loss: 0.5305 - acc: 0.7521 - val_loss: 0.4900 - val_acc: 0.8460

Epoch 00004: val_loss did not improve from 0.48655
Epoch 5/30
 - 17s - loss: 0.5177 - acc: 0.7574 - val_loss: 0.4909 - val_acc: 0.8198

Epoch 00005: val_loss did not improve from 0.48655
Epoch 6/30
 - 17s - loss: 0.5098 - acc: 0.7596 - val_loss: 0.4971 - val_acc: 0.8110

Epoch 00006: val_loss did not improve from 0.48655
Epoch 7/30
 - 17s - loss: 0.5037 - acc: 0.7610 - val_loss: 0.4992 - val_acc: 0.8024

Epoch 00007: val_loss did not improve from 0.48655
Epoch 8/30
 - 17s - loss: 0.4987 - acc: 0.7614 - val_loss: 0.4850 - val_acc: 0.8290

Epoch 00008: val_loss improved from 0.48655 to 0.48497, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 17s - loss: 0.4948 - acc: 0.7655 - val_loss: 0.5330 - val_acc: 0.8062

Epoch 00009: val_loss did not improve from 0.48497
Epoch 10/30
 - 17s - loss: 0.4913 - acc: 0.7663 - val_loss: 0.4973 - val_acc: 0.7895

Epoch 00010: val_loss did not improve from 0.48497
Epoch 11/30
 - 17s - loss: 0.4890 - acc: 0.7672 - val_loss: 0.4962 - val_acc: 0.8109

Epoch 00011: val_loss did not improve from 0.48497
Epoch 12/30
 - 17s - loss: 0.4857 - acc: 0.7706 - val_loss: 0.5067 - val_acc: 0.7743

Epoch 00012: val_loss did not improve from 0.48497
Epoch 13/30
 - 17s - loss: 0.4825 - acc: 0.7682 - val_loss: 0.5385 - val_acc: 0.7691

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.48497
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 4s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 544/7440 [=>............................] - ETA: 2s
 704/7440 [=>............................] - ETA: 2s
 864/7440 [==>...........................] - ETA: 2s
1024/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1344/7440 [====>.........................] - ETA: 1s
1504/7440 [=====>........................] - ETA: 1s
1664/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
1984/7440 [=======>......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2304/7440 [========>.....................] - ETA: 1s
2464/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3264/7440 [============>.................] - ETA: 1s
3424/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 329us/step
current Test accuracy: 0.7690860215053763
current auc_score ------------------>  0.8398991212856977

  32/7440 [..............................] - ETA: 54:55
 160/7440 [..............................] - ETA: 10:50
 320/7440 [>.............................] - ETA: 5:19 
 480/7440 [>.............................] - ETA: 3:28
 640/7440 [=>............................] - ETA: 2:33
 800/7440 [==>...........................] - ETA: 2:00
 960/7440 [==>...........................] - ETA: 1:38
1120/7440 [===>..........................] - ETA: 1:22
1280/7440 [====>.........................] - ETA: 1:10
1440/7440 [====>.........................] - ETA: 1:01
1600/7440 [=====>........................] - ETA: 53s 
1760/7440 [======>.......................] - ETA: 47s
1920/7440 [======>.......................] - ETA: 42s
2080/7440 [=======>......................] - ETA: 38s
2240/7440 [========>.....................] - ETA: 34s
2400/7440 [========>.....................] - ETA: 31s
2560/7440 [=========>....................] - ETA: 28s
2720/7440 [=========>....................] - ETA: 26s
2880/7440 [==========>...................] - ETA: 24s
3040/7440 [===========>..................] - ETA: 22s
3200/7440 [===========>..................] - ETA: 20s
3360/7440 [============>.................] - ETA: 18s
3520/7440 [=============>................] - ETA: 17s
3680/7440 [=============>................] - ETA: 15s
3840/7440 [==============>...............] - ETA: 14s
4000/7440 [===============>..............] - ETA: 13s
4160/7440 [===============>..............] - ETA: 12s
4320/7440 [================>.............] - ETA: 11s
4480/7440 [=================>............] - ETA: 10s
4640/7440 [=================>............] - ETA: 9s 
4800/7440 [==================>...........] - ETA: 8s
4960/7440 [===================>..........] - ETA: 7s
5120/7440 [===================>..........] - ETA: 7s
5280/7440 [====================>.........] - ETA: 6s
5440/7440 [====================>.........] - ETA: 5s
5600/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 4s
5920/7440 [======================>.......] - ETA: 4s
6080/7440 [=======================>......] - ETA: 3s
6240/7440 [========================>.....] - ETA: 3s
6400/7440 [========================>.....] - ETA: 2s
6560/7440 [=========================>....] - ETA: 2s
6720/7440 [==========================>...] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 17s 2ms/step
Best saved model Test accuracy: 0.8290322580645161
best saved model auc_score ------------------>  0.8696114435194819
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_39[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_391 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_391[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_392 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_392[0][0]             
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_158[0][0]            
__________________________________________________________________________________________________
activation_393 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_393[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_394 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_394[0][0]             
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 48, 96, 96)   0           concatenate_158[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_159[0][0]            
__________________________________________________________________________________________________
activation_395 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_395[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_396 (Activation)     (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_396[0][0]             
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 64, 96, 96)   0           concatenate_159[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_160[0][0]            
__________________________________________________________________________________________________
activation_397 (Activation)     (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_397[0][0]             
__________________________________________________________________________________________________
average_pooling2d_39 (AveragePo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_39[0][0]       
__________________________________________________________________________________________________
activation_398 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_398[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_399 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_399[0][0]             
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 48, 48, 48)   0           average_pooling2d_39[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_161[0][0]            
__________________________________________________________________________________________________
activation_400 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_400[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_401 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_401[0][0]             
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 64, 48, 48)   0           concatenate_161[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_162[0][0]            
__________________________________________________________________________________________________
activation_402 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_402[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_403 (Activation)     (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_403[0][0]             
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 80, 48, 48)   0           concatenate_162[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 80, 48, 48)   320         concatenate_163[0][0]            
__________________________________________________________________________________________________
activation_404 (Activation)     (None, 80, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 40, 48, 48)   3200        activation_404[0][0]             
__________________________________________________________________________________________________
average_pooling2d_40 (AveragePo (None, 40, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 40, 24, 24)   160         average_pooling2d_40[0][0]       
__________________________________________________________________________________________________
activation_405 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   2560        activation_405[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_406 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_406[0][0]             
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 56, 24, 24)   0           average_pooling2d_40[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 56, 24, 24)   224         concatenate_164[0][0]            
__________________________________________________________________________________________________
activation_407 (Activation)     (None, 56, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   3584        activation_407[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_408 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_408[0][0]             
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 72, 24, 24)   0           concatenate_164[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 72, 24, 24)   288         concatenate_165[0][0]            
__________________________________________________________________________________________________
activation_409 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 64, 24, 24)   4608        activation_409[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_410 (Activation)     (None, 64, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_410[0][0]             
__________________________________________________________________________________________________
concatenate_166 (Concatenate)   (None, 88, 24, 24)   0           concatenate_165[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 88, 24, 24)   352         concatenate_166[0][0]            
__________________________________________________________________________________________________
activation_411 (Activation)     (None, 88, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_39 (Gl (None, 88)           0           activation_411[0][0]             
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 1)            89          global_average_pooling2d_39[0][0]
==================================================================================================
Total params: 119,545
Trainable params: 117,113
Non-trainable params: 2,432
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 90s - loss: 0.5694 - acc: 0.7871 - val_loss: 0.6932 - val_acc: 0.7770

Epoch 00001: val_loss improved from inf to 0.69324, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 53s - loss: 0.4776 - acc: 0.8315 - val_loss: 0.6406 - val_acc: 0.7704

Epoch 00002: val_loss improved from 0.69324 to 0.64059, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 53s - loss: 0.4280 - acc: 0.8579 - val_loss: 0.7620 - val_acc: 0.7351

Epoch 00003: val_loss did not improve from 0.64059
Epoch 4/30
 - 53s - loss: 0.3856 - acc: 0.8805 - val_loss: 0.6110 - val_acc: 0.8086

Epoch 00004: val_loss improved from 0.64059 to 0.61105, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 53s - loss: 0.3549 - acc: 0.8933 - val_loss: 0.6128 - val_acc: 0.7703

Epoch 00005: val_loss did not improve from 0.61105
Epoch 6/30
 - 53s - loss: 0.3280 - acc: 0.9049 - val_loss: 0.6010 - val_acc: 0.7903

Epoch 00006: val_loss improved from 0.61105 to 0.60103, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 53s - loss: 0.3078 - acc: 0.9131 - val_loss: 0.9419 - val_acc: 0.7086

Epoch 00007: val_loss did not improve from 0.60103
Epoch 8/30
 - 53s - loss: 0.2836 - acc: 0.9254 - val_loss: 1.0353 - val_acc: 0.6696

Epoch 00008: val_loss did not improve from 0.60103
Epoch 9/30
 - 53s - loss: 0.2632 - acc: 0.9338 - val_loss: 0.6403 - val_acc: 0.7931

Epoch 00009: val_loss did not improve from 0.60103
Epoch 10/30
 - 53s - loss: 0.2526 - acc: 0.9382 - val_loss: 0.7096 - val_acc: 0.8261

Epoch 00010: val_loss did not improve from 0.60103
Epoch 11/30
 - 53s - loss: 0.2356 - acc: 0.9451 - val_loss: 0.6464 - val_acc: 0.7985

Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00011: val_loss did not improve from 0.60103
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 7s
 128/7440 [..............................] - ETA: 5s
 224/7440 [..............................] - ETA: 4s
 320/7440 [>.............................] - ETA: 4s
 416/7440 [>.............................] - ETA: 4s
 512/7440 [=>............................] - ETA: 4s
 608/7440 [=>............................] - ETA: 4s
 704/7440 [=>............................] - ETA: 4s
 800/7440 [==>...........................] - ETA: 4s
 896/7440 [==>...........................] - ETA: 4s
 992/7440 [===>..........................] - ETA: 3s
1088/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1280/7440 [====>.........................] - ETA: 3s
1376/7440 [====>.........................] - ETA: 3s
1472/7440 [====>.........................] - ETA: 3s
1568/7440 [=====>........................] - ETA: 3s
1664/7440 [=====>........................] - ETA: 3s
1760/7440 [======>.......................] - ETA: 3s
1856/7440 [======>.......................] - ETA: 3s
1952/7440 [======>.......................] - ETA: 3s
2048/7440 [=======>......................] - ETA: 3s
2144/7440 [=======>......................] - ETA: 3s
2240/7440 [========>.....................] - ETA: 3s
2336/7440 [========>.....................] - ETA: 3s
2432/7440 [========>.....................] - ETA: 3s
2528/7440 [=========>....................] - ETA: 2s
2624/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2816/7440 [==========>...................] - ETA: 2s
2912/7440 [==========>...................] - ETA: 2s
3008/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 2s
3296/7440 [============>.................] - ETA: 2s
3392/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 2s
3584/7440 [=============>................] - ETA: 2s
3680/7440 [=============>................] - ETA: 2s
3776/7440 [==============>...............] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
3968/7440 [===============>..............] - ETA: 2s
4064/7440 [===============>..............] - ETA: 2s
4160/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4928/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5696/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 603us/step
current Test accuracy: 0.798521505376344
current auc_score ------------------>  0.896029382009481

  32/7440 [..............................] - ETA: 59:19
  96/7440 [..............................] - ETA: 19:40
 192/7440 [..............................] - ETA: 9:44 
 288/7440 [>.............................] - ETA: 6:26
 384/7440 [>.............................] - ETA: 4:46
 480/7440 [>.............................] - ETA: 3:47
 576/7440 [=>............................] - ETA: 3:07
 672/7440 [=>............................] - ETA: 2:38
 768/7440 [==>...........................] - ETA: 2:17
 864/7440 [==>...........................] - ETA: 2:01
 960/7440 [==>...........................] - ETA: 1:47
1056/7440 [===>..........................] - ETA: 1:36
1152/7440 [===>..........................] - ETA: 1:27
1248/7440 [====>.........................] - ETA: 1:20
1344/7440 [====>.........................] - ETA: 1:13
1440/7440 [====>.........................] - ETA: 1:07
1536/7440 [=====>........................] - ETA: 1:02
1632/7440 [=====>........................] - ETA: 58s 
1728/7440 [=====>........................] - ETA: 54s
1824/7440 [======>.......................] - ETA: 50s
1920/7440 [======>.......................] - ETA: 47s
2016/7440 [=======>......................] - ETA: 44s
2112/7440 [=======>......................] - ETA: 42s
2208/7440 [=======>......................] - ETA: 39s
2304/7440 [========>.....................] - ETA: 37s
2400/7440 [========>.....................] - ETA: 35s
2496/7440 [=========>....................] - ETA: 33s
2592/7440 [=========>....................] - ETA: 31s
2688/7440 [=========>....................] - ETA: 30s
2784/7440 [==========>...................] - ETA: 28s
2880/7440 [==========>...................] - ETA: 27s
2976/7440 [===========>..................] - ETA: 25s
3072/7440 [===========>..................] - ETA: 24s
3168/7440 [===========>..................] - ETA: 23s
3264/7440 [============>.................] - ETA: 22s
3360/7440 [============>.................] - ETA: 21s
3456/7440 [============>.................] - ETA: 20s
3552/7440 [=============>................] - ETA: 19s
3648/7440 [=============>................] - ETA: 18s
3744/7440 [==============>...............] - ETA: 17s
3840/7440 [==============>...............] - ETA: 16s
3936/7440 [==============>...............] - ETA: 15s
4032/7440 [===============>..............] - ETA: 15s
4128/7440 [===============>..............] - ETA: 14s
4224/7440 [================>.............] - ETA: 13s
4320/7440 [================>.............] - ETA: 12s
4416/7440 [================>.............] - ETA: 12s
4512/7440 [=================>............] - ETA: 11s
4608/7440 [=================>............] - ETA: 11s
4704/7440 [=================>............] - ETA: 10s
4800/7440 [==================>...........] - ETA: 10s
4896/7440 [==================>...........] - ETA: 9s 
4992/7440 [===================>..........] - ETA: 9s
5088/7440 [===================>..........] - ETA: 8s
5184/7440 [===================>..........] - ETA: 8s
5280/7440 [====================>.........] - ETA: 7s
5376/7440 [====================>.........] - ETA: 7s
5472/7440 [=====================>........] - ETA: 6s
5568/7440 [=====================>........] - ETA: 6s
5664/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 5s
5856/7440 [======================>.......] - ETA: 5s
5952/7440 [=======================>......] - ETA: 4s
6048/7440 [=======================>......] - ETA: 4s
6144/7440 [=======================>......] - ETA: 4s
6240/7440 [========================>.....] - ETA: 3s
6336/7440 [========================>.....] - ETA: 3s
6432/7440 [========================>.....] - ETA: 3s
6528/7440 [=========================>....] - ETA: 2s
6624/7440 [=========================>....] - ETA: 2s
6720/7440 [==========================>...] - ETA: 2s
6816/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7008/7440 [===========================>..] - ETA: 1s
7104/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 20s 3ms/step
Best saved model Test accuracy: 0.7903225806451613
best saved model auc_score ------------------>  0.8714810960804716
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_40 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
final_bn (BatchNormalization (None, 16, 96, 96)        64        
_________________________________________________________________
activation_412 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
global_average_pooling2d_40  (None, 16)                0         
_________________________________________________________________
dense_40 (Dense)             (None, 1)                 17        
=================================================================
Total params: 369
Trainable params: 337
Non-trainable params: 32
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 43s - loss: 0.6737 - acc: 0.6096 - val_loss: 0.6470 - val_acc: 0.6808

Epoch 00001: val_loss improved from inf to 0.64702, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 9s - loss: 0.6484 - acc: 0.6757 - val_loss: 0.5958 - val_acc: 0.7884

Epoch 00002: val_loss improved from 0.64702 to 0.59581, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 9s - loss: 0.6347 - acc: 0.6814 - val_loss: 0.5768 - val_acc: 0.8112

Epoch 00003: val_loss improved from 0.59581 to 0.57681, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 9s - loss: 0.6284 - acc: 0.6849 - val_loss: 0.5670 - val_acc: 0.7915

Epoch 00004: val_loss improved from 0.57681 to 0.56705, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 9s - loss: 0.6246 - acc: 0.6842 - val_loss: 0.5478 - val_acc: 0.8129

Epoch 00005: val_loss improved from 0.56705 to 0.54779, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 9s - loss: 0.6216 - acc: 0.6839 - val_loss: 0.5543 - val_acc: 0.7965

Epoch 00006: val_loss did not improve from 0.54779
Epoch 7/30
 - 9s - loss: 0.6201 - acc: 0.6842 - val_loss: 0.5507 - val_acc: 0.7934

Epoch 00007: val_loss did not improve from 0.54779
Epoch 8/30
 - 9s - loss: 0.6184 - acc: 0.6862 - val_loss: 0.5397 - val_acc: 0.8027

Epoch 00008: val_loss improved from 0.54779 to 0.53975, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 9s - loss: 0.6171 - acc: 0.6849 - val_loss: 0.5422 - val_acc: 0.7929

Epoch 00009: val_loss did not improve from 0.53975
Epoch 10/30
 - 9s - loss: 0.6163 - acc: 0.6872 - val_loss: 0.5359 - val_acc: 0.8000

Epoch 00010: val_loss improved from 0.53975 to 0.53585, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 9s - loss: 0.6155 - acc: 0.6861 - val_loss: 0.5379 - val_acc: 0.7958

Epoch 00011: val_loss did not improve from 0.53585
Epoch 12/30
 - 9s - loss: 0.6146 - acc: 0.6865 - val_loss: 0.5323 - val_acc: 0.7997

Epoch 00012: val_loss improved from 0.53585 to 0.53231, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 13/30
 - 9s - loss: 0.6128 - acc: 0.6889 - val_loss: 0.5309 - val_acc: 0.7988

Epoch 00013: val_loss improved from 0.53231 to 0.53092, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 14/30
 - 9s - loss: 0.6124 - acc: 0.6885 - val_loss: 0.5301 - val_acc: 0.7996

Epoch 00014: val_loss improved from 0.53092 to 0.53009, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 15/30
 - 9s - loss: 0.6112 - acc: 0.6899 - val_loss: 0.5377 - val_acc: 0.7884

Epoch 00015: val_loss did not improve from 0.53009
Epoch 16/30
 - 9s - loss: 0.6095 - acc: 0.6907 - val_loss: 0.5196 - val_acc: 0.8019

Epoch 00016: val_loss improved from 0.53009 to 0.51960, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 17/30
 - 9s - loss: 0.6086 - acc: 0.6922 - val_loss: 0.5201 - val_acc: 0.8016

Epoch 00017: val_loss did not improve from 0.51960
Epoch 18/30
 - 9s - loss: 0.6075 - acc: 0.6922 - val_loss: 0.5215 - val_acc: 0.7996

Epoch 00018: val_loss did not improve from 0.51960
Epoch 19/30
 - 9s - loss: 0.6059 - acc: 0.6919 - val_loss: 0.5239 - val_acc: 0.8030

Epoch 00019: val_loss did not improve from 0.51960
Epoch 20/30
 - 9s - loss: 0.6044 - acc: 0.6950 - val_loss: 0.5171 - val_acc: 0.7993

Epoch 00020: val_loss improved from 0.51960 to 0.51713, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 21/30
 - 9s - loss: 0.6032 - acc: 0.6956 - val_loss: 0.5219 - val_acc: 0.7940

Epoch 00021: val_loss did not improve from 0.51713
Epoch 22/30
 - 9s - loss: 0.6021 - acc: 0.6973 - val_loss: 0.5350 - val_acc: 0.7863

Epoch 00022: val_loss did not improve from 0.51713
Epoch 23/30
 - 9s - loss: 0.6013 - acc: 0.6969 - val_loss: 0.5351 - val_acc: 0.7772

Epoch 00023: val_loss did not improve from 0.51713
Epoch 24/30
 - 9s - loss: 0.5995 - acc: 0.6986 - val_loss: 0.5370 - val_acc: 0.7776

Epoch 00024: val_loss did not improve from 0.51713
Epoch 25/30
 - 9s - loss: 0.5990 - acc: 0.6991 - val_loss: 0.5062 - val_acc: 0.8165

Epoch 00025: val_loss improved from 0.51713 to 0.50622, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 26/30
 - 9s - loss: 0.5967 - acc: 0.7016 - val_loss: 0.5082 - val_acc: 0.8032

Epoch 00026: val_loss did not improve from 0.50622
Epoch 27/30
 - 9s - loss: 0.5960 - acc: 0.7014 - val_loss: 0.5108 - val_acc: 0.8091

Epoch 00027: val_loss did not improve from 0.50622
Epoch 28/30
 - 9s - loss: 0.5950 - acc: 0.7020 - val_loss: 0.5124 - val_acc: 0.7961

Epoch 00028: val_loss did not improve from 0.50622
Epoch 29/30
 - 9s - loss: 0.5938 - acc: 0.7027 - val_loss: 0.5320 - val_acc: 0.7671

Epoch 00029: val_loss did not improve from 0.50622
Epoch 30/30
 - 9s - loss: 0.5918 - acc: 0.7069 - val_loss: 0.5334 - val_acc: 0.7719

Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00030: val_loss did not improve from 0.50622
Epoch 00030: early stopping

  32/7440 [..............................] - ETA: 5s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 640/7440 [=>............................] - ETA: 1s
 864/7440 [==>...........................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1984/7440 [=======>......................] - ETA: 1s
2208/7440 [=======>......................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2656/7440 [=========>....................] - ETA: 1s
2880/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 0s
4000/7440 [===============>..............] - ETA: 0s
4224/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5568/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 253us/step
current Test accuracy: 0.7719086021505376
current auc_score ------------------>  0.8666157937333796

  32/7440 [..............................] - ETA: 57:40
 192/7440 [..............................] - ETA: 9:26 
 416/7440 [>.............................] - ETA: 4:14
 640/7440 [=>............................] - ETA: 2:40
 864/7440 [==>...........................] - ETA: 1:55
1088/7440 [===>..........................] - ETA: 1:28
1312/7440 [====>.........................] - ETA: 1:11
1536/7440 [=====>........................] - ETA: 58s 
1760/7440 [======>.......................] - ETA: 49s
1984/7440 [=======>......................] - ETA: 42s
2208/7440 [=======>......................] - ETA: 36s
2432/7440 [========>.....................] - ETA: 32s
2656/7440 [=========>....................] - ETA: 28s
2880/7440 [==========>...................] - ETA: 24s
3104/7440 [===========>..................] - ETA: 21s
3328/7440 [============>.................] - ETA: 19s
3552/7440 [=============>................] - ETA: 17s
3776/7440 [==============>...............] - ETA: 15s
4000/7440 [===============>..............] - ETA: 13s
4224/7440 [================>.............] - ETA: 12s
4448/7440 [================>.............] - ETA: 10s
4672/7440 [=================>............] - ETA: 9s 
4896/7440 [==================>...........] - ETA: 8s
5120/7440 [===================>..........] - ETA: 7s
5344/7440 [====================>.........] - ETA: 6s
5568/7440 [=====================>........] - ETA: 5s
5792/7440 [======================>.......] - ETA: 4s
6016/7440 [=======================>......] - ETA: 3s
6240/7440 [========================>.....] - ETA: 3s
6464/7440 [=========================>....] - ETA: 2s
6688/7440 [=========================>....] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7136/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 17s 2ms/step
Best saved model Test accuracy: 0.8165322580645161
best saved model auc_score ------------------>  0.8582301422129727
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_41[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_413 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_413[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_414 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_414[0][0]             
__________________________________________________________________________________________________
concatenate_167 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 24, 96, 96)   96          concatenate_167[0][0]            
__________________________________________________________________________________________________
activation_415 (Activation)     (None, 24, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 12, 96, 96)   288         activation_415[0][0]             
__________________________________________________________________________________________________
average_pooling2d_41 (AveragePo (None, 12, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 12, 48, 48)   48          average_pooling2d_41[0][0]       
__________________________________________________________________________________________________
activation_416 (Activation)     (None, 12, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   384         activation_416[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_417 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_417[0][0]             
__________________________________________________________________________________________________
concatenate_168 (Concatenate)   (None, 20, 48, 48)   0           average_pooling2d_41[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 20, 48, 48)   80          concatenate_168[0][0]            
__________________________________________________________________________________________________
activation_418 (Activation)     (None, 20, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 10, 48, 48)   200         activation_418[0][0]             
__________________________________________________________________________________________________
average_pooling2d_42 (AveragePo (None, 10, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 10, 24, 24)   40          average_pooling2d_42[0][0]       
__________________________________________________________________________________________________
activation_419 (Activation)     (None, 10, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   320         activation_419[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_420 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_420[0][0]             
__________________________________________________________________________________________________
concatenate_169 (Concatenate)   (None, 18, 24, 24)   0           average_pooling2d_42[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 18, 24, 24)   72          concatenate_169[0][0]            
__________________________________________________________________________________________________
activation_421 (Activation)     (None, 18, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_41 (Gl (None, 18)           0           activation_421[0][0]             
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 1)            19          global_average_pooling2d_41[0][0]
==================================================================================================
Total params: 9,707
Trainable params: 9,315
Non-trainable params: 392
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 54s - loss: 0.5953 - acc: 0.7163 - val_loss: 0.4786 - val_acc: 0.8344

Epoch 00001: val_loss improved from inf to 0.47864, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 19s - loss: 0.4938 - acc: 0.7831 - val_loss: 0.4651 - val_acc: 0.8102

Epoch 00002: val_loss improved from 0.47864 to 0.46506, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 19s - loss: 0.4591 - acc: 0.7961 - val_loss: 0.4918 - val_acc: 0.7844

Epoch 00003: val_loss did not improve from 0.46506
Epoch 4/30
 - 19s - loss: 0.4394 - acc: 0.8037 - val_loss: 0.4544 - val_acc: 0.8149

Epoch 00004: val_loss improved from 0.46506 to 0.45439, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 19s - loss: 0.4262 - acc: 0.8083 - val_loss: 0.5448 - val_acc: 0.7683

Epoch 00005: val_loss did not improve from 0.45439
Epoch 6/30
 - 19s - loss: 0.4165 - acc: 0.8135 - val_loss: 0.4864 - val_acc: 0.7849

Epoch 00006: val_loss did not improve from 0.45439
Epoch 7/30
 - 19s - loss: 0.4076 - acc: 0.8180 - val_loss: 0.4840 - val_acc: 0.8043

Epoch 00007: val_loss did not improve from 0.45439
Epoch 8/30
 - 19s - loss: 0.3993 - acc: 0.8233 - val_loss: 0.4463 - val_acc: 0.8245

Epoch 00008: val_loss improved from 0.45439 to 0.44629, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 9/30
 - 19s - loss: 0.3947 - acc: 0.8273 - val_loss: 0.4918 - val_acc: 0.7913

Epoch 00009: val_loss did not improve from 0.44629
Epoch 10/30
 - 19s - loss: 0.3903 - acc: 0.8288 - val_loss: 0.5250 - val_acc: 0.7578

Epoch 00010: val_loss did not improve from 0.44629
Epoch 11/30
 - 19s - loss: 0.3851 - acc: 0.8311 - val_loss: 0.4814 - val_acc: 0.7910

Epoch 00011: val_loss did not improve from 0.44629
Epoch 12/30
 - 19s - loss: 0.3801 - acc: 0.8340 - val_loss: 0.5780 - val_acc: 0.8027

Epoch 00012: val_loss did not improve from 0.44629
Epoch 13/30
 - 19s - loss: 0.3760 - acc: 0.8370 - val_loss: 0.4864 - val_acc: 0.8129

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.44629
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 4s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2464/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 322us/step
current Test accuracy: 0.8129032258064516
current auc_score ------------------>  0.8894590415076886

  32/7440 [..............................] - ETA: 1:01:52
 160/7440 [..............................] - ETA: 12:12  
 320/7440 [>.............................] - ETA: 5:59 
 480/7440 [>.............................] - ETA: 3:54
 640/7440 [=>............................] - ETA: 2:52
 800/7440 [==>...........................] - ETA: 2:15
 960/7440 [==>...........................] - ETA: 1:50
1120/7440 [===>..........................] - ETA: 1:32
1280/7440 [====>.........................] - ETA: 1:19
1440/7440 [====>.........................] - ETA: 1:08
1600/7440 [=====>........................] - ETA: 1:00
1760/7440 [======>.......................] - ETA: 53s 
1920/7440 [======>.......................] - ETA: 47s
2080/7440 [=======>......................] - ETA: 43s
2240/7440 [========>.....................] - ETA: 38s
2400/7440 [========>.....................] - ETA: 35s
2560/7440 [=========>....................] - ETA: 32s
2720/7440 [=========>....................] - ETA: 29s
2880/7440 [==========>...................] - ETA: 26s
3040/7440 [===========>..................] - ETA: 24s
3200/7440 [===========>..................] - ETA: 22s
3360/7440 [============>.................] - ETA: 20s
3520/7440 [=============>................] - ETA: 19s
3680/7440 [=============>................] - ETA: 17s
3840/7440 [==============>...............] - ETA: 16s
4000/7440 [===============>..............] - ETA: 14s
4160/7440 [===============>..............] - ETA: 13s
4320/7440 [================>.............] - ETA: 12s
4480/7440 [=================>............] - ETA: 11s
4640/7440 [=================>............] - ETA: 10s
4800/7440 [==================>...........] - ETA: 9s 
4960/7440 [===================>..........] - ETA: 8s
5120/7440 [===================>..........] - ETA: 8s
5280/7440 [====================>.........] - ETA: 7s
5440/7440 [====================>.........] - ETA: 6s
5600/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 5s
5920/7440 [======================>.......] - ETA: 4s
6080/7440 [=======================>......] - ETA: 4s
6240/7440 [========================>.....] - ETA: 3s
6400/7440 [========================>.....] - ETA: 2s
6560/7440 [=========================>....] - ETA: 2s
6720/7440 [==========================>...] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 1s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 19s 2ms/step
Best saved model Test accuracy: 0.8244623655913978
best saved model auc_score ------------------>  0.8931658645508151
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_42 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_42[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_422 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_422[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_423 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_423[0][0]             
__________________________________________________________________________________________________
concatenate_170 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_170[0][0]            
__________________________________________________________________________________________________
activation_424 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_424[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_425 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_425[0][0]             
__________________________________________________________________________________________________
concatenate_171 (Concatenate)   (None, 40, 96, 96)   0           concatenate_170[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 40, 96, 96)   160         concatenate_171[0][0]            
__________________________________________________________________________________________________
activation_426 (Activation)     (None, 40, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_42 (Gl (None, 40)           0           activation_426[0][0]             
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 1)            41          global_average_pooling2d_42[0][0]
==================================================================================================
Total params: 13,529
Trainable params: 13,169
Non-trainable params: 360
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 60s - loss: 0.5981 - acc: 0.7230 - val_loss: 0.5111 - val_acc: 0.8069

Epoch 00001: val_loss improved from inf to 0.51108, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 25s - loss: 0.5502 - acc: 0.7505 - val_loss: 0.5731 - val_acc: 0.8070

Epoch 00002: val_loss did not improve from 0.51108
Epoch 3/30
 - 24s - loss: 0.5290 - acc: 0.7558 - val_loss: 0.5046 - val_acc: 0.8272

Epoch 00003: val_loss improved from 0.51108 to 0.50456, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 4/30
 - 24s - loss: 0.5104 - acc: 0.7665 - val_loss: 0.4877 - val_acc: 0.8097

Epoch 00004: val_loss improved from 0.50456 to 0.48767, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 5/30
 - 24s - loss: 0.4964 - acc: 0.7710 - val_loss: 0.5001 - val_acc: 0.8173

Epoch 00005: val_loss did not improve from 0.48767
Epoch 6/30
 - 24s - loss: 0.4858 - acc: 0.7738 - val_loss: 0.6050 - val_acc: 0.7390

Epoch 00006: val_loss did not improve from 0.48767
Epoch 7/30
 - 24s - loss: 0.4732 - acc: 0.7826 - val_loss: 0.4830 - val_acc: 0.8016

Epoch 00007: val_loss improved from 0.48767 to 0.48298, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 8/30
 - 24s - loss: 0.4662 - acc: 0.7851 - val_loss: 0.5534 - val_acc: 0.7425

Epoch 00008: val_loss did not improve from 0.48298
Epoch 9/30
 - 24s - loss: 0.4565 - acc: 0.7908 - val_loss: 0.4551 - val_acc: 0.8180

Epoch 00009: val_loss improved from 0.48298 to 0.45513, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 10/30
 - 24s - loss: 0.4509 - acc: 0.7933 - val_loss: 0.4932 - val_acc: 0.7981

Epoch 00010: val_loss did not improve from 0.45513
Epoch 11/30
 - 24s - loss: 0.4443 - acc: 0.7985 - val_loss: 0.4680 - val_acc: 0.7953

Epoch 00011: val_loss did not improve from 0.45513
Epoch 12/30
 - 24s - loss: 0.4408 - acc: 0.7981 - val_loss: 0.4466 - val_acc: 0.8254

Epoch 00012: val_loss improved from 0.45513 to 0.44656, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 13/30
 - 24s - loss: 0.4347 - acc: 0.8027 - val_loss: 0.4623 - val_acc: 0.7942

Epoch 00013: val_loss did not improve from 0.44656
Epoch 14/30
 - 24s - loss: 0.4290 - acc: 0.8064 - val_loss: 0.4896 - val_acc: 0.7757

Epoch 00014: val_loss did not improve from 0.44656
Epoch 15/30
 - 24s - loss: 0.4270 - acc: 0.8056 - val_loss: 0.4160 - val_acc: 0.8183

Epoch 00015: val_loss improved from 0.44656 to 0.41599, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 16/30
 - 24s - loss: 0.4246 - acc: 0.8065 - val_loss: 0.4915 - val_acc: 0.7767

Epoch 00016: val_loss did not improve from 0.41599
Epoch 17/30
 - 24s - loss: 0.4203 - acc: 0.8119 - val_loss: 0.4273 - val_acc: 0.8194

Epoch 00017: val_loss did not improve from 0.41599
Epoch 18/30
 - 24s - loss: 0.4158 - acc: 0.8120 - val_loss: 0.6258 - val_acc: 0.7484

Epoch 00018: val_loss did not improve from 0.41599
Epoch 19/30
 - 24s - loss: 0.4124 - acc: 0.8153 - val_loss: 0.4899 - val_acc: 0.8034

Epoch 00019: val_loss did not improve from 0.41599
Epoch 20/30
 - 24s - loss: 0.4100 - acc: 0.8151 - val_loss: 0.5307 - val_acc: 0.7638

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00020: val_loss did not improve from 0.41599
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 320/7440 [>.............................] - ETA: 3s
 480/7440 [>.............................] - ETA: 2s
 640/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1216/7440 [===>..........................] - ETA: 2s
1376/7440 [====>.........................] - ETA: 2s
1536/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1856/7440 [======>.......................] - ETA: 2s
2016/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2656/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3136/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3456/7440 [============>.................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
3936/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 1s
4576/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 387us/step
current Test accuracy: 0.7638440860215053
current auc_score ------------------>  0.8999258946120938

  32/7440 [..............................] - ETA: 59:37
 160/7440 [..............................] - ETA: 11:45
 288/7440 [>.............................] - ETA: 6:26 
 416/7440 [>.............................] - ETA: 4:23
 576/7440 [=>............................] - ETA: 3:06
 736/7440 [=>............................] - ETA: 2:23
 896/7440 [==>...........................] - ETA: 1:55
1056/7440 [===>..........................] - ETA: 1:35
1216/7440 [===>..........................] - ETA: 1:21
1344/7440 [====>.........................] - ETA: 1:12
1472/7440 [====>.........................] - ETA: 1:04
1632/7440 [=====>........................] - ETA: 57s 
1792/7440 [======>.......................] - ETA: 50s
1952/7440 [======>.......................] - ETA: 45s
2112/7440 [=======>......................] - ETA: 41s
2272/7440 [========>.....................] - ETA: 37s
2432/7440 [========>.....................] - ETA: 33s
2592/7440 [=========>....................] - ETA: 30s
2752/7440 [==========>...................] - ETA: 28s
2912/7440 [==========>...................] - ETA: 25s
3040/7440 [===========>..................] - ETA: 24s
3200/7440 [===========>..................] - ETA: 22s
3360/7440 [============>.................] - ETA: 20s
3488/7440 [=============>................] - ETA: 19s
3648/7440 [=============>................] - ETA: 17s
3808/7440 [==============>...............] - ETA: 16s
3968/7440 [===============>..............] - ETA: 14s
4128/7440 [===============>..............] - ETA: 13s
4288/7440 [================>.............] - ETA: 12s
4448/7440 [================>.............] - ETA: 11s
4608/7440 [=================>............] - ETA: 10s
4768/7440 [==================>...........] - ETA: 9s 
4928/7440 [==================>...........] - ETA: 8s
5056/7440 [===================>..........] - ETA: 8s
5216/7440 [====================>.........] - ETA: 7s
5376/7440 [====================>.........] - ETA: 6s
5536/7440 [=====================>........] - ETA: 6s
5696/7440 [=====================>........] - ETA: 5s
5856/7440 [======================>.......] - ETA: 4s
6016/7440 [=======================>......] - ETA: 4s
6176/7440 [=======================>......] - ETA: 3s
6336/7440 [========================>.....] - ETA: 3s
6496/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 2s
6816/7440 [==========================>...] - ETA: 1s
6976/7440 [===========================>..] - ETA: 1s
7136/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 18s 2ms/step
Best saved model Test accuracy: 0.8182795698924731
best saved model auc_score ------------------>  0.9027628555324315
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_43[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_427 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_427[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_428 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_428[0][0]             
__________________________________________________________________________________________________
concatenate_172 (Concatenate)   (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_172[0][0]            
__________________________________________________________________________________________________
activation_429 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_429[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_430 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_430[0][0]             
__________________________________________________________________________________________________
concatenate_173 (Concatenate)   (None, 40, 96, 96)   0           concatenate_172[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_173[0][0]            
__________________________________________________________________________________________________
activation_431 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_431[0][0]             
__________________________________________________________________________________________________
average_pooling2d_43 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_43[0][0]       
__________________________________________________________________________________________________
activation_432 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_432[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_433 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_433[0][0]             
__________________________________________________________________________________________________
concatenate_174 (Concatenate)   (None, 32, 48, 48)   0           average_pooling2d_43[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_174[0][0]            
__________________________________________________________________________________________________
activation_434 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_434[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_435 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_435[0][0]             
__________________________________________________________________________________________________
concatenate_175 (Concatenate)   (None, 44, 48, 48)   0           concatenate_174[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_175[0][0]            
__________________________________________________________________________________________________
activation_436 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_436[0][0]             
__________________________________________________________________________________________________
average_pooling2d_44 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_44[0][0]       
__________________________________________________________________________________________________
activation_437 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 48, 24, 24)   1056        activation_437[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_438 (Activation)     (None, 48, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_438[0][0]             
__________________________________________________________________________________________________
concatenate_176 (Concatenate)   (None, 34, 24, 24)   0           average_pooling2d_44[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 34, 24, 24)   136         concatenate_176[0][0]            
__________________________________________________________________________________________________
activation_439 (Activation)     (None, 34, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 48, 24, 24)   1632        activation_439[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 48, 24, 24)   192         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_440 (Activation)     (None, 48, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 12, 24, 24)   5184        activation_440[0][0]             
__________________________________________________________________________________________________
concatenate_177 (Concatenate)   (None, 46, 24, 24)   0           concatenate_176[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_177[0][0]            
__________________________________________________________________________________________________
activation_441 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_43 (Gl (None, 46)           0           activation_441[0][0]             
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 1)            47          global_average_pooling2d_43[0][0]
==================================================================================================
Total params: 42,783
Trainable params: 41,643
Non-trainable params: 1,140
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 71s - loss: 0.5435 - acc: 0.7723 - val_loss: 0.5322 - val_acc: 0.8020

Epoch 00001: val_loss improved from inf to 0.53220, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 2/30
 - 32s - loss: 0.4632 - acc: 0.8102 - val_loss: 0.5141 - val_acc: 0.7675

Epoch 00002: val_loss improved from 0.53220 to 0.51405, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 3/30
 - 33s - loss: 0.4295 - acc: 0.8268 - val_loss: 0.5198 - val_acc: 0.8051

Epoch 00003: val_loss did not improve from 0.51405
Epoch 4/30
 - 33s - loss: 0.4024 - acc: 0.8425 - val_loss: 0.5703 - val_acc: 0.7544

Epoch 00004: val_loss did not improve from 0.51405
Epoch 5/30
 - 33s - loss: 0.3769 - acc: 0.8581 - val_loss: 0.5128 - val_acc: 0.7879

Epoch 00005: val_loss improved from 0.51405 to 0.51280, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 6/30
 - 32s - loss: 0.3578 - acc: 0.8678 - val_loss: 0.4763 - val_acc: 0.8309

Epoch 00006: val_loss improved from 0.51280 to 0.47632, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 7/30
 - 32s - loss: 0.3394 - acc: 0.8783 - val_loss: 0.6165 - val_acc: 0.7962

Epoch 00007: val_loss did not improve from 0.47632
Epoch 8/30
 - 32s - loss: 0.3238 - acc: 0.8861 - val_loss: 0.6514 - val_acc: 0.7622

Epoch 00008: val_loss did not improve from 0.47632
Epoch 9/30
 - 32s - loss: 0.3108 - acc: 0.8918 - val_loss: 0.5803 - val_acc: 0.7552

Epoch 00009: val_loss did not improve from 0.47632
Epoch 10/30
 - 32s - loss: 0.2992 - acc: 0.8986 - val_loss: 0.4485 - val_acc: 0.8364

Epoch 00010: val_loss improved from 0.47632 to 0.44852, saving model to keras_densenet_simple_wt_28Sept_1352.h5
Epoch 11/30
 - 32s - loss: 0.2899 - acc: 0.9026 - val_loss: 0.4736 - val_acc: 0.8368

Epoch 00011: val_loss did not improve from 0.44852
Epoch 12/30
 - 32s - loss: 0.2782 - acc: 0.9081 - val_loss: 0.7844 - val_acc: 0.7688

Epoch 00012: val_loss did not improve from 0.44852
Epoch 13/30
 - 32s - loss: 0.2688 - acc: 0.9101 - val_loss: 0.5829 - val_acc: 0.8031

Epoch 00013: val_loss did not improve from 0.44852
Epoch 14/30
 - 32s - loss: 0.2626 - acc: 0.9152 - val_loss: 0.6892 - val_acc: 0.7770

Epoch 00014: val_loss did not improve from 0.44852
Epoch 15/30
 - 32s - loss: 0.2529 - acc: 0.9181 - val_loss: 0.5838 - val_acc: 0.8126

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.44852
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 426us/step
current Test accuracy: 0.8126344086021505
current auc_score ------------------>  0.8978650711064864

  32/7440 [..............................] - ETA: 1:06:41
 128/7440 [..............................] - ETA: 16:30  
 256/7440 [>.............................] - ETA: 8:08 
 384/7440 [>.............................] - ETA: 5:20
 512/7440 [=>............................] - ETA: 3:56
 640/7440 [=>............................] - ETA: 3:06
 768/7440 [==>...........................] - ETA: 2:33
 896/7440 [==>...........................] - ETA: 2:09
1024/7440 [===>..........................] - ETA: 1:51
1152/7440 [===>..........................] - ETA: 1:37
1280/7440 [====>.........................] - ETA: 1:25
1408/7440 [====>.........................] - ETA: 1:16
1536/7440 [=====>........................] - ETA: 1:08
1664/7440 [=====>........................] - ETA: 1:02
1792/7440 [======>.......................] - ETA: 56s 
1920/7440 [======>.......................] - ETA: 52s
2048/7440 [=======>......................] - ETA: 47s
2176/7440 [=======>......................] - ETA: 44s
2304/7440 [========>.....................] - ETA: 40s
2432/7440 [========>.....................] - ETA: 37s
2560/7440 [=========>....................] - ETA: 35s
2688/7440 [=========>....................] - ETA: 32s
2816/7440 [==========>...................] - ETA: 30s
2944/7440 [==========>...................] - ETA: 28s
3072/7440 [===========>..................] - ETA: 26s
3200/7440 [===========>..................] - ETA: 24s
3328/7440 [============>.................] - ETA: 23s
3456/7440 [============>.................] - ETA: 21s
3584/7440 [=============>................] - ETA: 20s
3712/7440 [=============>................] - ETA: 18s
3840/7440 [==============>...............] - ETA: 17s
3968/7440 [===============>..............] - ETA: 16s
4096/7440 [===============>..............] - ETA: 15s
4224/7440 [================>.............] - ETA: 14s
4352/7440 [================>.............] - ETA: 13s
4480/7440 [=================>............] - ETA: 12s
4608/7440 [=================>............] - ETA: 11s
4736/7440 [==================>...........] - ETA: 11s
4864/7440 [==================>...........] - ETA: 10s
4992/7440 [===================>..........] - ETA: 9s 
5120/7440 [===================>..........] - ETA: 8s
5248/7440 [====================>.........] - ETA: 8s
5376/7440 [====================>.........] - ETA: 7s
5504/7440 [=====================>........] - ETA: 6s
5632/7440 [=====================>........] - ETA: 6s
5760/7440 [======================>.......] - ETA: 5s
5888/7440 [======================>.......] - ETA: 5s
6016/7440 [=======================>......] - ETA: 4s
6144/7440 [=======================>......] - ETA: 4s
6272/7440 [========================>.....] - ETA: 3s
6400/7440 [========================>.....] - ETA: 3s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 2s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 1s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 20s 3ms/step
