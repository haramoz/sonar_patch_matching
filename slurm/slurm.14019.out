python hello-world.py
python hyperas_simple.py
python hyperas_contrastive_loss.py
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
python keras_densenet_simple.py
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import pickle
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'depth': hp.choice('depth', [7,10,13,16,19,22,25]),
        'nb_dense_block': hp.choice('nb_dense_block', [1,2,3]),
        'growth_rate': hp.choice('growth_rate', [6,8,10,12,14,16]),
    }

>>> Functions
  1: def process_data():
  2:     f = h5py.File('matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  3:     ln_training = 39840
  4:     X_train = f['X_train'].value
  5:     X_train_resize = X_train[0:ln_training,:,:,:]
  6:     #print(X_train_resize.shape)
  7:     y_train = f['y_train'].value
  8:     y_train_resize = y_train[0:ln_training,]
  9:     y_train_categorical = np_utils.to_categorical(y_train_resize, 2)
 10:     #X_reshaped = X_train_resize.reshape(*X_train_resize.shape[:1], -2)
 11:     #print(X_reshaped.shape)
 12:     ln_validation = 7440
 13:     X_val = f['X_val'].value
 14:     X_val_resize = X_val[0:ln_validation,:,:,:]
 15: 
 16:     #X_val_reshaped = X_val
 17:     #X_val_reshaped = X_val_resize.reshape(*X_val_resize.shape[:1], -2)
 18: 
 19:     y_val = f['y_val'].value
 20:     y_val_reshaped = y_val[0:ln_validation]
 21:     y_val_categorical = np_utils.to_categorical(y_val_reshaped, 2)
 22:     return X_train,y_train,X_val,y_val
 23: 
 24: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 30
   4:     #input_shape = (1,96,96)
   5:     es_patience = 5
   6:     lr_patience = 5
   7:     dropout = None
   8:     depth = space['depth']
   9:     nb_dense_block = space['nb_dense_block']
  10:     nb_filter = 16
  11:     growth_rate = space['growth_rate']
  12:     weight_decay = 1E-4
  13:     lr = 3E-4
  14:     weight_file = 'keras_densenet_simple_wt_28Sept_1340.h5'
  15:     
  16:     nb_classes = 1
  17:     img_dim = (2,96,96) 
  18:     n_channels = 2 
  19: 
  20:     
  21:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  22:                  growth_rate=growth_rate, nb_filter=nb_filter,
  23:                  dropout_rate=dropout,activation='sigmoid',
  24:                  input_shape=img_dim,include_top=True,
  25:                  bottleneck=True,reduction=0.5,
  26:                  classes=nb_classes,pooling='avg',
  27:                  weights=None)
  28:     
  29: 
  30:     model.summary()
  31:     opt = Adam(lr=lr)
  32:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  33: 
  34:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  35:     #es = EarlyStopping(monitor='val_acc', patience=es_patience,verbose=1,restore_best_weights=True)
  36:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  37: 
  38:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  39: 
  40:     model.fit(X_train,y_train,
  41:           batch_size=64,
  42:           epochs=epochs,
  43:           callbacks=[es,lr_reducer,checkpointer],
  44:           validation_data=(X_val,y_val),
  45:           verbose=2)
  46:     
  47:     score, acc = model.evaluate(X_val, y_val)
  48:     print('current Test accuracy:', acc)
  49:     pred = model.predict(X_val)
  50:     auc_score = roc_auc_score(y_val,pred)
  51:     print("current auc_score ------------------> ",auc_score)
  52: 
  53:     model = load_model(weight_file) #This is the best model
  54:     score, acc = model.evaluate(X_val, y_val)
  55:     print('Best saved model Test accuracy:', acc)
  56:     pred = model.predict(X_val)
  57:     auc_score = roc_auc_score(y_val,pred)
  58:     print("best saved model auc_score ------------------> ",auc_score)
  59: 
  60:     
  61:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  62: 
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_1 (Activation)    (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_1 (Average (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_2 (Activation)    (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_2 (Average (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_3 (Activation)    (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_1 ( (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 10s - loss: 0.6763 - acc: 0.6085 - val_loss: 0.6219 - val_acc: 0.7895

Epoch 00001: val_loss improved from inf to 0.62190, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 6s - loss: 0.6476 - acc: 0.6755 - val_loss: 0.5864 - val_acc: 0.7965

Epoch 00002: val_loss improved from 0.62190 to 0.58638, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 6s - loss: 0.6275 - acc: 0.6856 - val_loss: 0.5571 - val_acc: 0.7840

Epoch 00003: val_loss improved from 0.58638 to 0.55710, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 6s - loss: 0.6106 - acc: 0.6967 - val_loss: 0.5333 - val_acc: 0.7966

Epoch 00004: val_loss improved from 0.55710 to 0.53332, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 6s - loss: 0.5949 - acc: 0.7116 - val_loss: 0.5253 - val_acc: 0.8188

Epoch 00005: val_loss improved from 0.53332 to 0.52527, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 6s - loss: 0.5800 - acc: 0.7216 - val_loss: 0.5252 - val_acc: 0.7997

Epoch 00006: val_loss improved from 0.52527 to 0.52516, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 6s - loss: 0.5686 - acc: 0.7310 - val_loss: 0.5224 - val_acc: 0.8079

Epoch 00007: val_loss improved from 0.52516 to 0.52241, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 6s - loss: 0.5575 - acc: 0.7365 - val_loss: 0.5264 - val_acc: 0.8339

Epoch 00008: val_loss did not improve from 0.52241
Epoch 9/30
 - 6s - loss: 0.5479 - acc: 0.7432 - val_loss: 0.5261 - val_acc: 0.7789

Epoch 00009: val_loss did not improve from 0.52241
Epoch 10/30
 - 6s - loss: 0.5401 - acc: 0.7474 - val_loss: 0.5037 - val_acc: 0.8538

Epoch 00010: val_loss improved from 0.52241 to 0.50372, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 6s - loss: 0.5331 - acc: 0.7508 - val_loss: 0.5204 - val_acc: 0.8051

Epoch 00011: val_loss did not improve from 0.50372
Epoch 12/30
 - 6s - loss: 0.5265 - acc: 0.7535 - val_loss: 0.5462 - val_acc: 0.7875

Epoch 00012: val_loss did not improve from 0.50372
Epoch 13/30
 - 6s - loss: 0.5205 - acc: 0.7562 - val_loss: 0.7131 - val_acc: 0.7344

Epoch 00013: val_loss did not improve from 0.50372
Epoch 14/30
 - 6s - loss: 0.5164 - acc: 0.7575 - val_loss: 0.5338 - val_acc: 0.8058

Epoch 00014: val_loss did not improve from 0.50372
Epoch 15/30
 - 6s - loss: 0.5125 - acc: 0.7577 - val_loss: 0.5445 - val_acc: 0.8140

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.50372
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 0s
 704/7440 [=>............................] - ETA: 0s
1376/7440 [====>.........................] - ETA: 0s
2080/7440 [=======>......................] - ETA: 0s
2784/7440 [==========>...................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7440/7440 [==============================] - 1s 75us/step
current Test accuracy: 0.8139784946236559
current auc_score ------------------>  0.8573465140478669

  32/7440 [..............................] - ETA: 33s
 352/7440 [>.............................] - ETA: 4s 
 672/7440 [=>............................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 186us/step
Best saved model Test accuracy: 0.831989247311828
best saved model auc_score ------------------>  0.873588420626662
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_4[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_6[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 32, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 96, 96)   128         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 32)           0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            33          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 6,753
Trainable params: 6,481
Non-trainable params: 272
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 18s - loss: 0.6168 - acc: 0.7045 - val_loss: 0.5483 - val_acc: 0.7931

Epoch 00001: val_loss improved from inf to 0.54832, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 16s - loss: 0.5608 - acc: 0.7500 - val_loss: 0.5875 - val_acc: 0.7477

Epoch 00002: val_loss did not improve from 0.54832
Epoch 3/30
 - 16s - loss: 0.5339 - acc: 0.7579 - val_loss: 0.5250 - val_acc: 0.8142

Epoch 00003: val_loss improved from 0.54832 to 0.52497, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 16s - loss: 0.5190 - acc: 0.7601 - val_loss: 0.5183 - val_acc: 0.8095

Epoch 00004: val_loss improved from 0.52497 to 0.51830, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 16s - loss: 0.5099 - acc: 0.7623 - val_loss: 0.5133 - val_acc: 0.7898

Epoch 00005: val_loss improved from 0.51830 to 0.51333, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 16s - loss: 0.5042 - acc: 0.7631 - val_loss: 0.5185 - val_acc: 0.8105

Epoch 00006: val_loss did not improve from 0.51333
Epoch 7/30
 - 16s - loss: 0.4964 - acc: 0.7660 - val_loss: 0.4983 - val_acc: 0.8142

Epoch 00007: val_loss improved from 0.51333 to 0.49832, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 16s - loss: 0.4908 - acc: 0.7689 - val_loss: 0.4978 - val_acc: 0.8116

Epoch 00008: val_loss improved from 0.49832 to 0.49782, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 16s - loss: 0.4834 - acc: 0.7752 - val_loss: 0.4859 - val_acc: 0.8216

Epoch 00009: val_loss improved from 0.49782 to 0.48592, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 16s - loss: 0.4780 - acc: 0.7766 - val_loss: 0.4676 - val_acc: 0.8155

Epoch 00010: val_loss improved from 0.48592 to 0.46761, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 16s - loss: 0.4708 - acc: 0.7819 - val_loss: 0.5020 - val_acc: 0.8293

Epoch 00011: val_loss did not improve from 0.46761
Epoch 12/30
 - 16s - loss: 0.4646 - acc: 0.7846 - val_loss: 0.5395 - val_acc: 0.7870

Epoch 00012: val_loss did not improve from 0.46761
Epoch 13/30
 - 16s - loss: 0.4594 - acc: 0.7875 - val_loss: 0.5071 - val_acc: 0.7977

Epoch 00013: val_loss did not improve from 0.46761
Epoch 14/30
 - 16s - loss: 0.4564 - acc: 0.7891 - val_loss: 0.4768 - val_acc: 0.8293

Epoch 00014: val_loss did not improve from 0.46761
Epoch 15/30
 - 16s - loss: 0.4496 - acc: 0.7920 - val_loss: 0.4838 - val_acc: 0.8091

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.46761
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4192/7440 [===============>..............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 165us/step
current Test accuracy: 0.8091397849462365
current auc_score ------------------>  0.8683324661810613

  32/7440 [..............................] - ETA: 45s
 320/7440 [>.............................] - ETA: 5s 
 640/7440 [=>............................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 2s
1216/7440 [===>..........................] - ETA: 2s
1536/7440 [=====>........................] - ETA: 1s
1856/7440 [======>.......................] - ETA: 1s
2176/7440 [=======>......................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
3136/7440 [===========>..................] - ETA: 1s
3456/7440 [============>.................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 199us/step
Best saved model Test accuracy: 0.8154569892473118
best saved model auc_score ------------------>  0.8693138657648283
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_9[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_11[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_12[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 36, 96, 96)   0           concatenate_3[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_13[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 46, 96, 96)   0           concatenate_4[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_15[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_16[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 33, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_18[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 43, 48, 48)   0           concatenate_6[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_20[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 53, 48, 48)   0           concatenate_7[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 53, 48, 48)   212         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 53, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 53)           0           activation_22[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            54          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 32,144
Trainable params: 31,112
Non-trainable params: 1,032
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 37s - loss: 0.5508 - acc: 0.7648 - val_loss: 0.5093 - val_acc: 0.8172

Epoch 00001: val_loss improved from inf to 0.50933, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 34s - loss: 0.4794 - acc: 0.7973 - val_loss: 0.5060 - val_acc: 0.7938

Epoch 00002: val_loss improved from 0.50933 to 0.50602, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 34s - loss: 0.4506 - acc: 0.8074 - val_loss: 0.4684 - val_acc: 0.8030

Epoch 00003: val_loss improved from 0.50602 to 0.46844, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 34s - loss: 0.4324 - acc: 0.8177 - val_loss: 0.5079 - val_acc: 0.7844

Epoch 00004: val_loss did not improve from 0.46844
Epoch 5/30
 - 34s - loss: 0.4172 - acc: 0.8270 - val_loss: 0.4956 - val_acc: 0.8110

Epoch 00005: val_loss did not improve from 0.46844
Epoch 6/30
 - 34s - loss: 0.4014 - acc: 0.8339 - val_loss: 0.5030 - val_acc: 0.8194

Epoch 00006: val_loss did not improve from 0.46844
Epoch 7/30
 - 34s - loss: 0.3908 - acc: 0.8397 - val_loss: 0.5668 - val_acc: 0.7843

Epoch 00007: val_loss did not improve from 0.46844
Epoch 8/30
 - 34s - loss: 0.3781 - acc: 0.8468 - val_loss: 0.4737 - val_acc: 0.7813

Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00008: val_loss did not improve from 0.46844
Epoch 00008: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 384/7440 [>.............................] - ETA: 2s
 576/7440 [=>............................] - ETA: 2s
 768/7440 [==>...........................] - ETA: 2s
 960/7440 [==>...........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 1s
1344/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1728/7440 [=====>........................] - ETA: 1s
1920/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3136/7440 [===========>..................] - ETA: 1s
3328/7440 [============>.................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3904/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4288/7440 [================>.............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4864/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 310us/step
current Test accuracy: 0.7813172043010753
current auc_score ------------------>  0.8800302058041392

  32/7440 [..............................] - ETA: 1:55
 192/7440 [..............................] - ETA: 20s 
 352/7440 [>.............................] - ETA: 12s
 512/7440 [=>............................] - ETA: 8s 
 672/7440 [=>............................] - ETA: 7s
 832/7440 [==>...........................] - ETA: 5s
1024/7440 [===>..........................] - ETA: 5s
1184/7440 [===>..........................] - ETA: 4s
1376/7440 [====>.........................] - ETA: 4s
1568/7440 [=====>........................] - ETA: 3s
1760/7440 [======>.......................] - ETA: 3s
1952/7440 [======>.......................] - ETA: 3s
2144/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2528/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2912/7440 [==========>...................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5568/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 376us/step
Best saved model Test accuracy: 0.8029569892473118
best saved model auc_score ------------------>  0.8958453289397619
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_23[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_24[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_25[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 48, 96, 96)   0           concatenate_9[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 48, 96, 96)   192         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 64, 96, 96)   3072        activation_27[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 64, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_28[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 64, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 64, 96, 96)   256         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 64, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 32, 96, 96)   2048        activation_29[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 32, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 32, 48, 48)   128         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 32, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   2048        activation_30[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 48, 48, 48)   0           average_pooling2d_4[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 48, 48, 48)   192         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 48, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   3072        activation_32[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 64, 48, 48)   0           concatenate_12[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 64, 48, 48)   256         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 64, 48, 48)   4096        activation_34[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 64, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 80, 48, 48)   0           concatenate_13[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 80, 48, 48)   320         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 80, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 80)           0           activation_36[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            81          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 76,145
Trainable params: 74,609
Non-trainable params: 1,536
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 48s - loss: 0.5777 - acc: 0.7619 - val_loss: 0.5336 - val_acc: 0.8429

Epoch 00001: val_loss improved from inf to 0.53362, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 44s - loss: 0.4973 - acc: 0.7973 - val_loss: 0.4989 - val_acc: 0.8065

Epoch 00002: val_loss improved from 0.53362 to 0.49889, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 44s - loss: 0.4569 - acc: 0.8153 - val_loss: 0.5839 - val_acc: 0.7778

Epoch 00003: val_loss did not improve from 0.49889
Epoch 4/30
 - 44s - loss: 0.4326 - acc: 0.8277 - val_loss: 0.8841 - val_acc: 0.6931

Epoch 00004: val_loss did not improve from 0.49889
Epoch 5/30
 - 44s - loss: 0.4124 - acc: 0.8410 - val_loss: 0.5053 - val_acc: 0.8046

Epoch 00005: val_loss did not improve from 0.49889
Epoch 6/30
 - 44s - loss: 0.3983 - acc: 0.8459 - val_loss: 0.4776 - val_acc: 0.8003

Epoch 00006: val_loss improved from 0.49889 to 0.47758, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 44s - loss: 0.3817 - acc: 0.8558 - val_loss: 0.5907 - val_acc: 0.7801

Epoch 00007: val_loss did not improve from 0.47758
Epoch 8/30
 - 44s - loss: 0.3690 - acc: 0.8627 - val_loss: 0.4979 - val_acc: 0.8138

Epoch 00008: val_loss did not improve from 0.47758
Epoch 9/30
 - 44s - loss: 0.3595 - acc: 0.8665 - val_loss: 0.7813 - val_acc: 0.6950

Epoch 00009: val_loss did not improve from 0.47758
Epoch 10/30
 - 44s - loss: 0.3482 - acc: 0.8731 - val_loss: 0.4875 - val_acc: 0.8220

Epoch 00010: val_loss did not improve from 0.47758
Epoch 11/30
 - 44s - loss: 0.3363 - acc: 0.8786 - val_loss: 0.6568 - val_acc: 0.7534

Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00011: val_loss did not improve from 0.47758
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 3s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 544/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 405us/step
current Test accuracy: 0.7533602150537635
current auc_score ------------------>  0.8539933807376574

  32/7440 [..............................] - ETA: 2:51
 160/7440 [..............................] - ETA: 36s 
 288/7440 [>.............................] - ETA: 21s
 416/7440 [>.............................] - ETA: 15s
 544/7440 [=>............................] - ETA: 12s
 672/7440 [=>............................] - ETA: 10s
 800/7440 [==>...........................] - ETA: 8s 
 928/7440 [==>...........................] - ETA: 7s
1056/7440 [===>..........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 6s
1312/7440 [====>.........................] - ETA: 5s
1440/7440 [====>.........................] - ETA: 5s
1600/7440 [=====>........................] - ETA: 4s
1728/7440 [=====>........................] - ETA: 4s
1856/7440 [======>.......................] - ETA: 4s
1984/7440 [=======>......................] - ETA: 4s
2112/7440 [=======>......................] - ETA: 3s
2240/7440 [========>.....................] - ETA: 3s
2368/7440 [========>.....................] - ETA: 3s
2496/7440 [=========>....................] - ETA: 3s
2624/7440 [=========>....................] - ETA: 3s
2752/7440 [==========>...................] - ETA: 3s
2880/7440 [==========>...................] - ETA: 2s
3008/7440 [===========>..................] - ETA: 2s
3136/7440 [===========>..................] - ETA: 2s
3264/7440 [============>.................] - ETA: 2s
3392/7440 [============>.................] - ETA: 2s
3520/7440 [=============>................] - ETA: 2s
3648/7440 [=============>................] - ETA: 2s
3808/7440 [==============>...............] - ETA: 2s
3936/7440 [==============>...............] - ETA: 2s
4096/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4416/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 1s
4704/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 1s
5088/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 493us/step
Best saved model Test accuracy: 0.8002688172043011
best saved model auc_score ------------------>  0.8976694198751303
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_37[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_39[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_40[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 40, 96, 96)   0           concatenate_15[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_41[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_42[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 52, 96, 96)   0           concatenate_16[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 52, 96, 96)   208         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 52, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 52)           0           activation_43[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            53          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 21,045
Trainable params: 20,485
Non-trainable params: 560
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 31s - loss: 0.5817 - acc: 0.7406 - val_loss: 0.5243 - val_acc: 0.8372

Epoch 00001: val_loss improved from inf to 0.52433, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 28s - loss: 0.5209 - acc: 0.7646 - val_loss: 0.4780 - val_acc: 0.8106

Epoch 00002: val_loss improved from 0.52433 to 0.47805, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 28s - loss: 0.4937 - acc: 0.7729 - val_loss: 0.4732 - val_acc: 0.8220

Epoch 00003: val_loss improved from 0.47805 to 0.47317, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 28s - loss: 0.4766 - acc: 0.7838 - val_loss: 0.4570 - val_acc: 0.8328

Epoch 00004: val_loss improved from 0.47317 to 0.45704, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 28s - loss: 0.4613 - acc: 0.7911 - val_loss: 0.4704 - val_acc: 0.8270

Epoch 00005: val_loss did not improve from 0.45704
Epoch 6/30
 - 28s - loss: 0.4503 - acc: 0.7963 - val_loss: 0.5734 - val_acc: 0.7461

Epoch 00006: val_loss did not improve from 0.45704
Epoch 7/30
 - 28s - loss: 0.4401 - acc: 0.8012 - val_loss: 0.4328 - val_acc: 0.8254

Epoch 00007: val_loss improved from 0.45704 to 0.43281, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 28s - loss: 0.4315 - acc: 0.8077 - val_loss: 0.4194 - val_acc: 0.8376

Epoch 00008: val_loss improved from 0.43281 to 0.41941, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 28s - loss: 0.4246 - acc: 0.8117 - val_loss: 0.4599 - val_acc: 0.8086

Epoch 00009: val_loss did not improve from 0.41941
Epoch 10/30
 - 28s - loss: 0.4186 - acc: 0.8127 - val_loss: 0.4648 - val_acc: 0.7926

Epoch 00010: val_loss did not improve from 0.41941
Epoch 11/30
 - 28s - loss: 0.4118 - acc: 0.8176 - val_loss: 0.4199 - val_acc: 0.8254

Epoch 00011: val_loss did not improve from 0.41941
Epoch 12/30
 - 28s - loss: 0.4068 - acc: 0.8196 - val_loss: 0.4358 - val_acc: 0.8161

Epoch 00012: val_loss did not improve from 0.41941
Epoch 13/30
 - 28s - loss: 0.4012 - acc: 0.8224 - val_loss: 0.4101 - val_acc: 0.8227

Epoch 00013: val_loss improved from 0.41941 to 0.41006, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 14/30
 - 28s - loss: 0.3949 - acc: 0.8270 - val_loss: 0.4308 - val_acc: 0.8249

Epoch 00014: val_loss did not improve from 0.41006
Epoch 15/30
 - 28s - loss: 0.3906 - acc: 0.8280 - val_loss: 0.4360 - val_acc: 0.8235

Epoch 00015: val_loss did not improve from 0.41006
Epoch 16/30
 - 28s - loss: 0.3865 - acc: 0.8312 - val_loss: 0.3820 - val_acc: 0.8523

Epoch 00016: val_loss improved from 0.41006 to 0.38203, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 17/30
 - 28s - loss: 0.3828 - acc: 0.8304 - val_loss: 0.4558 - val_acc: 0.8121

Epoch 00017: val_loss did not improve from 0.38203
Epoch 18/30
 - 28s - loss: 0.3790 - acc: 0.8356 - val_loss: 0.4935 - val_acc: 0.8056

Epoch 00018: val_loss did not improve from 0.38203
Epoch 19/30
 - 28s - loss: 0.3761 - acc: 0.8392 - val_loss: 0.4176 - val_acc: 0.8254

Epoch 00019: val_loss did not improve from 0.38203
Epoch 20/30
 - 28s - loss: 0.3710 - acc: 0.8405 - val_loss: 0.4553 - val_acc: 0.8177

Epoch 00020: val_loss did not improve from 0.38203
Epoch 21/30
 - 28s - loss: 0.3692 - acc: 0.8415 - val_loss: 0.4210 - val_acc: 0.8228

Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00021: val_loss did not improve from 0.38203
Epoch 00021: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 285us/step
current Test accuracy: 0.8228494623655914
current auc_score ------------------>  0.9104493294022431

  32/7440 [..............................] - ETA: 2:57
 224/7440 [..............................] - ETA: 26s 
 416/7440 [>.............................] - ETA: 14s
 608/7440 [=>............................] - ETA: 10s
 800/7440 [==>...........................] - ETA: 8s 
 992/7440 [===>..........................] - ETA: 6s
1184/7440 [===>..........................] - ETA: 5s
1376/7440 [====>.........................] - ETA: 5s
1568/7440 [=====>........................] - ETA: 4s
1760/7440 [======>.......................] - ETA: 4s
1952/7440 [======>.......................] - ETA: 3s
2144/7440 [=======>......................] - ETA: 3s
2336/7440 [========>.....................] - ETA: 3s
2528/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2912/7440 [==========>...................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3296/7440 [============>.................] - ETA: 2s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4448/7440 [================>.............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 386us/step
Best saved model Test accuracy: 0.8522849462365591
best saved model auc_score ------------------>  0.9266712192160944
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_44[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_45[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_46[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_47[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 36, 96, 96)   0           concatenate_18[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_48[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_49[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 46, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_50[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_51[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_52[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 33, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_53[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_54[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 43, 48, 48)   0           concatenate_21[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_55[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_56[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 53, 48, 48)   0           concatenate_22[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 53, 48, 48)   212         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 53, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 26, 48, 48)   1378        activation_57[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 26, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 26, 24, 24)   104         average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 26, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   1040        activation_58[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_59[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 36, 24, 24)   0           average_pooling2d_6[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 36, 24, 24)   144         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 36, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 40, 24, 24)   1440        activation_60[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 40, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_61[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 46, 24, 24)   0           concatenate_24[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 46, 24, 24)   184         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 46, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 40, 24, 24)   1840        activation_62[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 40, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_63[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 56, 24, 24)   0           concatenate_25[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 56, 24, 24)   224         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 56, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 56)           0           activation_64[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            57          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 49,781
Trainable params: 48,181
Non-trainable params: 1,600
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 42s - loss: 0.5616 - acc: 0.7702 - val_loss: 0.6238 - val_acc: 0.7250

Epoch 00001: val_loss improved from inf to 0.62376, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 37s - loss: 0.4693 - acc: 0.8164 - val_loss: 0.5658 - val_acc: 0.7727

Epoch 00002: val_loss improved from 0.62376 to 0.56580, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 37s - loss: 0.4243 - acc: 0.8391 - val_loss: 0.5045 - val_acc: 0.8163

Epoch 00003: val_loss improved from 0.56580 to 0.50454, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 37s - loss: 0.3920 - acc: 0.8563 - val_loss: 0.5654 - val_acc: 0.8044

Epoch 00004: val_loss did not improve from 0.50454
Epoch 5/30
 - 37s - loss: 0.3651 - acc: 0.8702 - val_loss: 0.5028 - val_acc: 0.7923

Epoch 00005: val_loss improved from 0.50454 to 0.50278, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 37s - loss: 0.3435 - acc: 0.8817 - val_loss: 0.6758 - val_acc: 0.7495

Epoch 00006: val_loss did not improve from 0.50278
Epoch 7/30
 - 37s - loss: 0.3272 - acc: 0.8892 - val_loss: 0.6866 - val_acc: 0.7511

Epoch 00007: val_loss did not improve from 0.50278
Epoch 8/30
 - 37s - loss: 0.3079 - acc: 0.8995 - val_loss: 0.6363 - val_acc: 0.7949

Epoch 00008: val_loss did not improve from 0.50278
Epoch 9/30
 - 37s - loss: 0.2949 - acc: 0.9038 - val_loss: 0.7142 - val_acc: 0.7638

Epoch 00009: val_loss did not improve from 0.50278
Epoch 10/30
 - 37s - loss: 0.2789 - acc: 0.9129 - val_loss: 0.6044 - val_acc: 0.7985

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.50278
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 2s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 358us/step
current Test accuracy: 0.798521505376344
current auc_score ------------------>  0.8823730344548504

  32/7440 [..............................] - ETA: 4:48
 192/7440 [..............................] - ETA: 49s 
 352/7440 [>.............................] - ETA: 27s
 512/7440 [=>............................] - ETA: 19s
 672/7440 [=>............................] - ETA: 14s
 832/7440 [==>...........................] - ETA: 12s
 992/7440 [===>..........................] - ETA: 10s
1152/7440 [===>..........................] - ETA: 9s 
1312/7440 [====>.........................] - ETA: 7s
1472/7440 [====>.........................] - ETA: 7s
1632/7440 [=====>........................] - ETA: 6s
1792/7440 [======>.......................] - ETA: 5s
1952/7440 [======>.......................] - ETA: 5s
2112/7440 [=======>......................] - ETA: 5s
2272/7440 [========>.....................] - ETA: 4s
2432/7440 [========>.....................] - ETA: 4s
2592/7440 [=========>....................] - ETA: 4s
2752/7440 [==========>...................] - ETA: 3s
2912/7440 [==========>...................] - ETA: 3s
3072/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 2s
3552/7440 [=============>................] - ETA: 2s
3712/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 523us/step
Best saved model Test accuracy: 0.7923387096774194
best saved model auc_score ------------------>  0.8882807260955023
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_65[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_66[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_67[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_68[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 28, 96, 96)   0           concatenate_27[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_69[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_70[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 34, 96, 96)   0           concatenate_28[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 34, 96, 96)   136         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 34, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 34)           0           activation_71[0][0]              
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            35          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 6,483
Trainable params: 6,139
Non-trainable params: 344
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 25s - loss: 0.6297 - acc: 0.6845 - val_loss: 0.5413 - val_acc: 0.7606

Epoch 00001: val_loss improved from inf to 0.54131, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 21s - loss: 0.5612 - acc: 0.7511 - val_loss: 0.4895 - val_acc: 0.8391

Epoch 00002: val_loss improved from 0.54131 to 0.48953, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 21s - loss: 0.5259 - acc: 0.7607 - val_loss: 0.5063 - val_acc: 0.8246

Epoch 00003: val_loss did not improve from 0.48953
Epoch 4/30
 - 21s - loss: 0.5062 - acc: 0.7660 - val_loss: 0.5070 - val_acc: 0.7891

Epoch 00004: val_loss did not improve from 0.48953
Epoch 5/30
 - 21s - loss: 0.4956 - acc: 0.7656 - val_loss: 0.5275 - val_acc: 0.8075

Epoch 00005: val_loss did not improve from 0.48953
Epoch 6/30
 - 21s - loss: 0.4852 - acc: 0.7728 - val_loss: 0.6022 - val_acc: 0.7910

Epoch 00006: val_loss did not improve from 0.48953
Epoch 7/30
 - 21s - loss: 0.4774 - acc: 0.7754 - val_loss: 0.4706 - val_acc: 0.7931

Epoch 00007: val_loss improved from 0.48953 to 0.47059, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 21s - loss: 0.4709 - acc: 0.7777 - val_loss: 0.4803 - val_acc: 0.8034

Epoch 00008: val_loss did not improve from 0.47059
Epoch 9/30
 - 21s - loss: 0.4636 - acc: 0.7830 - val_loss: 0.4450 - val_acc: 0.8230

Epoch 00009: val_loss improved from 0.47059 to 0.44498, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 21s - loss: 0.4598 - acc: 0.7839 - val_loss: 0.4723 - val_acc: 0.8024

Epoch 00010: val_loss did not improve from 0.44498
Epoch 11/30
 - 21s - loss: 0.4523 - acc: 0.7900 - val_loss: 0.4631 - val_acc: 0.8235

Epoch 00011: val_loss did not improve from 0.44498
Epoch 12/30
 - 21s - loss: 0.4491 - acc: 0.7902 - val_loss: 0.4517 - val_acc: 0.7949

Epoch 00012: val_loss did not improve from 0.44498
Epoch 13/30
 - 21s - loss: 0.4435 - acc: 0.7925 - val_loss: 0.4419 - val_acc: 0.7987

Epoch 00013: val_loss improved from 0.44498 to 0.44194, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 14/30
 - 21s - loss: 0.4393 - acc: 0.7951 - val_loss: 0.4855 - val_acc: 0.7910

Epoch 00014: val_loss did not improve from 0.44194
Epoch 15/30
 - 21s - loss: 0.4358 - acc: 0.7987 - val_loss: 0.5522 - val_acc: 0.7797

Epoch 00015: val_loss did not improve from 0.44194
Epoch 16/30
 - 21s - loss: 0.4294 - acc: 0.8005 - val_loss: 0.4404 - val_acc: 0.8077

Epoch 00016: val_loss improved from 0.44194 to 0.44042, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 17/30
 - 21s - loss: 0.4253 - acc: 0.8030 - val_loss: 0.4195 - val_acc: 0.8157

Epoch 00017: val_loss improved from 0.44042 to 0.41947, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 18/30
 - 21s - loss: 0.4230 - acc: 0.8064 - val_loss: 0.4014 - val_acc: 0.8309

Epoch 00018: val_loss improved from 0.41947 to 0.40142, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 19/30
 - 21s - loss: 0.4205 - acc: 0.8065 - val_loss: 0.4237 - val_acc: 0.8020

Epoch 00019: val_loss did not improve from 0.40142
Epoch 20/30
 - 21s - loss: 0.4163 - acc: 0.8081 - val_loss: 0.4339 - val_acc: 0.8098

Epoch 00020: val_loss did not improve from 0.40142
Epoch 21/30
 - 21s - loss: 0.4119 - acc: 0.8108 - val_loss: 0.5108 - val_acc: 0.7903

Epoch 00021: val_loss did not improve from 0.40142
Epoch 22/30
 - 21s - loss: 0.4109 - acc: 0.8113 - val_loss: 0.6348 - val_acc: 0.7543

Epoch 00022: val_loss did not improve from 0.40142
Epoch 23/30
 - 21s - loss: 0.4077 - acc: 0.8133 - val_loss: 0.4400 - val_acc: 0.8042

Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00023: val_loss did not improve from 0.40142
Epoch 00023: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 0s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 230us/step
current Test accuracy: 0.8041666666666667
current auc_score ------------------>  0.8911202884726558

  32/7440 [..............................] - ETA: 4:55
 256/7440 [>.............................] - ETA: 37s 
 480/7440 [>.............................] - ETA: 20s
 704/7440 [=>............................] - ETA: 13s
 928/7440 [==>...........................] - ETA: 10s
1152/7440 [===>..........................] - ETA: 8s 
1376/7440 [====>.........................] - ETA: 7s
1600/7440 [=====>........................] - ETA: 6s
1824/7440 [======>.......................] - ETA: 5s
2048/7440 [=======>......................] - ETA: 4s
2272/7440 [========>.....................] - ETA: 4s
2496/7440 [=========>....................] - ETA: 3s
2720/7440 [=========>....................] - ETA: 3s
2944/7440 [==========>...................] - ETA: 2s
3168/7440 [===========>..................] - ETA: 2s
3392/7440 [============>.................] - ETA: 2s
3648/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4320/7440 [================>.............] - ETA: 1s
4544/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5216/7440 [====================>.........] - ETA: 1s
5440/7440 [====================>.........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 399us/step
Best saved model Test accuracy: 0.8309139784946237
best saved model auc_score ------------------>  0.9067685570586196
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_72[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_73[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_74[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_75[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 48, 96, 96)   0           concatenate_30[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 48, 96, 96)   192         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 48, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 24, 96, 96)   1152        activation_76[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 24, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 24, 48, 48)   96          average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 24, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1536        activation_77[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_78[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 40, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 40, 48, 48)   160         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 40, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_79[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_80[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 56, 48, 48)   0           concatenate_32[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 56, 48, 48)   224         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 56, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 28, 48, 48)   1568        activation_81[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 28, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 28, 24, 24)   112         average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 28, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   1792        activation_82[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_83[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 44, 24, 24)   0           average_pooling2d_8[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 44, 24, 24)   176         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 44, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_84[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_85[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 60, 24, 24)   0           concatenate_34[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 60, 24, 24)   240         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 60, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 60)           0           activation_86[0][0]              
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            61          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 73,069
Trainable params: 71,605
Non-trainable params: 1,464
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 38s - loss: 0.5526 - acc: 0.7786 - val_loss: 0.5788 - val_acc: 0.7769

Epoch 00001: val_loss improved from inf to 0.57878, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 33s - loss: 0.4667 - acc: 0.8178 - val_loss: 0.8178 - val_acc: 0.7204

Epoch 00002: val_loss did not improve from 0.57878
Epoch 3/30
 - 33s - loss: 0.4214 - acc: 0.8418 - val_loss: 0.5881 - val_acc: 0.7728

Epoch 00003: val_loss did not improve from 0.57878
Epoch 4/30
 - 33s - loss: 0.3894 - acc: 0.8566 - val_loss: 0.5188 - val_acc: 0.7941

Epoch 00004: val_loss improved from 0.57878 to 0.51877, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 33s - loss: 0.3658 - acc: 0.8721 - val_loss: 0.7104 - val_acc: 0.7503

Epoch 00005: val_loss did not improve from 0.51877
Epoch 6/30
 - 33s - loss: 0.3413 - acc: 0.8846 - val_loss: 0.6878 - val_acc: 0.7735

Epoch 00006: val_loss did not improve from 0.51877
Epoch 7/30
 - 33s - loss: 0.3211 - acc: 0.8926 - val_loss: 0.9094 - val_acc: 0.6825

Epoch 00007: val_loss did not improve from 0.51877
Epoch 8/30
 - 33s - loss: 0.3019 - acc: 0.9027 - val_loss: 0.5177 - val_acc: 0.8249

Epoch 00008: val_loss improved from 0.51877 to 0.51770, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 33s - loss: 0.2911 - acc: 0.9072 - val_loss: 0.5642 - val_acc: 0.8215

Epoch 00009: val_loss did not improve from 0.51770
Epoch 10/30
 - 33s - loss: 0.2759 - acc: 0.9143 - val_loss: 0.6166 - val_acc: 0.7781

Epoch 00010: val_loss did not improve from 0.51770
Epoch 11/30
 - 33s - loss: 0.2634 - acc: 0.9199 - val_loss: 0.6870 - val_acc: 0.7801

Epoch 00011: val_loss did not improve from 0.51770
Epoch 12/30
 - 33s - loss: 0.2554 - acc: 0.9233 - val_loss: 0.9135 - val_acc: 0.7216

Epoch 00012: val_loss did not improve from 0.51770
Epoch 13/30
 - 33s - loss: 0.2392 - acc: 0.9309 - val_loss: 0.6251 - val_acc: 0.7694

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.51770
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 330us/step
current Test accuracy: 0.7693548387096775
current auc_score ------------------>  0.8785288761706556

  32/7440 [..............................] - ETA: 6:09
 192/7440 [..............................] - ETA: 1:02
 352/7440 [>.............................] - ETA: 34s 
 512/7440 [=>............................] - ETA: 23s
 672/7440 [=>............................] - ETA: 18s
 832/7440 [==>...........................] - ETA: 14s
 992/7440 [===>..........................] - ETA: 12s
1152/7440 [===>..........................] - ETA: 10s
1312/7440 [====>.........................] - ETA: 9s 
1472/7440 [====>.........................] - ETA: 8s
1632/7440 [=====>........................] - ETA: 7s
1792/7440 [======>.......................] - ETA: 6s
1952/7440 [======>.......................] - ETA: 6s
2112/7440 [=======>......................] - ETA: 5s
2272/7440 [========>.....................] - ETA: 5s
2432/7440 [========>.....................] - ETA: 4s
2592/7440 [=========>....................] - ETA: 4s
2752/7440 [==========>...................] - ETA: 4s
2912/7440 [==========>...................] - ETA: 3s
3072/7440 [===========>..................] - ETA: 3s
3232/7440 [============>.................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3552/7440 [=============>................] - ETA: 3s
3712/7440 [=============>................] - ETA: 2s
3872/7440 [==============>...............] - ETA: 2s
4032/7440 [===============>..............] - ETA: 2s
4192/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 1s
4832/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 543us/step
Best saved model Test accuracy: 0.8248655913978494
best saved model auc_score ------------------>  0.9064028355879292
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_87[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_89[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_90[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 36, 96, 96)   0           concatenate_36[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_91[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_92[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 46, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 96, 96)   184         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 46, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 46)           0           activation_93[0][0]              
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            47          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 15,231
Trainable params: 14,743
Non-trainable params: 488
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.5914 - acc: 0.7298 - val_loss: 0.5570 - val_acc: 0.7789

Epoch 00001: val_loss improved from inf to 0.55703, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 27s - loss: 0.5231 - acc: 0.7665 - val_loss: 0.4999 - val_acc: 0.8060

Epoch 00002: val_loss improved from 0.55703 to 0.49991, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 27s - loss: 0.5000 - acc: 0.7725 - val_loss: 0.5078 - val_acc: 0.7797

Epoch 00003: val_loss did not improve from 0.49991
Epoch 4/30
 - 27s - loss: 0.4841 - acc: 0.7771 - val_loss: 0.5032 - val_acc: 0.8247

Epoch 00004: val_loss did not improve from 0.49991
Epoch 5/30
 - 27s - loss: 0.4700 - acc: 0.7831 - val_loss: 0.4726 - val_acc: 0.8129

Epoch 00005: val_loss improved from 0.49991 to 0.47259, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 27s - loss: 0.4588 - acc: 0.7887 - val_loss: 0.4687 - val_acc: 0.8226

Epoch 00006: val_loss improved from 0.47259 to 0.46866, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 27s - loss: 0.4459 - acc: 0.7965 - val_loss: 0.5041 - val_acc: 0.7684

Epoch 00007: val_loss did not improve from 0.46866
Epoch 8/30
 - 27s - loss: 0.4409 - acc: 0.7991 - val_loss: 0.4883 - val_acc: 0.8103

Epoch 00008: val_loss did not improve from 0.46866
Epoch 9/30
 - 27s - loss: 0.4325 - acc: 0.8040 - val_loss: 0.4458 - val_acc: 0.8265

Epoch 00009: val_loss improved from 0.46866 to 0.44582, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 28s - loss: 0.4260 - acc: 0.8062 - val_loss: 0.4848 - val_acc: 0.8433

Epoch 00010: val_loss did not improve from 0.44582
Epoch 11/30
 - 27s - loss: 0.4213 - acc: 0.8094 - val_loss: 0.5472 - val_acc: 0.7730

Epoch 00011: val_loss did not improve from 0.44582
Epoch 12/30
 - 27s - loss: 0.4155 - acc: 0.8137 - val_loss: 0.4444 - val_acc: 0.8187

Epoch 00012: val_loss improved from 0.44582 to 0.44444, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 13/30
 - 27s - loss: 0.4109 - acc: 0.8168 - val_loss: 0.5286 - val_acc: 0.7731

Epoch 00013: val_loss did not improve from 0.44444
Epoch 14/30
 - 27s - loss: 0.4068 - acc: 0.8182 - val_loss: 0.4631 - val_acc: 0.7878

Epoch 00014: val_loss did not improve from 0.44444
Epoch 15/30
 - 27s - loss: 0.4017 - acc: 0.8210 - val_loss: 0.4569 - val_acc: 0.7970

Epoch 00015: val_loss did not improve from 0.44444
Epoch 16/30
 - 27s - loss: 0.3959 - acc: 0.8243 - val_loss: 0.4753 - val_acc: 0.8110

Epoch 00016: val_loss did not improve from 0.44444
Epoch 17/30
 - 27s - loss: 0.3939 - acc: 0.8262 - val_loss: 0.5086 - val_acc: 0.7853

Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00017: val_loss did not improve from 0.44444
Epoch 00017: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 287us/step
current Test accuracy: 0.7853494623655914
current auc_score ------------------>  0.8652889784946236

  32/7440 [..............................] - ETA: 9:07
 192/7440 [..............................] - ETA: 1:31
 384/7440 [>.............................] - ETA: 45s 
 576/7440 [=>............................] - ETA: 30s
 768/7440 [==>...........................] - ETA: 22s
 960/7440 [==>...........................] - ETA: 17s
1152/7440 [===>..........................] - ETA: 14s
1344/7440 [====>.........................] - ETA: 12s
1536/7440 [=====>........................] - ETA: 10s
1728/7440 [=====>........................] - ETA: 9s 
1920/7440 [======>.......................] - ETA: 8s
2112/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2496/7440 [=========>....................] - ETA: 6s
2688/7440 [=========>....................] - ETA: 5s
2880/7440 [==========>...................] - ETA: 5s
3072/7440 [===========>..................] - ETA: 4s
3264/7440 [============>.................] - ETA: 4s
3456/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 2s
4224/7440 [================>.............] - ETA: 2s
4416/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4800/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 611us/step
Best saved model Test accuracy: 0.8186827956989248
best saved model auc_score ------------------>  0.888152315296566
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_94[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_95[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_96[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_97[0][0]              
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 28, 96, 96)   0           concatenate_39[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_98[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   336         activation_99[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_100[0][0]             
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 20, 48, 48)   80          concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 20, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   480         activation_101[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_102[0][0]             
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 26, 48, 48)   0           concatenate_41[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 26)           0           activation_103[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            27          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 8,507
Trainable params: 8,063
Non-trainable params: 444
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 26s - loss: 0.5849 - acc: 0.7339 - val_loss: 0.5569 - val_acc: 0.8036

Epoch 00001: val_loss improved from inf to 0.55689, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 21s - loss: 0.5169 - acc: 0.7693 - val_loss: 0.5014 - val_acc: 0.8270

Epoch 00002: val_loss improved from 0.55689 to 0.50143, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 21s - loss: 0.4862 - acc: 0.7835 - val_loss: 0.4798 - val_acc: 0.7941

Epoch 00003: val_loss improved from 0.50143 to 0.47979, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 21s - loss: 0.4659 - acc: 0.7910 - val_loss: 0.5055 - val_acc: 0.7784

Epoch 00004: val_loss did not improve from 0.47979
Epoch 5/30
 - 20s - loss: 0.4521 - acc: 0.7974 - val_loss: 0.4687 - val_acc: 0.8042

Epoch 00005: val_loss improved from 0.47979 to 0.46871, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 20s - loss: 0.4414 - acc: 0.8029 - val_loss: 0.4399 - val_acc: 0.8069

Epoch 00006: val_loss improved from 0.46871 to 0.43992, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 20s - loss: 0.4321 - acc: 0.8066 - val_loss: 0.5085 - val_acc: 0.8192

Epoch 00007: val_loss did not improve from 0.43992
Epoch 8/30
 - 20s - loss: 0.4241 - acc: 0.8104 - val_loss: 0.5154 - val_acc: 0.7825

Epoch 00008: val_loss did not improve from 0.43992
Epoch 9/30
 - 20s - loss: 0.4199 - acc: 0.8112 - val_loss: 0.4546 - val_acc: 0.7821

Epoch 00009: val_loss did not improve from 0.43992
Epoch 10/30
 - 20s - loss: 0.4145 - acc: 0.8137 - val_loss: 0.3905 - val_acc: 0.8368

Epoch 00010: val_loss improved from 0.43992 to 0.39054, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 20s - loss: 0.4104 - acc: 0.8160 - val_loss: 0.3923 - val_acc: 0.8458

Epoch 00011: val_loss did not improve from 0.39054
Epoch 12/30
 - 20s - loss: 0.4045 - acc: 0.8210 - val_loss: 0.4624 - val_acc: 0.8128

Epoch 00012: val_loss did not improve from 0.39054
Epoch 13/30
 - 20s - loss: 0.4001 - acc: 0.8231 - val_loss: 0.4434 - val_acc: 0.8026

Epoch 00013: val_loss did not improve from 0.39054
Epoch 14/30
 - 20s - loss: 0.3950 - acc: 0.8252 - val_loss: 0.4560 - val_acc: 0.8054

Epoch 00014: val_loss did not improve from 0.39054
Epoch 15/30
 - 20s - loss: 0.3931 - acc: 0.8278 - val_loss: 0.5945 - val_acc: 0.7663

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.39054
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 234us/step
current Test accuracy: 0.766263440860215
current auc_score ------------------>  0.8852996372412996

  32/7440 [..............................] - ETA: 7:41
 256/7440 [>.............................] - ETA: 57s 
 480/7440 [>.............................] - ETA: 30s
 704/7440 [=>............................] - ETA: 20s
 928/7440 [==>...........................] - ETA: 15s
1184/7440 [===>..........................] - ETA: 11s
1440/7440 [====>.........................] - ETA: 9s 
1696/7440 [=====>........................] - ETA: 8s
1920/7440 [======>.......................] - ETA: 6s
2176/7440 [=======>......................] - ETA: 5s
2432/7440 [========>.....................] - ETA: 5s
2688/7440 [=========>....................] - ETA: 4s
2944/7440 [==========>...................] - ETA: 4s
3168/7440 [===========>..................] - ETA: 3s
3392/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 2s
3904/7440 [==============>...............] - ETA: 2s
4128/7440 [===============>..............] - ETA: 2s
4384/7440 [================>.............] - ETA: 2s
4640/7440 [=================>............] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
5088/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5536/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 491us/step
Best saved model Test accuracy: 0.8368279569892473
best saved model auc_score ------------------>  0.9179976008787143
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_11[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_104[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_105[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_106[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_107[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 40, 96, 96)   0           concatenate_43[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_108[0][0]             
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   960         activation_109[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_110[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_10[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 32, 48, 48)   128         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1536        activation_111[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_112[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 44, 48, 48)   0           concatenate_45[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_11 (Gl (None, 44)           0           activation_113[0][0]             
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            45          global_average_pooling2d_11[0][0]
==================================================================================================
Total params: 27,965
Trainable params: 27,221
Non-trainable params: 744
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 33s - loss: 0.5686 - acc: 0.7495 - val_loss: 0.5266 - val_acc: 0.8008

Epoch 00001: val_loss improved from inf to 0.52661, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 27s - loss: 0.4997 - acc: 0.7810 - val_loss: 0.4810 - val_acc: 0.8069

Epoch 00002: val_loss improved from 0.52661 to 0.48101, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 27s - loss: 0.4707 - acc: 0.7920 - val_loss: 0.4707 - val_acc: 0.8250

Epoch 00003: val_loss improved from 0.48101 to 0.47069, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 27s - loss: 0.4489 - acc: 0.8044 - val_loss: 0.5720 - val_acc: 0.7726

Epoch 00004: val_loss did not improve from 0.47069
Epoch 5/30
 - 27s - loss: 0.4313 - acc: 0.8148 - val_loss: 0.4531 - val_acc: 0.8203

Epoch 00005: val_loss improved from 0.47069 to 0.45310, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 27s - loss: 0.4193 - acc: 0.8227 - val_loss: 0.4545 - val_acc: 0.8220

Epoch 00006: val_loss did not improve from 0.45310
Epoch 7/30
 - 27s - loss: 0.4047 - acc: 0.8303 - val_loss: 0.4671 - val_acc: 0.8044

Epoch 00007: val_loss did not improve from 0.45310
Epoch 8/30
 - 27s - loss: 0.3959 - acc: 0.8343 - val_loss: 0.5161 - val_acc: 0.8138

Epoch 00008: val_loss did not improve from 0.45310
Epoch 9/30
 - 27s - loss: 0.3852 - acc: 0.8418 - val_loss: 0.5240 - val_acc: 0.7723

Epoch 00009: val_loss did not improve from 0.45310
Epoch 10/30
 - 27s - loss: 0.3771 - acc: 0.8446 - val_loss: 0.4655 - val_acc: 0.7913

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.45310
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 283us/step
current Test accuracy: 0.791263440860215
current auc_score ------------------>  0.8756419817319923

  32/7440 [..............................] - ETA: 8:33
 192/7440 [..............................] - ETA: 1:25
 384/7440 [>.............................] - ETA: 42s 
 576/7440 [=>............................] - ETA: 28s
 768/7440 [==>...........................] - ETA: 21s
 960/7440 [==>...........................] - ETA: 16s
1152/7440 [===>..........................] - ETA: 13s
1344/7440 [====>.........................] - ETA: 11s
1536/7440 [=====>........................] - ETA: 10s
1728/7440 [=====>........................] - ETA: 8s 
1920/7440 [======>.......................] - ETA: 7s
2112/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2496/7440 [=========>....................] - ETA: 5s
2688/7440 [=========>....................] - ETA: 5s
2880/7440 [==========>...................] - ETA: 4s
3072/7440 [===========>..................] - ETA: 4s
3264/7440 [============>.................] - ETA: 4s
3456/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 2s
4224/7440 [================>.............] - ETA: 2s
4416/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4800/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5184/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 583us/step
Best saved model Test accuracy: 0.8202956989247312
best saved model auc_score ------------------>  0.9009206989247311
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_114 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_11 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_115 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_12  (None, 8)                 0         
_________________________________________________________________
dense_12 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 13s - loss: 0.6655 - acc: 0.6210 - val_loss: 0.6154 - val_acc: 0.8019

Epoch 00001: val_loss improved from inf to 0.61539, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 7s - loss: 0.6277 - acc: 0.6878 - val_loss: 0.5768 - val_acc: 0.8102

Epoch 00002: val_loss improved from 0.61539 to 0.57685, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 7s - loss: 0.6083 - acc: 0.7056 - val_loss: 0.5430 - val_acc: 0.8227

Epoch 00003: val_loss improved from 0.57685 to 0.54296, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 7s - loss: 0.5932 - acc: 0.7122 - val_loss: 0.6209 - val_acc: 0.6883

Epoch 00004: val_loss did not improve from 0.54296
Epoch 5/30
 - 7s - loss: 0.5835 - acc: 0.7169 - val_loss: 0.5188 - val_acc: 0.8187

Epoch 00005: val_loss improved from 0.54296 to 0.51883, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 7s - loss: 0.5733 - acc: 0.7224 - val_loss: 0.5383 - val_acc: 0.7863

Epoch 00006: val_loss did not improve from 0.51883
Epoch 7/30
 - 7s - loss: 0.5647 - acc: 0.7266 - val_loss: 0.5158 - val_acc: 0.8352

Epoch 00007: val_loss improved from 0.51883 to 0.51584, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 7s - loss: 0.5578 - acc: 0.7296 - val_loss: 0.4900 - val_acc: 0.8606

Epoch 00008: val_loss improved from 0.51584 to 0.48999, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 7s - loss: 0.5518 - acc: 0.7333 - val_loss: 0.5124 - val_acc: 0.8324

Epoch 00009: val_loss did not improve from 0.48999
Epoch 10/30
 - 7s - loss: 0.5481 - acc: 0.7355 - val_loss: 0.4999 - val_acc: 0.8144

Epoch 00010: val_loss did not improve from 0.48999
Epoch 11/30
 - 7s - loss: 0.5426 - acc: 0.7387 - val_loss: 0.5487 - val_acc: 0.7868

Epoch 00011: val_loss did not improve from 0.48999
Epoch 12/30
 - 7s - loss: 0.5399 - acc: 0.7394 - val_loss: 0.5634 - val_acc: 0.8144

Epoch 00012: val_loss did not improve from 0.48999
Epoch 13/30
 - 7s - loss: 0.5374 - acc: 0.7413 - val_loss: 0.4806 - val_acc: 0.8520

Epoch 00013: val_loss improved from 0.48999 to 0.48055, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 14/30
 - 7s - loss: 0.5356 - acc: 0.7438 - val_loss: 0.4809 - val_acc: 0.8499

Epoch 00014: val_loss did not improve from 0.48055
Epoch 15/30
 - 7s - loss: 0.5328 - acc: 0.7454 - val_loss: 0.5445 - val_acc: 0.8129

Epoch 00015: val_loss did not improve from 0.48055
Epoch 16/30
 - 7s - loss: 0.5312 - acc: 0.7465 - val_loss: 0.4866 - val_acc: 0.8462

Epoch 00016: val_loss did not improve from 0.48055
Epoch 17/30
 - 7s - loss: 0.5293 - acc: 0.7483 - val_loss: 0.4820 - val_acc: 0.8484

Epoch 00017: val_loss did not improve from 0.48055
Epoch 18/30
 - 7s - loss: 0.5278 - acc: 0.7491 - val_loss: 0.5517 - val_acc: 0.8379

Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00018: val_loss did not improve from 0.48055
Epoch 00018: early stopping

  32/7440 [..............................] - ETA: 1s
 416/7440 [>.............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 0s
1184/7440 [===>..........................] - ETA: 0s
1568/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2720/7440 [=========>....................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 133us/step
current Test accuracy: 0.8379032258064516
current auc_score ------------------>  0.8706260478089951

  32/7440 [..............................] - ETA: 8:27
 384/7440 [>.............................] - ETA: 41s 
 768/7440 [==>...........................] - ETA: 19s
1152/7440 [===>..........................] - ETA: 12s
1536/7440 [=====>........................] - ETA: 9s 
1920/7440 [======>.......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 5s
2720/7440 [=========>....................] - ETA: 4s
3136/7440 [===========>..................] - ETA: 3s
3552/7440 [=============>................] - ETA: 2s
3936/7440 [==============>...............] - ETA: 2s
4320/7440 [================>.............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 3s 426us/step
Best saved model Test accuracy: 0.8520161290322581
best saved model auc_score ------------------>  0.8675780437044744
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_13[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_116[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_117[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_118[0][0]             
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   672         activation_119[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_120[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 26, 48, 48)   0           average_pooling2d_12[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_13 (Gl (None, 26)           0           activation_121[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            27          global_average_pooling2d_13[0][0]
==================================================================================================
Total params: 13,235
Trainable params: 12,875
Non-trainable params: 360
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 23s - loss: 0.5881 - acc: 0.7281 - val_loss: 0.6196 - val_acc: 0.7062

Epoch 00001: val_loss improved from inf to 0.61958, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 17s - loss: 0.5199 - acc: 0.7682 - val_loss: 0.4737 - val_acc: 0.8129

Epoch 00002: val_loss improved from 0.61958 to 0.47372, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 16s - loss: 0.4927 - acc: 0.7756 - val_loss: 0.4711 - val_acc: 0.7953

Epoch 00003: val_loss improved from 0.47372 to 0.47115, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 16s - loss: 0.4745 - acc: 0.7838 - val_loss: 0.4579 - val_acc: 0.8089

Epoch 00004: val_loss improved from 0.47115 to 0.45788, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 16s - loss: 0.4629 - acc: 0.7892 - val_loss: 0.4668 - val_acc: 0.8169

Epoch 00005: val_loss did not improve from 0.45788
Epoch 6/30
 - 16s - loss: 0.4520 - acc: 0.7947 - val_loss: 0.4509 - val_acc: 0.8086

Epoch 00006: val_loss improved from 0.45788 to 0.45087, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 16s - loss: 0.4431 - acc: 0.7989 - val_loss: 0.4602 - val_acc: 0.8017

Epoch 00007: val_loss did not improve from 0.45087
Epoch 8/30
 - 16s - loss: 0.4348 - acc: 0.8030 - val_loss: 0.4999 - val_acc: 0.7926

Epoch 00008: val_loss did not improve from 0.45087
Epoch 9/30
 - 16s - loss: 0.4269 - acc: 0.8099 - val_loss: 0.4590 - val_acc: 0.8198

Epoch 00009: val_loss did not improve from 0.45087
Epoch 10/30
 - 17s - loss: 0.4216 - acc: 0.8107 - val_loss: 0.4377 - val_acc: 0.8222

Epoch 00010: val_loss improved from 0.45087 to 0.43772, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 16s - loss: 0.4152 - acc: 0.8138 - val_loss: 0.5122 - val_acc: 0.8081

Epoch 00011: val_loss did not improve from 0.43772
Epoch 12/30
 - 16s - loss: 0.4101 - acc: 0.8152 - val_loss: 0.4576 - val_acc: 0.7977

Epoch 00012: val_loss did not improve from 0.43772
Epoch 13/30
 - 16s - loss: 0.4026 - acc: 0.8216 - val_loss: 0.5894 - val_acc: 0.7724

Epoch 00013: val_loss did not improve from 0.43772
Epoch 14/30
 - 16s - loss: 0.4020 - acc: 0.8226 - val_loss: 0.4499 - val_acc: 0.8206

Epoch 00014: val_loss did not improve from 0.43772
Epoch 15/30
 - 16s - loss: 0.3965 - acc: 0.8266 - val_loss: 0.5091 - val_acc: 0.7957

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.43772
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1056/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2080/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 213us/step
current Test accuracy: 0.7956989247311828
current auc_score ------------------>  0.8753519554283733

  32/7440 [..............................] - ETA: 9:37
 256/7440 [>.............................] - ETA: 1:11
 512/7440 [=>............................] - ETA: 35s 
 768/7440 [==>...........................] - ETA: 23s
1024/7440 [===>..........................] - ETA: 16s
1280/7440 [====>.........................] - ETA: 13s
1536/7440 [=====>........................] - ETA: 10s
1792/7440 [======>.......................] - ETA: 9s 
2048/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2560/7440 [=========>....................] - ETA: 5s
2816/7440 [==========>...................] - ETA: 5s
3072/7440 [===========>..................] - ETA: 4s
3328/7440 [============>.................] - ETA: 3s
3584/7440 [=============>................] - ETA: 3s
3840/7440 [==============>...............] - ETA: 3s
4096/7440 [===============>..............] - ETA: 2s
4352/7440 [================>.............] - ETA: 2s
4608/7440 [=================>............] - ETA: 2s
4864/7440 [==================>...........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5888/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 543us/step
Best saved model Test accuracy: 0.8221774193548387
best saved model auc_score ------------------>  0.8922264857208925
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_14[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_122[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_123[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_124[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_125[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 32, 96, 96)   0           concatenate_49[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 96, 96)   128         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 32, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_14 (Gl (None, 32)           0           activation_126[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            33          global_average_pooling2d_14[0][0]
==================================================================================================
Total params: 6,753
Trainable params: 6,481
Non-trainable params: 272
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 25s - loss: 0.6267 - acc: 0.6775 - val_loss: 0.5361 - val_acc: 0.8125

Epoch 00001: val_loss improved from inf to 0.53615, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 18s - loss: 0.5709 - acc: 0.7379 - val_loss: 0.5406 - val_acc: 0.8387

Epoch 00002: val_loss did not improve from 0.53615
Epoch 3/30
 - 18s - loss: 0.5394 - acc: 0.7536 - val_loss: 0.5214 - val_acc: 0.8204

Epoch 00003: val_loss improved from 0.53615 to 0.52137, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 18s - loss: 0.5188 - acc: 0.7567 - val_loss: 0.4993 - val_acc: 0.8332

Epoch 00004: val_loss improved from 0.52137 to 0.49931, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 18s - loss: 0.5049 - acc: 0.7650 - val_loss: 0.5278 - val_acc: 0.7798

Epoch 00005: val_loss did not improve from 0.49931
Epoch 6/30
 - 18s - loss: 0.4940 - acc: 0.7686 - val_loss: 0.4924 - val_acc: 0.8047

Epoch 00006: val_loss improved from 0.49931 to 0.49243, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 18s - loss: 0.4850 - acc: 0.7724 - val_loss: 0.5281 - val_acc: 0.7899

Epoch 00007: val_loss did not improve from 0.49243
Epoch 8/30
 - 18s - loss: 0.4740 - acc: 0.7775 - val_loss: 0.4794 - val_acc: 0.8086

Epoch 00008: val_loss improved from 0.49243 to 0.47943, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 18s - loss: 0.4672 - acc: 0.7820 - val_loss: 0.4519 - val_acc: 0.8242

Epoch 00009: val_loss improved from 0.47943 to 0.45191, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 18s - loss: 0.4598 - acc: 0.7853 - val_loss: 0.4843 - val_acc: 0.8040

Epoch 00010: val_loss did not improve from 0.45191
Epoch 11/30
 - 18s - loss: 0.4536 - acc: 0.7894 - val_loss: 0.4465 - val_acc: 0.8017

Epoch 00011: val_loss improved from 0.45191 to 0.44646, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 12/30
 - 18s - loss: 0.4487 - acc: 0.7903 - val_loss: 0.4627 - val_acc: 0.8200

Epoch 00012: val_loss did not improve from 0.44646
Epoch 13/30
 - 18s - loss: 0.4421 - acc: 0.7948 - val_loss: 0.4382 - val_acc: 0.8133

Epoch 00013: val_loss improved from 0.44646 to 0.43816, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 14/30
 - 18s - loss: 0.4374 - acc: 0.7975 - val_loss: 0.4402 - val_acc: 0.8052

Epoch 00014: val_loss did not improve from 0.43816
Epoch 15/30
 - 18s - loss: 0.4334 - acc: 0.8002 - val_loss: 0.4441 - val_acc: 0.8048

Epoch 00015: val_loss did not improve from 0.43816
Epoch 16/30
 - 18s - loss: 0.4284 - acc: 0.8019 - val_loss: 0.4457 - val_acc: 0.8196

Epoch 00016: val_loss did not improve from 0.43816
Epoch 17/30
 - 18s - loss: 0.4267 - acc: 0.8041 - val_loss: 0.4470 - val_acc: 0.8224

Epoch 00017: val_loss did not improve from 0.43816
Epoch 18/30
 - 18s - loss: 0.4202 - acc: 0.8088 - val_loss: 0.4332 - val_acc: 0.8263

Epoch 00018: val_loss improved from 0.43816 to 0.43320, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 19/30
 - 18s - loss: 0.4168 - acc: 0.8087 - val_loss: 0.4118 - val_acc: 0.8343

Epoch 00019: val_loss improved from 0.43320 to 0.41179, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 20/30
 - 18s - loss: 0.4137 - acc: 0.8112 - val_loss: 0.5232 - val_acc: 0.7931

Epoch 00020: val_loss did not improve from 0.41179
Epoch 21/30
 - 18s - loss: 0.4113 - acc: 0.8111 - val_loss: 0.4660 - val_acc: 0.7871

Epoch 00021: val_loss did not improve from 0.41179
Epoch 22/30
 - 18s - loss: 0.4094 - acc: 0.8129 - val_loss: 0.4149 - val_acc: 0.8337

Epoch 00022: val_loss did not improve from 0.41179
Epoch 23/30
 - 18s - loss: 0.4057 - acc: 0.8154 - val_loss: 0.3911 - val_acc: 0.8337

Epoch 00023: val_loss improved from 0.41179 to 0.39109, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 24/30
 - 18s - loss: 0.4057 - acc: 0.8136 - val_loss: 0.4043 - val_acc: 0.8290

Epoch 00024: val_loss did not improve from 0.39109
Epoch 25/30
 - 18s - loss: 0.4051 - acc: 0.8152 - val_loss: 0.4436 - val_acc: 0.8147

Epoch 00025: val_loss did not improve from 0.39109
Epoch 26/30
 - 18s - loss: 0.4006 - acc: 0.8183 - val_loss: 0.4435 - val_acc: 0.8155

Epoch 00026: val_loss did not improve from 0.39109
Epoch 27/30
 - 18s - loss: 0.4015 - acc: 0.8166 - val_loss: 0.4501 - val_acc: 0.8219

Epoch 00027: val_loss did not improve from 0.39109
Epoch 28/30
 - 18s - loss: 0.3989 - acc: 0.8198 - val_loss: 0.4101 - val_acc: 0.8163

Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00028: val_loss did not improve from 0.39109
Epoch 00028: early stopping

  32/7440 [..............................] - ETA: 2s
 256/7440 [>.............................] - ETA: 1s
 512/7440 [=>............................] - ETA: 1s
 768/7440 [==>...........................] - ETA: 1s
1024/7440 [===>..........................] - ETA: 1s
1280/7440 [====>.........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2016/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2784/7440 [==========>...................] - ETA: 1s
3040/7440 [===========>..................] - ETA: 0s
3264/7440 [============>.................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4000/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 222us/step
current Test accuracy: 0.8162634408602151
current auc_score ------------------>  0.9068909700543415

  32/7440 [..............................] - ETA: 9:57
 256/7440 [>.............................] - ETA: 1:13
 480/7440 [>.............................] - ETA: 38s 
 704/7440 [=>............................] - ETA: 26s
 928/7440 [==>...........................] - ETA: 19s
1152/7440 [===>..........................] - ETA: 15s
1376/7440 [====>.........................] - ETA: 12s
1600/7440 [=====>........................] - ETA: 10s
1824/7440 [======>.......................] - ETA: 9s 
2048/7440 [=======>......................] - ETA: 7s
2304/7440 [========>.....................] - ETA: 6s
2528/7440 [=========>....................] - ETA: 6s
2752/7440 [==========>...................] - ETA: 5s
2976/7440 [===========>..................] - ETA: 4s
3200/7440 [===========>..................] - ETA: 4s
3424/7440 [============>.................] - ETA: 3s
3648/7440 [=============>................] - ETA: 3s
3872/7440 [==============>...............] - ETA: 3s
4096/7440 [===============>..............] - ETA: 2s
4320/7440 [================>.............] - ETA: 2s
4576/7440 [=================>............] - ETA: 2s
4800/7440 [==================>...........] - ETA: 2s
5056/7440 [===================>..........] - ETA: 1s
5312/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
6016/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 571us/step
Best saved model Test accuracy: 0.833736559139785
best saved model auc_score ------------------>  0.91810346571858
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_15[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_127[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_128[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_129[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_130[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 28, 96, 96)   0           concatenate_51[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_131[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_132[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 34, 96, 96)   0           concatenate_52[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 34, 96, 96)   136         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 34, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 17, 96, 96)   578         activation_133[0][0]             
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 17, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 17, 48, 48)   68          average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 17, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   408         activation_134[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_135[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 23, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 23, 48, 48)   92          concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 23, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   552         activation_136[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_137[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 29, 48, 48)   0           concatenate_54[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 29, 48, 48)   116         concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 29, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 24, 48, 48)   696         activation_138[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 24, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_139[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 35, 48, 48)   0           concatenate_55[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 35, 48, 48)   140         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 35, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_15 (Gl (None, 35)           0           activation_140[0][0]             
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 1)            36          global_average_pooling2d_15[0][0]
==================================================================================================
Total params: 13,310
Trainable params: 12,614
Non-trainable params: 696
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 36s - loss: 0.5884 - acc: 0.7363 - val_loss: 0.5684 - val_acc: 0.7907

Epoch 00001: val_loss improved from inf to 0.56844, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 28s - loss: 0.5094 - acc: 0.7729 - val_loss: 0.5037 - val_acc: 0.8094

Epoch 00002: val_loss improved from 0.56844 to 0.50375, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 28s - loss: 0.4773 - acc: 0.7832 - val_loss: 0.4942 - val_acc: 0.7829

Epoch 00003: val_loss improved from 0.50375 to 0.49420, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 28s - loss: 0.4588 - acc: 0.7939 - val_loss: 0.4639 - val_acc: 0.8230

Epoch 00004: val_loss improved from 0.49420 to 0.46391, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 28s - loss: 0.4447 - acc: 0.7980 - val_loss: 0.5697 - val_acc: 0.7422

Epoch 00005: val_loss did not improve from 0.46391
Epoch 6/30
 - 28s - loss: 0.4319 - acc: 0.8066 - val_loss: 0.4954 - val_acc: 0.8305

Epoch 00006: val_loss did not improve from 0.46391
Epoch 7/30
 - 28s - loss: 0.4188 - acc: 0.8154 - val_loss: 0.5018 - val_acc: 0.7702

Epoch 00007: val_loss did not improve from 0.46391
Epoch 8/30
 - 28s - loss: 0.4117 - acc: 0.8195 - val_loss: 0.4746 - val_acc: 0.8138

Epoch 00008: val_loss did not improve from 0.46391
Epoch 9/30
 - 28s - loss: 0.4015 - acc: 0.8255 - val_loss: 0.4903 - val_acc: 0.8208

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.46391
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 384/7440 [>.............................] - ETA: 2s
 544/7440 [=>............................] - ETA: 2s
 704/7440 [=>............................] - ETA: 2s
 896/7440 [==>...........................] - ETA: 2s
1088/7440 [===>..........................] - ETA: 1s
1280/7440 [====>.........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1664/7440 [=====>........................] - ETA: 1s
1856/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2240/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
3008/7440 [===========>..................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3776/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4160/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 0s
4544/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5696/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 306us/step
current Test accuracy: 0.8208333333333333
current auc_score ------------------>  0.889690390507573

  32/7440 [..............................] - ETA: 12:00
 192/7440 [..............................] - ETA: 1:59 
 384/7440 [>.............................] - ETA: 59s 
 576/7440 [=>............................] - ETA: 39s
 768/7440 [==>...........................] - ETA: 29s
 960/7440 [==>...........................] - ETA: 22s
1152/7440 [===>..........................] - ETA: 18s
1344/7440 [====>.........................] - ETA: 15s
1536/7440 [=====>........................] - ETA: 13s
1728/7440 [=====>........................] - ETA: 11s
1920/7440 [======>.......................] - ETA: 10s
2112/7440 [=======>......................] - ETA: 9s 
2304/7440 [========>.....................] - ETA: 8s
2496/7440 [=========>....................] - ETA: 7s
2688/7440 [=========>....................] - ETA: 6s
2880/7440 [==========>...................] - ETA: 6s
3072/7440 [===========>..................] - ETA: 5s
3264/7440 [============>.................] - ETA: 5s
3456/7440 [============>.................] - ETA: 4s
3648/7440 [=============>................] - ETA: 4s
3840/7440 [==============>...............] - ETA: 3s
4032/7440 [===============>..............] - ETA: 3s
4224/7440 [================>.............] - ETA: 3s
4416/7440 [================>.............] - ETA: 3s
4608/7440 [=================>............] - ETA: 2s
4800/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5184/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 1s
5760/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 5s 718us/step
Best saved model Test accuracy: 0.822983870967742
best saved model auc_score ------------------>  0.8980915062434963
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_16 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_141 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_14 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 8, 48, 48)         32        
_________________________________________________________________
activation_142 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
global_average_pooling2d_16  (None, 8)                 0         
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 9         
=================================================================
Total params: 521
Trainable params: 473
Non-trainable params: 48
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 14s - loss: 0.7948 - acc: 0.5000 - val_loss: 0.6906 - val_acc: 0.5000

Epoch 00001: val_loss improved from inf to 0.69062, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 7s - loss: 0.6841 - acc: 0.5204 - val_loss: 0.6435 - val_acc: 0.5819

Epoch 00002: val_loss improved from 0.69062 to 0.64350, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 7s - loss: 0.6664 - acc: 0.6283 - val_loss: 0.6226 - val_acc: 0.7402

Epoch 00003: val_loss improved from 0.64350 to 0.62257, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 7s - loss: 0.6592 - acc: 0.6623 - val_loss: 0.6099 - val_acc: 0.7762

Epoch 00004: val_loss improved from 0.62257 to 0.60993, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 7s - loss: 0.6540 - acc: 0.6640 - val_loss: 0.5997 - val_acc: 0.7796

Epoch 00005: val_loss improved from 0.60993 to 0.59966, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 7s - loss: 0.6501 - acc: 0.6638 - val_loss: 0.5877 - val_acc: 0.7798

Epoch 00006: val_loss improved from 0.59966 to 0.58769, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 7s - loss: 0.6465 - acc: 0.6644 - val_loss: 0.5842 - val_acc: 0.7843

Epoch 00007: val_loss improved from 0.58769 to 0.58422, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 7s - loss: 0.6433 - acc: 0.6633 - val_loss: 0.5900 - val_acc: 0.7742

Epoch 00008: val_loss did not improve from 0.58422
Epoch 9/30
 - 7s - loss: 0.6404 - acc: 0.6651 - val_loss: 0.5759 - val_acc: 0.7702

Epoch 00009: val_loss improved from 0.58422 to 0.57593, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 7s - loss: 0.6367 - acc: 0.6685 - val_loss: 0.5722 - val_acc: 0.7718

Epoch 00010: val_loss improved from 0.57593 to 0.57218, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 7s - loss: 0.6346 - acc: 0.6692 - val_loss: 0.5684 - val_acc: 0.7829

Epoch 00011: val_loss improved from 0.57218 to 0.56844, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 12/30
 - 7s - loss: 0.6318 - acc: 0.6710 - val_loss: 0.5797 - val_acc: 0.7633

Epoch 00012: val_loss did not improve from 0.56844
Epoch 13/30
 - 7s - loss: 0.6284 - acc: 0.6732 - val_loss: 0.5698 - val_acc: 0.7642

Epoch 00013: val_loss did not improve from 0.56844
Epoch 14/30
 - 7s - loss: 0.6255 - acc: 0.6791 - val_loss: 0.5566 - val_acc: 0.7691

Epoch 00014: val_loss improved from 0.56844 to 0.55659, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 15/30
 - 7s - loss: 0.6235 - acc: 0.6787 - val_loss: 0.5579 - val_acc: 0.7569

Epoch 00015: val_loss did not improve from 0.55659
Epoch 16/30
 - 7s - loss: 0.6199 - acc: 0.6814 - val_loss: 0.5571 - val_acc: 0.7661

Epoch 00016: val_loss did not improve from 0.55659
Epoch 17/30
 - 7s - loss: 0.6179 - acc: 0.6803 - val_loss: 0.5513 - val_acc: 0.7649

Epoch 00017: val_loss improved from 0.55659 to 0.55135, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 18/30
 - 7s - loss: 0.6153 - acc: 0.6826 - val_loss: 0.5576 - val_acc: 0.7711

Epoch 00018: val_loss did not improve from 0.55135
Epoch 19/30
 - 7s - loss: 0.6125 - acc: 0.6864 - val_loss: 0.5591 - val_acc: 0.7699

Epoch 00019: val_loss did not improve from 0.55135
Epoch 20/30
 - 7s - loss: 0.6111 - acc: 0.6867 - val_loss: 0.5459 - val_acc: 0.7567

Epoch 00020: val_loss improved from 0.55135 to 0.54595, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 21/30
 - 7s - loss: 0.6087 - acc: 0.6896 - val_loss: 0.5756 - val_acc: 0.7267

Epoch 00021: val_loss did not improve from 0.54595
Epoch 22/30
 - 7s - loss: 0.6068 - acc: 0.6894 - val_loss: 0.5473 - val_acc: 0.7640

Epoch 00022: val_loss did not improve from 0.54595
Epoch 23/30
 - 7s - loss: 0.6046 - acc: 0.6909 - val_loss: 0.5479 - val_acc: 0.7610

Epoch 00023: val_loss did not improve from 0.54595
Epoch 24/30
 - 7s - loss: 0.6031 - acc: 0.6946 - val_loss: 0.5499 - val_acc: 0.7585

Epoch 00024: val_loss did not improve from 0.54595
Epoch 25/30
 - 7s - loss: 0.6012 - acc: 0.6939 - val_loss: 0.5376 - val_acc: 0.7559

Epoch 00025: val_loss improved from 0.54595 to 0.53758, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 26/30
 - 7s - loss: 0.5994 - acc: 0.6962 - val_loss: 0.5539 - val_acc: 0.7558

Epoch 00026: val_loss did not improve from 0.53758
Epoch 27/30
 - 7s - loss: 0.5972 - acc: 0.6976 - val_loss: 0.5404 - val_acc: 0.7594

Epoch 00027: val_loss did not improve from 0.53758
Epoch 28/30
 - 7s - loss: 0.5957 - acc: 0.6993 - val_loss: 0.5381 - val_acc: 0.7551

Epoch 00028: val_loss did not improve from 0.53758
Epoch 29/30
 - 7s - loss: 0.5938 - acc: 0.6986 - val_loss: 0.5552 - val_acc: 0.7414

Epoch 00029: val_loss did not improve from 0.53758
Epoch 30/30
 - 7s - loss: 0.5927 - acc: 0.7003 - val_loss: 0.5705 - val_acc: 0.7148

Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00030: val_loss did not improve from 0.53758
Epoch 00030: early stopping

  32/7440 [..............................] - ETA: 2s
 384/7440 [>.............................] - ETA: 1s
 736/7440 [=>............................] - ETA: 1s
1088/7440 [===>..........................] - ETA: 0s
1440/7440 [====>.........................] - ETA: 0s
1792/7440 [======>.......................] - ETA: 0s
2144/7440 [=======>......................] - ETA: 0s
2464/7440 [========>.....................] - ETA: 0s
2816/7440 [==========>...................] - ETA: 0s
3168/7440 [===========>..................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4224/7440 [================>.............] - ETA: 0s
4576/7440 [=================>............] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 153us/step
current Test accuracy: 0.7147849462365592
current auc_score ------------------>  0.8288393889466991

  32/7440 [..............................] - ETA: 12:04
 352/7440 [>.............................] - ETA: 1:04 
 704/7440 [=>............................] - ETA: 30s 
1056/7440 [===>..........................] - ETA: 19s
1376/7440 [====>.........................] - ETA: 14s
1728/7440 [=====>........................] - ETA: 11s
2080/7440 [=======>......................] - ETA: 8s 
2432/7440 [========>.....................] - ETA: 7s
2784/7440 [==========>...................] - ETA: 5s
3136/7440 [===========>..................] - ETA: 4s
3488/7440 [=============>................] - ETA: 4s
3840/7440 [==============>...............] - ETA: 3s
4192/7440 [===============>..............] - ETA: 2s
4544/7440 [=================>............] - ETA: 2s
4896/7440 [==================>...........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 1s
5632/7440 [=====================>........] - ETA: 1s
5984/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 569us/step
Best saved model Test accuracy: 0.7559139784946236
best saved model auc_score ------------------>  0.8401779107411261
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_17[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_143[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_144[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_145[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_146[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 32, 96, 96)   0           concatenate_57[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_147[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_148[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 40, 96, 96)   0           concatenate_58[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_149[0][0]             
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_150[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_151[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 28, 48, 48)   0           average_pooling2d_15[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_152[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_153[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 36, 48, 48)   0           concatenate_60[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_154[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_155[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 44, 48, 48)   0           concatenate_61[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 44, 48, 48)   176         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 44, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_17 (Gl (None, 44)           0           activation_156[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            45          global_average_pooling2d_17[0][0]
==================================================================================================
Total params: 21,677
Trainable params: 20,813
Non-trainable params: 864
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 41s - loss: 0.5792 - acc: 0.7417 - val_loss: 0.5242 - val_acc: 0.8468

Epoch 00001: val_loss improved from inf to 0.52422, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 32s - loss: 0.5028 - acc: 0.7779 - val_loss: 0.5006 - val_acc: 0.8325

Epoch 00002: val_loss improved from 0.52422 to 0.50061, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 32s - loss: 0.4647 - acc: 0.7956 - val_loss: 0.4747 - val_acc: 0.8359

Epoch 00003: val_loss improved from 0.50061 to 0.47470, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 32s - loss: 0.4425 - acc: 0.8086 - val_loss: 0.4044 - val_acc: 0.8435

Epoch 00004: val_loss improved from 0.47470 to 0.40440, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 32s - loss: 0.4272 - acc: 0.8134 - val_loss: 0.4812 - val_acc: 0.8281

Epoch 00005: val_loss did not improve from 0.40440
Epoch 6/30
 - 32s - loss: 0.4131 - acc: 0.8209 - val_loss: 0.4896 - val_acc: 0.7860

Epoch 00006: val_loss did not improve from 0.40440
Epoch 7/30
 - 32s - loss: 0.4012 - acc: 0.8284 - val_loss: 0.3826 - val_acc: 0.8673

Epoch 00007: val_loss improved from 0.40440 to 0.38262, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 32s - loss: 0.3932 - acc: 0.8328 - val_loss: 0.5731 - val_acc: 0.7719

Epoch 00008: val_loss did not improve from 0.38262
Epoch 9/30
 - 32s - loss: 0.3853 - acc: 0.8360 - val_loss: 0.5352 - val_acc: 0.7421

Epoch 00009: val_loss did not improve from 0.38262
Epoch 10/30
 - 32s - loss: 0.3769 - acc: 0.8418 - val_loss: 0.5368 - val_acc: 0.7825

Epoch 00010: val_loss did not improve from 0.38262
Epoch 11/30
 - 32s - loss: 0.3739 - acc: 0.8456 - val_loss: 0.4619 - val_acc: 0.8118

Epoch 00011: val_loss did not improve from 0.38262
Epoch 12/30
 - 32s - loss: 0.3664 - acc: 0.8469 - val_loss: 0.6123 - val_acc: 0.7483

Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00012: val_loss did not improve from 0.38262
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 1s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 338us/step
current Test accuracy: 0.748252688172043
current auc_score ------------------>  0.905327566770725

  32/7440 [..............................] - ETA: 14:23
 192/7440 [..............................] - ETA: 2:23 
 352/7440 [>.............................] - ETA: 1:17
 512/7440 [=>............................] - ETA: 52s 
 672/7440 [=>............................] - ETA: 39s
 832/7440 [==>...........................] - ETA: 31s
 992/7440 [===>..........................] - ETA: 26s
1152/7440 [===>..........................] - ETA: 22s
1312/7440 [====>.........................] - ETA: 19s
1472/7440 [====>.........................] - ETA: 17s
1632/7440 [=====>........................] - ETA: 15s
1792/7440 [======>.......................] - ETA: 13s
1952/7440 [======>.......................] - ETA: 12s
2112/7440 [=======>......................] - ETA: 11s
2272/7440 [========>.....................] - ETA: 10s
2432/7440 [========>.....................] - ETA: 9s 
2592/7440 [=========>....................] - ETA: 8s
2752/7440 [==========>...................] - ETA: 7s
2912/7440 [==========>...................] - ETA: 7s
3072/7440 [===========>..................] - ETA: 6s
3232/7440 [============>.................] - ETA: 6s
3392/7440 [============>.................] - ETA: 5s
3552/7440 [=============>................] - ETA: 5s
3712/7440 [=============>................] - ETA: 5s
3872/7440 [==============>...............] - ETA: 4s
4032/7440 [===============>..............] - ETA: 4s
4192/7440 [===============>..............] - ETA: 4s
4352/7440 [================>.............] - ETA: 3s
4512/7440 [=================>............] - ETA: 3s
4672/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 2s
4992/7440 [===================>..........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5312/7440 [====================>.........] - ETA: 2s
5472/7440 [=====================>........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5952/7440 [=======================>......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6272/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 850us/step
Best saved model Test accuracy: 0.8673387096774193
best saved model auc_score ------------------>  0.927219548502717
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_18 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_18[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_157[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_158[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_159[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 28, 96, 96)   0           concatenate_63[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_161[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_162[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 34, 96, 96)   0           concatenate_64[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 34, 96, 96)   136         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 34, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_18 (Gl (None, 34)           0           activation_163[0][0]             
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 1)            35          global_average_pooling2d_18[0][0]
==================================================================================================
Total params: 6,483
Trainable params: 6,139
Non-trainable params: 344
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 31s - loss: 0.6285 - acc: 0.6809 - val_loss: 0.5491 - val_acc: 0.8121

Epoch 00001: val_loss improved from inf to 0.54915, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 22s - loss: 0.5663 - acc: 0.7478 - val_loss: 0.5110 - val_acc: 0.8454

Epoch 00002: val_loss improved from 0.54915 to 0.51095, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 22s - loss: 0.5363 - acc: 0.7591 - val_loss: 0.4994 - val_acc: 0.8300

Epoch 00003: val_loss improved from 0.51095 to 0.49938, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 22s - loss: 0.5130 - acc: 0.7638 - val_loss: 0.4967 - val_acc: 0.8226

Epoch 00004: val_loss improved from 0.49938 to 0.49669, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 22s - loss: 0.4974 - acc: 0.7696 - val_loss: 0.5052 - val_acc: 0.8048

Epoch 00005: val_loss did not improve from 0.49669
Epoch 6/30
 - 22s - loss: 0.4856 - acc: 0.7755 - val_loss: 0.4896 - val_acc: 0.8083

Epoch 00006: val_loss improved from 0.49669 to 0.48956, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 22s - loss: 0.4780 - acc: 0.7744 - val_loss: 0.5084 - val_acc: 0.8026

Epoch 00007: val_loss did not improve from 0.48956
Epoch 8/30
 - 22s - loss: 0.4689 - acc: 0.7812 - val_loss: 0.4734 - val_acc: 0.7941

Epoch 00008: val_loss improved from 0.48956 to 0.47336, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 22s - loss: 0.4618 - acc: 0.7840 - val_loss: 0.4782 - val_acc: 0.8017

Epoch 00009: val_loss did not improve from 0.47336
Epoch 10/30
 - 22s - loss: 0.4560 - acc: 0.7859 - val_loss: 0.4546 - val_acc: 0.8140

Epoch 00010: val_loss improved from 0.47336 to 0.45458, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 22s - loss: 0.4519 - acc: 0.7878 - val_loss: 0.4962 - val_acc: 0.8122

Epoch 00011: val_loss did not improve from 0.45458
Epoch 12/30
 - 22s - loss: 0.4448 - acc: 0.7914 - val_loss: 0.4715 - val_acc: 0.8028

Epoch 00012: val_loss did not improve from 0.45458
Epoch 13/30
 - 22s - loss: 0.4395 - acc: 0.7941 - val_loss: 0.4797 - val_acc: 0.7841

Epoch 00013: val_loss did not improve from 0.45458
Epoch 14/30
 - 22s - loss: 0.4358 - acc: 0.7962 - val_loss: 0.6130 - val_acc: 0.7448

Epoch 00014: val_loss did not improve from 0.45458
Epoch 15/30
 - 22s - loss: 0.4292 - acc: 0.8007 - val_loss: 0.4998 - val_acc: 0.7832

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.45458
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 276us/step
current Test accuracy: 0.7831989247311828
current auc_score ------------------>  0.8396389033414267

  32/7440 [..............................] - ETA: 14:25
 224/7440 [..............................] - ETA: 2:02 
 416/7440 [>.............................] - ETA: 1:04
 608/7440 [=>............................] - ETA: 43s 
 800/7440 [==>...........................] - ETA: 32s
 992/7440 [===>..........................] - ETA: 26s
1184/7440 [===>..........................] - ETA: 21s
1376/7440 [====>.........................] - ETA: 18s
1568/7440 [=====>........................] - ETA: 15s
1760/7440 [======>.......................] - ETA: 13s
1952/7440 [======>.......................] - ETA: 12s
2144/7440 [=======>......................] - ETA: 10s
2336/7440 [========>.....................] - ETA: 9s 
2528/7440 [=========>....................] - ETA: 8s
2720/7440 [=========>....................] - ETA: 7s
2912/7440 [==========>...................] - ETA: 7s
3104/7440 [===========>..................] - ETA: 6s
3296/7440 [============>.................] - ETA: 5s
3488/7440 [=============>................] - ETA: 5s
3680/7440 [=============>................] - ETA: 4s
3872/7440 [==============>...............] - ETA: 4s
4064/7440 [===============>..............] - ETA: 4s
4256/7440 [================>.............] - ETA: 3s
4448/7440 [================>.............] - ETA: 3s
4640/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 2s
5024/7440 [===================>..........] - ETA: 2s
5216/7440 [====================>.........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 1s
5600/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5984/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 778us/step
Best saved model Test accuracy: 0.8139784946236559
best saved model auc_score ------------------>  0.8859548791767835
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_19[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_164[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_166[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 28, 96, 96)   0           concatenate_66[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 28, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 24, 96, 96)   672         activation_168[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 24, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_169[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 34, 96, 96)   0           concatenate_67[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 34, 96, 96)   136         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 34, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_19 (Gl (None, 34)           0           activation_170[0][0]             
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 1)            35          global_average_pooling2d_19[0][0]
==================================================================================================
Total params: 6,483
Trainable params: 6,139
Non-trainable params: 344
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.6064 - acc: 0.7157 - val_loss: 0.5945 - val_acc: 0.7472

Epoch 00001: val_loss improved from inf to 0.59447, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 22s - loss: 0.5450 - acc: 0.7559 - val_loss: 0.5285 - val_acc: 0.8234

Epoch 00002: val_loss improved from 0.59447 to 0.52852, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 21s - loss: 0.5178 - acc: 0.7621 - val_loss: 0.4962 - val_acc: 0.8202

Epoch 00003: val_loss improved from 0.52852 to 0.49625, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 22s - loss: 0.5007 - acc: 0.7687 - val_loss: 0.5221 - val_acc: 0.7825

Epoch 00004: val_loss did not improve from 0.49625
Epoch 5/30
 - 21s - loss: 0.4882 - acc: 0.7719 - val_loss: 0.5132 - val_acc: 0.7981

Epoch 00005: val_loss did not improve from 0.49625
Epoch 6/30
 - 21s - loss: 0.4779 - acc: 0.7763 - val_loss: 0.4923 - val_acc: 0.7902

Epoch 00006: val_loss improved from 0.49625 to 0.49225, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 21s - loss: 0.4689 - acc: 0.7807 - val_loss: 0.4986 - val_acc: 0.8051

Epoch 00007: val_loss did not improve from 0.49225
Epoch 8/30
 - 22s - loss: 0.4635 - acc: 0.7829 - val_loss: 0.5283 - val_acc: 0.7825

Epoch 00008: val_loss did not improve from 0.49225
Epoch 9/30
 - 21s - loss: 0.4559 - acc: 0.7870 - val_loss: 0.5217 - val_acc: 0.7993

Epoch 00009: val_loss did not improve from 0.49225
Epoch 10/30
 - 21s - loss: 0.4536 - acc: 0.7887 - val_loss: 0.4912 - val_acc: 0.7937

Epoch 00010: val_loss improved from 0.49225 to 0.49116, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 21s - loss: 0.4470 - acc: 0.7931 - val_loss: 0.4776 - val_acc: 0.8251

Epoch 00011: val_loss improved from 0.49116 to 0.47760, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 12/30
 - 22s - loss: 0.4411 - acc: 0.7964 - val_loss: 0.5049 - val_acc: 0.8004

Epoch 00012: val_loss did not improve from 0.47760
Epoch 13/30
 - 22s - loss: 0.4389 - acc: 0.7969 - val_loss: 0.6331 - val_acc: 0.7044

Epoch 00013: val_loss did not improve from 0.47760
Epoch 14/30
 - 21s - loss: 0.4351 - acc: 0.8002 - val_loss: 0.5209 - val_acc: 0.7632

Epoch 00014: val_loss did not improve from 0.47760
Epoch 15/30
 - 21s - loss: 0.4313 - acc: 0.8017 - val_loss: 0.5547 - val_acc: 0.7573

Epoch 00015: val_loss did not improve from 0.47760
Epoch 16/30
 - 22s - loss: 0.4296 - acc: 0.8018 - val_loss: 0.4761 - val_acc: 0.8222

Epoch 00016: val_loss improved from 0.47760 to 0.47610, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 17/30
 - 22s - loss: 0.4259 - acc: 0.8039 - val_loss: 0.4794 - val_acc: 0.7895

Epoch 00017: val_loss did not improve from 0.47610
Epoch 18/30
 - 21s - loss: 0.4222 - acc: 0.8068 - val_loss: 0.5775 - val_acc: 0.7645

Epoch 00018: val_loss did not improve from 0.47610
Epoch 19/30
 - 21s - loss: 0.4188 - acc: 0.8096 - val_loss: 0.4825 - val_acc: 0.7954

Epoch 00019: val_loss did not improve from 0.47610
Epoch 20/30
 - 22s - loss: 0.4183 - acc: 0.8079 - val_loss: 0.5128 - val_acc: 0.8208

Epoch 00020: val_loss did not improve from 0.47610
Epoch 21/30
 - 21s - loss: 0.4155 - acc: 0.8111 - val_loss: 0.4990 - val_acc: 0.8164

Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00021: val_loss did not improve from 0.47610
Epoch 00021: early stopping

  32/7440 [..............................] - ETA: 2s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 280us/step
current Test accuracy: 0.8163978494623656
current auc_score ------------------>  0.8697902936755694

  32/7440 [..............................] - ETA: 15:36
 224/7440 [..............................] - ETA: 2:12 
 416/7440 [>.............................] - ETA: 1:10
 608/7440 [=>............................] - ETA: 47s 
 800/7440 [==>...........................] - ETA: 35s
 992/7440 [===>..........................] - ETA: 28s
1184/7440 [===>..........................] - ETA: 23s
1376/7440 [====>.........................] - ETA: 19s
1568/7440 [=====>........................] - ETA: 16s
1760/7440 [======>.......................] - ETA: 14s
1952/7440 [======>.......................] - ETA: 12s
2144/7440 [=======>......................] - ETA: 11s
2336/7440 [========>.....................] - ETA: 10s
2528/7440 [=========>....................] - ETA: 9s 
2720/7440 [=========>....................] - ETA: 8s
2912/7440 [==========>...................] - ETA: 7s
3104/7440 [===========>..................] - ETA: 6s
3296/7440 [============>.................] - ETA: 6s
3488/7440 [=============>................] - ETA: 5s
3680/7440 [=============>................] - ETA: 5s
3872/7440 [==============>...............] - ETA: 4s
4064/7440 [===============>..............] - ETA: 4s
4256/7440 [================>.............] - ETA: 3s
4448/7440 [================>.............] - ETA: 3s
4640/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 2s
5024/7440 [===================>..........] - ETA: 2s
5216/7440 [====================>.........] - ETA: 2s
5408/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
5984/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 820us/step
Best saved model Test accuracy: 0.8221774193548387
best saved model auc_score ------------------>  0.8679534122441901
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_20 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_20[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 24, 96, 96)   384         activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 24, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 22, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 22, 96, 96)   88          concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 22, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 24, 96, 96)   528         activation_173[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 24, 96, 96)   96          dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 6, 96, 96)    1296        activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 28, 96, 96)   0           concatenate_69[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 28, 96, 96)   112         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 28, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 14, 96, 96)   392         activation_175[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 14, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 14, 48, 48)   56          average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 14, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 24, 48, 48)   336         activation_176[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_177[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 20, 48, 48)   0           average_pooling2d_16[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 20, 48, 48)   80          concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 20, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 24, 48, 48)   480         activation_178[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 24, 48, 48)   96          dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 24, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 6, 48, 48)    1296        activation_179[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 26, 48, 48)   0           concatenate_71[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 26, 48, 48)   104         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 26, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_20 (Gl (None, 26)           0           activation_180[0][0]             
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 1)            27          global_average_pooling2d_20[0][0]
==================================================================================================
Total params: 8,507
Trainable params: 8,063
Non-trainable params: 444
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 33s - loss: 0.5941 - acc: 0.7120 - val_loss: 0.5042 - val_acc: 0.8478

Epoch 00001: val_loss improved from inf to 0.50417, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 21s - loss: 0.5201 - acc: 0.7644 - val_loss: 0.4836 - val_acc: 0.8374

Epoch 00002: val_loss improved from 0.50417 to 0.48359, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 21s - loss: 0.4905 - acc: 0.7755 - val_loss: 0.6075 - val_acc: 0.7442

Epoch 00003: val_loss did not improve from 0.48359
Epoch 4/30
 - 21s - loss: 0.4702 - acc: 0.7852 - val_loss: 0.4527 - val_acc: 0.8324

Epoch 00004: val_loss improved from 0.48359 to 0.45274, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 21s - loss: 0.4523 - acc: 0.7931 - val_loss: 0.4661 - val_acc: 0.8226

Epoch 00005: val_loss did not improve from 0.45274
Epoch 6/30
 - 21s - loss: 0.4400 - acc: 0.8016 - val_loss: 0.4366 - val_acc: 0.8286

Epoch 00006: val_loss improved from 0.45274 to 0.43664, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 21s - loss: 0.4303 - acc: 0.8045 - val_loss: 0.4569 - val_acc: 0.8324

Epoch 00007: val_loss did not improve from 0.43664
Epoch 8/30
 - 21s - loss: 0.4193 - acc: 0.8114 - val_loss: 0.6354 - val_acc: 0.7302

Epoch 00008: val_loss did not improve from 0.43664
Epoch 9/30
 - 21s - loss: 0.4121 - acc: 0.8137 - val_loss: 0.4396 - val_acc: 0.7946

Epoch 00009: val_loss did not improve from 0.43664
Epoch 10/30
 - 21s - loss: 0.4074 - acc: 0.8185 - val_loss: 0.4179 - val_acc: 0.8108

Epoch 00010: val_loss improved from 0.43664 to 0.41789, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 21s - loss: 0.3988 - acc: 0.8224 - val_loss: 0.5087 - val_acc: 0.7929

Epoch 00011: val_loss did not improve from 0.41789
Epoch 12/30
 - 21s - loss: 0.3970 - acc: 0.8242 - val_loss: 0.5159 - val_acc: 0.7953

Epoch 00012: val_loss did not improve from 0.41789
Epoch 13/30
 - 21s - loss: 0.3914 - acc: 0.8288 - val_loss: 0.4491 - val_acc: 0.8020

Epoch 00013: val_loss did not improve from 0.41789
Epoch 14/30
 - 21s - loss: 0.3855 - acc: 0.8338 - val_loss: 0.4370 - val_acc: 0.8223

Epoch 00014: val_loss did not improve from 0.41789
Epoch 15/30
 - 21s - loss: 0.3813 - acc: 0.8338 - val_loss: 0.4333 - val_acc: 0.8048

Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00015: val_loss did not improve from 0.41789
Epoch 00015: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 265us/step
current Test accuracy: 0.8048387096774193
current auc_score ------------------>  0.8905585905885074

  32/7440 [..............................] - ETA: 16:56
 224/7440 [..............................] - ETA: 2:23 
 448/7440 [>.............................] - ETA: 1:10
 672/7440 [=>............................] - ETA: 45s 
 896/7440 [==>...........................] - ETA: 33s
1120/7440 [===>..........................] - ETA: 26s
1312/7440 [====>.........................] - ETA: 22s
1504/7440 [=====>........................] - ETA: 18s
1696/7440 [=====>........................] - ETA: 16s
1888/7440 [======>.......................] - ETA: 14s
2080/7440 [=======>......................] - ETA: 12s
2304/7440 [========>.....................] - ETA: 11s
2528/7440 [=========>....................] - ETA: 9s 
2752/7440 [==========>...................] - ETA: 8s
2976/7440 [===========>..................] - ETA: 7s
3200/7440 [===========>..................] - ETA: 6s
3424/7440 [============>.................] - ETA: 6s
3648/7440 [=============>................] - ETA: 5s
3872/7440 [==============>...............] - ETA: 4s
4064/7440 [===============>..............] - ETA: 4s
4288/7440 [================>.............] - ETA: 4s
4512/7440 [=================>............] - ETA: 3s
4736/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 2s
5152/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 1s
5792/7440 [======================>.......] - ETA: 1s
6016/7440 [=======================>......] - ETA: 1s
6208/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 850us/step
Best saved model Test accuracy: 0.810752688172043
best saved model auc_score ------------------>  0.9011987657532663
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_21[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_181[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_182[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 28, 96, 96)   112         concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 28, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_21 (Gl (None, 28)           0           activation_183[0][0]             
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 1)            29          global_average_pooling2d_21[0][0]
==================================================================================================
Total params: 6,637
Trainable params: 6,453
Non-trainable params: 184
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 25s - loss: 0.6360 - acc: 0.6829 - val_loss: 0.5381 - val_acc: 0.8300

Epoch 00001: val_loss improved from inf to 0.53809, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 14s - loss: 0.5942 - acc: 0.7229 - val_loss: 0.5321 - val_acc: 0.8034

Epoch 00002: val_loss improved from 0.53809 to 0.53205, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 14s - loss: 0.5655 - acc: 0.7392 - val_loss: 0.5196 - val_acc: 0.8488

Epoch 00003: val_loss improved from 0.53205 to 0.51964, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 14s - loss: 0.5445 - acc: 0.7462 - val_loss: 0.4949 - val_acc: 0.8324

Epoch 00004: val_loss improved from 0.51964 to 0.49493, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 14s - loss: 0.5307 - acc: 0.7539 - val_loss: 0.4919 - val_acc: 0.8292

Epoch 00005: val_loss improved from 0.49493 to 0.49190, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 14s - loss: 0.5197 - acc: 0.7544 - val_loss: 0.5199 - val_acc: 0.8067

Epoch 00006: val_loss did not improve from 0.49190
Epoch 7/30
 - 14s - loss: 0.5153 - acc: 0.7546 - val_loss: 0.4941 - val_acc: 0.8188

Epoch 00007: val_loss did not improve from 0.49190
Epoch 8/30
 - 14s - loss: 0.5098 - acc: 0.7590 - val_loss: 0.5196 - val_acc: 0.7878

Epoch 00008: val_loss did not improve from 0.49190
Epoch 9/30
 - 14s - loss: 0.5064 - acc: 0.7589 - val_loss: 0.5670 - val_acc: 0.7956

Epoch 00009: val_loss did not improve from 0.49190
Epoch 10/30
 - 14s - loss: 0.5019 - acc: 0.7597 - val_loss: 0.5148 - val_acc: 0.8247

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.49190
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1056/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2080/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 216us/step
current Test accuracy: 0.8247311827956989
current auc_score ------------------>  0.8706918429876287

  32/7440 [..............................] - ETA: 17:18
 256/7440 [>.............................] - ETA: 2:07 
 512/7440 [=>............................] - ETA: 1:02
 768/7440 [==>...........................] - ETA: 40s 
1024/7440 [===>..........................] - ETA: 29s
1280/7440 [====>.........................] - ETA: 22s
1536/7440 [=====>........................] - ETA: 18s
1792/7440 [======>.......................] - ETA: 15s
2048/7440 [=======>......................] - ETA: 12s
2304/7440 [========>.....................] - ETA: 11s
2560/7440 [=========>....................] - ETA: 9s 
2816/7440 [==========>...................] - ETA: 8s
3072/7440 [===========>..................] - ETA: 7s
3328/7440 [============>.................] - ETA: 6s
3552/7440 [=============>................] - ETA: 5s
3808/7440 [==============>...............] - ETA: 5s
4064/7440 [===============>..............] - ETA: 4s
4320/7440 [================>.............] - ETA: 3s
4576/7440 [=================>............] - ETA: 3s
4832/7440 [==================>...........] - ETA: 2s
5088/7440 [===================>..........] - ETA: 2s
5344/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 1s
5856/7440 [======================>.......] - ETA: 1s
6112/7440 [=======================>......] - ETA: 1s
6368/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 6s 823us/step
Best saved model Test accuracy: 0.8291666666666667
best saved model auc_score ------------------>  0.8724172230893744
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_22 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_22[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_184[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_185[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 30, 96, 96)   120         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 30, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 15, 96, 96)   450         activation_186[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 15, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 15, 48, 48)   60          average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 15, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   840         activation_187[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_188[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 29, 48, 48)   0           average_pooling2d_17[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 29, 48, 48)   116         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_189 (Activation)     (None, 29, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 14, 48, 48)   406         activation_189[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 14, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 14, 24, 24)   56          average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
activation_190 (Activation)     (None, 14, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 56, 24, 24)   784         activation_190[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_191 (Activation)     (None, 56, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_191[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 28, 24, 24)   0           average_pooling2d_18[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 28, 24, 24)   112         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_192 (Activation)     (None, 28, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_22 (Gl (None, 28)           0           activation_192[0][0]             
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 1)            29          global_average_pooling2d_22[0][0]
==================================================================================================
Total params: 26,061
Trainable params: 25,461
Non-trainable params: 600
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 32s - loss: 0.5451 - acc: 0.7603 - val_loss: 0.4934 - val_acc: 0.8109

Epoch 00001: val_loss improved from inf to 0.49336, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 19s - loss: 0.4683 - acc: 0.7974 - val_loss: 0.4767 - val_acc: 0.7923

Epoch 00002: val_loss improved from 0.49336 to 0.47666, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 19s - loss: 0.4341 - acc: 0.8151 - val_loss: 0.4917 - val_acc: 0.8106

Epoch 00003: val_loss did not improve from 0.47666
Epoch 4/30
 - 19s - loss: 0.4122 - acc: 0.8271 - val_loss: 0.4494 - val_acc: 0.8157

Epoch 00004: val_loss improved from 0.47666 to 0.44941, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 19s - loss: 0.3941 - acc: 0.8352 - val_loss: 0.5093 - val_acc: 0.7989

Epoch 00005: val_loss did not improve from 0.44941
Epoch 6/30
 - 19s - loss: 0.3836 - acc: 0.8401 - val_loss: 0.4319 - val_acc: 0.8512

Epoch 00006: val_loss improved from 0.44941 to 0.43195, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 19s - loss: 0.3735 - acc: 0.8459 - val_loss: 0.5874 - val_acc: 0.7355

Epoch 00007: val_loss did not improve from 0.43195
Epoch 8/30
 - 19s - loss: 0.3655 - acc: 0.8501 - val_loss: 0.6287 - val_acc: 0.7618

Epoch 00008: val_loss did not improve from 0.43195
Epoch 9/30
 - 19s - loss: 0.3590 - acc: 0.8554 - val_loss: 0.5933 - val_acc: 0.7556

Epoch 00009: val_loss did not improve from 0.43195
Epoch 10/30
 - 19s - loss: 0.3499 - acc: 0.8591 - val_loss: 0.6075 - val_acc: 0.7737

Epoch 00010: val_loss did not improve from 0.43195
Epoch 11/30
 - 19s - loss: 0.3430 - acc: 0.8610 - val_loss: 0.5414 - val_acc: 0.7806

Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00011: val_loss did not improve from 0.43195
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2016/7440 [=======>......................] - ETA: 1s
2240/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2624/7440 [=========>....................] - ETA: 1s
2816/7440 [==========>...................] - ETA: 1s
3040/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3424/7440 [============>.................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3808/7440 [==============>...............] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4224/7440 [================>.............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5056/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5728/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 263us/step
current Test accuracy: 0.7806451612903226
current auc_score ------------------>  0.8764073520060123

  32/7440 [..............................] - ETA: 18:37
 224/7440 [..............................] - ETA: 2:37 
 416/7440 [>.............................] - ETA: 1:23
 608/7440 [=>............................] - ETA: 56s 
 800/7440 [==>...........................] - ETA: 41s
 992/7440 [===>..........................] - ETA: 33s
1184/7440 [===>..........................] - ETA: 27s
1376/7440 [====>.........................] - ETA: 22s
1568/7440 [=====>........................] - ETA: 19s
1760/7440 [======>.......................] - ETA: 17s
1952/7440 [======>.......................] - ETA: 15s
2144/7440 [=======>......................] - ETA: 13s
2336/7440 [========>.....................] - ETA: 11s
2560/7440 [=========>....................] - ETA: 10s
2752/7440 [==========>...................] - ETA: 9s 
2944/7440 [==========>...................] - ETA: 8s
3136/7440 [===========>..................] - ETA: 7s
3328/7440 [============>.................] - ETA: 7s
3520/7440 [=============>................] - ETA: 6s
3712/7440 [=============>................] - ETA: 5s
3904/7440 [==============>...............] - ETA: 5s
4096/7440 [===============>..............] - ETA: 4s
4288/7440 [================>.............] - ETA: 4s
4480/7440 [=================>............] - ETA: 3s
4672/7440 [=================>............] - ETA: 3s
4864/7440 [==================>...........] - ETA: 3s
5056/7440 [===================>..........] - ETA: 2s
5248/7440 [====================>.........] - ETA: 2s
5440/7440 [====================>.........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5824/7440 [======================>.......] - ETA: 1s
6016/7440 [=======================>......] - ETA: 1s
6208/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6592/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 912us/step
Best saved model Test accuracy: 0.8512096774193548
best saved model auc_score ------------------>  0.9006337437854087
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_23[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_193[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_194[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_195[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_196[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 40, 96, 96)   0           concatenate_77[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_197[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_198[0][0]             
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 52, 96, 96)   0           concatenate_78[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 52, 96, 96)   208         concatenate_79[0][0]             
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 52, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_23 (Gl (None, 52)           0           activation_199[0][0]             
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 1)            53          global_average_pooling2d_23[0][0]
==================================================================================================
Total params: 21,045
Trainable params: 20,485
Non-trainable params: 560
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 43s - loss: 0.6048 - acc: 0.7150 - val_loss: 0.5688 - val_acc: 0.7917

Epoch 00001: val_loss improved from inf to 0.56879, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 31s - loss: 0.5259 - acc: 0.7667 - val_loss: 0.5250 - val_acc: 0.8176

Epoch 00002: val_loss improved from 0.56879 to 0.52503, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 30s - loss: 0.4978 - acc: 0.7747 - val_loss: 0.5488 - val_acc: 0.7903

Epoch 00003: val_loss did not improve from 0.52503
Epoch 4/30
 - 30s - loss: 0.4800 - acc: 0.7799 - val_loss: 0.7381 - val_acc: 0.7203

Epoch 00004: val_loss did not improve from 0.52503
Epoch 5/30
 - 30s - loss: 0.4684 - acc: 0.7849 - val_loss: 0.4839 - val_acc: 0.8038

Epoch 00005: val_loss improved from 0.52503 to 0.48386, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 30s - loss: 0.4564 - acc: 0.7908 - val_loss: 0.4764 - val_acc: 0.8027

Epoch 00006: val_loss improved from 0.48386 to 0.47643, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 30s - loss: 0.4453 - acc: 0.7975 - val_loss: 0.4811 - val_acc: 0.8219

Epoch 00007: val_loss did not improve from 0.47643
Epoch 8/30
 - 30s - loss: 0.4347 - acc: 0.8034 - val_loss: 0.4836 - val_acc: 0.7913

Epoch 00008: val_loss did not improve from 0.47643
Epoch 9/30
 - 30s - loss: 0.4275 - acc: 0.8078 - val_loss: 0.4487 - val_acc: 0.8060

Epoch 00009: val_loss improved from 0.47643 to 0.44873, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 31s - loss: 0.4224 - acc: 0.8126 - val_loss: 0.4589 - val_acc: 0.8305

Epoch 00010: val_loss did not improve from 0.44873
Epoch 11/30
 - 31s - loss: 0.4144 - acc: 0.8151 - val_loss: 0.5436 - val_acc: 0.7868

Epoch 00011: val_loss did not improve from 0.44873
Epoch 12/30
 - 31s - loss: 0.4078 - acc: 0.8174 - val_loss: 0.4738 - val_acc: 0.8138

Epoch 00012: val_loss did not improve from 0.44873
Epoch 13/30
 - 30s - loss: 0.4045 - acc: 0.8186 - val_loss: 0.4575 - val_acc: 0.8116

Epoch 00013: val_loss did not improve from 0.44873
Epoch 14/30
 - 30s - loss: 0.3979 - acc: 0.8246 - val_loss: 0.4563 - val_acc: 0.8050

Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00014: val_loss did not improve from 0.44873
Epoch 00014: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 355us/step
current Test accuracy: 0.8049731182795699
current auc_score ------------------>  0.8940238755925541

  32/7440 [..............................] - ETA: 19:38
 160/7440 [..............................] - ETA: 3:54 
 320/7440 [>.............................] - ETA: 1:55
 480/7440 [>.............................] - ETA: 1:16
 640/7440 [=>............................] - ETA: 56s 
 800/7440 [==>...........................] - ETA: 44s
 960/7440 [==>...........................] - ETA: 36s
1120/7440 [===>..........................] - ETA: 30s
1280/7440 [====>.........................] - ETA: 26s
1440/7440 [====>.........................] - ETA: 23s
1600/7440 [=====>........................] - ETA: 20s
1760/7440 [======>.......................] - ETA: 18s
1920/7440 [======>.......................] - ETA: 16s
2080/7440 [=======>......................] - ETA: 15s
2240/7440 [========>.....................] - ETA: 13s
2400/7440 [========>.....................] - ETA: 12s
2560/7440 [=========>....................] - ETA: 11s
2720/7440 [=========>....................] - ETA: 10s
2880/7440 [==========>...................] - ETA: 9s 
3040/7440 [===========>..................] - ETA: 8s
3200/7440 [===========>..................] - ETA: 8s
3360/7440 [============>.................] - ETA: 7s
3520/7440 [=============>................] - ETA: 7s
3680/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 6s
4000/7440 [===============>..............] - ETA: 5s
4160/7440 [===============>..............] - ETA: 5s
4320/7440 [================>.............] - ETA: 4s
4480/7440 [=================>............] - ETA: 4s
4640/7440 [=================>............] - ETA: 4s
4800/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 3s
5280/7440 [====================>.........] - ETA: 2s
5440/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 1s
6080/7440 [=======================>......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8060483870967742
best saved model auc_score ------------------>  0.8842333289975719
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_24 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_24[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_200[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_201[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 24, 96, 96)   96          concatenate_80[0][0]             
__________________________________________________________________________________________________
activation_202 (Activation)     (None, 24, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_24 (Gl (None, 24)           0           activation_202[0][0]             
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 1)            25          global_average_pooling2d_24[0][0]
==================================================================================================
Total params: 3,417
Trainable params: 3,273
Non-trainable params: 144
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 25s - loss: 0.6324 - acc: 0.6758 - val_loss: 0.5430 - val_acc: 0.8171

Epoch 00001: val_loss improved from inf to 0.54303, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 12s - loss: 0.5847 - acc: 0.7300 - val_loss: 0.5119 - val_acc: 0.8417

Epoch 00002: val_loss improved from 0.54303 to 0.51186, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 12s - loss: 0.5599 - acc: 0.7384 - val_loss: 0.4989 - val_acc: 0.8520

Epoch 00003: val_loss improved from 0.51186 to 0.49890, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 12s - loss: 0.5437 - acc: 0.7442 - val_loss: 0.5111 - val_acc: 0.8398

Epoch 00004: val_loss did not improve from 0.49890
Epoch 5/30
 - 12s - loss: 0.5320 - acc: 0.7493 - val_loss: 0.4952 - val_acc: 0.8441

Epoch 00005: val_loss improved from 0.49890 to 0.49520, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 13s - loss: 0.5249 - acc: 0.7512 - val_loss: 0.5062 - val_acc: 0.8257

Epoch 00006: val_loss did not improve from 0.49520
Epoch 7/30
 - 13s - loss: 0.5195 - acc: 0.7524 - val_loss: 0.5039 - val_acc: 0.8327

Epoch 00007: val_loss did not improve from 0.49520
Epoch 8/30
 - 13s - loss: 0.5159 - acc: 0.7550 - val_loss: 0.4898 - val_acc: 0.8272

Epoch 00008: val_loss improved from 0.49520 to 0.48977, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 12s - loss: 0.5115 - acc: 0.7564 - val_loss: 0.5045 - val_acc: 0.8293

Epoch 00009: val_loss did not improve from 0.48977
Epoch 10/30
 - 12s - loss: 0.5084 - acc: 0.7566 - val_loss: 0.5109 - val_acc: 0.7910

Epoch 00010: val_loss did not improve from 0.48977
Epoch 11/30
 - 12s - loss: 0.5063 - acc: 0.7574 - val_loss: 0.5276 - val_acc: 0.8180

Epoch 00011: val_loss did not improve from 0.48977
Epoch 12/30
 - 12s - loss: 0.5028 - acc: 0.7593 - val_loss: 0.5128 - val_acc: 0.8168

Epoch 00012: val_loss did not improve from 0.48977
Epoch 13/30
 - 13s - loss: 0.5001 - acc: 0.7593 - val_loss: 0.5017 - val_acc: 0.8183

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.48977
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 2s
 288/7440 [>.............................] - ETA: 1s
 544/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
1056/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2080/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3872/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4384/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 214us/step
current Test accuracy: 0.8182795698924731
current auc_score ------------------>  0.8398683373800439

  32/7440 [..............................] - ETA: 19:57
 256/7440 [>.............................] - ETA: 2:26 
 512/7440 [=>............................] - ETA: 1:11
 768/7440 [==>...........................] - ETA: 46s 
1024/7440 [===>..........................] - ETA: 33s
1280/7440 [====>.........................] - ETA: 26s
1536/7440 [=====>........................] - ETA: 21s
1792/7440 [======>.......................] - ETA: 17s
2048/7440 [=======>......................] - ETA: 14s
2304/7440 [========>.....................] - ETA: 12s
2560/7440 [=========>....................] - ETA: 10s
2816/7440 [==========>...................] - ETA: 9s 
3072/7440 [===========>..................] - ETA: 8s
3328/7440 [============>.................] - ETA: 7s
3584/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 5s
4096/7440 [===============>..............] - ETA: 4s
4352/7440 [================>.............] - ETA: 4s
4608/7440 [=================>............] - ETA: 3s
4864/7440 [==================>...........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 2s
5376/7440 [====================>.........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5888/7440 [======================>.......] - ETA: 1s
6144/7440 [=======================>......] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 7s 907us/step
Best saved model Test accuracy: 0.8271505376344086
best saved model auc_score ------------------>  0.8525734550237021
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_25[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_203 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_203[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_204 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_204[0][0]             
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_81[0][0]             
__________________________________________________________________________________________________
activation_205 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_205[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_206 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_206[0][0]             
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 40, 96, 96)   0           concatenate_81[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_82[0][0]             
__________________________________________________________________________________________________
activation_207 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_207[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_208 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_208[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 52, 96, 96)   0           concatenate_82[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 52, 96, 96)   208         concatenate_83[0][0]             
__________________________________________________________________________________________________
activation_209 (Activation)     (None, 52, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_25 (Gl (None, 52)           0           activation_209[0][0]             
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 1)            53          global_average_pooling2d_25[0][0]
==================================================================================================
Total params: 21,045
Trainable params: 20,485
Non-trainable params: 560
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 43s - loss: 0.5993 - acc: 0.7233 - val_loss: 0.5251 - val_acc: 0.8391

Epoch 00001: val_loss improved from inf to 0.52514, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 30s - loss: 0.5402 - acc: 0.7579 - val_loss: 0.5606 - val_acc: 0.8066

Epoch 00002: val_loss did not improve from 0.52514
Epoch 3/30
 - 30s - loss: 0.5127 - acc: 0.7685 - val_loss: 0.5685 - val_acc: 0.7997

Epoch 00003: val_loss did not improve from 0.52514
Epoch 4/30
 - 30s - loss: 0.4932 - acc: 0.7763 - val_loss: 0.4860 - val_acc: 0.8126

Epoch 00004: val_loss improved from 0.52514 to 0.48596, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 30s - loss: 0.4764 - acc: 0.7853 - val_loss: 0.5664 - val_acc: 0.7810

Epoch 00005: val_loss did not improve from 0.48596
Epoch 6/30
 - 30s - loss: 0.4652 - acc: 0.7866 - val_loss: 0.4662 - val_acc: 0.8079

Epoch 00006: val_loss improved from 0.48596 to 0.46618, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 30s - loss: 0.4527 - acc: 0.7950 - val_loss: 0.4607 - val_acc: 0.8181

Epoch 00007: val_loss improved from 0.46618 to 0.46072, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 30s - loss: 0.4434 - acc: 0.8002 - val_loss: 0.4566 - val_acc: 0.8122

Epoch 00008: val_loss improved from 0.46072 to 0.45661, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 30s - loss: 0.4351 - acc: 0.8035 - val_loss: 0.4938 - val_acc: 0.8196

Epoch 00009: val_loss did not improve from 0.45661
Epoch 10/30
 - 30s - loss: 0.4285 - acc: 0.8097 - val_loss: 0.5320 - val_acc: 0.8058

Epoch 00010: val_loss did not improve from 0.45661
Epoch 11/30
 - 30s - loss: 0.4207 - acc: 0.8133 - val_loss: 0.4647 - val_acc: 0.8223

Epoch 00011: val_loss did not improve from 0.45661
Epoch 12/30
 - 30s - loss: 0.4146 - acc: 0.8165 - val_loss: 0.4271 - val_acc: 0.8397

Epoch 00012: val_loss improved from 0.45661 to 0.42707, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 13/30
 - 30s - loss: 0.4078 - acc: 0.8213 - val_loss: 0.4474 - val_acc: 0.8281

Epoch 00013: val_loss did not improve from 0.42707
Epoch 14/30
 - 30s - loss: 0.4047 - acc: 0.8215 - val_loss: 0.4226 - val_acc: 0.8286

Epoch 00014: val_loss improved from 0.42707 to 0.42262, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 15/30
 - 30s - loss: 0.4007 - acc: 0.8223 - val_loss: 0.4246 - val_acc: 0.8310

Epoch 00015: val_loss did not improve from 0.42262
Epoch 16/30
 - 30s - loss: 0.3943 - acc: 0.8273 - val_loss: 0.4545 - val_acc: 0.8082

Epoch 00016: val_loss did not improve from 0.42262
Epoch 17/30
 - 30s - loss: 0.3902 - acc: 0.8304 - val_loss: 0.4936 - val_acc: 0.7978

Epoch 00017: val_loss did not improve from 0.42262
Epoch 18/30
 - 30s - loss: 0.3883 - acc: 0.8284 - val_loss: 0.5037 - val_acc: 0.7859

Epoch 00018: val_loss did not improve from 0.42262
Epoch 19/30
 - 30s - loss: 0.3819 - acc: 0.8340 - val_loss: 0.4609 - val_acc: 0.8126

Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00019: val_loss did not improve from 0.42262
Epoch 00019: early stopping

  32/7440 [..............................] - ETA: 3s
 192/7440 [..............................] - ETA: 2s
 352/7440 [>.............................] - ETA: 2s
 512/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 832/7440 [==>...........................] - ETA: 2s
 992/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2112/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3072/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3392/7440 [============>.................] - ETA: 1s
3552/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4032/7440 [===============>..............] - ETA: 1s
4192/7440 [===============>..............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4672/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5312/7440 [====================>.........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 345us/step
current Test accuracy: 0.8126344086021505
current auc_score ------------------>  0.8844974129957219

  32/7440 [..............................] - ETA: 20:41
 160/7440 [..............................] - ETA: 4:06 
 320/7440 [>.............................] - ETA: 2:01
 480/7440 [>.............................] - ETA: 1:20
 640/7440 [=>............................] - ETA: 59s 
 800/7440 [==>...........................] - ETA: 46s
 960/7440 [==>...........................] - ETA: 38s
1120/7440 [===>..........................] - ETA: 32s
1280/7440 [====>.........................] - ETA: 27s
1440/7440 [====>.........................] - ETA: 24s
1600/7440 [=====>........................] - ETA: 21s
1760/7440 [======>.......................] - ETA: 19s
1920/7440 [======>.......................] - ETA: 17s
2080/7440 [=======>......................] - ETA: 15s
2240/7440 [========>.....................] - ETA: 14s
2400/7440 [========>.....................] - ETA: 13s
2560/7440 [=========>....................] - ETA: 11s
2720/7440 [=========>....................] - ETA: 10s
2880/7440 [==========>...................] - ETA: 10s
3040/7440 [===========>..................] - ETA: 9s 
3200/7440 [===========>..................] - ETA: 8s
3360/7440 [============>.................] - ETA: 7s
3520/7440 [=============>................] - ETA: 7s
3680/7440 [=============>................] - ETA: 6s
3840/7440 [==============>...............] - ETA: 6s
4000/7440 [===============>..............] - ETA: 5s
4160/7440 [===============>..............] - ETA: 5s
4320/7440 [================>.............] - ETA: 4s
4480/7440 [=================>............] - ETA: 4s
4640/7440 [=================>............] - ETA: 4s
4800/7440 [==================>...........] - ETA: 3s
4960/7440 [===================>..........] - ETA: 3s
5120/7440 [===================>..........] - ETA: 3s
5280/7440 [====================>.........] - ETA: 2s
5440/7440 [====================>.........] - ETA: 2s
5600/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 1s
6080/7440 [=======================>......] - ETA: 1s
6240/7440 [========================>.....] - ETA: 1s
6400/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8286290322580645
best saved model auc_score ------------------>  0.9006471123829344
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_26 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_26[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_210 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 48, 96, 96)   768         activation_210[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_211 (Activation)     (None, 48, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_211[0][0]             
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 28, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 28, 96, 96)   112         concatenate_84[0][0]             
__________________________________________________________________________________________________
activation_212 (Activation)     (None, 28, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 48, 96, 96)   1344        activation_212[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_213 (Activation)     (None, 48, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_213[0][0]             
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 40, 96, 96)   0           concatenate_84[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 40, 96, 96)   160         concatenate_85[0][0]             
__________________________________________________________________________________________________
activation_214 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 48, 96, 96)   1920        activation_214[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 48, 96, 96)   192         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_215 (Activation)     (None, 48, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   5184        activation_215[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 52, 96, 96)   0           concatenate_85[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 52, 96, 96)   208         concatenate_86[0][0]             
__________________________________________________________________________________________________
activation_216 (Activation)     (None, 52, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 26, 96, 96)   1352        activation_216[0][0]             
__________________________________________________________________________________________________
average_pooling2d_19 (AveragePo (None, 26, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 26, 48, 48)   104         average_pooling2d_19[0][0]       
__________________________________________________________________________________________________
activation_217 (Activation)     (None, 26, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 48, 48, 48)   1248        activation_217[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_218 (Activation)     (None, 48, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_218[0][0]             
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 38, 48, 48)   0           average_pooling2d_19[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 38, 48, 48)   152         concatenate_87[0][0]             
__________________________________________________________________________________________________
activation_219 (Activation)     (None, 38, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 48, 48, 48)   1824        activation_219[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_220 (Activation)     (None, 48, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_220[0][0]             
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 50, 48, 48)   0           concatenate_87[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 50, 48, 48)   200         concatenate_88[0][0]             
__________________________________________________________________________________________________
activation_221 (Activation)     (None, 50, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 48, 48, 48)   2400        activation_221[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 48, 48, 48)   192         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_222 (Activation)     (None, 48, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 12, 48, 48)   5184        activation_222[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 62, 48, 48)   0           concatenate_88[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 62, 48, 48)   248         concatenate_89[0][0]             
__________________________________________________________________________________________________
activation_223 (Activation)     (None, 62, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_26 (Gl (None, 62)           0           activation_223[0][0]             
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 1)            63          global_average_pooling2d_26[0][0]
==================================================================================================
Total params: 44,711
Trainable params: 43,511
Non-trainable params: 1,200
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 54s - loss: 0.5791 - acc: 0.7532 - val_loss: 0.5540 - val_acc: 0.8156

Epoch 00001: val_loss improved from inf to 0.55396, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 39s - loss: 0.5020 - acc: 0.7848 - val_loss: 0.5371 - val_acc: 0.7980

Epoch 00002: val_loss improved from 0.55396 to 0.53709, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 39s - loss: 0.4668 - acc: 0.8007 - val_loss: 0.5413 - val_acc: 0.8074

Epoch 00003: val_loss did not improve from 0.53709
Epoch 4/30
 - 39s - loss: 0.4448 - acc: 0.8134 - val_loss: 0.5878 - val_acc: 0.7933

Epoch 00004: val_loss did not improve from 0.53709
Epoch 5/30
 - 39s - loss: 0.4243 - acc: 0.8272 - val_loss: 0.5225 - val_acc: 0.7867

Epoch 00005: val_loss improved from 0.53709 to 0.52248, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 39s - loss: 0.4075 - acc: 0.8361 - val_loss: 0.5465 - val_acc: 0.7855

Epoch 00006: val_loss did not improve from 0.52248
Epoch 7/30
 - 39s - loss: 0.3937 - acc: 0.8430 - val_loss: 0.5223 - val_acc: 0.8007

Epoch 00007: val_loss improved from 0.52248 to 0.52233, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 39s - loss: 0.3836 - acc: 0.8464 - val_loss: 0.6059 - val_acc: 0.7919

Epoch 00008: val_loss did not improve from 0.52233
Epoch 9/30
 - 39s - loss: 0.3714 - acc: 0.8537 - val_loss: 1.0290 - val_acc: 0.7395

Epoch 00009: val_loss did not improve from 0.52233
Epoch 10/30
 - 39s - loss: 0.3631 - acc: 0.8593 - val_loss: 0.5836 - val_acc: 0.7968

Epoch 00010: val_loss did not improve from 0.52233
Epoch 11/30
 - 39s - loss: 0.3560 - acc: 0.8617 - val_loss: 0.8616 - val_acc: 0.6929

Epoch 00011: val_loss did not improve from 0.52233
Epoch 12/30
 - 39s - loss: 0.3449 - acc: 0.8683 - val_loss: 0.9516 - val_acc: 0.7597

Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00012: val_loss did not improve from 0.52233
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 420us/step
current Test accuracy: 0.7596774193548387
current auc_score ------------------>  0.806216000404671

  32/7440 [..............................] - ETA: 23:10
 160/7440 [..............................] - ETA: 4:36 
 288/7440 [>.............................] - ETA: 2:32
 416/7440 [>.............................] - ETA: 1:44
 544/7440 [=>............................] - ETA: 1:19
 672/7440 [=>............................] - ETA: 1:03
 800/7440 [==>...........................] - ETA: 52s 
 928/7440 [==>...........................] - ETA: 44s
1056/7440 [===>..........................] - ETA: 38s
1184/7440 [===>..........................] - ETA: 34s
1312/7440 [====>.........................] - ETA: 30s
1440/7440 [====>.........................] - ETA: 27s
1568/7440 [=====>........................] - ETA: 24s
1696/7440 [=====>........................] - ETA: 22s
1824/7440 [======>.......................] - ETA: 20s
1952/7440 [======>.......................] - ETA: 19s
2080/7440 [=======>......................] - ETA: 17s
2208/7440 [=======>......................] - ETA: 16s
2336/7440 [========>.....................] - ETA: 15s
2464/7440 [========>.....................] - ETA: 14s
2592/7440 [=========>....................] - ETA: 13s
2720/7440 [=========>....................] - ETA: 12s
2848/7440 [==========>...................] - ETA: 11s
2976/7440 [===========>..................] - ETA: 10s
3104/7440 [===========>..................] - ETA: 10s
3232/7440 [============>.................] - ETA: 9s 
3360/7440 [============>.................] - ETA: 8s
3488/7440 [=============>................] - ETA: 8s
3616/7440 [=============>................] - ETA: 7s
3744/7440 [==============>...............] - ETA: 7s
3872/7440 [==============>...............] - ETA: 7s
4000/7440 [===============>..............] - ETA: 6s
4128/7440 [===============>..............] - ETA: 6s
4256/7440 [================>.............] - ETA: 5s
4384/7440 [================>.............] - ETA: 5s
4512/7440 [=================>............] - ETA: 5s
4640/7440 [=================>............] - ETA: 4s
4768/7440 [==================>...........] - ETA: 4s
4896/7440 [==================>...........] - ETA: 4s
5024/7440 [===================>..........] - ETA: 3s
5152/7440 [===================>..........] - ETA: 3s
5280/7440 [====================>.........] - ETA: 3s
5408/7440 [====================>.........] - ETA: 3s
5536/7440 [=====================>........] - ETA: 2s
5664/7440 [=====================>........] - ETA: 2s
5792/7440 [======================>.......] - ETA: 2s
5920/7440 [======================>.......] - ETA: 2s
6048/7440 [=======================>......] - ETA: 1s
6176/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 9s 1ms/step
Best saved model Test accuracy: 0.8006720430107527
best saved model auc_score ------------------>  0.872352078274945
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_27[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_224 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 56, 96, 96)   896         activation_224[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 56, 96, 96)   224         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_225 (Activation)     (None, 56, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 14, 96, 96)   7056        activation_225[0][0]             
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 30, 96, 96)   120         concatenate_90[0][0]             
__________________________________________________________________________________________________
activation_226 (Activation)     (None, 30, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 15, 96, 96)   450         activation_226[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 15, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 15, 48, 48)   60          average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_227 (Activation)     (None, 15, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 56, 48, 48)   840         activation_227[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 56, 48, 48)   224         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_228 (Activation)     (None, 56, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 14, 48, 48)   7056        activation_228[0][0]             
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 29, 48, 48)   0           average_pooling2d_20[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 29, 48, 48)   116         concatenate_91[0][0]             
__________________________________________________________________________________________________
activation_229 (Activation)     (None, 29, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 14, 48, 48)   406         activation_229[0][0]             
__________________________________________________________________________________________________
average_pooling2d_21 (AveragePo (None, 14, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 14, 24, 24)   56          average_pooling2d_21[0][0]       
__________________________________________________________________________________________________
activation_230 (Activation)     (None, 14, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 56, 24, 24)   784         activation_230[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 56, 24, 24)   224         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_231 (Activation)     (None, 56, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 14, 24, 24)   7056        activation_231[0][0]             
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 28, 24, 24)   0           average_pooling2d_21[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 28, 24, 24)   112         concatenate_92[0][0]             
__________________________________________________________________________________________________
activation_232 (Activation)     (None, 28, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_27 (Gl (None, 28)           0           activation_232[0][0]             
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 1)            29          global_average_pooling2d_27[0][0]
==================================================================================================
Total params: 26,061
Trainable params: 25,461
Non-trainable params: 600
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 35s - loss: 0.5565 - acc: 0.7560 - val_loss: 0.4652 - val_acc: 0.8140

Epoch 00001: val_loss improved from inf to 0.46524, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 20s - loss: 0.4832 - acc: 0.7882 - val_loss: 0.4581 - val_acc: 0.8380

Epoch 00002: val_loss improved from 0.46524 to 0.45806, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 20s - loss: 0.4519 - acc: 0.8005 - val_loss: 0.4897 - val_acc: 0.8005

Epoch 00003: val_loss did not improve from 0.45806
Epoch 4/30
 - 20s - loss: 0.4291 - acc: 0.8144 - val_loss: 0.4144 - val_acc: 0.8336

Epoch 00004: val_loss improved from 0.45806 to 0.41440, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 20s - loss: 0.4159 - acc: 0.8196 - val_loss: 0.4074 - val_acc: 0.8378

Epoch 00005: val_loss improved from 0.41440 to 0.40739, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 20s - loss: 0.4016 - acc: 0.8295 - val_loss: 0.5406 - val_acc: 0.7788

Epoch 00006: val_loss did not improve from 0.40739
Epoch 7/30
 - 20s - loss: 0.3930 - acc: 0.8347 - val_loss: 0.7224 - val_acc: 0.6995

Epoch 00007: val_loss did not improve from 0.40739
Epoch 8/30
 - 20s - loss: 0.3840 - acc: 0.8398 - val_loss: 0.4557 - val_acc: 0.7949

Epoch 00008: val_loss did not improve from 0.40739
Epoch 9/30
 - 20s - loss: 0.3761 - acc: 0.8438 - val_loss: 0.4520 - val_acc: 0.8020

Epoch 00009: val_loss did not improve from 0.40739
Epoch 10/30
 - 20s - loss: 0.3690 - acc: 0.8469 - val_loss: 0.4887 - val_acc: 0.7978

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.40739
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 268us/step
current Test accuracy: 0.7978494623655914
current auc_score ------------------>  0.8871443953058156

  32/7440 [..............................] - ETA: 24:18
 224/7440 [..............................] - ETA: 3:24 
 448/7440 [>.............................] - ETA: 1:40
 640/7440 [=>............................] - ETA: 1:08
 832/7440 [==>...........................] - ETA: 51s 
1024/7440 [===>..........................] - ETA: 41s
1216/7440 [===>..........................] - ETA: 33s
1440/7440 [====>.........................] - ETA: 27s
1664/7440 [=====>........................] - ETA: 23s
1888/7440 [======>.......................] - ETA: 19s
2112/7440 [=======>......................] - ETA: 17s
2304/7440 [========>.....................] - ETA: 15s
2528/7440 [=========>....................] - ETA: 13s
2752/7440 [==========>...................] - ETA: 11s
2976/7440 [===========>..................] - ETA: 10s
3200/7440 [===========>..................] - ETA: 9s 
3424/7440 [============>.................] - ETA: 8s
3616/7440 [=============>................] - ETA: 7s
3840/7440 [==============>...............] - ETA: 6s
4064/7440 [===============>..............] - ETA: 6s
4288/7440 [================>.............] - ETA: 5s
4512/7440 [=================>............] - ETA: 4s
4736/7440 [==================>...........] - ETA: 4s
4960/7440 [===================>..........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 3s
5408/7440 [====================>.........] - ETA: 2s
5632/7440 [=====================>........] - ETA: 2s
5856/7440 [======================>.......] - ETA: 2s
6080/7440 [=======================>......] - ETA: 1s
6304/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 8s 1ms/step
Best saved model Test accuracy: 0.8377688172043011
best saved model auc_score ------------------>  0.9177841368944386
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_28 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_28[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_233 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_233[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_234 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_234[0][0]             
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 32, 96, 96)   128         concatenate_93[0][0]             
__________________________________________________________________________________________________
activation_235 (Activation)     (None, 32, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 16, 96, 96)   512         activation_235[0][0]             
__________________________________________________________________________________________________
average_pooling2d_22 (AveragePo (None, 16, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 16, 48, 48)   64          average_pooling2d_22[0][0]       
__________________________________________________________________________________________________
activation_236 (Activation)     (None, 16, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1024        activation_236[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_237 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_237[0][0]             
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 32, 48, 48)   0           average_pooling2d_22[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 32, 48, 48)   128         concatenate_94[0][0]             
__________________________________________________________________________________________________
activation_238 (Activation)     (None, 32, 48, 48)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_28 (Gl (None, 32)           0           activation_238[0][0]             
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 1)            33          global_average_pooling2d_28[0][0]
==================================================================================================
Total params: 22,209
Trainable params: 21,761
Non-trainable params: 448
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 35s - loss: 0.5706 - acc: 0.7376 - val_loss: 0.5278 - val_acc: 0.7907

Epoch 00001: val_loss improved from inf to 0.52776, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 19s - loss: 0.5097 - acc: 0.7703 - val_loss: 0.4801 - val_acc: 0.8230

Epoch 00002: val_loss improved from 0.52776 to 0.48014, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 19s - loss: 0.4842 - acc: 0.7803 - val_loss: 0.4790 - val_acc: 0.7966

Epoch 00003: val_loss improved from 0.48014 to 0.47896, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 19s - loss: 0.4637 - acc: 0.7902 - val_loss: 0.5004 - val_acc: 0.8012

Epoch 00004: val_loss did not improve from 0.47896
Epoch 5/30
 - 19s - loss: 0.4515 - acc: 0.7960 - val_loss: 0.4652 - val_acc: 0.8211

Epoch 00005: val_loss improved from 0.47896 to 0.46521, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 19s - loss: 0.4383 - acc: 0.8051 - val_loss: 0.4980 - val_acc: 0.8034

Epoch 00006: val_loss did not improve from 0.46521
Epoch 7/30
 - 19s - loss: 0.4312 - acc: 0.8069 - val_loss: 0.5116 - val_acc: 0.8181

Epoch 00007: val_loss did not improve from 0.46521
Epoch 8/30
 - 19s - loss: 0.4213 - acc: 0.8116 - val_loss: 0.4964 - val_acc: 0.8191

Epoch 00008: val_loss did not improve from 0.46521
Epoch 9/30
 - 19s - loss: 0.4162 - acc: 0.8148 - val_loss: 0.4791 - val_acc: 0.7821

Epoch 00009: val_loss did not improve from 0.46521
Epoch 10/30
 - 19s - loss: 0.4075 - acc: 0.8198 - val_loss: 0.5191 - val_acc: 0.8015

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.46521
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 3s
 224/7440 [..............................] - ETA: 2s
 416/7440 [>.............................] - ETA: 2s
 608/7440 [=>............................] - ETA: 1s
 800/7440 [==>...........................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
1952/7440 [======>.......................] - ETA: 1s
2144/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 1s
2528/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3296/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3872/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4256/7440 [================>.............] - ETA: 0s
4448/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4832/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5984/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7136/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 269us/step
current Test accuracy: 0.801478494623656
current auc_score ------------------>  0.8424239796508268

  32/7440 [..............................] - ETA: 25:22
 192/7440 [..............................] - ETA: 4:10 
 384/7440 [>.............................] - ETA: 2:02
 576/7440 [=>............................] - ETA: 1:20
 768/7440 [==>...........................] - ETA: 59s 
 960/7440 [==>...........................] - ETA: 46s
1152/7440 [===>..........................] - ETA: 37s
1344/7440 [====>.........................] - ETA: 31s
1536/7440 [=====>........................] - ETA: 26s
1728/7440 [=====>........................] - ETA: 23s
1920/7440 [======>.......................] - ETA: 20s
2112/7440 [=======>......................] - ETA: 18s
2304/7440 [========>.....................] - ETA: 16s
2496/7440 [=========>....................] - ETA: 14s
2688/7440 [=========>....................] - ETA: 12s
2880/7440 [==========>...................] - ETA: 11s
3072/7440 [===========>..................] - ETA: 10s
3264/7440 [============>.................] - ETA: 9s 
3456/7440 [============>.................] - ETA: 8s
3648/7440 [=============>................] - ETA: 7s
3840/7440 [==============>...............] - ETA: 7s
4032/7440 [===============>..............] - ETA: 6s
4224/7440 [================>.............] - ETA: 5s
4416/7440 [================>.............] - ETA: 5s
4608/7440 [=================>............] - ETA: 4s
4800/7440 [==================>...........] - ETA: 4s
4992/7440 [===================>..........] - ETA: 3s
5184/7440 [===================>..........] - ETA: 3s
5376/7440 [====================>.........] - ETA: 3s
5568/7440 [=====================>........] - ETA: 2s
5760/7440 [======================>.......] - ETA: 2s
5952/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 1s
6336/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6720/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 9s 1ms/step
Best saved model Test accuracy: 0.8211021505376344
best saved model auc_score ------------------>  0.8946490201179328
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_29[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_239 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_239[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_240 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_240[0][0]             
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_95[0][0]             
__________________________________________________________________________________________________
activation_241 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_241[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_242 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_242[0][0]             
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 32, 96, 96)   0           concatenate_95[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_96[0][0]             
__________________________________________________________________________________________________
activation_243 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_243[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_244 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_244[0][0]             
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 40, 96, 96)   0           concatenate_96[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_97[0][0]             
__________________________________________________________________________________________________
activation_245 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_245[0][0]             
__________________________________________________________________________________________________
average_pooling2d_23 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_23[0][0]       
__________________________________________________________________________________________________
activation_246 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_246[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_247 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_247[0][0]             
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 28, 48, 48)   0           average_pooling2d_23[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_98[0][0]             
__________________________________________________________________________________________________
activation_248 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_248[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_249 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_249[0][0]             
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 36, 48, 48)   0           concatenate_98[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_99[0][0]             
__________________________________________________________________________________________________
activation_250 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_250[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_251 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_251[0][0]             
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 44, 48, 48)   0           concatenate_99[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_100[0][0]            
__________________________________________________________________________________________________
activation_252 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_252[0][0]             
__________________________________________________________________________________________________
average_pooling2d_24 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_24[0][0]       
__________________________________________________________________________________________________
activation_253 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_253[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_254 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_254[0][0]             
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_24[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_101[0][0]            
__________________________________________________________________________________________________
activation_255 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_255[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_256 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_256[0][0]             
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 38, 24, 24)   0           concatenate_101[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_102[0][0]            
__________________________________________________________________________________________________
activation_257 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_257[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_258 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_258[0][0]             
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 46, 24, 24)   0           concatenate_102[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_103[0][0]            
__________________________________________________________________________________________________
activation_259 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_29 (Gl (None, 46)           0           activation_259[0][0]             
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 1)            47          global_average_pooling2d_29[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 55s - loss: 0.5565 - acc: 0.7637 - val_loss: 0.5047 - val_acc: 0.8181

Epoch 00001: val_loss improved from inf to 0.50474, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 36s - loss: 0.4616 - acc: 0.8103 - val_loss: 0.5187 - val_acc: 0.7973

Epoch 00002: val_loss did not improve from 0.50474
Epoch 3/30
 - 36s - loss: 0.4191 - acc: 0.8334 - val_loss: 0.5323 - val_acc: 0.8078

Epoch 00003: val_loss did not improve from 0.50474
Epoch 4/30
 - 36s - loss: 0.3861 - acc: 0.8519 - val_loss: 0.5203 - val_acc: 0.7962

Epoch 00004: val_loss did not improve from 0.50474
Epoch 5/30
 - 36s - loss: 0.3652 - acc: 0.8617 - val_loss: 0.5828 - val_acc: 0.7872

Epoch 00005: val_loss did not improve from 0.50474
Epoch 6/30
 - 36s - loss: 0.3464 - acc: 0.8724 - val_loss: 0.4890 - val_acc: 0.8015

Epoch 00006: val_loss improved from 0.50474 to 0.48897, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 36s - loss: 0.3310 - acc: 0.8805 - val_loss: 0.4724 - val_acc: 0.8161

Epoch 00007: val_loss improved from 0.48897 to 0.47244, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 36s - loss: 0.3180 - acc: 0.8852 - val_loss: 0.5469 - val_acc: 0.8290

Epoch 00008: val_loss did not improve from 0.47244
Epoch 9/30
 - 36s - loss: 0.3084 - acc: 0.8914 - val_loss: 0.4586 - val_acc: 0.7996

Epoch 00009: val_loss improved from 0.47244 to 0.45855, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 36s - loss: 0.2961 - acc: 0.8962 - val_loss: 0.5587 - val_acc: 0.8056

Epoch 00010: val_loss did not improve from 0.45855
Epoch 11/30
 - 36s - loss: 0.2856 - acc: 0.9038 - val_loss: 0.4556 - val_acc: 0.8270

Epoch 00011: val_loss improved from 0.45855 to 0.45562, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 12/30
 - 36s - loss: 0.2780 - acc: 0.9053 - val_loss: 0.7312 - val_acc: 0.7652

Epoch 00012: val_loss did not improve from 0.45562
Epoch 13/30
 - 36s - loss: 0.2679 - acc: 0.9115 - val_loss: 0.5102 - val_acc: 0.7983

Epoch 00013: val_loss did not improve from 0.45562
Epoch 14/30
 - 36s - loss: 0.2593 - acc: 0.9138 - val_loss: 0.5360 - val_acc: 0.8046

Epoch 00014: val_loss did not improve from 0.45562
Epoch 15/30
 - 36s - loss: 0.2549 - acc: 0.9147 - val_loss: 0.3951 - val_acc: 0.8444

Epoch 00015: val_loss improved from 0.45562 to 0.39511, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 16/30
 - 36s - loss: 0.2473 - acc: 0.9204 - val_loss: 0.5759 - val_acc: 0.8106

Epoch 00016: val_loss did not improve from 0.39511
Epoch 17/30
 - 36s - loss: 0.2409 - acc: 0.9227 - val_loss: 0.4470 - val_acc: 0.8164

Epoch 00017: val_loss did not improve from 0.39511
Epoch 18/30
 - 36s - loss: 0.2328 - acc: 0.9257 - val_loss: 0.6309 - val_acc: 0.8121

Epoch 00018: val_loss did not improve from 0.39511
Epoch 19/30
 - 36s - loss: 0.2284 - acc: 0.9280 - val_loss: 0.4865 - val_acc: 0.8323

Epoch 00019: val_loss did not improve from 0.39511
Epoch 20/30
 - 36s - loss: 0.2232 - acc: 0.9297 - val_loss: 0.6349 - val_acc: 0.7706

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00020: val_loss did not improve from 0.39511
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 2s
 544/7440 [=>............................] - ETA: 2s
 704/7440 [=>............................] - ETA: 2s
 864/7440 [==>...........................] - ETA: 2s
1024/7440 [===>..........................] - ETA: 2s
1152/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1472/7440 [====>.........................] - ETA: 2s
1632/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2112/7440 [=======>......................] - ETA: 2s
2272/7440 [========>.....................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 1s
2592/7440 [=========>....................] - ETA: 1s
2752/7440 [==========>...................] - ETA: 1s
2912/7440 [==========>...................] - ETA: 1s
3040/7440 [===========>..................] - ETA: 1s
3200/7440 [===========>..................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3520/7440 [=============>................] - ETA: 1s
3680/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4160/7440 [===============>..............] - ETA: 1s
4320/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4800/7440 [==================>...........] - ETA: 1s
4960/7440 [===================>..........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5600/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 390us/step
current Test accuracy: 0.7705645161290322
current auc_score ------------------>  0.897826157648283

  32/7440 [..............................] - ETA: 32:28
 160/7440 [..............................] - ETA: 6:25 
 288/7440 [>.............................] - ETA: 3:31
 416/7440 [>.............................] - ETA: 2:24
 544/7440 [=>............................] - ETA: 1:49
 672/7440 [=>............................] - ETA: 1:27
 800/7440 [==>...........................] - ETA: 1:12
 928/7440 [==>...........................] - ETA: 1:01
1056/7440 [===>..........................] - ETA: 53s 
1184/7440 [===>..........................] - ETA: 47s
1312/7440 [====>.........................] - ETA: 41s
1440/7440 [====>.........................] - ETA: 37s
1568/7440 [=====>........................] - ETA: 33s
1696/7440 [=====>........................] - ETA: 30s
1824/7440 [======>.......................] - ETA: 28s
1952/7440 [======>.......................] - ETA: 25s
2080/7440 [=======>......................] - ETA: 23s
2208/7440 [=======>......................] - ETA: 22s
2336/7440 [========>.....................] - ETA: 20s
2464/7440 [========>.....................] - ETA: 19s
2592/7440 [=========>....................] - ETA: 17s
2720/7440 [=========>....................] - ETA: 16s
2848/7440 [==========>...................] - ETA: 15s
2976/7440 [===========>..................] - ETA: 14s
3104/7440 [===========>..................] - ETA: 13s
3232/7440 [============>.................] - ETA: 12s
3360/7440 [============>.................] - ETA: 11s
3488/7440 [=============>................] - ETA: 11s
3616/7440 [=============>................] - ETA: 10s
3744/7440 [==============>...............] - ETA: 9s 
3872/7440 [==============>...............] - ETA: 9s
4000/7440 [===============>..............] - ETA: 8s
4128/7440 [===============>..............] - ETA: 8s
4256/7440 [================>.............] - ETA: 7s
4384/7440 [================>.............] - ETA: 7s
4512/7440 [=================>............] - ETA: 6s
4640/7440 [=================>............] - ETA: 6s
4768/7440 [==================>...........] - ETA: 5s
4896/7440 [==================>...........] - ETA: 5s
5024/7440 [===================>..........] - ETA: 5s
5152/7440 [===================>..........] - ETA: 4s
5280/7440 [====================>.........] - ETA: 4s
5408/7440 [====================>.........] - ETA: 3s
5536/7440 [=====================>........] - ETA: 3s
5664/7440 [=====================>........] - ETA: 3s
5792/7440 [======================>.......] - ETA: 3s
5920/7440 [======================>.......] - ETA: 2s
6048/7440 [=======================>......] - ETA: 2s
6176/7440 [=======================>......] - ETA: 2s
6304/7440 [========================>.....] - ETA: 1s
6432/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 1s
6816/7440 [==========================>...] - ETA: 1s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 11s 2ms/step
Best saved model Test accuracy: 0.8443548387096774
best saved model auc_score ------------------>  0.9297744320152619
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_30 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_30[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_260 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_260[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_261 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_261[0][0]             
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_104[0][0]            
__________________________________________________________________________________________________
activation_262 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_262[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_263 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_263[0][0]             
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 32, 96, 96)   0           concatenate_104[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_105[0][0]            
__________________________________________________________________________________________________
activation_264 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_264[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_265 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_265[0][0]             
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 40, 96, 96)   0           concatenate_105[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_106[0][0]            
__________________________________________________________________________________________________
activation_266 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_266[0][0]             
__________________________________________________________________________________________________
average_pooling2d_25 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_25[0][0]       
__________________________________________________________________________________________________
activation_267 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_267[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_268 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_268[0][0]             
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_25[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_107[0][0]            
__________________________________________________________________________________________________
activation_269 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_269[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_270 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_270[0][0]             
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 36, 48, 48)   0           concatenate_107[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_108[0][0]            
__________________________________________________________________________________________________
activation_271 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_271[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_272 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_272[0][0]             
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 44, 48, 48)   0           concatenate_108[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_109[0][0]            
__________________________________________________________________________________________________
activation_273 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_273[0][0]             
__________________________________________________________________________________________________
average_pooling2d_26 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_26[0][0]       
__________________________________________________________________________________________________
activation_274 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_274[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_275 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_275[0][0]             
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_26[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_110[0][0]            
__________________________________________________________________________________________________
activation_276 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_276[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_277 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_277[0][0]             
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 38, 24, 24)   0           concatenate_110[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_111[0][0]            
__________________________________________________________________________________________________
activation_278 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_278[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_279 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_279[0][0]             
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 46, 24, 24)   0           concatenate_111[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_112[0][0]            
__________________________________________________________________________________________________
activation_280 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_30 (Gl (None, 46)           0           activation_280[0][0]             
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 1)            47          global_average_pooling2d_30[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 57s - loss: 0.5475 - acc: 0.7687 - val_loss: 0.4886 - val_acc: 0.8110

Epoch 00001: val_loss improved from inf to 0.48864, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 36s - loss: 0.4651 - acc: 0.8082 - val_loss: 0.4893 - val_acc: 0.8157

Epoch 00002: val_loss did not improve from 0.48864
Epoch 3/30
 - 36s - loss: 0.4295 - acc: 0.8270 - val_loss: 0.4674 - val_acc: 0.8109

Epoch 00003: val_loss improved from 0.48864 to 0.46735, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 36s - loss: 0.4016 - acc: 0.8426 - val_loss: 0.6684 - val_acc: 0.7797

Epoch 00004: val_loss did not improve from 0.46735
Epoch 5/30
 - 36s - loss: 0.3765 - acc: 0.8569 - val_loss: 0.4627 - val_acc: 0.8296

Epoch 00005: val_loss improved from 0.46735 to 0.46271, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 36s - loss: 0.3604 - acc: 0.8661 - val_loss: 0.4379 - val_acc: 0.8321

Epoch 00006: val_loss improved from 0.46271 to 0.43793, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 36s - loss: 0.3406 - acc: 0.8763 - val_loss: 0.5809 - val_acc: 0.8152

Epoch 00007: val_loss did not improve from 0.43793
Epoch 8/30
 - 36s - loss: 0.3247 - acc: 0.8835 - val_loss: 0.4079 - val_acc: 0.8452

Epoch 00008: val_loss improved from 0.43793 to 0.40788, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 36s - loss: 0.3107 - acc: 0.8893 - val_loss: 0.4464 - val_acc: 0.8465

Epoch 00009: val_loss did not improve from 0.40788
Epoch 10/30
 - 36s - loss: 0.2973 - acc: 0.8994 - val_loss: 0.5860 - val_acc: 0.7921

Epoch 00010: val_loss did not improve from 0.40788
Epoch 11/30
 - 36s - loss: 0.2896 - acc: 0.9007 - val_loss: 0.7629 - val_acc: 0.7792

Epoch 00011: val_loss did not improve from 0.40788
Epoch 12/30
 - 36s - loss: 0.2796 - acc: 0.9069 - val_loss: 0.7016 - val_acc: 0.7867

Epoch 00012: val_loss did not improve from 0.40788
Epoch 13/30
 - 36s - loss: 0.2700 - acc: 0.9094 - val_loss: 0.5378 - val_acc: 0.8044

Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00013: val_loss did not improve from 0.40788
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 415us/step
current Test accuracy: 0.8044354838709677
current auc_score ------------------>  0.8992823592322813

  32/7440 [..............................] - ETA: 34:26
 128/7440 [..............................] - ETA: 8:32 
 256/7440 [>.............................] - ETA: 4:13
 384/7440 [>.............................] - ETA: 2:47
 512/7440 [=>............................] - ETA: 2:03
 640/7440 [=>............................] - ETA: 1:37
 768/7440 [==>...........................] - ETA: 1:20
 896/7440 [==>...........................] - ETA: 1:07
1024/7440 [===>..........................] - ETA: 58s 
1152/7440 [===>..........................] - ETA: 51s
1280/7440 [====>.........................] - ETA: 45s
1408/7440 [====>.........................] - ETA: 40s
1536/7440 [=====>........................] - ETA: 36s
1664/7440 [=====>........................] - ETA: 33s
1792/7440 [======>.......................] - ETA: 30s
1920/7440 [======>.......................] - ETA: 28s
2048/7440 [=======>......................] - ETA: 25s
2176/7440 [=======>......................] - ETA: 23s
2304/7440 [========>.....................] - ETA: 22s
2432/7440 [========>.....................] - ETA: 20s
2560/7440 [=========>....................] - ETA: 19s
2688/7440 [=========>....................] - ETA: 17s
2816/7440 [==========>...................] - ETA: 16s
2944/7440 [==========>...................] - ETA: 15s
3072/7440 [===========>..................] - ETA: 14s
3200/7440 [===========>..................] - ETA: 13s
3328/7440 [============>.................] - ETA: 12s
3456/7440 [============>.................] - ETA: 11s
3584/7440 [=============>................] - ETA: 11s
3712/7440 [=============>................] - ETA: 10s
3840/7440 [==============>...............] - ETA: 9s 
3968/7440 [===============>..............] - ETA: 9s
4096/7440 [===============>..............] - ETA: 8s
4224/7440 [================>.............] - ETA: 8s
4352/7440 [================>.............] - ETA: 7s
4480/7440 [=================>............] - ETA: 7s
4608/7440 [=================>............] - ETA: 6s
4736/7440 [==================>...........] - ETA: 6s
4864/7440 [==================>...........] - ETA: 5s
4992/7440 [===================>..........] - ETA: 5s
5120/7440 [===================>..........] - ETA: 5s
5248/7440 [====================>.........] - ETA: 4s
5376/7440 [====================>.........] - ETA: 4s
5504/7440 [=====================>........] - ETA: 3s
5632/7440 [=====================>........] - ETA: 3s
5760/7440 [======================>.......] - ETA: 3s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 2s
6144/7440 [=======================>......] - ETA: 2s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 1s
6528/7440 [=========================>....] - ETA: 1s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 12s 2ms/step
Best saved model Test accuracy: 0.8451612903225807
best saved model auc_score ------------------>  0.9293955226037692
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_31[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_281 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_281[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_282 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_282[0][0]             
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_113[0][0]            
__________________________________________________________________________________________________
activation_283 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_283[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_284 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_284[0][0]             
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 32, 96, 96)   0           concatenate_113[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_114[0][0]            
__________________________________________________________________________________________________
activation_285 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_285[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_286 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_286[0][0]             
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 40, 96, 96)   0           concatenate_114[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_115[0][0]            
__________________________________________________________________________________________________
activation_287 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_287[0][0]             
__________________________________________________________________________________________________
average_pooling2d_27 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_27[0][0]       
__________________________________________________________________________________________________
activation_288 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_288[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_289 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_289[0][0]             
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_27[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_116[0][0]            
__________________________________________________________________________________________________
activation_290 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_290[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_291 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_291[0][0]             
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 36, 48, 48)   0           concatenate_116[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_117[0][0]            
__________________________________________________________________________________________________
activation_292 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_292[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_293 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_293[0][0]             
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 44, 48, 48)   0           concatenate_117[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_118[0][0]            
__________________________________________________________________________________________________
activation_294 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_294[0][0]             
__________________________________________________________________________________________________
average_pooling2d_28 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_28[0][0]       
__________________________________________________________________________________________________
activation_295 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_295[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_296 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_296[0][0]             
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_28[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_119[0][0]            
__________________________________________________________________________________________________
activation_297 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_297[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_298 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_298[0][0]             
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 38, 24, 24)   0           concatenate_119[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_120[0][0]            
__________________________________________________________________________________________________
activation_299 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_299[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_300 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_300[0][0]             
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 46, 24, 24)   0           concatenate_120[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_121[0][0]            
__________________________________________________________________________________________________
activation_301 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_31 (Gl (None, 46)           0           activation_301[0][0]             
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 1)            47          global_average_pooling2d_31[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 60s - loss: 0.5603 - acc: 0.7600 - val_loss: 0.5447 - val_acc: 0.7691

Epoch 00001: val_loss improved from inf to 0.54474, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 36s - loss: 0.4631 - acc: 0.8111 - val_loss: 0.5063 - val_acc: 0.8138

Epoch 00002: val_loss improved from 0.54474 to 0.50630, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 36s - loss: 0.4163 - acc: 0.8348 - val_loss: 0.5124 - val_acc: 0.7914

Epoch 00003: val_loss did not improve from 0.50630
Epoch 4/30
 - 36s - loss: 0.3831 - acc: 0.8542 - val_loss: 0.4664 - val_acc: 0.8339

Epoch 00004: val_loss improved from 0.50630 to 0.46643, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 36s - loss: 0.3608 - acc: 0.8680 - val_loss: 0.4510 - val_acc: 0.8300

Epoch 00005: val_loss improved from 0.46643 to 0.45097, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 36s - loss: 0.3397 - acc: 0.8764 - val_loss: 0.4938 - val_acc: 0.8157

Epoch 00006: val_loss did not improve from 0.45097
Epoch 7/30
 - 36s - loss: 0.3222 - acc: 0.8860 - val_loss: 0.4581 - val_acc: 0.8180

Epoch 00007: val_loss did not improve from 0.45097
Epoch 8/30
 - 36s - loss: 0.3096 - acc: 0.8925 - val_loss: 0.4528 - val_acc: 0.8290

Epoch 00008: val_loss did not improve from 0.45097
Epoch 9/30
 - 36s - loss: 0.2969 - acc: 0.8981 - val_loss: 0.5014 - val_acc: 0.8168

Epoch 00009: val_loss did not improve from 0.45097
Epoch 10/30
 - 36s - loss: 0.2861 - acc: 0.9034 - val_loss: 0.9371 - val_acc: 0.7552

Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00010: val_loss did not improve from 0.45097
Epoch 00010: early stopping

  32/7440 [..............................] - ETA: 4s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 418us/step
current Test accuracy: 0.755241935483871
current auc_score ------------------>  0.8059531448722396

  32/7440 [..............................] - ETA: 36:55
 160/7440 [..............................] - ETA: 7:18 
 288/7440 [>.............................] - ETA: 4:00
 416/7440 [>.............................] - ETA: 2:44
 544/7440 [=>............................] - ETA: 2:04
 672/7440 [=>............................] - ETA: 1:39
 800/7440 [==>...........................] - ETA: 1:22
 928/7440 [==>...........................] - ETA: 1:09
1056/7440 [===>..........................] - ETA: 1:00
1184/7440 [===>..........................] - ETA: 53s 
1312/7440 [====>.........................] - ETA: 47s
1440/7440 [====>.........................] - ETA: 42s
1568/7440 [=====>........................] - ETA: 38s
1696/7440 [=====>........................] - ETA: 34s
1824/7440 [======>.......................] - ETA: 31s
1952/7440 [======>.......................] - ETA: 29s
2080/7440 [=======>......................] - ETA: 26s
2208/7440 [=======>......................] - ETA: 24s
2336/7440 [========>.....................] - ETA: 23s
2464/7440 [========>.....................] - ETA: 21s
2592/7440 [=========>....................] - ETA: 19s
2720/7440 [=========>....................] - ETA: 18s
2848/7440 [==========>...................] - ETA: 17s
2976/7440 [===========>..................] - ETA: 16s
3104/7440 [===========>..................] - ETA: 15s
3232/7440 [============>.................] - ETA: 14s
3360/7440 [============>.................] - ETA: 13s
3488/7440 [=============>................] - ETA: 12s
3616/7440 [=============>................] - ETA: 11s
3744/7440 [==============>...............] - ETA: 10s
3872/7440 [==============>...............] - ETA: 10s
4000/7440 [===============>..............] - ETA: 9s 
4128/7440 [===============>..............] - ETA: 9s
4256/7440 [================>.............] - ETA: 8s
4384/7440 [================>.............] - ETA: 7s
4512/7440 [=================>............] - ETA: 7s
4640/7440 [=================>............] - ETA: 6s
4768/7440 [==================>...........] - ETA: 6s
4896/7440 [==================>...........] - ETA: 6s
5024/7440 [===================>..........] - ETA: 5s
5152/7440 [===================>..........] - ETA: 5s
5280/7440 [====================>.........] - ETA: 4s
5408/7440 [====================>.........] - ETA: 4s
5536/7440 [=====================>........] - ETA: 4s
5664/7440 [=====================>........] - ETA: 3s
5792/7440 [======================>.......] - ETA: 3s
5920/7440 [======================>.......] - ETA: 3s
6048/7440 [=======================>......] - ETA: 2s
6176/7440 [=======================>......] - ETA: 2s
6304/7440 [========================>.....] - ETA: 2s
6432/7440 [========================>.....] - ETA: 1s
6560/7440 [=========================>....] - ETA: 1s
6688/7440 [=========================>....] - ETA: 1s
6816/7440 [==========================>...] - ETA: 1s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 13s 2ms/step
Best saved model Test accuracy: 0.8299731182795699
best saved model auc_score ------------------>  0.9156659729448492
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_32 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_32[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_302 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_302[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_303 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_303[0][0]             
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_122[0][0]            
__________________________________________________________________________________________________
activation_304 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_304[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_305 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_305[0][0]             
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 32, 96, 96)   0           concatenate_122[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_123[0][0]            
__________________________________________________________________________________________________
activation_306 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_306[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_307 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_307[0][0]             
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 40, 96, 96)   0           concatenate_123[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_124[0][0]            
__________________________________________________________________________________________________
activation_308 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_308[0][0]             
__________________________________________________________________________________________________
average_pooling2d_29 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_29[0][0]       
__________________________________________________________________________________________________
activation_309 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_309[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_310 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_310[0][0]             
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_29[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_125[0][0]            
__________________________________________________________________________________________________
activation_311 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_311[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_312 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_312[0][0]             
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 36, 48, 48)   0           concatenate_125[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_126[0][0]            
__________________________________________________________________________________________________
activation_313 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_313[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_314 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_314[0][0]             
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 44, 48, 48)   0           concatenate_126[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_127[0][0]            
__________________________________________________________________________________________________
activation_315 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_315[0][0]             
__________________________________________________________________________________________________
average_pooling2d_30 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_30[0][0]       
__________________________________________________________________________________________________
activation_316 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_316[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_317 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_317[0][0]             
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_30[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_128[0][0]            
__________________________________________________________________________________________________
activation_318 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_318[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_319 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_319[0][0]             
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 38, 24, 24)   0           concatenate_128[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_129[0][0]            
__________________________________________________________________________________________________
activation_320 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_320[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_321 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_321[0][0]             
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 46, 24, 24)   0           concatenate_129[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_130[0][0]            
__________________________________________________________________________________________________
activation_322 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_32 (Gl (None, 46)           0           activation_322[0][0]             
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 1)            47          global_average_pooling2d_32[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 63s - loss: 0.5634 - acc: 0.7580 - val_loss: 0.5862 - val_acc: 0.7880

Epoch 00001: val_loss improved from inf to 0.58617, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 38s - loss: 0.4697 - acc: 0.8068 - val_loss: 0.5428 - val_acc: 0.8058

Epoch 00002: val_loss improved from 0.58617 to 0.54275, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 37s - loss: 0.4279 - acc: 0.8274 - val_loss: 0.4630 - val_acc: 0.8254

Epoch 00003: val_loss improved from 0.54275 to 0.46303, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 37s - loss: 0.3982 - acc: 0.8436 - val_loss: 0.4295 - val_acc: 0.8464

Epoch 00004: val_loss improved from 0.46303 to 0.42947, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 37s - loss: 0.3787 - acc: 0.8556 - val_loss: 0.6195 - val_acc: 0.7913

Epoch 00005: val_loss did not improve from 0.42947
Epoch 6/30
 - 37s - loss: 0.3584 - acc: 0.8661 - val_loss: 0.5306 - val_acc: 0.7913

Epoch 00006: val_loss did not improve from 0.42947
Epoch 7/30
 - 37s - loss: 0.3407 - acc: 0.8758 - val_loss: 0.4834 - val_acc: 0.8094

Epoch 00007: val_loss did not improve from 0.42947
Epoch 8/30
 - 37s - loss: 0.3294 - acc: 0.8825 - val_loss: 0.4827 - val_acc: 0.8172

Epoch 00008: val_loss did not improve from 0.42947
Epoch 9/30
 - 37s - loss: 0.3123 - acc: 0.8898 - val_loss: 0.5157 - val_acc: 0.7952

Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00009: val_loss did not improve from 0.42947
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 427us/step
current Test accuracy: 0.7951612903225806
current auc_score ------------------>  0.8952919412648861

  32/7440 [..............................] - ETA: 42:50
  64/7440 [..............................] - ETA: 21:26
 192/7440 [..............................] - ETA: 7:03 
 320/7440 [>.............................] - ETA: 4:10
 448/7440 [>.............................] - ETA: 2:56
 576/7440 [=>............................] - ETA: 2:15
 704/7440 [=>............................] - ETA: 1:49
 832/7440 [==>...........................] - ETA: 1:31
 960/7440 [==>...........................] - ETA: 1:18
1088/7440 [===>..........................] - ETA: 1:07
1216/7440 [===>..........................] - ETA: 59s 
1344/7440 [====>.........................] - ETA: 53s
1472/7440 [====>.........................] - ETA: 47s
1600/7440 [=====>........................] - ETA: 43s
1728/7440 [=====>........................] - ETA: 39s
1856/7440 [======>.......................] - ETA: 35s
1984/7440 [=======>......................] - ETA: 33s
2112/7440 [=======>......................] - ETA: 30s
2240/7440 [========>.....................] - ETA: 28s
2368/7440 [========>.....................] - ETA: 26s
2496/7440 [=========>....................] - ETA: 24s
2624/7440 [=========>....................] - ETA: 22s
2752/7440 [==========>...................] - ETA: 21s
2880/7440 [==========>...................] - ETA: 19s
3008/7440 [===========>..................] - ETA: 18s
3136/7440 [===========>..................] - ETA: 17s
3264/7440 [============>.................] - ETA: 16s
3392/7440 [============>.................] - ETA: 15s
3520/7440 [=============>................] - ETA: 14s
3648/7440 [=============>................] - ETA: 13s
3776/7440 [==============>...............] - ETA: 12s
3904/7440 [==============>...............] - ETA: 11s
4032/7440 [===============>..............] - ETA: 10s
4160/7440 [===============>..............] - ETA: 10s
4288/7440 [================>.............] - ETA: 9s 
4416/7440 [================>.............] - ETA: 8s
4544/7440 [=================>............] - ETA: 8s
4672/7440 [=================>............] - ETA: 7s
4800/7440 [==================>...........] - ETA: 7s
4928/7440 [==================>...........] - ETA: 6s
5056/7440 [===================>..........] - ETA: 6s
5184/7440 [===================>..........] - ETA: 5s
5312/7440 [====================>.........] - ETA: 5s
5440/7440 [====================>.........] - ETA: 4s
5568/7440 [=====================>........] - ETA: 4s
5696/7440 [=====================>........] - ETA: 4s
5824/7440 [======================>.......] - ETA: 3s
5952/7440 [=======================>......] - ETA: 3s
6080/7440 [=======================>......] - ETA: 3s
6208/7440 [========================>.....] - ETA: 2s
6336/7440 [========================>.....] - ETA: 2s
6464/7440 [=========================>....] - ETA: 2s
6592/7440 [=========================>....] - ETA: 1s
6720/7440 [==========================>...] - ETA: 1s
6848/7440 [==========================>...] - ETA: 1s
6976/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7360/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 14s 2ms/step
Best saved model Test accuracy: 0.8463709677419354
best saved model auc_score ------------------>  0.9208748843796971
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_33[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_323 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_323[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_324 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_324[0][0]             
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_131[0][0]            
__________________________________________________________________________________________________
activation_325 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_325[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_326 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_326[0][0]             
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 32, 96, 96)   0           concatenate_131[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_132[0][0]            
__________________________________________________________________________________________________
activation_327 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_327[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_328 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_328[0][0]             
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 40, 96, 96)   0           concatenate_132[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_133[0][0]            
__________________________________________________________________________________________________
activation_329 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_329[0][0]             
__________________________________________________________________________________________________
average_pooling2d_31 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_31[0][0]       
__________________________________________________________________________________________________
activation_330 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_330[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_331 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_331[0][0]             
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_31[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_134[0][0]            
__________________________________________________________________________________________________
activation_332 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_332[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_333 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_333[0][0]             
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 36, 48, 48)   0           concatenate_134[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_135[0][0]            
__________________________________________________________________________________________________
activation_334 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_334[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_335 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_335[0][0]             
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 44, 48, 48)   0           concatenate_135[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_136[0][0]            
__________________________________________________________________________________________________
activation_336 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_336[0][0]             
__________________________________________________________________________________________________
average_pooling2d_32 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_32[0][0]       
__________________________________________________________________________________________________
activation_337 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_337[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_338 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_338[0][0]             
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_32[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_137[0][0]            
__________________________________________________________________________________________________
activation_339 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_339[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_340 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_340[0][0]             
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 38, 24, 24)   0           concatenate_137[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_138[0][0]            
__________________________________________________________________________________________________
activation_341 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_341[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_342 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_342[0][0]             
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 46, 24, 24)   0           concatenate_138[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_139[0][0]            
__________________________________________________________________________________________________
activation_343 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_33 (Gl (None, 46)           0           activation_343[0][0]             
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 1)            47          global_average_pooling2d_33[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 66s - loss: 0.5671 - acc: 0.7571 - val_loss: 0.5317 - val_acc: 0.8078

Epoch 00001: val_loss improved from inf to 0.53171, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 37s - loss: 0.4724 - acc: 0.8064 - val_loss: 0.5045 - val_acc: 0.8134

Epoch 00002: val_loss improved from 0.53171 to 0.50447, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 37s - loss: 0.4307 - acc: 0.8269 - val_loss: 0.4722 - val_acc: 0.8405

Epoch 00003: val_loss improved from 0.50447 to 0.47222, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 37s - loss: 0.4036 - acc: 0.8446 - val_loss: 0.4704 - val_acc: 0.8379

Epoch 00004: val_loss improved from 0.47222 to 0.47042, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 37s - loss: 0.3809 - acc: 0.8552 - val_loss: 0.5049 - val_acc: 0.8077

Epoch 00005: val_loss did not improve from 0.47042
Epoch 6/30
 - 37s - loss: 0.3626 - acc: 0.8675 - val_loss: 0.4751 - val_acc: 0.8487

Epoch 00006: val_loss did not improve from 0.47042
Epoch 7/30
 - 37s - loss: 0.3436 - acc: 0.8769 - val_loss: 0.4952 - val_acc: 0.8208

Epoch 00007: val_loss did not improve from 0.47042
Epoch 8/30
 - 37s - loss: 0.3294 - acc: 0.8819 - val_loss: 0.4472 - val_acc: 0.8332

Epoch 00008: val_loss improved from 0.47042 to 0.44715, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 37s - loss: 0.3153 - acc: 0.8884 - val_loss: 0.4172 - val_acc: 0.8457

Epoch 00009: val_loss improved from 0.44715 to 0.41722, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 37s - loss: 0.3014 - acc: 0.8966 - val_loss: 0.8229 - val_acc: 0.7035

Epoch 00010: val_loss did not improve from 0.41722
Epoch 11/30
 - 37s - loss: 0.2908 - acc: 0.9024 - val_loss: 0.5721 - val_acc: 0.7582

Epoch 00011: val_loss did not improve from 0.41722
Epoch 12/30
 - 37s - loss: 0.2829 - acc: 0.9048 - val_loss: 0.4526 - val_acc: 0.8200

Epoch 00012: val_loss did not improve from 0.41722
Epoch 13/30
 - 37s - loss: 0.2714 - acc: 0.9094 - val_loss: 0.5200 - val_acc: 0.7853

Epoch 00013: val_loss did not improve from 0.41722
Epoch 14/30
 - 37s - loss: 0.2658 - acc: 0.9141 - val_loss: 0.5604 - val_acc: 0.8220

Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00014: val_loss did not improve from 0.41722
Epoch 00014: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 448us/step
current Test accuracy: 0.8220430107526882
current auc_score ------------------>  0.8934305989131691

  32/7440 [..............................] - ETA: 44:59
 128/7440 [..............................] - ETA: 11:09
 256/7440 [>.............................] - ETA: 5:30 
 384/7440 [>.............................] - ETA: 3:37
 512/7440 [=>............................] - ETA: 2:40
 640/7440 [=>............................] - ETA: 2:06
 768/7440 [==>...........................] - ETA: 1:44
 896/7440 [==>...........................] - ETA: 1:28
1024/7440 [===>..........................] - ETA: 1:15
1152/7440 [===>..........................] - ETA: 1:06
1280/7440 [====>.........................] - ETA: 58s 
1408/7440 [====>.........................] - ETA: 52s
1536/7440 [=====>........................] - ETA: 47s
1664/7440 [=====>........................] - ETA: 43s
1792/7440 [======>.......................] - ETA: 39s
1920/7440 [======>.......................] - ETA: 35s
2048/7440 [=======>......................] - ETA: 33s
2176/7440 [=======>......................] - ETA: 30s
2304/7440 [========>.....................] - ETA: 28s
2432/7440 [========>.....................] - ETA: 26s
2560/7440 [=========>....................] - ETA: 24s
2688/7440 [=========>....................] - ETA: 22s
2816/7440 [==========>...................] - ETA: 21s
2944/7440 [==========>...................] - ETA: 19s
3072/7440 [===========>..................] - ETA: 18s
3200/7440 [===========>..................] - ETA: 17s
3328/7440 [============>.................] - ETA: 16s
3456/7440 [============>.................] - ETA: 15s
3584/7440 [=============>................] - ETA: 14s
3712/7440 [=============>................] - ETA: 13s
3840/7440 [==============>...............] - ETA: 12s
3968/7440 [===============>..............] - ETA: 11s
4096/7440 [===============>..............] - ETA: 10s
4224/7440 [================>.............] - ETA: 10s
4352/7440 [================>.............] - ETA: 9s 
4480/7440 [=================>............] - ETA: 9s
4608/7440 [=================>............] - ETA: 8s
4736/7440 [==================>...........] - ETA: 7s
4864/7440 [==================>...........] - ETA: 7s
4992/7440 [===================>..........] - ETA: 6s
5120/7440 [===================>..........] - ETA: 6s
5248/7440 [====================>.........] - ETA: 5s
5376/7440 [====================>.........] - ETA: 5s
5504/7440 [=====================>........] - ETA: 4s
5632/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 3s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 15s 2ms/step
Best saved model Test accuracy: 0.8456989247311828
best saved model auc_score ------------------>  0.9231354058272634
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_34 (InputLayer)        (None, 2, 96, 96)         0         
_________________________________________________________________
initial_conv2D (Conv2D)      (None, 16, 96, 96)        288       
_________________________________________________________________
tr_0_bn (BatchNormalization) (None, 16, 96, 96)        64        
_________________________________________________________________
activation_344 (Activation)  (None, 16, 96, 96)        0         
_________________________________________________________________
tr_0_conv2D (Conv2D)         (None, 8, 96, 96)         128       
_________________________________________________________________
average_pooling2d_33 (Averag (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_bn (BatchNormalization) (None, 8, 48, 48)         32        
_________________________________________________________________
activation_345 (Activation)  (None, 8, 48, 48)         0         
_________________________________________________________________
tr_1_conv2D (Conv2D)         (None, 4, 48, 48)         32        
_________________________________________________________________
average_pooling2d_34 (Averag (None, 4, 24, 24)         0         
_________________________________________________________________
final_bn (BatchNormalization (None, 4, 24, 24)         16        
_________________________________________________________________
activation_346 (Activation)  (None, 4, 24, 24)         0         
_________________________________________________________________
global_average_pooling2d_34  (None, 4)                 0         
_________________________________________________________________
dense_34 (Dense)             (None, 1)                 5         
=================================================================
Total params: 565
Trainable params: 509
Non-trainable params: 56
_________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 36s - loss: 0.6344 - acc: 0.6872 - val_loss: 0.5554 - val_acc: 0.8263

Epoch 00001: val_loss improved from inf to 0.55536, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 10s - loss: 0.6096 - acc: 0.7108 - val_loss: 0.5434 - val_acc: 0.8382

Epoch 00002: val_loss improved from 0.55536 to 0.54343, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 10s - loss: 0.5957 - acc: 0.7177 - val_loss: 0.5244 - val_acc: 0.8372

Epoch 00003: val_loss improved from 0.54343 to 0.52442, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 10s - loss: 0.5807 - acc: 0.7278 - val_loss: 0.5037 - val_acc: 0.8616

Epoch 00004: val_loss improved from 0.52442 to 0.50370, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 10s - loss: 0.5689 - acc: 0.7338 - val_loss: 0.4940 - val_acc: 0.8626

Epoch 00005: val_loss improved from 0.50370 to 0.49402, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 10s - loss: 0.5590 - acc: 0.7395 - val_loss: 0.5122 - val_acc: 0.8372

Epoch 00006: val_loss did not improve from 0.49402
Epoch 7/30
 - 10s - loss: 0.5514 - acc: 0.7432 - val_loss: 0.4976 - val_acc: 0.8501

Epoch 00007: val_loss did not improve from 0.49402
Epoch 8/30
 - 10s - loss: 0.5433 - acc: 0.7464 - val_loss: 0.4930 - val_acc: 0.8618

Epoch 00008: val_loss improved from 0.49402 to 0.49300, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 10s - loss: 0.5363 - acc: 0.7500 - val_loss: 0.4964 - val_acc: 0.8591

Epoch 00009: val_loss did not improve from 0.49300
Epoch 10/30
 - 10s - loss: 0.5300 - acc: 0.7528 - val_loss: 0.5170 - val_acc: 0.8392

Epoch 00010: val_loss did not improve from 0.49300
Epoch 11/30
 - 10s - loss: 0.5244 - acc: 0.7555 - val_loss: 0.4864 - val_acc: 0.8456

Epoch 00011: val_loss improved from 0.49300 to 0.48639, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 12/30
 - 10s - loss: 0.5189 - acc: 0.7583 - val_loss: 0.6620 - val_acc: 0.7309

Epoch 00012: val_loss did not improve from 0.48639
Epoch 13/30
 - 10s - loss: 0.5140 - acc: 0.7591 - val_loss: 0.4816 - val_acc: 0.8423

Epoch 00013: val_loss improved from 0.48639 to 0.48160, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 14/30
 - 10s - loss: 0.5108 - acc: 0.7583 - val_loss: 0.4807 - val_acc: 0.8359

Epoch 00014: val_loss improved from 0.48160 to 0.48071, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 15/30
 - 10s - loss: 0.5076 - acc: 0.7602 - val_loss: 0.4743 - val_acc: 0.8163

Epoch 00015: val_loss improved from 0.48071 to 0.47431, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 16/30
 - 10s - loss: 0.5053 - acc: 0.7604 - val_loss: 0.4799 - val_acc: 0.8219

Epoch 00016: val_loss did not improve from 0.47431
Epoch 17/30
 - 10s - loss: 0.5028 - acc: 0.7624 - val_loss: 0.4933 - val_acc: 0.7993

Epoch 00017: val_loss did not improve from 0.47431
Epoch 18/30
 - 10s - loss: 0.4993 - acc: 0.7639 - val_loss: 0.4805 - val_acc: 0.8103

Epoch 00018: val_loss did not improve from 0.47431
Epoch 19/30
 - 10s - loss: 0.4975 - acc: 0.7634 - val_loss: 0.4930 - val_acc: 0.8200

Epoch 00019: val_loss did not improve from 0.47431
Epoch 20/30
 - 10s - loss: 0.4958 - acc: 0.7650 - val_loss: 0.4743 - val_acc: 0.8138

Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00020: val_loss did not improve from 0.47431
Epoch 00020: early stopping

  32/7440 [..............................] - ETA: 3s
 256/7440 [>.............................] - ETA: 2s
 480/7440 [>.............................] - ETA: 1s
 704/7440 [=>............................] - ETA: 1s
 928/7440 [==>...........................] - ETA: 1s
1152/7440 [===>..........................] - ETA: 1s
1376/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 1s
1824/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2272/7440 [========>.....................] - ETA: 1s
2496/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2944/7440 [==========>...................] - ETA: 1s
3168/7440 [===========>..................] - ETA: 1s
3392/7440 [============>.................] - ETA: 0s
3616/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4288/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6752/7440 [==========================>...] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 2s 243us/step
current Test accuracy: 0.8138440860215054
current auc_score ------------------>  0.8734339952595676

  32/7440 [..............................] - ETA: 45:22
 192/7440 [..............................] - ETA: 7:26 
 384/7440 [>.............................] - ETA: 3:38
 608/7440 [=>............................] - ETA: 2:13
 832/7440 [==>...........................] - ETA: 1:35
1056/7440 [===>..........................] - ETA: 1:12
1280/7440 [====>.........................] - ETA: 58s 
1504/7440 [=====>........................] - ETA: 47s
1728/7440 [=====>........................] - ETA: 40s
1952/7440 [======>.......................] - ETA: 34s
2176/7440 [=======>......................] - ETA: 29s
2400/7440 [========>.....................] - ETA: 25s
2624/7440 [=========>....................] - ETA: 22s
2848/7440 [==========>...................] - ETA: 20s
3072/7440 [===========>..................] - ETA: 17s
3296/7440 [============>.................] - ETA: 15s
3520/7440 [=============>................] - ETA: 14s
3744/7440 [==============>...............] - ETA: 12s
3968/7440 [===============>..............] - ETA: 11s
4192/7440 [===============>..............] - ETA: 9s 
4416/7440 [================>.............] - ETA: 8s
4640/7440 [=================>............] - ETA: 7s
4864/7440 [==================>...........] - ETA: 6s
5088/7440 [===================>..........] - ETA: 6s
5312/7440 [====================>.........] - ETA: 5s
5536/7440 [=====================>........] - ETA: 4s
5760/7440 [======================>.......] - ETA: 3s
5984/7440 [=======================>......] - ETA: 3s
6208/7440 [========================>.....] - ETA: 2s
6432/7440 [========================>.....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7104/7440 [===========================>..] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 14s 2ms/step
Best saved model Test accuracy: 0.8162634408602151
best saved model auc_score ------------------>  0.8748055411030176
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_35[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_347 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_347[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_348 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_348[0][0]             
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_140[0][0]            
__________________________________________________________________________________________________
activation_349 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_349[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_350 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_350[0][0]             
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 32, 96, 96)   0           concatenate_140[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_141[0][0]            
__________________________________________________________________________________________________
activation_351 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_351[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_352 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_352[0][0]             
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 40, 96, 96)   0           concatenate_141[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_142[0][0]            
__________________________________________________________________________________________________
activation_353 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_353[0][0]             
__________________________________________________________________________________________________
average_pooling2d_35 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_35[0][0]       
__________________________________________________________________________________________________
activation_354 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_354[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_355 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_355[0][0]             
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_35[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_143[0][0]            
__________________________________________________________________________________________________
activation_356 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_356[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_357 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_357[0][0]             
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 36, 48, 48)   0           concatenate_143[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_144[0][0]            
__________________________________________________________________________________________________
activation_358 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_358[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_359 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_359[0][0]             
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 44, 48, 48)   0           concatenate_144[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_145[0][0]            
__________________________________________________________________________________________________
activation_360 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_360[0][0]             
__________________________________________________________________________________________________
average_pooling2d_36 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_36[0][0]       
__________________________________________________________________________________________________
activation_361 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_361[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_362 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_362[0][0]             
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_36[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_146[0][0]            
__________________________________________________________________________________________________
activation_363 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_363[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_364 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_364[0][0]             
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 38, 24, 24)   0           concatenate_146[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_147[0][0]            
__________________________________________________________________________________________________
activation_365 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_365[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_366 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_366[0][0]             
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 46, 24, 24)   0           concatenate_147[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_148[0][0]            
__________________________________________________________________________________________________
activation_367 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_35 (Gl (None, 46)           0           activation_367[0][0]             
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 1)            47          global_average_pooling2d_35[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 68s - loss: 0.5570 - acc: 0.7650 - val_loss: 0.6483 - val_acc: 0.7022

Epoch 00001: val_loss improved from inf to 0.64832, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 37s - loss: 0.4685 - acc: 0.8054 - val_loss: 0.4747 - val_acc: 0.7944

Epoch 00002: val_loss improved from 0.64832 to 0.47473, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 37s - loss: 0.4224 - acc: 0.8324 - val_loss: 0.5045 - val_acc: 0.7755

Epoch 00003: val_loss did not improve from 0.47473
Epoch 4/30
 - 37s - loss: 0.3930 - acc: 0.8490 - val_loss: 0.5427 - val_acc: 0.7696

Epoch 00004: val_loss did not improve from 0.47473
Epoch 5/30
 - 37s - loss: 0.3673 - acc: 0.8616 - val_loss: 0.5735 - val_acc: 0.7575

Epoch 00005: val_loss did not improve from 0.47473
Epoch 6/30
 - 37s - loss: 0.3490 - acc: 0.8722 - val_loss: 0.5043 - val_acc: 0.7935

Epoch 00006: val_loss did not improve from 0.47473
Epoch 7/30
 - 37s - loss: 0.3316 - acc: 0.8816 - val_loss: 0.4159 - val_acc: 0.8402

Epoch 00007: val_loss improved from 0.47473 to 0.41588, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 8/30
 - 37s - loss: 0.3185 - acc: 0.8886 - val_loss: 0.8563 - val_acc: 0.7114

Epoch 00008: val_loss did not improve from 0.41588
Epoch 9/30
 - 37s - loss: 0.3038 - acc: 0.8947 - val_loss: 0.4854 - val_acc: 0.8214

Epoch 00009: val_loss did not improve from 0.41588
Epoch 10/30
 - 37s - loss: 0.2904 - acc: 0.9027 - val_loss: 0.5147 - val_acc: 0.8043

Epoch 00010: val_loss did not improve from 0.41588
Epoch 11/30
 - 37s - loss: 0.2778 - acc: 0.9085 - val_loss: 0.7978 - val_acc: 0.7695

Epoch 00011: val_loss did not improve from 0.41588
Epoch 12/30
 - 37s - loss: 0.2725 - acc: 0.9107 - val_loss: 0.6841 - val_acc: 0.7626

Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00012: val_loss did not improve from 0.41588
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 461us/step
current Test accuracy: 0.7626344086021506
current auc_score ------------------>  0.9137383295756736

  32/7440 [..............................] - ETA: 50:35
 128/7440 [..............................] - ETA: 12:32
 256/7440 [>.............................] - ETA: 6:11 
 384/7440 [>.............................] - ETA: 4:04
 512/7440 [=>............................] - ETA: 3:00
 640/7440 [=>............................] - ETA: 2:22
 768/7440 [==>...........................] - ETA: 1:56
 896/7440 [==>...........................] - ETA: 1:38
1024/7440 [===>..........................] - ETA: 1:25
1152/7440 [===>..........................] - ETA: 1:14
1280/7440 [====>.........................] - ETA: 1:05
1408/7440 [====>.........................] - ETA: 58s 
1536/7440 [=====>........................] - ETA: 53s
1664/7440 [=====>........................] - ETA: 48s
1792/7440 [======>.......................] - ETA: 43s
1920/7440 [======>.......................] - ETA: 40s
2048/7440 [=======>......................] - ETA: 36s
2176/7440 [=======>......................] - ETA: 34s
2304/7440 [========>.....................] - ETA: 31s
2432/7440 [========>.....................] - ETA: 29s
2560/7440 [=========>....................] - ETA: 27s
2688/7440 [=========>....................] - ETA: 25s
2816/7440 [==========>...................] - ETA: 23s
2944/7440 [==========>...................] - ETA: 22s
3072/7440 [===========>..................] - ETA: 20s
3200/7440 [===========>..................] - ETA: 19s
3328/7440 [============>.................] - ETA: 18s
3456/7440 [============>.................] - ETA: 16s
3584/7440 [=============>................] - ETA: 15s
3712/7440 [=============>................] - ETA: 14s
3840/7440 [==============>...............] - ETA: 13s
3968/7440 [===============>..............] - ETA: 13s
4096/7440 [===============>..............] - ETA: 12s
4224/7440 [================>.............] - ETA: 11s
4352/7440 [================>.............] - ETA: 10s
4480/7440 [=================>............] - ETA: 10s
4608/7440 [=================>............] - ETA: 9s 
4736/7440 [==================>...........] - ETA: 8s
4864/7440 [==================>...........] - ETA: 8s
4992/7440 [===================>..........] - ETA: 7s
5120/7440 [===================>..........] - ETA: 6s
5248/7440 [====================>.........] - ETA: 6s
5376/7440 [====================>.........] - ETA: 5s
5504/7440 [=====================>........] - ETA: 5s
5632/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 4s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 2s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 16s 2ms/step
Best saved model Test accuracy: 0.8401881720430108
best saved model auc_score ------------------>  0.9258607208925889
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_36 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_36[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_368 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_368[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_369 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_369[0][0]             
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_149[0][0]            
__________________________________________________________________________________________________
activation_370 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_370[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_371 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_371[0][0]             
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 36, 96, 96)   0           concatenate_149[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 36, 96, 96)   144         concatenate_150[0][0]            
__________________________________________________________________________________________________
activation_372 (Activation)     (None, 36, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 18, 96, 96)   648         activation_372[0][0]             
__________________________________________________________________________________________________
average_pooling2d_37 (AveragePo (None, 18, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 18, 48, 48)   72          average_pooling2d_37[0][0]       
__________________________________________________________________________________________________
activation_373 (Activation)     (None, 18, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   720         activation_373[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_374 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_374[0][0]             
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_37[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_151[0][0]            
__________________________________________________________________________________________________
activation_375 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1120        activation_375[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_376 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_376[0][0]             
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 38, 48, 48)   0           concatenate_151[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 38, 48, 48)   152         concatenate_152[0][0]            
__________________________________________________________________________________________________
activation_377 (Activation)     (None, 38, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 19, 48, 48)   722         activation_377[0][0]             
__________________________________________________________________________________________________
average_pooling2d_38 (AveragePo (None, 19, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 19, 24, 24)   76          average_pooling2d_38[0][0]       
__________________________________________________________________________________________________
activation_378 (Activation)     (None, 19, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   760         activation_378[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_379 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_379[0][0]             
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 29, 24, 24)   0           average_pooling2d_38[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 29, 24, 24)   116         concatenate_153[0][0]            
__________________________________________________________________________________________________
activation_380 (Activation)     (None, 29, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 40, 24, 24)   1160        activation_380[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_381 (Activation)     (None, 40, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_381[0][0]             
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 39, 24, 24)   0           concatenate_153[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 39, 24, 24)   156         concatenate_154[0][0]            
__________________________________________________________________________________________________
activation_382 (Activation)     (None, 39, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_36 (Gl (None, 39)           0           activation_382[0][0]             
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 1)            40          global_average_pooling2d_36[0][0]
==================================================================================================
Total params: 30,694
Trainable params: 29,716
Non-trainable params: 978
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 62s - loss: 0.5616 - acc: 0.7594 - val_loss: 0.4764 - val_acc: 0.8364

Epoch 00001: val_loss improved from inf to 0.47636, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 30s - loss: 0.4723 - acc: 0.8026 - val_loss: 0.5372 - val_acc: 0.8270

Epoch 00002: val_loss did not improve from 0.47636
Epoch 3/30
 - 30s - loss: 0.4345 - acc: 0.8229 - val_loss: 0.5121 - val_acc: 0.8044

Epoch 00003: val_loss did not improve from 0.47636
Epoch 4/30
 - 30s - loss: 0.4101 - acc: 0.8343 - val_loss: 0.5333 - val_acc: 0.7685

Epoch 00004: val_loss did not improve from 0.47636
Epoch 5/30
 - 30s - loss: 0.3859 - acc: 0.8473 - val_loss: 0.5922 - val_acc: 0.7720

Epoch 00005: val_loss did not improve from 0.47636
Epoch 6/30
 - 30s - loss: 0.3694 - acc: 0.8584 - val_loss: 0.5464 - val_acc: 0.7988

Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00006: val_loss did not improve from 0.47636
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 3s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 2s
 672/7440 [=>............................] - ETA: 2s
 800/7440 [==>...........................] - ETA: 2s
 928/7440 [==>...........................] - ETA: 2s
1056/7440 [===>..........................] - ETA: 2s
1184/7440 [===>..........................] - ETA: 2s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 1s
2720/7440 [=========>....................] - ETA: 1s
2848/7440 [==========>...................] - ETA: 1s
2976/7440 [===========>..................] - ETA: 1s
3104/7440 [===========>..................] - ETA: 1s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 0s
5152/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 404us/step
current Test accuracy: 0.7987903225806452
current auc_score ------------------>  0.909720632443057

  32/7440 [..............................] - ETA: 53:02
 128/7440 [..............................] - ETA: 13:08
 256/7440 [>.............................] - ETA: 6:28 
 384/7440 [>.............................] - ETA: 4:15
 512/7440 [=>............................] - ETA: 3:08
 640/7440 [=>............................] - ETA: 2:28
 768/7440 [==>...........................] - ETA: 2:02
 896/7440 [==>...........................] - ETA: 1:43
1024/7440 [===>..........................] - ETA: 1:28
1152/7440 [===>..........................] - ETA: 1:17
1280/7440 [====>.........................] - ETA: 1:08
1408/7440 [====>.........................] - ETA: 1:01
1536/7440 [=====>........................] - ETA: 55s 
1664/7440 [=====>........................] - ETA: 50s
1792/7440 [======>.......................] - ETA: 45s
1920/7440 [======>.......................] - ETA: 41s
2048/7440 [=======>......................] - ETA: 38s
2176/7440 [=======>......................] - ETA: 35s
2304/7440 [========>.....................] - ETA: 32s
2432/7440 [========>.....................] - ETA: 30s
2560/7440 [=========>....................] - ETA: 28s
2688/7440 [=========>....................] - ETA: 26s
2816/7440 [==========>...................] - ETA: 24s
2944/7440 [==========>...................] - ETA: 22s
3072/7440 [===========>..................] - ETA: 21s
3200/7440 [===========>..................] - ETA: 19s
3328/7440 [============>.................] - ETA: 18s
3456/7440 [============>.................] - ETA: 17s
3584/7440 [=============>................] - ETA: 16s
3712/7440 [=============>................] - ETA: 15s
3840/7440 [==============>...............] - ETA: 14s
3968/7440 [===============>..............] - ETA: 13s
4096/7440 [===============>..............] - ETA: 12s
4224/7440 [================>.............] - ETA: 11s
4352/7440 [================>.............] - ETA: 11s
4480/7440 [=================>............] - ETA: 10s
4608/7440 [=================>............] - ETA: 9s 
4736/7440 [==================>...........] - ETA: 8s
4864/7440 [==================>...........] - ETA: 8s
4992/7440 [===================>..........] - ETA: 7s
5120/7440 [===================>..........] - ETA: 7s
5248/7440 [====================>.........] - ETA: 6s
5376/7440 [====================>.........] - ETA: 6s
5504/7440 [=====================>........] - ETA: 5s
5632/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 4s
5888/7440 [======================>.......] - ETA: 4s
6016/7440 [=======================>......] - ETA: 3s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 3s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 1s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 17s 2ms/step
Best saved model Test accuracy: 0.8364247311827957
best saved model auc_score ------------------>  0.8987028124638686
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_37[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_383 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 32, 96, 96)   512         activation_383[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_384 (Activation)     (None, 32, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_384[0][0]             
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 24, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 24, 96, 96)   96          concatenate_155[0][0]            
__________________________________________________________________________________________________
activation_385 (Activation)     (None, 24, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 32, 96, 96)   768         activation_385[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_386 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_386[0][0]             
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 32, 96, 96)   0           concatenate_155[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_156[0][0]            
__________________________________________________________________________________________________
activation_387 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 32, 96, 96)   1024        activation_387[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 32, 96, 96)   128         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_388 (Activation)     (None, 32, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 8, 96, 96)    2304        activation_388[0][0]             
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 40, 96, 96)   0           concatenate_156[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 40, 96, 96)   160         concatenate_157[0][0]            
__________________________________________________________________________________________________
activation_389 (Activation)     (None, 40, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 20, 96, 96)   800         activation_389[0][0]             
__________________________________________________________________________________________________
average_pooling2d_39 (AveragePo (None, 20, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 20, 48, 48)   80          average_pooling2d_39[0][0]       
__________________________________________________________________________________________________
activation_390 (Activation)     (None, 20, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 32, 48, 48)   640         activation_390[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_391 (Activation)     (None, 32, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_391[0][0]             
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 28, 48, 48)   0           average_pooling2d_39[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 28, 48, 48)   112         concatenate_158[0][0]            
__________________________________________________________________________________________________
activation_392 (Activation)     (None, 28, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 32, 48, 48)   896         activation_392[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_393 (Activation)     (None, 32, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_393[0][0]             
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 36, 48, 48)   0           concatenate_158[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 36, 48, 48)   144         concatenate_159[0][0]            
__________________________________________________________________________________________________
activation_394 (Activation)     (None, 36, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 32, 48, 48)   1152        activation_394[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 32, 48, 48)   128         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_395 (Activation)     (None, 32, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 8, 48, 48)    2304        activation_395[0][0]             
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 44, 48, 48)   0           concatenate_159[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 44, 48, 48)   176         concatenate_160[0][0]            
__________________________________________________________________________________________________
activation_396 (Activation)     (None, 44, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 22, 48, 48)   968         activation_396[0][0]             
__________________________________________________________________________________________________
average_pooling2d_40 (AveragePo (None, 22, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 22, 24, 24)   88          average_pooling2d_40[0][0]       
__________________________________________________________________________________________________
activation_397 (Activation)     (None, 22, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 32, 24, 24)   704         activation_397[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_398 (Activation)     (None, 32, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_398[0][0]             
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 30, 24, 24)   0           average_pooling2d_40[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 30, 24, 24)   120         concatenate_161[0][0]            
__________________________________________________________________________________________________
activation_399 (Activation)     (None, 30, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 32, 24, 24)   960         activation_399[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_400 (Activation)     (None, 32, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_400[0][0]             
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 38, 24, 24)   0           concatenate_161[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 38, 24, 24)   152         concatenate_162[0][0]            
__________________________________________________________________________________________________
activation_401 (Activation)     (None, 38, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 32, 24, 24)   1216        activation_401[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 32, 24, 24)   128         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_402 (Activation)     (None, 32, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 8, 24, 24)    2304        activation_402[0][0]             
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 46, 24, 24)   0           concatenate_162[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 46, 24, 24)   184         concatenate_163[0][0]            
__________________________________________________________________________________________________
activation_403 (Activation)     (None, 46, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_37 (Gl (None, 46)           0           activation_403[0][0]             
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 1)            47          global_average_pooling2d_37[0][0]
==================================================================================================
Total params: 33,367
Trainable params: 32,039
Non-trainable params: 1,328
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 74s - loss: 0.5489 - acc: 0.7696 - val_loss: 0.5312 - val_acc: 0.7848

Epoch 00001: val_loss improved from inf to 0.53119, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 38s - loss: 0.4593 - acc: 0.8128 - val_loss: 0.4586 - val_acc: 0.8194

Epoch 00002: val_loss improved from 0.53119 to 0.45858, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 38s - loss: 0.4234 - acc: 0.8336 - val_loss: 0.5429 - val_acc: 0.7695

Epoch 00003: val_loss did not improve from 0.45858
Epoch 4/30
 - 38s - loss: 0.3902 - acc: 0.8525 - val_loss: 0.5168 - val_acc: 0.8159

Epoch 00004: val_loss did not improve from 0.45858
Epoch 5/30
 - 38s - loss: 0.3681 - acc: 0.8607 - val_loss: 0.4882 - val_acc: 0.8161

Epoch 00005: val_loss did not improve from 0.45858
Epoch 6/30
 - 38s - loss: 0.3484 - acc: 0.8724 - val_loss: 0.4911 - val_acc: 0.8118

Epoch 00006: val_loss did not improve from 0.45858
Epoch 7/30
 - 38s - loss: 0.3317 - acc: 0.8799 - val_loss: 0.7783 - val_acc: 0.7316

Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00007: val_loss did not improve from 0.45858
Epoch 00007: early stopping

  32/7440 [..............................] - ETA: 5s
 160/7440 [..............................] - ETA: 4s
 288/7440 [>.............................] - ETA: 3s
 416/7440 [>.............................] - ETA: 3s
 544/7440 [=>............................] - ETA: 3s
 672/7440 [=>............................] - ETA: 3s
 800/7440 [==>...........................] - ETA: 3s
 928/7440 [==>...........................] - ETA: 3s
1056/7440 [===>..........................] - ETA: 3s
1184/7440 [===>..........................] - ETA: 3s
1312/7440 [====>.........................] - ETA: 2s
1440/7440 [====>.........................] - ETA: 2s
1568/7440 [=====>........................] - ETA: 2s
1696/7440 [=====>........................] - ETA: 2s
1824/7440 [======>.......................] - ETA: 2s
1952/7440 [======>.......................] - ETA: 2s
2080/7440 [=======>......................] - ETA: 2s
2208/7440 [=======>......................] - ETA: 2s
2336/7440 [========>.....................] - ETA: 2s
2464/7440 [========>.....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2720/7440 [=========>....................] - ETA: 2s
2848/7440 [==========>...................] - ETA: 2s
2976/7440 [===========>..................] - ETA: 2s
3104/7440 [===========>..................] - ETA: 2s
3232/7440 [============>.................] - ETA: 1s
3360/7440 [============>.................] - ETA: 1s
3488/7440 [=============>................] - ETA: 1s
3616/7440 [=============>................] - ETA: 1s
3744/7440 [==============>...............] - ETA: 1s
3872/7440 [==============>...............] - ETA: 1s
4000/7440 [===============>..............] - ETA: 1s
4128/7440 [===============>..............] - ETA: 1s
4256/7440 [================>.............] - ETA: 1s
4384/7440 [================>.............] - ETA: 1s
4512/7440 [=================>............] - ETA: 1s
4640/7440 [=================>............] - ETA: 1s
4768/7440 [==================>...........] - ETA: 1s
4896/7440 [==================>...........] - ETA: 1s
5024/7440 [===================>..........] - ETA: 1s
5152/7440 [===================>..........] - ETA: 1s
5280/7440 [====================>.........] - ETA: 1s
5408/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
5920/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6176/7440 [=======================>......] - ETA: 0s
6304/7440 [========================>.....] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7072/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 3s 469us/step
current Test accuracy: 0.7315860215053763
current auc_score ------------------>  0.8841056046941843

  32/7440 [..............................] - ETA: 57:28
 128/7440 [..............................] - ETA: 14:14
 256/7440 [>.............................] - ETA: 7:01 
 384/7440 [>.............................] - ETA: 4:37
 512/7440 [=>............................] - ETA: 3:24
 640/7440 [=>............................] - ETA: 2:41
 768/7440 [==>...........................] - ETA: 2:12
 896/7440 [==>...........................] - ETA: 1:51
1024/7440 [===>..........................] - ETA: 1:36
1152/7440 [===>..........................] - ETA: 1:24
1280/7440 [====>.........................] - ETA: 1:14
1408/7440 [====>.........................] - ETA: 1:06
1536/7440 [=====>........................] - ETA: 1:00
1664/7440 [=====>........................] - ETA: 54s 
1792/7440 [======>.......................] - ETA: 49s
1920/7440 [======>.......................] - ETA: 45s
2048/7440 [=======>......................] - ETA: 41s
2176/7440 [=======>......................] - ETA: 38s
2304/7440 [========>.....................] - ETA: 35s
2432/7440 [========>.....................] - ETA: 33s
2560/7440 [=========>....................] - ETA: 30s
2688/7440 [=========>....................] - ETA: 28s
2816/7440 [==========>...................] - ETA: 26s
2944/7440 [==========>...................] - ETA: 24s
3072/7440 [===========>..................] - ETA: 23s
3200/7440 [===========>..................] - ETA: 21s
3328/7440 [============>.................] - ETA: 20s
3456/7440 [============>.................] - ETA: 19s
3584/7440 [=============>................] - ETA: 17s
3712/7440 [=============>................] - ETA: 16s
3840/7440 [==============>...............] - ETA: 15s
3968/7440 [===============>..............] - ETA: 14s
4096/7440 [===============>..............] - ETA: 13s
4224/7440 [================>.............] - ETA: 12s
4352/7440 [================>.............] - ETA: 12s
4480/7440 [=================>............] - ETA: 11s
4608/7440 [=================>............] - ETA: 10s
4736/7440 [==================>...........] - ETA: 9s 
4864/7440 [==================>...........] - ETA: 9s
4992/7440 [===================>..........] - ETA: 8s
5120/7440 [===================>..........] - ETA: 7s
5248/7440 [====================>.........] - ETA: 7s
5376/7440 [====================>.........] - ETA: 6s
5504/7440 [=====================>........] - ETA: 6s
5632/7440 [=====================>........] - ETA: 5s
5760/7440 [======================>.......] - ETA: 5s
5888/7440 [======================>.......] - ETA: 4s
6016/7440 [=======================>......] - ETA: 4s
6144/7440 [=======================>......] - ETA: 3s
6272/7440 [========================>.....] - ETA: 3s
6400/7440 [========================>.....] - ETA: 2s
6528/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 2s
6784/7440 [==========================>...] - ETA: 1s
6912/7440 [==========================>...] - ETA: 1s
7040/7440 [===========================>..] - ETA: 1s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 18s 2ms/step
Best saved model Test accuracy: 0.8193548387096774
best saved model auc_score ------------------>  0.9037834214938143
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_38 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_38[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_404 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 64, 96, 96)   1024        activation_404[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_405 (Activation)     (None, 64, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_405[0][0]             
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 32, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 32, 96, 96)   128         concatenate_164[0][0]            
__________________________________________________________________________________________________
activation_406 (Activation)     (None, 32, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 64, 96, 96)   2048        activation_406[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 64, 96, 96)   256         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_407 (Activation)     (None, 64, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 16, 96, 96)   9216        activation_407[0][0]             
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 48, 96, 96)   0           concatenate_164[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 48, 96, 96)   192         concatenate_165[0][0]            
__________________________________________________________________________________________________
activation_408 (Activation)     (None, 48, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 24, 96, 96)   1152        activation_408[0][0]             
__________________________________________________________________________________________________
average_pooling2d_41 (AveragePo (None, 24, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 24, 48, 48)   96          average_pooling2d_41[0][0]       
__________________________________________________________________________________________________
activation_409 (Activation)     (None, 24, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 64, 48, 48)   1536        activation_409[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_410 (Activation)     (None, 64, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_410[0][0]             
__________________________________________________________________________________________________
concatenate_166 (Concatenate)   (None, 40, 48, 48)   0           average_pooling2d_41[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 40, 48, 48)   160         concatenate_166[0][0]            
__________________________________________________________________________________________________
activation_411 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 64, 48, 48)   2560        activation_411[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 64, 48, 48)   256         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_412 (Activation)     (None, 64, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 16, 48, 48)   9216        activation_412[0][0]             
__________________________________________________________________________________________________
concatenate_167 (Concatenate)   (None, 56, 48, 48)   0           concatenate_166[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 56, 48, 48)   224         concatenate_167[0][0]            
__________________________________________________________________________________________________
activation_413 (Activation)     (None, 56, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 28, 48, 48)   1568        activation_413[0][0]             
__________________________________________________________________________________________________
average_pooling2d_42 (AveragePo (None, 28, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 28, 24, 24)   112         average_pooling2d_42[0][0]       
__________________________________________________________________________________________________
activation_414 (Activation)     (None, 28, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 64, 24, 24)   1792        activation_414[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_415 (Activation)     (None, 64, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_415[0][0]             
__________________________________________________________________________________________________
concatenate_168 (Concatenate)   (None, 44, 24, 24)   0           average_pooling2d_42[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 44, 24, 24)   176         concatenate_168[0][0]            
__________________________________________________________________________________________________
activation_416 (Activation)     (None, 44, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 64, 24, 24)   2816        activation_416[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 64, 24, 24)   256         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_417 (Activation)     (None, 64, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 16, 24, 24)   9216        activation_417[0][0]             
__________________________________________________________________________________________________
concatenate_169 (Concatenate)   (None, 60, 24, 24)   0           concatenate_168[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 60, 24, 24)   240         concatenate_169[0][0]            
__________________________________________________________________________________________________
activation_418 (Activation)     (None, 60, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_38 (Gl (None, 60)           0           activation_418[0][0]             
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 1)            61          global_average_pooling2d_38[0][0]
==================================================================================================
Total params: 73,069
Trainable params: 71,605
Non-trainable params: 1,464
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 73s - loss: 0.5560 - acc: 0.7726 - val_loss: 0.5751 - val_acc: 0.7626

Epoch 00001: val_loss improved from inf to 0.57509, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 37s - loss: 0.4664 - acc: 0.8198 - val_loss: 0.5462 - val_acc: 0.7933

Epoch 00002: val_loss improved from 0.57509 to 0.54617, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 37s - loss: 0.4279 - acc: 0.8376 - val_loss: 0.6422 - val_acc: 0.7677

Epoch 00003: val_loss did not improve from 0.54617
Epoch 4/30
 - 37s - loss: 0.3954 - acc: 0.8551 - val_loss: 0.6330 - val_acc: 0.8254

Epoch 00004: val_loss did not improve from 0.54617
Epoch 5/30
 - 37s - loss: 0.3707 - acc: 0.8670 - val_loss: 0.4706 - val_acc: 0.8081

Epoch 00005: val_loss improved from 0.54617 to 0.47064, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 6/30
 - 37s - loss: 0.3461 - acc: 0.8817 - val_loss: 0.5221 - val_acc: 0.7782

Epoch 00006: val_loss did not improve from 0.47064
Epoch 7/30
 - 37s - loss: 0.3246 - acc: 0.8926 - val_loss: 0.5419 - val_acc: 0.7839

Epoch 00007: val_loss did not improve from 0.47064
Epoch 8/30
 - 37s - loss: 0.3075 - acc: 0.8991 - val_loss: 0.4510 - val_acc: 0.8224

Epoch 00008: val_loss improved from 0.47064 to 0.45096, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 9/30
 - 37s - loss: 0.2912 - acc: 0.9073 - val_loss: 0.4298 - val_acc: 0.8394

Epoch 00009: val_loss improved from 0.45096 to 0.42981, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 37s - loss: 0.2776 - acc: 0.9129 - val_loss: 0.4833 - val_acc: 0.8044

Epoch 00010: val_loss did not improve from 0.42981
Epoch 11/30
 - 37s - loss: 0.2628 - acc: 0.9203 - val_loss: 0.6964 - val_acc: 0.8059

Epoch 00011: val_loss did not improve from 0.42981
Epoch 12/30
 - 37s - loss: 0.2506 - acc: 0.9258 - val_loss: 0.4471 - val_acc: 0.8470

Epoch 00012: val_loss did not improve from 0.42981
Epoch 13/30
 - 37s - loss: 0.2410 - acc: 0.9297 - val_loss: 0.4904 - val_acc: 0.8344

Epoch 00013: val_loss did not improve from 0.42981
Epoch 14/30
 - 37s - loss: 0.2345 - acc: 0.9315 - val_loss: 0.5193 - val_acc: 0.7855

Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00014: val_loss did not improve from 0.42981
Epoch 00014: early stopping

  32/7440 [..............................] - ETA: 7s
 128/7440 [..............................] - ETA: 4s
 256/7440 [>.............................] - ETA: 4s
 384/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 640/7440 [=>............................] - ETA: 3s
 768/7440 [==>...........................] - ETA: 3s
 896/7440 [==>...........................] - ETA: 3s
1024/7440 [===>..........................] - ETA: 3s
1152/7440 [===>..........................] - ETA: 3s
1280/7440 [====>.........................] - ETA: 3s
1408/7440 [====>.........................] - ETA: 3s
1536/7440 [=====>........................] - ETA: 2s
1664/7440 [=====>........................] - ETA: 2s
1792/7440 [======>.......................] - ETA: 2s
1920/7440 [======>.......................] - ETA: 2s
2048/7440 [=======>......................] - ETA: 2s
2176/7440 [=======>......................] - ETA: 2s
2304/7440 [========>.....................] - ETA: 2s
2432/7440 [========>.....................] - ETA: 2s
2560/7440 [=========>....................] - ETA: 2s
2688/7440 [=========>....................] - ETA: 2s
2816/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3456/7440 [============>.................] - ETA: 1s
3584/7440 [=============>................] - ETA: 1s
3712/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4608/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5504/7440 [=====================>........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7040/7440 [===========================>..] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 491us/step
current Test accuracy: 0.785483870967742
current auc_score ------------------>  0.8756455948664585

  32/7440 [..............................] - ETA: 1:02:22
  96/7440 [..............................] - ETA: 20:40  
 224/7440 [..............................] - ETA: 8:44 
 352/7440 [>.............................] - ETA: 5:29
 480/7440 [>.............................] - ETA: 3:57
 608/7440 [=>............................] - ETA: 3:05
 736/7440 [=>............................] - ETA: 2:30
 864/7440 [==>...........................] - ETA: 2:06
 992/7440 [===>..........................] - ETA: 1:48
1120/7440 [===>..........................] - ETA: 1:34
1248/7440 [====>.........................] - ETA: 1:23
1376/7440 [====>.........................] - ETA: 1:14
1504/7440 [=====>........................] - ETA: 1:06
1632/7440 [=====>........................] - ETA: 1:00
1760/7440 [======>.......................] - ETA: 54s 
1888/7440 [======>.......................] - ETA: 50s
2016/7440 [=======>......................] - ETA: 46s
2144/7440 [=======>......................] - ETA: 42s
2272/7440 [========>.....................] - ETA: 39s
2400/7440 [========>.....................] - ETA: 36s
2528/7440 [=========>....................] - ETA: 33s
2656/7440 [=========>....................] - ETA: 31s
2784/7440 [==========>...................] - ETA: 29s
2912/7440 [==========>...................] - ETA: 27s
3040/7440 [===========>..................] - ETA: 25s
3168/7440 [===========>..................] - ETA: 23s
3296/7440 [============>.................] - ETA: 22s
3424/7440 [============>.................] - ETA: 20s
3552/7440 [=============>................] - ETA: 19s
3680/7440 [=============>................] - ETA: 18s
3808/7440 [==============>...............] - ETA: 17s
3936/7440 [==============>...............] - ETA: 16s
4064/7440 [===============>..............] - ETA: 15s
4192/7440 [===============>..............] - ETA: 14s
4320/7440 [================>.............] - ETA: 13s
4448/7440 [================>.............] - ETA: 12s
4576/7440 [=================>............] - ETA: 11s
4704/7440 [=================>............] - ETA: 10s
4832/7440 [==================>...........] - ETA: 10s
4960/7440 [===================>..........] - ETA: 9s 
5088/7440 [===================>..........] - ETA: 8s
5216/7440 [====================>.........] - ETA: 7s
5344/7440 [====================>.........] - ETA: 7s
5472/7440 [=====================>........] - ETA: 6s
5600/7440 [=====================>........] - ETA: 6s
5728/7440 [======================>.......] - ETA: 5s
5856/7440 [======================>.......] - ETA: 5s
5984/7440 [=======================>......] - ETA: 4s
6112/7440 [=======================>......] - ETA: 4s
6240/7440 [========================>.....] - ETA: 3s
6368/7440 [========================>.....] - ETA: 3s
6496/7440 [=========================>....] - ETA: 2s
6624/7440 [=========================>....] - ETA: 2s
6752/7440 [==========================>...] - ETA: 1s
6880/7440 [==========================>...] - ETA: 1s
7008/7440 [===========================>..] - ETA: 1s
7136/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 20s 3ms/step
Best saved model Test accuracy: 0.8393817204301075
best saved model auc_score ------------------>  0.9243064949705169
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_39[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_419 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_419[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_420 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_420[0][0]             
__________________________________________________________________________________________________
concatenate_170 (Concatenate)   (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 26, 96, 96)   104         concatenate_170[0][0]            
__________________________________________________________________________________________________
activation_421 (Activation)     (None, 26, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 40, 96, 96)   1040        activation_421[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_422 (Activation)     (None, 40, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_422[0][0]             
__________________________________________________________________________________________________
concatenate_171 (Concatenate)   (None, 36, 96, 96)   0           concatenate_170[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 36, 96, 96)   144         concatenate_171[0][0]            
__________________________________________________________________________________________________
activation_423 (Activation)     (None, 36, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 40, 96, 96)   1440        activation_423[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_424 (Activation)     (None, 40, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_424[0][0]             
__________________________________________________________________________________________________
concatenate_172 (Concatenate)   (None, 46, 96, 96)   0           concatenate_171[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 46, 96, 96)   184         concatenate_172[0][0]            
__________________________________________________________________________________________________
activation_425 (Activation)     (None, 46, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 23, 96, 96)   1058        activation_425[0][0]             
__________________________________________________________________________________________________
average_pooling2d_43 (AveragePo (None, 23, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 23, 48, 48)   92          average_pooling2d_43[0][0]       
__________________________________________________________________________________________________
activation_426 (Activation)     (None, 23, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   920         activation_426[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_427 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_427[0][0]             
__________________________________________________________________________________________________
concatenate_173 (Concatenate)   (None, 33, 48, 48)   0           average_pooling2d_43[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 33, 48, 48)   132         concatenate_173[0][0]            
__________________________________________________________________________________________________
activation_428 (Activation)     (None, 33, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 40, 48, 48)   1320        activation_428[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_429 (Activation)     (None, 40, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_429[0][0]             
__________________________________________________________________________________________________
concatenate_174 (Concatenate)   (None, 43, 48, 48)   0           concatenate_173[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 43, 48, 48)   172         concatenate_174[0][0]            
__________________________________________________________________________________________________
activation_430 (Activation)     (None, 43, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 40, 48, 48)   1720        activation_430[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_431 (Activation)     (None, 40, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_431[0][0]             
__________________________________________________________________________________________________
concatenate_175 (Concatenate)   (None, 53, 48, 48)   0           concatenate_174[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 53, 48, 48)   212         concatenate_175[0][0]            
__________________________________________________________________________________________________
activation_432 (Activation)     (None, 53, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 26, 48, 48)   1378        activation_432[0][0]             
__________________________________________________________________________________________________
average_pooling2d_44 (AveragePo (None, 26, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 26, 24, 24)   104         average_pooling2d_44[0][0]       
__________________________________________________________________________________________________
activation_433 (Activation)     (None, 26, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   1040        activation_433[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_434 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_434[0][0]             
__________________________________________________________________________________________________
concatenate_176 (Concatenate)   (None, 36, 24, 24)   0           average_pooling2d_44[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 36, 24, 24)   144         concatenate_176[0][0]            
__________________________________________________________________________________________________
activation_435 (Activation)     (None, 36, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 40, 24, 24)   1440        activation_435[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_436 (Activation)     (None, 40, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_436[0][0]             
__________________________________________________________________________________________________
concatenate_177 (Concatenate)   (None, 46, 24, 24)   0           concatenate_176[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 46, 24, 24)   184         concatenate_177[0][0]            
__________________________________________________________________________________________________
activation_437 (Activation)     (None, 46, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 40, 24, 24)   1840        activation_437[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_438 (Activation)     (None, 40, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_438[0][0]             
__________________________________________________________________________________________________
concatenate_178 (Concatenate)   (None, 56, 24, 24)   0           concatenate_177[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 56, 24, 24)   224         concatenate_178[0][0]            
__________________________________________________________________________________________________
activation_439 (Activation)     (None, 56, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_39 (Gl (None, 56)           0           activation_439[0][0]             
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 1)            57          global_average_pooling2d_39[0][0]
==================================================================================================
Total params: 49,781
Trainable params: 48,181
Non-trainable params: 1,600
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 81s - loss: 0.5461 - acc: 0.7777 - val_loss: 0.4930 - val_acc: 0.8399

Epoch 00001: val_loss improved from inf to 0.49298, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 42s - loss: 0.4655 - acc: 0.8142 - val_loss: 0.5373 - val_acc: 0.8167

Epoch 00002: val_loss did not improve from 0.49298
Epoch 3/30
 - 42s - loss: 0.4207 - acc: 0.8383 - val_loss: 0.4594 - val_acc: 0.8474

Epoch 00003: val_loss improved from 0.49298 to 0.45935, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 43s - loss: 0.3876 - acc: 0.8574 - val_loss: 0.5256 - val_acc: 0.8075

Epoch 00004: val_loss did not improve from 0.45935
Epoch 5/30
 - 42s - loss: 0.3575 - acc: 0.8736 - val_loss: 0.4715 - val_acc: 0.8347

Epoch 00005: val_loss did not improve from 0.45935
Epoch 6/30
 - 42s - loss: 0.3326 - acc: 0.8876 - val_loss: 0.3959 - val_acc: 0.8591

Epoch 00006: val_loss improved from 0.45935 to 0.39588, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 7/30
 - 42s - loss: 0.3110 - acc: 0.8958 - val_loss: 0.4018 - val_acc: 0.8602

Epoch 00007: val_loss did not improve from 0.39588
Epoch 8/30
 - 42s - loss: 0.2975 - acc: 0.9026 - val_loss: 0.4400 - val_acc: 0.8501

Epoch 00008: val_loss did not improve from 0.39588
Epoch 9/30
 - 42s - loss: 0.2809 - acc: 0.9097 - val_loss: 0.5049 - val_acc: 0.8210

Epoch 00009: val_loss did not improve from 0.39588
Epoch 10/30
 - 42s - loss: 0.2696 - acc: 0.9155 - val_loss: 0.4776 - val_acc: 0.8407

Epoch 00010: val_loss did not improve from 0.39588
Epoch 11/30
 - 42s - loss: 0.2569 - acc: 0.9212 - val_loss: 0.4819 - val_acc: 0.8286

Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.486833431105525e-05.

Epoch 00011: val_loss did not improve from 0.39588
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 7s
 128/7440 [..............................] - ETA: 4s
 224/7440 [..............................] - ETA: 4s
 320/7440 [>.............................] - ETA: 4s
 416/7440 [>.............................] - ETA: 3s
 512/7440 [=>............................] - ETA: 3s
 608/7440 [=>............................] - ETA: 3s
 736/7440 [=>............................] - ETA: 3s
 864/7440 [==>...........................] - ETA: 3s
 992/7440 [===>..........................] - ETA: 3s
1120/7440 [===>..........................] - ETA: 3s
1248/7440 [====>.........................] - ETA: 3s
1376/7440 [====>.........................] - ETA: 3s
1504/7440 [=====>........................] - ETA: 3s
1632/7440 [=====>........................] - ETA: 3s
1728/7440 [=====>........................] - ETA: 3s
1856/7440 [======>.......................] - ETA: 2s
1984/7440 [=======>......................] - ETA: 2s
2112/7440 [=======>......................] - ETA: 2s
2240/7440 [========>.....................] - ETA: 2s
2368/7440 [========>.....................] - ETA: 2s
2496/7440 [=========>....................] - ETA: 2s
2592/7440 [=========>....................] - ETA: 2s
2688/7440 [=========>....................] - ETA: 2s
2816/7440 [==========>...................] - ETA: 2s
2944/7440 [==========>...................] - ETA: 2s
3072/7440 [===========>..................] - ETA: 2s
3200/7440 [===========>..................] - ETA: 2s
3328/7440 [============>.................] - ETA: 2s
3456/7440 [============>.................] - ETA: 2s
3584/7440 [=============>................] - ETA: 2s
3712/7440 [=============>................] - ETA: 1s
3840/7440 [==============>...............] - ETA: 1s
3968/7440 [===============>..............] - ETA: 1s
4096/7440 [===============>..............] - ETA: 1s
4224/7440 [================>.............] - ETA: 1s
4352/7440 [================>.............] - ETA: 1s
4480/7440 [=================>............] - ETA: 1s
4608/7440 [=================>............] - ETA: 1s
4736/7440 [==================>...........] - ETA: 1s
4864/7440 [==================>...........] - ETA: 1s
4992/7440 [===================>..........] - ETA: 1s
5120/7440 [===================>..........] - ETA: 1s
5248/7440 [====================>.........] - ETA: 1s
5376/7440 [====================>.........] - ETA: 1s
5472/7440 [=====================>........] - ETA: 1s
5568/7440 [=====================>........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6016/7440 [=======================>......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6464/7440 [=========================>....] - ETA: 0s
6592/7440 [=========================>....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6784/7440 [==========================>...] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 4s 522us/step
current Test accuracy: 0.8286290322580645
current auc_score ------------------>  0.9364184443288242

  32/7440 [..............................] - ETA: 1:04:34
  96/7440 [..............................] - ETA: 21:24  
 192/7440 [..............................] - ETA: 10:35
 288/7440 [>.............................] - ETA: 6:59 
 384/7440 [>.............................] - ETA: 5:11
 480/7440 [>.............................] - ETA: 4:06
 576/7440 [=>............................] - ETA: 3:23
 704/7440 [=>............................] - ETA: 2:43
 800/7440 [==>...........................] - ETA: 2:22
 896/7440 [==>...........................] - ETA: 2:05
 992/7440 [===>..........................] - ETA: 1:52
1088/7440 [===>..........................] - ETA: 1:41
1184/7440 [===>..........................] - ETA: 1:31
1280/7440 [====>.........................] - ETA: 1:23
1408/7440 [====>.........................] - ETA: 1:14
1504/7440 [=====>........................] - ETA: 1:09
1600/7440 [=====>........................] - ETA: 1:04
1696/7440 [=====>........................] - ETA: 59s 
1824/7440 [======>.......................] - ETA: 54s
1952/7440 [======>.......................] - ETA: 49s
2048/7440 [=======>......................] - ETA: 46s
2144/7440 [=======>......................] - ETA: 44s
2272/7440 [========>.....................] - ETA: 40s
2368/7440 [========>.....................] - ETA: 38s
2496/7440 [=========>....................] - ETA: 35s
2592/7440 [=========>....................] - ETA: 33s
2688/7440 [=========>....................] - ETA: 32s
2784/7440 [==========>...................] - ETA: 30s
2880/7440 [==========>...................] - ETA: 28s
2976/7440 [===========>..................] - ETA: 27s
3072/7440 [===========>..................] - ETA: 26s
3168/7440 [===========>..................] - ETA: 24s
3264/7440 [============>.................] - ETA: 23s
3360/7440 [============>.................] - ETA: 22s
3456/7440 [============>.................] - ETA: 21s
3552/7440 [=============>................] - ETA: 20s
3648/7440 [=============>................] - ETA: 19s
3744/7440 [==============>...............] - ETA: 18s
3840/7440 [==============>...............] - ETA: 17s
3968/7440 [===============>..............] - ETA: 16s
4064/7440 [===============>..............] - ETA: 15s
4160/7440 [===============>..............] - ETA: 14s
4256/7440 [================>.............] - ETA: 14s
4352/7440 [================>.............] - ETA: 13s
4448/7440 [================>.............] - ETA: 12s
4544/7440 [=================>............] - ETA: 12s
4640/7440 [=================>............] - ETA: 11s
4736/7440 [==================>...........] - ETA: 10s
4864/7440 [==================>...........] - ETA: 10s
4992/7440 [===================>..........] - ETA: 9s 
5120/7440 [===================>..........] - ETA: 8s
5216/7440 [====================>.........] - ETA: 8s
5312/7440 [====================>.........] - ETA: 7s
5408/7440 [====================>.........] - ETA: 7s
5504/7440 [=====================>........] - ETA: 6s
5600/7440 [=====================>........] - ETA: 6s
5696/7440 [=====================>........] - ETA: 6s
5792/7440 [======================>.......] - ETA: 5s
5888/7440 [======================>.......] - ETA: 5s
5984/7440 [=======================>......] - ETA: 4s
6080/7440 [=======================>......] - ETA: 4s
6176/7440 [=======================>......] - ETA: 4s
6272/7440 [========================>.....] - ETA: 3s
6368/7440 [========================>.....] - ETA: 3s
6464/7440 [=========================>....] - ETA: 3s
6560/7440 [=========================>....] - ETA: 2s
6656/7440 [=========================>....] - ETA: 2s
6752/7440 [==========================>...] - ETA: 2s
6848/7440 [==========================>...] - ETA: 1s
6944/7440 [===========================>..] - ETA: 1s
7040/7440 [===========================>..] - ETA: 1s
7136/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7328/7440 [============================>.] - ETA: 0s
7424/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 21s 3ms/step
Best saved model Test accuracy: 0.8591397849462366
best saved model auc_score ------------------>  0.9358772690484449
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_40 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_40[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_440 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 40, 96, 96)   640         activation_440[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 40, 96, 96)   160         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_441 (Activation)     (None, 40, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 10, 96, 96)   3600        activation_441[0][0]             
__________________________________________________________________________________________________
concatenate_179 (Concatenate)   (None, 26, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 26, 96, 96)   104         concatenate_179[0][0]            
__________________________________________________________________________________________________
activation_442 (Activation)     (None, 26, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 13, 96, 96)   338         activation_442[0][0]             
__________________________________________________________________________________________________
average_pooling2d_45 (AveragePo (None, 13, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 13, 48, 48)   52          average_pooling2d_45[0][0]       
__________________________________________________________________________________________________
activation_443 (Activation)     (None, 13, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 40, 48, 48)   520         activation_443[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 40, 48, 48)   160         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_444 (Activation)     (None, 40, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 10, 48, 48)   3600        activation_444[0][0]             
__________________________________________________________________________________________________
concatenate_180 (Concatenate)   (None, 23, 48, 48)   0           average_pooling2d_45[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 23, 48, 48)   92          concatenate_180[0][0]            
__________________________________________________________________________________________________
activation_445 (Activation)     (None, 23, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 11, 48, 48)   253         activation_445[0][0]             
__________________________________________________________________________________________________
average_pooling2d_46 (AveragePo (None, 11, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 11, 24, 24)   44          average_pooling2d_46[0][0]       
__________________________________________________________________________________________________
activation_446 (Activation)     (None, 11, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 40, 24, 24)   440         activation_446[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 40, 24, 24)   160         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_447 (Activation)     (None, 40, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 10, 24, 24)   3600        activation_447[0][0]             
__________________________________________________________________________________________________
concatenate_181 (Concatenate)   (None, 21, 24, 24)   0           average_pooling2d_46[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 21, 24, 24)   84          concatenate_181[0][0]            
__________________________________________________________________________________________________
activation_448 (Activation)     (None, 21, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_40 (Gl (None, 21)           0           activation_448[0][0]             
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 1)            22          global_average_pooling2d_40[0][0]
==================================================================================================
Total params: 14,221
Trainable params: 13,761
Non-trainable params: 460
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/30
 - 59s - loss: 0.5695 - acc: 0.7347 - val_loss: 0.5142 - val_acc: 0.8112

Epoch 00001: val_loss improved from inf to 0.51419, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 2/30
 - 20s - loss: 0.4871 - acc: 0.7830 - val_loss: 0.5134 - val_acc: 0.7913

Epoch 00002: val_loss improved from 0.51419 to 0.51337, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 3/30
 - 20s - loss: 0.4529 - acc: 0.8010 - val_loss: 0.4861 - val_acc: 0.7972

Epoch 00003: val_loss improved from 0.51337 to 0.48612, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 4/30
 - 20s - loss: 0.4313 - acc: 0.8106 - val_loss: 0.4498 - val_acc: 0.8022

Epoch 00004: val_loss improved from 0.48612 to 0.44976, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 5/30
 - 20s - loss: 0.4164 - acc: 0.8184 - val_loss: 0.5598 - val_acc: 0.7938

Epoch 00005: val_loss did not improve from 0.44976
Epoch 6/30
 - 20s - loss: 0.4036 - acc: 0.8263 - val_loss: 0.4670 - val_acc: 0.8046

Epoch 00006: val_loss did not improve from 0.44976
Epoch 7/30
 - 20s - loss: 0.3952 - acc: 0.8289 - val_loss: 0.5111 - val_acc: 0.7860

Epoch 00007: val_loss did not improve from 0.44976
Epoch 8/30
 - 20s - loss: 0.3865 - acc: 0.8326 - val_loss: 0.8623 - val_acc: 0.7325

Epoch 00008: val_loss did not improve from 0.44976
Epoch 9/30
 - 20s - loss: 0.3812 - acc: 0.8359 - val_loss: 0.4469 - val_acc: 0.8069

Epoch 00009: val_loss improved from 0.44976 to 0.44693, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 10/30
 - 20s - loss: 0.3743 - acc: 0.8410 - val_loss: 0.4087 - val_acc: 0.8282

Epoch 00010: val_loss improved from 0.44693 to 0.40866, saving model to keras_densenet_simple_wt_28Sept_1340.h5
Epoch 11/30
 - 20s - loss: 0.3705 - acc: 0.8436 - val_loss: 0.6254 - val_acc: 0.7812

Epoch 00011: val_loss did not improve from 0.40866
Epoch 12/30
 - 20s - loss: 0.3673 - acc: 0.8441 - val_loss: 0.7366 - val_acc: 0.7638

Epoch 00012: val_loss did not improve from 0.40866
Epoch 13/30
 - 20s - loss: 0.3617 - acc: 0.8468 - val_loss: 0.6335 - val_acc: 0.7692

Epoch 00013: val_loss did not improve from 0.40866
Epoch 14/30
 - 20s - loss: 0.3565 - acc: 0.8503 - val_loss: 0.4660 - val_acc: 0.8009

Epoch 00014: val_loss did not improve from 0.40866
Epoch 15/30
 - 20s - loss: 0.3533 - acc: 0.8517 - val_loss: 0.4898 - val_acc: 0.8089
