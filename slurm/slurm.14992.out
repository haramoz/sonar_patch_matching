python keras_dn_simple_multigpu.py -g 4
python hyperas_dn_lr.py -n 40
no of  evals 20
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam, Adadelta, Adamax, Nadam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from keras.utils import multi_gpu_model
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from sklearn.model_selection import train_test_split
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import argparse
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'opt': hp.choice('opt', [Adam(lr=1E-2), RMSprop(lr=1E-2),Adadelta(),Adamax(lr=1E-2),Nadam()]),
    }

>>> Functions
  1: def process_data():
  2:     random_seed = 7
  3: 
  4:     f = h5py.File('/home/amalli2s/thesis/keras/matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  5:     X_train = f['X_train'].value
  6:     y_train = f['y_train'].value
  7:     X_test = f['X_val'].value
  8:     y_test = f['y_val'].value
  9:     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed)
 10:  
 11:     return X_train,y_train,X_val,y_val,X_test,y_test
 12: 
 13: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val,X_test,y_test = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 25
   4:     es_patience = 5
   5:     lr_patience = 3
   6:     dropout = None
   7:     depth = 25
   8:     nb_dense_block = 3
   9:     nb_filter = 16
  10:     growth_rate = 18
  11:     bn = True
  12:     reduction_ = 0.5
  13:     bs = 32
  14:     lr = 1E-2
  15:     opt = space['opt']
  16:     weight_file = 'hyperas_dn_lr_optimizer_wt_3Oct_1425.h5'
  17:     nb_classes = 1
  18:     img_dim = (2,96,96) 
  19:     n_channels = 2 
  20: 
  21:     
  22:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  23:                  growth_rate=growth_rate, nb_filter=nb_filter,
  24:                  dropout_rate=dropout,activation='sigmoid',
  25:                  input_shape=img_dim,include_top=True,
  26:                  bottleneck=bn,reduction=reduction_,
  27:                  classes=nb_classes,pooling='avg',
  28:                  weights=None)
  29:     
  30:     model.summary()
  31:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  32: 
  33:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  34:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  35: 
  36:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  37: 
  38:     model.fit(X_train,y_train,
  39:           batch_size=bs,
  40:           epochs=epochs,
  41:           callbacks=[lr_reducer,checkpointer,es],
  42:           validation_data=(X_val,y_val),
  43:           verbose=2)
  44:     
  45:     score, acc = model.evaluate(X_val,y_val)
  46:     print("current val accuracy:%0.3f"%acc)
  47:     pred = model.predict(X_val)
  48:     auc_score = roc_auc_score(y_val,pred)
  49:     print("current auc_score ------------------> %0.3f"%auc_score)
  50: 
  51:     model = load_model(weight_file) #This is the best model
  52:     score, acc = model.evaluate(X_val,y_val)
  53:     print("Best saved model val accuracy:%0.3f"% acc)
  54:     pred = model.predict(X_val)
  55:     auc_score = roc_auc_score(y_val,pred)
  56:     print("best saved model auc_score ------------------> %0.3f"%auc_score)
  57: 
  58:     
  59:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  60: 
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_1[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_1[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_3[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_5[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 70, 96, 96)   0           concatenate_2[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_7[0][0]               
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_8[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 53, 48, 48)   0           average_pooling2d_1[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_10[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 71, 48, 48)   0           concatenate_4[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_12[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_13[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 89, 48, 48)   0           concatenate_5[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_14[0][0]              
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_15[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 62, 24, 24)   0           average_pooling2d_2[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_17[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 80, 24, 24)   0           concatenate_7[0][0]              
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_19[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 98, 24, 24)   0           concatenate_8[0][0]              
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 98)           0           activation_21[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            99          global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 54s - loss: 0.5386 - acc: 0.7500 - val_loss: 1.0599 - val_acc: 0.6445

Epoch 00001: val_loss improved from inf to 1.05987, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 50s - loss: 0.4717 - acc: 0.7771 - val_loss: 0.5873 - val_acc: 0.6958

Epoch 00002: val_loss improved from 1.05987 to 0.58732, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 50s - loss: 0.4504 - acc: 0.7858 - val_loss: 0.6076 - val_acc: 0.7629

Epoch 00003: val_loss did not improve from 0.58732
Epoch 4/25
 - 50s - loss: 0.4321 - acc: 0.8014 - val_loss: 1.8422 - val_acc: 0.5061

Epoch 00004: val_loss did not improve from 0.58732
Epoch 5/25
 - 50s - loss: 0.4122 - acc: 0.8141 - val_loss: 0.6611 - val_acc: 0.7268

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0031622775894859655.

Epoch 00005: val_loss did not improve from 0.58732
Epoch 6/25
 - 50s - loss: 0.3653 - acc: 0.8412 - val_loss: 0.3781 - val_acc: 0.8411

Epoch 00006: val_loss improved from 0.58732 to 0.37808, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 50s - loss: 0.3481 - acc: 0.8506 - val_loss: 0.7416 - val_acc: 0.7210

Epoch 00007: val_loss did not improve from 0.37808
Epoch 8/25
 - 50s - loss: 0.3436 - acc: 0.8552 - val_loss: 0.4198 - val_acc: 0.8263

Epoch 00008: val_loss did not improve from 0.37808
Epoch 9/25
 - 50s - loss: 0.3350 - acc: 0.8596 - val_loss: 0.3620 - val_acc: 0.8471

Epoch 00009: val_loss improved from 0.37808 to 0.36196, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 50s - loss: 0.3269 - acc: 0.8634 - val_loss: 0.3557 - val_acc: 0.8494

Epoch 00010: val_loss improved from 0.36196 to 0.35572, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 50s - loss: 0.3207 - acc: 0.8673 - val_loss: 0.3602 - val_acc: 0.8488

Epoch 00011: val_loss did not improve from 0.35572
Epoch 12/25
 - 50s - loss: 0.3185 - acc: 0.8678 - val_loss: 0.7529 - val_acc: 0.6993

Epoch 00012: val_loss did not improve from 0.35572
Epoch 13/25
 - 50s - loss: 0.3120 - acc: 0.8724 - val_loss: 0.3611 - val_acc: 0.8508

Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999903331056.

Epoch 00013: val_loss did not improve from 0.35572
Epoch 14/25
 - 50s - loss: 0.2842 - acc: 0.8860 - val_loss: 0.2817 - val_acc: 0.8893

Epoch 00014: val_loss improved from 0.35572 to 0.28167, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 50s - loss: 0.2783 - acc: 0.8876 - val_loss: 0.3043 - val_acc: 0.8794

Epoch 00015: val_loss did not improve from 0.28167
Epoch 16/25
 - 50s - loss: 0.2740 - acc: 0.8905 - val_loss: 0.3459 - val_acc: 0.8623

Epoch 00016: val_loss did not improve from 0.28167
Epoch 17/25
 - 50s - loss: 0.2724 - acc: 0.8914 - val_loss: 0.2764 - val_acc: 0.8926

Epoch 00017: val_loss improved from 0.28167 to 0.27638, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 50s - loss: 0.2673 - acc: 0.8956 - val_loss: 0.2800 - val_acc: 0.8887

Epoch 00018: val_loss did not improve from 0.27638
Epoch 19/25
 - 50s - loss: 0.2632 - acc: 0.8961 - val_loss: 0.3330 - val_acc: 0.8690

Epoch 00019: val_loss did not improve from 0.27638
Epoch 20/25
 - 50s - loss: 0.2630 - acc: 0.8969 - val_loss: 0.2714 - val_acc: 0.8962

Epoch 00020: val_loss improved from 0.27638 to 0.27136, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 50s - loss: 0.2596 - acc: 0.8977 - val_loss: 0.2596 - val_acc: 0.8942

Epoch 00021: val_loss improved from 0.27136 to 0.25962, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 50s - loss: 0.2583 - acc: 0.8965 - val_loss: 0.2605 - val_acc: 0.8971

Epoch 00022: val_loss did not improve from 0.25962
Epoch 23/25
 - 50s - loss: 0.2539 - acc: 0.8997 - val_loss: 0.4203 - val_acc: 0.8424

Epoch 00023: val_loss did not improve from 0.25962
Epoch 24/25
 - 50s - loss: 0.2497 - acc: 0.9032 - val_loss: 0.2977 - val_acc: 0.8815

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.

Epoch 00024: val_loss did not improve from 0.25962
Epoch 25/25
 - 50s - loss: 0.2416 - acc: 0.9053 - val_loss: 0.2394 - val_acc: 0.9071

Epoch 00025: val_loss improved from 0.25962 to 0.23939, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 3s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 2s
1568/7968 [====>.........................] - ETA: 2s
1696/7968 [=====>........................] - ETA: 2s
1824/7968 [=====>........................] - ETA: 2s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 1s
3872/7968 [=============>................] - ETA: 1s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 0s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 462us/step
current val accuracy:0.907
current auc_score ------------------> 0.969

  32/7968 [..............................] - ETA: 1:57
 160/7968 [..............................] - ETA: 26s 
 288/7968 [>.............................] - ETA: 15s
 416/7968 [>.............................] - ETA: 11s
 544/7968 [=>............................] - ETA: 9s 
 672/7968 [=>............................] - ETA: 8s
 800/7968 [==>...........................] - ETA: 7s
 928/7968 [==>...........................] - ETA: 6s
1056/7968 [==>...........................] - ETA: 6s
1184/7968 [===>..........................] - ETA: 5s
1312/7968 [===>..........................] - ETA: 5s
1440/7968 [====>.........................] - ETA: 5s
1568/7968 [====>.........................] - ETA: 4s
1696/7968 [=====>........................] - ETA: 4s
1824/7968 [=====>........................] - ETA: 4s
1952/7968 [======>.......................] - ETA: 4s
2080/7968 [======>.......................] - ETA: 4s
2208/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2464/7968 [========>.....................] - ETA: 3s
2592/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2848/7968 [=========>....................] - ETA: 3s
2976/7968 [==========>...................] - ETA: 3s
3104/7968 [==========>...................] - ETA: 3s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4384/7968 [===============>..............] - ETA: 2s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 529us/step
Best saved model val accuracy:0.907
best saved model auc_score ------------------> 0.969
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_22[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_24[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 52, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_26[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 70, 96, 96)   0           concatenate_11[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_28[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_29[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_31[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 71, 48, 48)   0           concatenate_13[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_33[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 89, 48, 48)   0           concatenate_14[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_35[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_36[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_4[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_38[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_39[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 80, 24, 24)   0           concatenate_16[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_40[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_41[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 98, 24, 24)   0           concatenate_17[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 98)           0           activation_42[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            99          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 56s - loss: 0.5343 - acc: 0.7829 - val_loss: 0.6096 - val_acc: 0.7569

Epoch 00001: val_loss improved from inf to 0.60958, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 52s - loss: 0.4047 - acc: 0.8352 - val_loss: 0.4527 - val_acc: 0.8144

Epoch 00002: val_loss improved from 0.60958 to 0.45275, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 52s - loss: 0.3484 - acc: 0.8630 - val_loss: 0.3639 - val_acc: 0.8529

Epoch 00003: val_loss improved from 0.45275 to 0.36391, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 52s - loss: 0.3140 - acc: 0.8801 - val_loss: 0.3886 - val_acc: 0.8571

Epoch 00004: val_loss did not improve from 0.36391
Epoch 5/25
 - 52s - loss: 0.2861 - acc: 0.8951 - val_loss: 0.2953 - val_acc: 0.8899

Epoch 00005: val_loss improved from 0.36391 to 0.29527, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 52s - loss: 0.2613 - acc: 0.9083 - val_loss: 0.4101 - val_acc: 0.8547

Epoch 00006: val_loss did not improve from 0.29527
Epoch 7/25
 - 52s - loss: 0.2480 - acc: 0.9147 - val_loss: 0.3791 - val_acc: 0.8362

Epoch 00007: val_loss did not improve from 0.29527
Epoch 8/25
 - 52s - loss: 0.2354 - acc: 0.9201 - val_loss: 0.2700 - val_acc: 0.9015

Epoch 00008: val_loss improved from 0.29527 to 0.27000, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 52s - loss: 0.2204 - acc: 0.9274 - val_loss: 0.4116 - val_acc: 0.8624

Epoch 00009: val_loss did not improve from 0.27000
Epoch 10/25
 - 52s - loss: 0.2119 - acc: 0.9311 - val_loss: 0.2264 - val_acc: 0.9265

Epoch 00010: val_loss improved from 0.27000 to 0.22639, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 52s - loss: 0.2010 - acc: 0.9356 - val_loss: 0.6311 - val_acc: 0.8143

Epoch 00011: val_loss did not improve from 0.22639
Epoch 12/25
 - 52s - loss: 0.1928 - acc: 0.9400 - val_loss: 0.3858 - val_acc: 0.8566

Epoch 00012: val_loss did not improve from 0.22639
Epoch 13/25
 - 52s - loss: 0.1818 - acc: 0.9443 - val_loss: 0.4184 - val_acc: 0.8592

Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0006324555620737017.

Epoch 00013: val_loss did not improve from 0.22639
Epoch 14/25
 - 52s - loss: 0.1343 - acc: 0.9647 - val_loss: 0.1552 - val_acc: 0.9570

Epoch 00014: val_loss improved from 0.22639 to 0.15521, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 52s - loss: 0.1164 - acc: 0.9714 - val_loss: 0.1475 - val_acc: 0.9571

Epoch 00015: val_loss improved from 0.15521 to 0.14748, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 53s - loss: 0.1091 - acc: 0.9733 - val_loss: 0.1660 - val_acc: 0.9522

Epoch 00016: val_loss did not improve from 0.14748
Epoch 17/25
 - 53s - loss: 0.1063 - acc: 0.9739 - val_loss: 0.1596 - val_acc: 0.9544

Epoch 00017: val_loss did not improve from 0.14748
Epoch 18/25
 - 52s - loss: 0.1000 - acc: 0.9757 - val_loss: 0.1499 - val_acc: 0.9581

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020000001279212398.

Epoch 00018: val_loss did not improve from 0.14748
Epoch 19/25
 - 53s - loss: 0.0777 - acc: 0.9863 - val_loss: 0.1133 - val_acc: 0.9691

Epoch 00019: val_loss improved from 0.14748 to 0.11331, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 53s - loss: 0.0730 - acc: 0.9874 - val_loss: 0.1188 - val_acc: 0.9686

Epoch 00020: val_loss did not improve from 0.11331
Epoch 21/25
 - 53s - loss: 0.0690 - acc: 0.9895 - val_loss: 0.1104 - val_acc: 0.9714

Epoch 00021: val_loss improved from 0.11331 to 0.11039, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 53s - loss: 0.0683 - acc: 0.9893 - val_loss: 0.1102 - val_acc: 0.9710

Epoch 00022: val_loss improved from 0.11039 to 0.11019, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 53s - loss: 0.0640 - acc: 0.9899 - val_loss: 0.1124 - val_acc: 0.9700

Epoch 00023: val_loss did not improve from 0.11019
Epoch 24/25
 - 52s - loss: 0.0599 - acc: 0.9920 - val_loss: 0.1284 - val_acc: 0.9650

Epoch 00024: val_loss did not improve from 0.11019
Epoch 25/25
 - 53s - loss: 0.0584 - acc: 0.9922 - val_loss: 0.1111 - val_acc: 0.9708

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.324555620737017e-05.

Epoch 00025: val_loss did not improve from 0.11019

  32/7968 [..............................] - ETA: 3s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 2s
1824/7968 [=====>........................] - ETA: 2s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 1s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 0s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 480us/step
current val accuracy:0.971
current auc_score ------------------> 0.996

  32/7968 [..............................] - ETA: 3:16
 160/7968 [..............................] - ETA: 41s 
 288/7968 [>.............................] - ETA: 24s
 416/7968 [>.............................] - ETA: 17s
 544/7968 [=>............................] - ETA: 14s
 672/7968 [=>............................] - ETA: 11s
 800/7968 [==>...........................] - ETA: 10s
 928/7968 [==>...........................] - ETA: 9s 
1056/7968 [==>...........................] - ETA: 8s
1184/7968 [===>..........................] - ETA: 7s
1312/7968 [===>..........................] - ETA: 7s
1440/7968 [====>.........................] - ETA: 6s
1568/7968 [====>.........................] - ETA: 6s
1696/7968 [=====>........................] - ETA: 5s
1824/7968 [=====>........................] - ETA: 5s
1952/7968 [======>.......................] - ETA: 5s
2080/7968 [======>.......................] - ETA: 5s
2208/7968 [=======>......................] - ETA: 4s
2336/7968 [=======>......................] - ETA: 4s
2464/7968 [========>.....................] - ETA: 4s
2592/7968 [========>.....................] - ETA: 4s
2720/7968 [=========>....................] - ETA: 4s
2848/7968 [=========>....................] - ETA: 3s
2976/7968 [==========>...................] - ETA: 3s
3104/7968 [==========>...................] - ETA: 3s
3232/7968 [===========>..................] - ETA: 3s
3360/7968 [===========>..................] - ETA: 3s
3488/7968 [============>.................] - ETA: 3s
3616/7968 [============>.................] - ETA: 3s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4384/7968 [===============>..............] - ETA: 2s
4512/7968 [===============>..............] - ETA: 2s
4640/7968 [================>.............] - ETA: 2s
4768/7968 [================>.............] - ETA: 2s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6304/7968 [======================>.......] - ETA: 1s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 578us/step
Best saved model val accuracy:0.971
best saved model auc_score ------------------> 0.996
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_43[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_44[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_45[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_46[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 52, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_47[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_48[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 70, 96, 96)   0           concatenate_20[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_49[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_50[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_51[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_52[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_53[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 71, 48, 48)   0           concatenate_22[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_54[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_55[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 89, 48, 48)   0           concatenate_23[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_56[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_57[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_58[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_6[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_59[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_60[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 80, 24, 24)   0           concatenate_25[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_61[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_62[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 98, 24, 24)   0           concatenate_26[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 98)           0           activation_63[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            99          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 58s - loss: 0.5777 - acc: 0.7809 - val_loss: 0.6054 - val_acc: 0.7423

Epoch 00001: val_loss improved from inf to 0.60540, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 53s - loss: 0.4677 - acc: 0.8304 - val_loss: 0.4101 - val_acc: 0.8586

Epoch 00002: val_loss improved from 0.60540 to 0.41011, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 53s - loss: 0.4027 - acc: 0.8611 - val_loss: 0.4061 - val_acc: 0.8523

Epoch 00003: val_loss improved from 0.41011 to 0.40615, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 53s - loss: 0.3534 - acc: 0.8801 - val_loss: 0.3716 - val_acc: 0.8670

Epoch 00004: val_loss improved from 0.40615 to 0.37160, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 53s - loss: 0.3187 - acc: 0.8945 - val_loss: 0.5353 - val_acc: 0.8180

Epoch 00005: val_loss did not improve from 0.37160
Epoch 6/25
 - 53s - loss: 0.2841 - acc: 0.9108 - val_loss: 0.3596 - val_acc: 0.8996

Epoch 00006: val_loss improved from 0.37160 to 0.35959, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 53s - loss: 0.2635 - acc: 0.9174 - val_loss: 0.4694 - val_acc: 0.8326

Epoch 00007: val_loss did not improve from 0.35959
Epoch 8/25
 - 53s - loss: 0.2465 - acc: 0.9237 - val_loss: 0.3250 - val_acc: 0.8991

Epoch 00008: val_loss improved from 0.35959 to 0.32497, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 53s - loss: 0.2287 - acc: 0.9320 - val_loss: 0.2796 - val_acc: 0.9088

Epoch 00009: val_loss improved from 0.32497 to 0.27961, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 53s - loss: 0.2083 - acc: 0.9401 - val_loss: 0.2763 - val_acc: 0.9125

Epoch 00010: val_loss improved from 0.27961 to 0.27631, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 53s - loss: 0.1976 - acc: 0.9445 - val_loss: 0.2938 - val_acc: 0.8978

Epoch 00011: val_loss did not improve from 0.27631
Epoch 12/25
 - 53s - loss: 0.1879 - acc: 0.9482 - val_loss: 0.2648 - val_acc: 0.9183

Epoch 00012: val_loss improved from 0.27631 to 0.26477, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 53s - loss: 0.1767 - acc: 0.9507 - val_loss: 0.2692 - val_acc: 0.9194

Epoch 00013: val_loss did not improve from 0.26477
Epoch 14/25
 - 53s - loss: 0.1692 - acc: 0.9538 - val_loss: 0.3136 - val_acc: 0.9051

Epoch 00014: val_loss did not improve from 0.26477
Epoch 15/25
 - 53s - loss: 0.1593 - acc: 0.9582 - val_loss: 0.2213 - val_acc: 0.9349

Epoch 00015: val_loss improved from 0.26477 to 0.22131, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 53s - loss: 0.1507 - acc: 0.9614 - val_loss: 0.3238 - val_acc: 0.9127

Epoch 00016: val_loss did not improve from 0.22131
Epoch 17/25
 - 53s - loss: 0.1504 - acc: 0.9604 - val_loss: 0.3436 - val_acc: 0.8928

Epoch 00017: val_loss did not improve from 0.22131
Epoch 18/25
 - 53s - loss: 0.1421 - acc: 0.9643 - val_loss: 0.1884 - val_acc: 0.9459

Epoch 00018: val_loss improved from 0.22131 to 0.18841, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 53s - loss: 0.1337 - acc: 0.9677 - val_loss: 0.2799 - val_acc: 0.9168

Epoch 00019: val_loss did not improve from 0.18841
Epoch 20/25
 - 53s - loss: 0.1308 - acc: 0.9691 - val_loss: 0.2071 - val_acc: 0.9429

Epoch 00020: val_loss did not improve from 0.18841
Epoch 21/25
 - 53s - loss: 0.1256 - acc: 0.9704 - val_loss: 0.3018 - val_acc: 0.9040

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.31622776601683794.

Epoch 00021: val_loss did not improve from 0.18841
Epoch 22/25
 - 53s - loss: 0.0829 - acc: 0.9879 - val_loss: 0.1410 - val_acc: 0.9645

Epoch 00022: val_loss improved from 0.18841 to 0.14104, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 53s - loss: 0.0744 - acc: 0.9908 - val_loss: 0.1353 - val_acc: 0.9686

Epoch 00023: val_loss improved from 0.14104 to 0.13534, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 53s - loss: 0.0683 - acc: 0.9926 - val_loss: 0.1498 - val_acc: 0.9647

Epoch 00024: val_loss did not improve from 0.13534
Epoch 25/25
 - 53s - loss: 0.0651 - acc: 0.9937 - val_loss: 0.1367 - val_acc: 0.9684

Epoch 00025: val_loss did not improve from 0.13534

  32/7968 [..............................] - ETA: 3s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 498us/step
current val accuracy:0.968
current auc_score ------------------> 0.995

  32/7968 [..............................] - ETA: 4:37
 128/7968 [..............................] - ETA: 1:11
 256/7968 [..............................] - ETA: 37s 
 384/7968 [>.............................] - ETA: 25s
 512/7968 [>.............................] - ETA: 19s
 640/7968 [=>............................] - ETA: 16s
 768/7968 [=>............................] - ETA: 13s
 896/7968 [==>...........................] - ETA: 12s
1024/7968 [==>...........................] - ETA: 10s
1152/7968 [===>..........................] - ETA: 9s 
1280/7968 [===>..........................] - ETA: 9s
1408/7968 [====>.........................] - ETA: 8s
1536/7968 [====>.........................] - ETA: 7s
1664/7968 [=====>........................] - ETA: 7s
1792/7968 [=====>........................] - ETA: 6s
1920/7968 [======>.......................] - ETA: 6s
2048/7968 [======>.......................] - ETA: 6s
2176/7968 [=======>......................] - ETA: 5s
2304/7968 [=======>......................] - ETA: 5s
2432/7968 [========>.....................] - ETA: 5s
2560/7968 [========>.....................] - ETA: 5s
2688/7968 [=========>....................] - ETA: 4s
2816/7968 [=========>....................] - ETA: 4s
2944/7968 [==========>...................] - ETA: 4s
3072/7968 [==========>...................] - ETA: 4s
3200/7968 [===========>..................] - ETA: 4s
3328/7968 [===========>..................] - ETA: 3s
3456/7968 [============>.................] - ETA: 3s
3584/7968 [============>.................] - ETA: 3s
3712/7968 [============>.................] - ETA: 3s
3840/7968 [=============>................] - ETA: 3s
3968/7968 [=============>................] - ETA: 3s
4096/7968 [==============>...............] - ETA: 2s
4224/7968 [==============>...............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4480/7968 [===============>..............] - ETA: 2s
4608/7968 [================>.............] - ETA: 2s
4736/7968 [================>.............] - ETA: 2s
4864/7968 [=================>............] - ETA: 2s
4992/7968 [=================>............] - ETA: 2s
5120/7968 [==================>...........] - ETA: 2s
5248/7968 [==================>...........] - ETA: 1s
5376/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5632/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6400/7968 [=======================>......] - ETA: 1s
6528/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 635us/step
Best saved model val accuracy:0.969
best saved model auc_score ------------------> 0.995
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_64[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_65[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_66[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_67[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 52, 96, 96)   0           concatenate_28[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_68[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_69[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 70, 96, 96)   0           concatenate_29[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_70[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_71[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_72[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_73[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_74[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 71, 48, 48)   0           concatenate_31[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_75[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_76[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 89, 48, 48)   0           concatenate_32[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_77[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_78[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_79[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_8[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_80[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_81[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 80, 24, 24)   0           concatenate_34[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_82[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_83[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 98, 24, 24)   0           concatenate_35[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 98)           0           activation_84[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            99          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 56s - loss: 0.5830 - acc: 0.7838 - val_loss: 0.6997 - val_acc: 0.7373

Epoch 00001: val_loss improved from inf to 0.69968, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 51s - loss: 0.4922 - acc: 0.8304 - val_loss: 0.4937 - val_acc: 0.8422

Epoch 00002: val_loss improved from 0.69968 to 0.49371, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 51s - loss: 0.4435 - acc: 0.8539 - val_loss: 0.4765 - val_acc: 0.8429

Epoch 00003: val_loss improved from 0.49371 to 0.47648, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 51s - loss: 0.3989 - acc: 0.8758 - val_loss: 0.7029 - val_acc: 0.7869

Epoch 00004: val_loss did not improve from 0.47648
Epoch 5/25
 - 51s - loss: 0.3647 - acc: 0.8913 - val_loss: 0.5574 - val_acc: 0.8311

Epoch 00005: val_loss did not improve from 0.47648
Epoch 6/25
 - 51s - loss: 0.3377 - acc: 0.9033 - val_loss: 0.3235 - val_acc: 0.9099

Epoch 00006: val_loss improved from 0.47648 to 0.32345, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 51s - loss: 0.3151 - acc: 0.9132 - val_loss: 0.3110 - val_acc: 0.9160

Epoch 00007: val_loss improved from 0.32345 to 0.31104, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 51s - loss: 0.2938 - acc: 0.9220 - val_loss: 0.3533 - val_acc: 0.8988

Epoch 00008: val_loss did not improve from 0.31104
Epoch 9/25
 - 51s - loss: 0.2755 - acc: 0.9279 - val_loss: 0.2911 - val_acc: 0.9285

Epoch 00009: val_loss improved from 0.31104 to 0.29112, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 51s - loss: 0.2581 - acc: 0.9360 - val_loss: 0.2770 - val_acc: 0.9300

Epoch 00010: val_loss improved from 0.29112 to 0.27701, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 51s - loss: 0.2474 - acc: 0.9391 - val_loss: 0.2828 - val_acc: 0.9291

Epoch 00011: val_loss did not improve from 0.27701
Epoch 12/25
 - 51s - loss: 0.2348 - acc: 0.9446 - val_loss: 0.3426 - val_acc: 0.8981

Epoch 00012: val_loss did not improve from 0.27701
Epoch 13/25
 - 51s - loss: 0.2240 - acc: 0.9477 - val_loss: 0.2303 - val_acc: 0.9472

Epoch 00013: val_loss improved from 0.27701 to 0.23033, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 51s - loss: 0.2144 - acc: 0.9521 - val_loss: 0.3517 - val_acc: 0.8985

Epoch 00014: val_loss did not improve from 0.23033
Epoch 15/25
 - 51s - loss: 0.2031 - acc: 0.9564 - val_loss: 0.4010 - val_acc: 0.8709

Epoch 00015: val_loss did not improve from 0.23033
Epoch 16/25
 - 51s - loss: 0.1958 - acc: 0.9587 - val_loss: 0.2876 - val_acc: 0.9256

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00016: val_loss did not improve from 0.23033
Epoch 17/25
 - 51s - loss: 0.1529 - acc: 0.9773 - val_loss: 0.2003 - val_acc: 0.9559

Epoch 00017: val_loss improved from 0.23033 to 0.20030, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 51s - loss: 0.1450 - acc: 0.9799 - val_loss: 0.1961 - val_acc: 0.9576

Epoch 00018: val_loss improved from 0.20030 to 0.19613, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 51s - loss: 0.1382 - acc: 0.9825 - val_loss: 0.1862 - val_acc: 0.9611

Epoch 00019: val_loss improved from 0.19613 to 0.18624, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 51s - loss: 0.1343 - acc: 0.9838 - val_loss: 0.1855 - val_acc: 0.9605

Epoch 00020: val_loss improved from 0.18624 to 0.18553, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 51s - loss: 0.1301 - acc: 0.9849 - val_loss: 0.1866 - val_acc: 0.9605

Epoch 00021: val_loss did not improve from 0.18553
Epoch 22/25
 - 51s - loss: 0.1277 - acc: 0.9851 - val_loss: 0.1944 - val_acc: 0.9572

Epoch 00022: val_loss did not improve from 0.18553
Epoch 23/25
 - 51s - loss: 0.1247 - acc: 0.9861 - val_loss: 0.1907 - val_acc: 0.9593

Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.

Epoch 00023: val_loss did not improve from 0.18553
Epoch 24/25
 - 51s - loss: 0.1136 - acc: 0.9909 - val_loss: 0.1756 - val_acc: 0.9657

Epoch 00024: val_loss improved from 0.18553 to 0.17556, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 51s - loss: 0.1098 - acc: 0.9930 - val_loss: 0.1756 - val_acc: 0.9651

Epoch 00025: val_loss did not improve from 0.17556

  32/7968 [..............................] - ETA: 3s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 494us/step
current val accuracy:0.965
current auc_score ------------------> 0.995

  32/7968 [..............................] - ETA: 5:40
 128/7968 [..............................] - ETA: 1:27
 256/7968 [..............................] - ETA: 44s 
 384/7968 [>.............................] - ETA: 30s
 512/7968 [>.............................] - ETA: 23s
 640/7968 [=>............................] - ETA: 19s
 768/7968 [=>............................] - ETA: 16s
 896/7968 [==>...........................] - ETA: 14s
1024/7968 [==>...........................] - ETA: 12s
1152/7968 [===>..........................] - ETA: 11s
1280/7968 [===>..........................] - ETA: 10s
1408/7968 [====>.........................] - ETA: 9s 
1536/7968 [====>.........................] - ETA: 8s
1664/7968 [=====>........................] - ETA: 8s
1792/7968 [=====>........................] - ETA: 7s
1920/7968 [======>.......................] - ETA: 7s
2048/7968 [======>.......................] - ETA: 6s
2176/7968 [=======>......................] - ETA: 6s
2304/7968 [=======>......................] - ETA: 6s
2432/7968 [========>.....................] - ETA: 5s
2560/7968 [========>.....................] - ETA: 5s
2688/7968 [=========>....................] - ETA: 5s
2816/7968 [=========>....................] - ETA: 5s
2944/7968 [==========>...................] - ETA: 4s
3072/7968 [==========>...................] - ETA: 4s
3200/7968 [===========>..................] - ETA: 4s
3328/7968 [===========>..................] - ETA: 4s
3456/7968 [============>.................] - ETA: 4s
3584/7968 [============>.................] - ETA: 3s
3712/7968 [============>.................] - ETA: 3s
3840/7968 [=============>................] - ETA: 3s
3968/7968 [=============>................] - ETA: 3s
4096/7968 [==============>...............] - ETA: 3s
4224/7968 [==============>...............] - ETA: 3s
4352/7968 [===============>..............] - ETA: 2s
4480/7968 [===============>..............] - ETA: 2s
4608/7968 [================>.............] - ETA: 2s
4736/7968 [================>.............] - ETA: 2s
4864/7968 [=================>............] - ETA: 2s
4992/7968 [=================>............] - ETA: 2s
5120/7968 [==================>...........] - ETA: 2s
5248/7968 [==================>...........] - ETA: 2s
5376/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5632/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6400/7968 [=======================>......] - ETA: 1s
6528/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 673us/step
Best saved model val accuracy:0.966
best saved model auc_score ------------------> 0.995
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_85[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_86[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_87[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 52, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_89[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_90[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 70, 96, 96)   0           concatenate_38[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_91[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_92[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_93[0][0]              
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_94[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_95[0][0]              
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 71, 48, 48)   0           concatenate_40[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_96[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_97[0][0]              
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 89, 48, 48)   0           concatenate_41[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_98[0][0]              
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_99[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_100[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_10[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_101[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_102[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 80, 24, 24)   0           concatenate_43[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_103[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_104[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 98, 24, 24)   0           concatenate_44[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 98)           0           activation_105[0][0]             
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            99          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 57s - loss: 0.5501 - acc: 0.7656 - val_loss: 0.7752 - val_acc: 0.7006

Epoch 00001: val_loss improved from inf to 0.77521, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 51s - loss: 0.4553 - acc: 0.8016 - val_loss: 0.4698 - val_acc: 0.7937

Epoch 00002: val_loss improved from 0.77521 to 0.46982, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 51s - loss: 0.4161 - acc: 0.8180 - val_loss: 0.6324 - val_acc: 0.7728

Epoch 00003: val_loss did not improve from 0.46982
Epoch 4/25
 - 51s - loss: 0.3749 - acc: 0.8440 - val_loss: 0.3899 - val_acc: 0.8422

Epoch 00004: val_loss improved from 0.46982 to 0.38991, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 51s - loss: 0.3451 - acc: 0.8606 - val_loss: 1.0868 - val_acc: 0.7289

Epoch 00005: val_loss did not improve from 0.38991
Epoch 6/25
 - 51s - loss: 0.3168 - acc: 0.8781 - val_loss: 0.8093 - val_acc: 0.7898

Epoch 00006: val_loss did not improve from 0.38991
Epoch 7/25
 - 51s - loss: 0.2985 - acc: 0.8855 - val_loss: 0.3204 - val_acc: 0.8800

Epoch 00007: val_loss improved from 0.38991 to 0.32042, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 51s - loss: 0.2789 - acc: 0.8967 - val_loss: 0.3158 - val_acc: 0.8809

Epoch 00008: val_loss improved from 0.32042 to 0.31580, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 51s - loss: 0.2621 - acc: 0.9066 - val_loss: 0.3610 - val_acc: 0.8731

Epoch 00009: val_loss did not improve from 0.31580
Epoch 10/25
 - 51s - loss: 0.2466 - acc: 0.9135 - val_loss: 0.2468 - val_acc: 0.9140

Epoch 00010: val_loss improved from 0.31580 to 0.24677, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 51s - loss: 0.2337 - acc: 0.9203 - val_loss: 0.2721 - val_acc: 0.9034

Epoch 00011: val_loss did not improve from 0.24677
Epoch 12/25
 - 51s - loss: 0.2200 - acc: 0.9276 - val_loss: 0.2906 - val_acc: 0.9006

Epoch 00012: val_loss did not improve from 0.24677
Epoch 13/25
 - 51s - loss: 0.2100 - acc: 0.9296 - val_loss: 0.2505 - val_acc: 0.9153

Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0031622775894859655.

Epoch 00013: val_loss did not improve from 0.24677
Epoch 14/25
 - 51s - loss: 0.1614 - acc: 0.9517 - val_loss: 0.1652 - val_acc: 0.9488

Epoch 00014: val_loss improved from 0.24677 to 0.16518, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 51s - loss: 0.1424 - acc: 0.9592 - val_loss: 0.1522 - val_acc: 0.9541

Epoch 00015: val_loss improved from 0.16518 to 0.15222, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 51s - loss: 0.1309 - acc: 0.9631 - val_loss: 0.1707 - val_acc: 0.9462

Epoch 00016: val_loss did not improve from 0.15222
Epoch 17/25
 - 52s - loss: 0.1229 - acc: 0.9661 - val_loss: 0.1338 - val_acc: 0.9602

Epoch 00017: val_loss improved from 0.15222 to 0.13384, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 52s - loss: 0.1159 - acc: 0.9677 - val_loss: 0.1377 - val_acc: 0.9597

Epoch 00018: val_loss did not improve from 0.13384
Epoch 19/25
 - 51s - loss: 0.1091 - acc: 0.9706 - val_loss: 0.1697 - val_acc: 0.9449

Epoch 00019: val_loss did not improve from 0.13384
Epoch 20/25
 - 51s - loss: 0.1053 - acc: 0.9716 - val_loss: 0.1487 - val_acc: 0.9553

Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0009999999903331056.

Epoch 00020: val_loss did not improve from 0.13384
Epoch 21/25
 - 51s - loss: 0.0852 - acc: 0.9804 - val_loss: 0.1127 - val_acc: 0.9698

Epoch 00021: val_loss improved from 0.13384 to 0.11265, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 51s - loss: 0.0785 - acc: 0.9825 - val_loss: 0.1190 - val_acc: 0.9664

Epoch 00022: val_loss did not improve from 0.11265
Epoch 23/25
 - 51s - loss: 0.0749 - acc: 0.9838 - val_loss: 0.1169 - val_acc: 0.9666

Epoch 00023: val_loss did not improve from 0.11265
Epoch 24/25
 - 51s - loss: 0.0752 - acc: 0.9839 - val_loss: 0.1277 - val_acc: 0.9627

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.

Epoch 00024: val_loss did not improve from 0.11265
Epoch 25/25
 - 51s - loss: 0.0701 - acc: 0.9865 - val_loss: 0.1120 - val_acc: 0.9688

Epoch 00025: val_loss improved from 0.11265 to 0.11196, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6400/7968 [=======================>......] - ETA: 0s
6528/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 507us/step
current val accuracy:0.969
current auc_score ------------------> 0.995

  32/7968 [..............................] - ETA: 7:10
 128/7968 [..............................] - ETA: 1:49
 224/7968 [..............................] - ETA: 1:03
 320/7968 [>.............................] - ETA: 45s 
 416/7968 [>.............................] - ETA: 35s
 544/7968 [=>............................] - ETA: 27s
 672/7968 [=>............................] - ETA: 22s
 800/7968 [==>...........................] - ETA: 19s
 928/7968 [==>...........................] - ETA: 16s
1056/7968 [==>...........................] - ETA: 14s
1184/7968 [===>..........................] - ETA: 13s
1312/7968 [===>..........................] - ETA: 12s
1408/7968 [====>.........................] - ETA: 11s
1536/7968 [====>.........................] - ETA: 10s
1664/7968 [=====>........................] - ETA: 9s 
1792/7968 [=====>........................] - ETA: 9s
1920/7968 [======>.......................] - ETA: 8s
2048/7968 [======>.......................] - ETA: 8s
2176/7968 [=======>......................] - ETA: 7s
2304/7968 [=======>......................] - ETA: 7s
2432/7968 [========>.....................] - ETA: 6s
2560/7968 [========>.....................] - ETA: 6s
2688/7968 [=========>....................] - ETA: 6s
2816/7968 [=========>....................] - ETA: 5s
2944/7968 [==========>...................] - ETA: 5s
3072/7968 [==========>...................] - ETA: 5s
3200/7968 [===========>..................] - ETA: 5s
3328/7968 [===========>..................] - ETA: 4s
3456/7968 [============>.................] - ETA: 4s
3584/7968 [============>.................] - ETA: 4s
3712/7968 [============>.................] - ETA: 4s
3840/7968 [=============>................] - ETA: 3s
3968/7968 [=============>................] - ETA: 3s
4096/7968 [==============>...............] - ETA: 3s
4224/7968 [==============>...............] - ETA: 3s
4352/7968 [===============>..............] - ETA: 3s
4480/7968 [===============>..............] - ETA: 3s
4608/7968 [================>.............] - ETA: 2s
4736/7968 [================>.............] - ETA: 2s
4864/7968 [=================>............] - ETA: 2s
4992/7968 [=================>............] - ETA: 2s
5120/7968 [==================>...........] - ETA: 2s
5248/7968 [==================>...........] - ETA: 2s
5344/7968 [===================>..........] - ETA: 2s
5472/7968 [===================>..........] - ETA: 2s
5600/7968 [====================>.........] - ETA: 1s
5728/7968 [====================>.........] - ETA: 1s
5856/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6208/7968 [======================>.......] - ETA: 1s
6336/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6592/7968 [=======================>......] - ETA: 1s
6720/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6976/7968 [=========================>....] - ETA: 0s
7104/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7360/7968 [==========================>...] - ETA: 0s
7488/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 6s 731us/step
Best saved model val accuracy:0.969
best saved model auc_score ------------------> 0.995
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_106[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_107[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_108[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_109[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 52, 96, 96)   0           concatenate_46[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_110[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_111[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 70, 96, 96)   0           concatenate_47[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_112[0][0]             
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_113[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_114[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_11[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_115[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_116[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 71, 48, 48)   0           concatenate_49[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_117[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_118[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 89, 48, 48)   0           concatenate_50[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_119[0][0]             
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_120[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_121[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_12[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_122[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_123[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 80, 24, 24)   0           concatenate_52[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_124[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_125[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 98, 24, 24)   0           concatenate_53[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 98)           0           activation_126[0][0]             
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            99          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 62s - loss: 0.6042 - acc: 0.7727 - val_loss: 0.6019 - val_acc: 0.7723

Epoch 00001: val_loss improved from inf to 0.60193, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.5243 - acc: 0.8135 - val_loss: 0.4920 - val_acc: 0.8302

Epoch 00002: val_loss improved from 0.60193 to 0.49202, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 54s - loss: 0.4718 - acc: 0.8402 - val_loss: 0.5000 - val_acc: 0.8273

Epoch 00003: val_loss did not improve from 0.49202
Epoch 4/25
 - 54s - loss: 0.4300 - acc: 0.8626 - val_loss: 0.4574 - val_acc: 0.8537

Epoch 00004: val_loss improved from 0.49202 to 0.45742, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 54s - loss: 0.3914 - acc: 0.8813 - val_loss: 0.4252 - val_acc: 0.8702

Epoch 00005: val_loss improved from 0.45742 to 0.42521, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 54s - loss: 0.3647 - acc: 0.8929 - val_loss: 0.3359 - val_acc: 0.9108

Epoch 00006: val_loss improved from 0.42521 to 0.33587, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 54s - loss: 0.3371 - acc: 0.9070 - val_loss: 0.3051 - val_acc: 0.9204

Epoch 00007: val_loss improved from 0.33587 to 0.30506, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 54s - loss: 0.3121 - acc: 0.9149 - val_loss: 0.6390 - val_acc: 0.8075

Epoch 00008: val_loss did not improve from 0.30506
Epoch 9/25
 - 54s - loss: 0.2953 - acc: 0.9219 - val_loss: 0.3407 - val_acc: 0.9004

Epoch 00009: val_loss did not improve from 0.30506
Epoch 10/25
 - 54s - loss: 0.2757 - acc: 0.9301 - val_loss: 0.4487 - val_acc: 0.8486

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.09999999932782062.

Epoch 00010: val_loss did not improve from 0.30506
Epoch 11/25
 - 54s - loss: 0.2297 - acc: 0.9524 - val_loss: 0.2444 - val_acc: 0.9439

Epoch 00011: val_loss improved from 0.30506 to 0.24443, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 54s - loss: 0.2161 - acc: 0.9572 - val_loss: 0.2389 - val_acc: 0.9463

Epoch 00012: val_loss improved from 0.24443 to 0.23888, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 54s - loss: 0.2050 - acc: 0.9616 - val_loss: 0.2575 - val_acc: 0.9378

Epoch 00013: val_loss did not improve from 0.23888
Epoch 14/25
 - 54s - loss: 0.1992 - acc: 0.9639 - val_loss: 0.2199 - val_acc: 0.9546

Epoch 00014: val_loss improved from 0.23888 to 0.21993, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.1967 - acc: 0.9629 - val_loss: 0.2359 - val_acc: 0.9472

Epoch 00015: val_loss did not improve from 0.21993
Epoch 16/25
 - 54s - loss: 0.1894 - acc: 0.9671 - val_loss: 0.2380 - val_acc: 0.9452

Epoch 00016: val_loss did not improve from 0.21993
Epoch 17/25
 - 54s - loss: 0.1783 - acc: 0.9730 - val_loss: 0.2185 - val_acc: 0.9526

Epoch 00017: val_loss improved from 0.21993 to 0.21853, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 54s - loss: 0.1747 - acc: 0.9726 - val_loss: 0.2147 - val_acc: 0.9559

Epoch 00018: val_loss improved from 0.21853 to 0.21466, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 54s - loss: 0.1714 - acc: 0.9739 - val_loss: 0.2231 - val_acc: 0.9504

Epoch 00019: val_loss did not improve from 0.21466
Epoch 20/25
 - 54s - loss: 0.1678 - acc: 0.9741 - val_loss: 0.2346 - val_acc: 0.9439

Epoch 00020: val_loss did not improve from 0.21466
Epoch 21/25
 - 54s - loss: 0.1615 - acc: 0.9773 - val_loss: 0.2204 - val_acc: 0.9507

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.031622777072899885.

Epoch 00021: val_loss did not improve from 0.21466
Epoch 22/25
 - 54s - loss: 0.1496 - acc: 0.9830 - val_loss: 0.2067 - val_acc: 0.9570

Epoch 00022: val_loss improved from 0.21466 to 0.20672, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 54s - loss: 0.1473 - acc: 0.9828 - val_loss: 0.2058 - val_acc: 0.9583

Epoch 00023: val_loss improved from 0.20672 to 0.20582, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 54s - loss: 0.1456 - acc: 0.9844 - val_loss: 0.1999 - val_acc: 0.9606

Epoch 00024: val_loss improved from 0.20582 to 0.19992, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 54s - loss: 0.1411 - acc: 0.9864 - val_loss: 0.1987 - val_acc: 0.9612

Epoch 00025: val_loss improved from 0.19992 to 0.19870, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 4s
 288/7968 [>.............................] - ETA: 3s
 384/7968 [>.............................] - ETA: 3s
 480/7968 [>.............................] - ETA: 3s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1504/7968 [====>.........................] - ETA: 3s
1600/7968 [=====>........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1792/7968 [=====>........................] - ETA: 3s
1920/7968 [======>.......................] - ETA: 3s
2016/7968 [======>.......................] - ETA: 3s
2112/7968 [======>.......................] - ETA: 3s
2208/7968 [=======>......................] - ETA: 3s
2304/7968 [=======>......................] - ETA: 2s
2400/7968 [========>.....................] - ETA: 2s
2496/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2688/7968 [=========>....................] - ETA: 2s
2784/7968 [=========>....................] - ETA: 2s
2880/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3072/7968 [==========>...................] - ETA: 2s
3168/7968 [==========>...................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4480/7968 [===============>..............] - ETA: 1s
4576/7968 [================>.............] - ETA: 1s
4704/7968 [================>.............] - ETA: 1s
4800/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5856/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6880/7968 [========================>.....] - ETA: 0s
6976/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7264/7968 [==========================>...] - ETA: 0s
7360/7968 [==========================>...] - ETA: 0s
7488/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 526us/step
current val accuracy:0.961
current auc_score ------------------> 0.992

  32/7968 [..............................] - ETA: 8:35
 128/7968 [..............................] - ETA: 2:10
 224/7968 [..............................] - ETA: 1:15
 320/7968 [>.............................] - ETA: 53s 
 416/7968 [>.............................] - ETA: 41s
 512/7968 [>.............................] - ETA: 33s
 640/7968 [=>............................] - ETA: 27s
 768/7968 [=>............................] - ETA: 23s
 896/7968 [==>...........................] - ETA: 19s
1024/7968 [==>...........................] - ETA: 17s
1152/7968 [===>..........................] - ETA: 15s
1280/7968 [===>..........................] - ETA: 14s
1376/7968 [====>.........................] - ETA: 13s
1504/7968 [====>.........................] - ETA: 12s
1632/7968 [=====>........................] - ETA: 11s
1760/7968 [=====>........................] - ETA: 10s
1888/7968 [======>.......................] - ETA: 9s 
1984/7968 [======>.......................] - ETA: 9s
2080/7968 [======>.......................] - ETA: 8s
2208/7968 [=======>......................] - ETA: 8s
2336/7968 [=======>......................] - ETA: 7s
2464/7968 [========>.....................] - ETA: 7s
2592/7968 [========>.....................] - ETA: 7s
2688/7968 [=========>....................] - ETA: 6s
2816/7968 [=========>....................] - ETA: 6s
2944/7968 [==========>...................] - ETA: 6s
3040/7968 [==========>...................] - ETA: 5s
3168/7968 [==========>...................] - ETA: 5s
3296/7968 [===========>..................] - ETA: 5s
3424/7968 [===========>..................] - ETA: 5s
3520/7968 [============>.................] - ETA: 4s
3648/7968 [============>.................] - ETA: 4s
3776/7968 [=============>................] - ETA: 4s
3904/7968 [=============>................] - ETA: 4s
4032/7968 [==============>...............] - ETA: 4s
4160/7968 [==============>...............] - ETA: 3s
4288/7968 [===============>..............] - ETA: 3s
4416/7968 [===============>..............] - ETA: 3s
4544/7968 [================>.............] - ETA: 3s
4672/7968 [================>.............] - ETA: 3s
4800/7968 [=================>............] - ETA: 2s
4928/7968 [=================>............] - ETA: 2s
5056/7968 [==================>...........] - ETA: 2s
5184/7968 [==================>...........] - ETA: 2s
5312/7968 [===================>..........] - ETA: 2s
5440/7968 [===================>..........] - ETA: 2s
5568/7968 [===================>..........] - ETA: 2s
5664/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6304/7968 [======================>.......] - ETA: 1s
6432/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6784/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7008/7968 [=========================>....] - ETA: 0s
7104/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 6s 776us/step
Best saved model val accuracy:0.961
best saved model auc_score ------------------> 0.992
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_127[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_128[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_129[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_130[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 52, 96, 96)   0           concatenate_55[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_131[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_132[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 70, 96, 96)   0           concatenate_56[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_133[0][0]             
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_134[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_135[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_136[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_137[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 71, 48, 48)   0           concatenate_58[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_138[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_139[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 89, 48, 48)   0           concatenate_59[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_140[0][0]             
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_141[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_142[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_14[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_143[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_144[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 80, 24, 24)   0           concatenate_61[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_145[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_146[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 98, 24, 24)   0           concatenate_62[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 98)           0           activation_147[0][0]             
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            99          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 59s - loss: 0.6289 - acc: 0.7658 - val_loss: 0.5937 - val_acc: 0.7845

Epoch 00001: val_loss improved from inf to 0.59371, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 51s - loss: 0.5468 - acc: 0.8044 - val_loss: 0.5115 - val_acc: 0.8194

Epoch 00002: val_loss improved from 0.59371 to 0.51153, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 51s - loss: 0.5094 - acc: 0.8228 - val_loss: 0.5105 - val_acc: 0.8323

Epoch 00003: val_loss improved from 0.51153 to 0.51049, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 51s - loss: 0.4764 - acc: 0.8409 - val_loss: 0.4735 - val_acc: 0.8390

Epoch 00004: val_loss improved from 0.51049 to 0.47351, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 51s - loss: 0.4517 - acc: 0.8561 - val_loss: 0.4550 - val_acc: 0.8614

Epoch 00005: val_loss improved from 0.47351 to 0.45500, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 51s - loss: 0.4284 - acc: 0.8693 - val_loss: 0.4822 - val_acc: 0.8386

Epoch 00006: val_loss did not improve from 0.45500
Epoch 7/25
 - 51s - loss: 0.4133 - acc: 0.8766 - val_loss: 0.3964 - val_acc: 0.8860

Epoch 00007: val_loss improved from 0.45500 to 0.39636, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 51s - loss: 0.3996 - acc: 0.8843 - val_loss: 0.4307 - val_acc: 0.8717

Epoch 00008: val_loss did not improve from 0.39636
Epoch 9/25
 - 51s - loss: 0.3790 - acc: 0.8943 - val_loss: 0.3772 - val_acc: 0.8981

Epoch 00009: val_loss improved from 0.39636 to 0.37721, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 51s - loss: 0.3644 - acc: 0.8995 - val_loss: 0.5033 - val_acc: 0.8326

Epoch 00010: val_loss did not improve from 0.37721
Epoch 11/25
 - 51s - loss: 0.3573 - acc: 0.9023 - val_loss: 0.3506 - val_acc: 0.9032

Epoch 00011: val_loss improved from 0.37721 to 0.35059, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 51s - loss: 0.3385 - acc: 0.9118 - val_loss: 0.3615 - val_acc: 0.9026

Epoch 00012: val_loss did not improve from 0.35059
Epoch 13/25
 - 51s - loss: 0.3316 - acc: 0.9143 - val_loss: 0.3445 - val_acc: 0.9054

Epoch 00013: val_loss improved from 0.35059 to 0.34450, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 51s - loss: 0.3211 - acc: 0.9192 - val_loss: 0.4467 - val_acc: 0.8475

Epoch 00014: val_loss did not improve from 0.34450
Epoch 15/25
 - 51s - loss: 0.3068 - acc: 0.9251 - val_loss: 0.3374 - val_acc: 0.9111

Epoch 00015: val_loss improved from 0.34450 to 0.33745, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 51s - loss: 0.3002 - acc: 0.9287 - val_loss: 0.3544 - val_acc: 0.9074

Epoch 00016: val_loss did not improve from 0.33745
Epoch 17/25
 - 51s - loss: 0.2928 - acc: 0.9313 - val_loss: 0.3598 - val_acc: 0.8948

Epoch 00017: val_loss did not improve from 0.33745
Epoch 18/25
 - 51s - loss: 0.2845 - acc: 0.9356 - val_loss: 0.3855 - val_acc: 0.8857

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00018: val_loss did not improve from 0.33745
Epoch 19/25
 - 51s - loss: 0.2544 - acc: 0.9502 - val_loss: 0.2922 - val_acc: 0.9297

Epoch 00019: val_loss improved from 0.33745 to 0.29218, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 51s - loss: 0.2475 - acc: 0.9532 - val_loss: 0.3069 - val_acc: 0.9226

Epoch 00020: val_loss did not improve from 0.29218
Epoch 21/25
 - 51s - loss: 0.2454 - acc: 0.9543 - val_loss: 0.2905 - val_acc: 0.9302

Epoch 00021: val_loss improved from 0.29218 to 0.29050, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 51s - loss: 0.2403 - acc: 0.9565 - val_loss: 0.2864 - val_acc: 0.9320

Epoch 00022: val_loss improved from 0.29050 to 0.28636, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 51s - loss: 0.2371 - acc: 0.9581 - val_loss: 0.3160 - val_acc: 0.9163

Epoch 00023: val_loss did not improve from 0.28636
Epoch 24/25
 - 51s - loss: 0.2347 - acc: 0.9589 - val_loss: 0.2914 - val_acc: 0.9300

Epoch 00024: val_loss did not improve from 0.28636
Epoch 25/25
 - 51s - loss: 0.2320 - acc: 0.9597 - val_loss: 0.3595 - val_acc: 0.9089

Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.

Epoch 00025: val_loss did not improve from 0.28636

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 4s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1024/7968 [==>...........................] - ETA: 3s
1152/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1408/7968 [====>.........................] - ETA: 3s
1504/7968 [====>.........................] - ETA: 3s
1600/7968 [=====>........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1792/7968 [=====>........................] - ETA: 3s
1920/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2176/7968 [=======>......................] - ETA: 2s
2304/7968 [=======>......................] - ETA: 2s
2400/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2656/7968 [=========>....................] - ETA: 2s
2784/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3040/7968 [==========>...................] - ETA: 2s
3168/7968 [==========>...................] - ETA: 2s
3264/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4480/7968 [===============>..............] - ETA: 1s
4608/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4960/7968 [=================>............] - ETA: 1s
5088/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5344/7968 [===================>..........] - ETA: 1s
5472/7968 [===================>..........] - ETA: 1s
5568/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6400/7968 [=======================>......] - ETA: 0s
6528/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6880/7968 [========================>.....] - ETA: 0s
6976/7968 [=========================>....] - ETA: 0s
7104/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7360/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 516us/step
current val accuracy:0.909
current auc_score ------------------> 0.979

  32/7968 [..............................] - ETA: 10:25
 128/7968 [..............................] - ETA: 2:37 
 224/7968 [..............................] - ETA: 1:30
 320/7968 [>.............................] - ETA: 1:04
 416/7968 [>.............................] - ETA: 49s 
 512/7968 [>.............................] - ETA: 40s
 608/7968 [=>............................] - ETA: 34s
 704/7968 [=>............................] - ETA: 29s
 832/7968 [==>...........................] - ETA: 25s
 928/7968 [==>...........................] - ETA: 22s
1024/7968 [==>...........................] - ETA: 20s
1120/7968 [===>..........................] - ETA: 18s
1216/7968 [===>..........................] - ETA: 17s
1312/7968 [===>..........................] - ETA: 16s
1408/7968 [====>.........................] - ETA: 15s
1504/7968 [====>.........................] - ETA: 14s
1600/7968 [=====>........................] - ETA: 13s
1696/7968 [=====>........................] - ETA: 12s
1792/7968 [=====>........................] - ETA: 11s
1888/7968 [======>.......................] - ETA: 11s
1984/7968 [======>.......................] - ETA: 10s
2080/7968 [======>.......................] - ETA: 10s
2176/7968 [=======>......................] - ETA: 9s 
2272/7968 [=======>......................] - ETA: 9s
2368/7968 [=======>......................] - ETA: 8s
2464/7968 [========>.....................] - ETA: 8s
2560/7968 [========>.....................] - ETA: 8s
2656/7968 [=========>....................] - ETA: 7s
2752/7968 [=========>....................] - ETA: 7s
2848/7968 [=========>....................] - ETA: 7s
2944/7968 [==========>...................] - ETA: 6s
3040/7968 [==========>...................] - ETA: 6s
3136/7968 [==========>...................] - ETA: 6s
3232/7968 [===========>..................] - ETA: 6s
3328/7968 [===========>..................] - ETA: 5s
3424/7968 [===========>..................] - ETA: 5s
3520/7968 [============>.................] - ETA: 5s
3616/7968 [============>.................] - ETA: 5s
3712/7968 [============>.................] - ETA: 5s
3840/7968 [=============>................] - ETA: 4s
3936/7968 [=============>................] - ETA: 4s
4032/7968 [==============>...............] - ETA: 4s
4128/7968 [==============>...............] - ETA: 4s
4224/7968 [==============>...............] - ETA: 4s
4320/7968 [===============>..............] - ETA: 4s
4416/7968 [===============>..............] - ETA: 3s
4512/7968 [===============>..............] - ETA: 3s
4608/7968 [================>.............] - ETA: 3s
4704/7968 [================>.............] - ETA: 3s
4800/7968 [=================>............] - ETA: 3s
4896/7968 [=================>............] - ETA: 3s
4992/7968 [=================>............] - ETA: 3s
5088/7968 [==================>...........] - ETA: 2s
5184/7968 [==================>...........] - ETA: 2s
5280/7968 [==================>...........] - ETA: 2s
5376/7968 [===================>..........] - ETA: 2s
5504/7968 [===================>..........] - ETA: 2s
5600/7968 [====================>.........] - ETA: 2s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 7s 843us/step
Best saved model val accuracy:0.932
best saved model auc_score ------------------> 0.983
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_148[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_149[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_150[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_151[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 52, 96, 96)   0           concatenate_64[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_152[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_153[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 70, 96, 96)   0           concatenate_65[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_154[0][0]             
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_155[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_156[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_15[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_157[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_158[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 71, 48, 48)   0           concatenate_67[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_159[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 89, 48, 48)   0           concatenate_68[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_161[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_162[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_163[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_16[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_164[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 80, 24, 24)   0           concatenate_70[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_166[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 98, 24, 24)   0           concatenate_71[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 98)           0           activation_168[0][0]             
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            99          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 63s - loss: 0.6917 - acc: 0.7307 - val_loss: 0.6384 - val_acc: 0.7580

Epoch 00001: val_loss improved from inf to 0.63841, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.6130 - acc: 0.7779 - val_loss: 0.6114 - val_acc: 0.7819

Epoch 00002: val_loss improved from 0.63841 to 0.61143, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 54s - loss: 0.5835 - acc: 0.7915 - val_loss: 0.5637 - val_acc: 0.7994

Epoch 00003: val_loss improved from 0.61143 to 0.56371, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 54s - loss: 0.5627 - acc: 0.8034 - val_loss: 0.5469 - val_acc: 0.8067

Epoch 00004: val_loss improved from 0.56371 to 0.54689, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 54s - loss: 0.5448 - acc: 0.8105 - val_loss: 0.5225 - val_acc: 0.8194

Epoch 00005: val_loss improved from 0.54689 to 0.52247, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 54s - loss: 0.5308 - acc: 0.8183 - val_loss: 0.5364 - val_acc: 0.8125

Epoch 00006: val_loss did not improve from 0.52247
Epoch 7/25
 - 54s - loss: 0.5164 - acc: 0.8255 - val_loss: 0.5013 - val_acc: 0.8341

Epoch 00007: val_loss improved from 0.52247 to 0.50127, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 54s - loss: 0.5064 - acc: 0.8311 - val_loss: 0.5430 - val_acc: 0.8144

Epoch 00008: val_loss did not improve from 0.50127
Epoch 9/25
 - 54s - loss: 0.4932 - acc: 0.8392 - val_loss: 0.4956 - val_acc: 0.8444

Epoch 00009: val_loss improved from 0.50127 to 0.49561, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 54s - loss: 0.4858 - acc: 0.8424 - val_loss: 0.4656 - val_acc: 0.8567

Epoch 00010: val_loss improved from 0.49561 to 0.46556, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 54s - loss: 0.4763 - acc: 0.8486 - val_loss: 0.4724 - val_acc: 0.8554

Epoch 00011: val_loss did not improve from 0.46556
Epoch 12/25
 - 54s - loss: 0.4689 - acc: 0.8515 - val_loss: 0.4589 - val_acc: 0.8635

Epoch 00012: val_loss improved from 0.46556 to 0.45888, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 54s - loss: 0.4594 - acc: 0.8580 - val_loss: 0.4782 - val_acc: 0.8529

Epoch 00013: val_loss did not improve from 0.45888
Epoch 14/25
 - 54s - loss: 0.4524 - acc: 0.8614 - val_loss: 0.4516 - val_acc: 0.8646

Epoch 00014: val_loss improved from 0.45888 to 0.45162, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.4429 - acc: 0.8674 - val_loss: 0.4506 - val_acc: 0.8717

Epoch 00015: val_loss improved from 0.45162 to 0.45061, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 54s - loss: 0.4378 - acc: 0.8713 - val_loss: 0.4377 - val_acc: 0.8712

Epoch 00016: val_loss improved from 0.45061 to 0.43774, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 54s - loss: 0.4323 - acc: 0.8718 - val_loss: 0.4349 - val_acc: 0.8764

Epoch 00017: val_loss improved from 0.43774 to 0.43486, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 54s - loss: 0.4259 - acc: 0.8769 - val_loss: 0.4304 - val_acc: 0.8770

Epoch 00018: val_loss improved from 0.43486 to 0.43036, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 54s - loss: 0.4164 - acc: 0.8817 - val_loss: 0.4148 - val_acc: 0.8838

Epoch 00019: val_loss improved from 0.43036 to 0.41476, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 54s - loss: 0.4112 - acc: 0.8833 - val_loss: 0.4226 - val_acc: 0.8806

Epoch 00020: val_loss did not improve from 0.41476
Epoch 21/25
 - 54s - loss: 0.4033 - acc: 0.8880 - val_loss: 0.4189 - val_acc: 0.8859

Epoch 00021: val_loss did not improve from 0.41476
Epoch 22/25
 - 54s - loss: 0.4002 - acc: 0.8890 - val_loss: 0.4697 - val_acc: 0.8558

Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0100000006396062.

Epoch 00022: val_loss did not improve from 0.41476
Epoch 23/25
 - 54s - loss: 0.3828 - acc: 0.9000 - val_loss: 0.3981 - val_acc: 0.8978

Epoch 00023: val_loss improved from 0.41476 to 0.39808, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 54s - loss: 0.3813 - acc: 0.8989 - val_loss: 0.3975 - val_acc: 0.8928

Epoch 00024: val_loss improved from 0.39808 to 0.39746, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 54s - loss: 0.3788 - acc: 0.9005 - val_loss: 0.3986 - val_acc: 0.8864

Epoch 00025: val_loss did not improve from 0.39746

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 4s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1600/7968 [=====>........................] - ETA: 3s
1728/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1920/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2176/7968 [=======>......................] - ETA: 3s
2272/7968 [=======>......................] - ETA: 2s
2368/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2560/7968 [========>.....................] - ETA: 2s
2688/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3136/7968 [==========>...................] - ETA: 2s
3264/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3712/7968 [============>.................] - ETA: 2s
3840/7968 [=============>................] - ETA: 2s
3936/7968 [=============>................] - ETA: 2s
4032/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 1s
4288/7968 [===============>..............] - ETA: 1s
4416/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4608/7968 [================>.............] - ETA: 1s
4704/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5056/7968 [==================>...........] - ETA: 1s
5184/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5728/7968 [====================>.........] - ETA: 1s
5856/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6112/7968 [======================>.......] - ETA: 0s
6240/7968 [======================>.......] - ETA: 0s
6336/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6528/7968 [=======================>......] - ETA: 0s
6624/7968 [=======================>......] - ETA: 0s
6720/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 520us/step
current val accuracy:0.886
current auc_score ------------------> 0.961

  32/7968 [..............................] - ETA: 12:49
 128/7968 [..............................] - ETA: 3:13 
 224/7968 [..............................] - ETA: 1:50
 320/7968 [>.............................] - ETA: 1:17
 416/7968 [>.............................] - ETA: 1:00
 512/7968 [>.............................] - ETA: 48s 
 608/7968 [=>............................] - ETA: 41s
 704/7968 [=>............................] - ETA: 35s
 800/7968 [==>...........................] - ETA: 31s
 896/7968 [==>...........................] - ETA: 28s
 992/7968 [==>...........................] - ETA: 25s
1088/7968 [===>..........................] - ETA: 23s
1184/7968 [===>..........................] - ETA: 21s
1280/7968 [===>..........................] - ETA: 19s
1376/7968 [====>.........................] - ETA: 18s
1472/7968 [====>.........................] - ETA: 17s
1568/7968 [====>.........................] - ETA: 16s
1664/7968 [=====>........................] - ETA: 15s
1760/7968 [=====>........................] - ETA: 14s
1856/7968 [=====>........................] - ETA: 13s
1952/7968 [======>.......................] - ETA: 12s
2048/7968 [======>.......................] - ETA: 12s
2144/7968 [=======>......................] - ETA: 11s
2240/7968 [=======>......................] - ETA: 10s
2336/7968 [=======>......................] - ETA: 10s
2464/7968 [========>.....................] - ETA: 9s 
2560/7968 [========>.....................] - ETA: 9s
2656/7968 [=========>....................] - ETA: 8s
2752/7968 [=========>....................] - ETA: 8s
2848/7968 [=========>....................] - ETA: 8s
2944/7968 [==========>...................] - ETA: 7s
3040/7968 [==========>...................] - ETA: 7s
3136/7968 [==========>...................] - ETA: 7s
3232/7968 [===========>..................] - ETA: 7s
3328/7968 [===========>..................] - ETA: 6s
3424/7968 [===========>..................] - ETA: 6s
3520/7968 [============>.................] - ETA: 6s
3616/7968 [============>.................] - ETA: 6s
3712/7968 [============>.................] - ETA: 5s
3808/7968 [=============>................] - ETA: 5s
3936/7968 [=============>................] - ETA: 5s
4032/7968 [==============>...............] - ETA: 5s
4128/7968 [==============>...............] - ETA: 4s
4224/7968 [==============>...............] - ETA: 4s
4320/7968 [===============>..............] - ETA: 4s
4416/7968 [===============>..............] - ETA: 4s
4544/7968 [================>.............] - ETA: 4s
4640/7968 [================>.............] - ETA: 3s
4736/7968 [================>.............] - ETA: 3s
4832/7968 [=================>............] - ETA: 3s
4928/7968 [=================>............] - ETA: 3s
5024/7968 [=================>............] - ETA: 3s
5120/7968 [==================>...........] - ETA: 3s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 2s
5408/7968 [===================>..........] - ETA: 2s
5536/7968 [===================>..........] - ETA: 2s
5632/7968 [====================>.........] - ETA: 2s
5728/7968 [====================>.........] - ETA: 2s
5824/7968 [====================>.........] - ETA: 2s
5920/7968 [=====================>........] - ETA: 2s
6016/7968 [=====================>........] - ETA: 2s
6112/7968 [======================>.......] - ETA: 1s
6208/7968 [======================>.......] - ETA: 1s
6304/7968 [======================>.......] - ETA: 1s
6400/7968 [=======================>......] - ETA: 1s
6496/7968 [=======================>......] - ETA: 1s
6592/7968 [=======================>......] - ETA: 1s
6688/7968 [========================>.....] - ETA: 1s
6784/7968 [========================>.....] - ETA: 1s
6880/7968 [========================>.....] - ETA: 1s
6976/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7264/7968 [==========================>...] - ETA: 0s
7360/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7648/7968 [===========================>..] - ETA: 0s
7744/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 7s 918us/step
Best saved model val accuracy:0.893
best saved model auc_score ------------------> 0.960
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_169[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_170[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 52, 96, 96)   0           concatenate_73[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_173[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 70, 96, 96)   0           concatenate_74[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_175[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_176[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_177[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_17[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_178[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_179[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 71, 48, 48)   0           concatenate_76[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_180[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_181[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 89, 48, 48)   0           concatenate_77[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_182[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_183[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_184[0][0]             
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_18[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_79[0][0]             
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_185[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_186[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 80, 24, 24)   0           concatenate_79[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_80[0][0]             
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_187[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_188[0][0]             
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 98, 24, 24)   0           concatenate_80[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_81[0][0]             
__________________________________________________________________________________________________
activation_189 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 98)           0           activation_189[0][0]             
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            99          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 65s - loss: 0.5895 - acc: 0.7862 - val_loss: 0.5531 - val_acc: 0.8035

Epoch 00001: val_loss improved from inf to 0.55308, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.5215 - acc: 0.8240 - val_loss: 0.5205 - val_acc: 0.8218

Epoch 00002: val_loss improved from 0.55308 to 0.52053, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 55s - loss: 0.4910 - acc: 0.8405 - val_loss: 0.4667 - val_acc: 0.8517

Epoch 00003: val_loss improved from 0.52053 to 0.46673, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 54s - loss: 0.4664 - acc: 0.8519 - val_loss: 0.4866 - val_acc: 0.8379

Epoch 00004: val_loss did not improve from 0.46673
Epoch 5/25
 - 54s - loss: 0.4452 - acc: 0.8638 - val_loss: 0.4402 - val_acc: 0.8687

Epoch 00005: val_loss improved from 0.46673 to 0.44016, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 54s - loss: 0.4269 - acc: 0.8727 - val_loss: 0.4104 - val_acc: 0.8795

Epoch 00006: val_loss improved from 0.44016 to 0.41035, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 54s - loss: 0.4109 - acc: 0.8790 - val_loss: 0.3984 - val_acc: 0.8829

Epoch 00007: val_loss improved from 0.41035 to 0.39839, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 54s - loss: 0.3955 - acc: 0.8874 - val_loss: 0.3990 - val_acc: 0.8840

Epoch 00008: val_loss did not improve from 0.39839
Epoch 9/25
 - 54s - loss: 0.3800 - acc: 0.8940 - val_loss: 0.4070 - val_acc: 0.8761

Epoch 00009: val_loss did not improve from 0.39839
Epoch 10/25
 - 54s - loss: 0.3665 - acc: 0.9007 - val_loss: 0.3662 - val_acc: 0.8997

Epoch 00010: val_loss improved from 0.39839 to 0.36616, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 54s - loss: 0.3566 - acc: 0.9075 - val_loss: 0.3869 - val_acc: 0.8921

Epoch 00011: val_loss did not improve from 0.36616
Epoch 12/25
 - 54s - loss: 0.3482 - acc: 0.9110 - val_loss: 0.3621 - val_acc: 0.9032

Epoch 00012: val_loss improved from 0.36616 to 0.36206, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 54s - loss: 0.3358 - acc: 0.9162 - val_loss: 0.3524 - val_acc: 0.9074

Epoch 00013: val_loss improved from 0.36206 to 0.35236, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 54s - loss: 0.3271 - acc: 0.9177 - val_loss: 0.3467 - val_acc: 0.9081

Epoch 00014: val_loss improved from 0.35236 to 0.34667, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.3137 - acc: 0.9247 - val_loss: 0.3592 - val_acc: 0.9031

Epoch 00015: val_loss did not improve from 0.34667
Epoch 16/25
 - 54s - loss: 0.3039 - acc: 0.9281 - val_loss: 0.3269 - val_acc: 0.9165

Epoch 00016: val_loss improved from 0.34667 to 0.32694, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 54s - loss: 0.2987 - acc: 0.9308 - val_loss: 0.3616 - val_acc: 0.9004

Epoch 00017: val_loss did not improve from 0.32694
Epoch 18/25
 - 54s - loss: 0.2920 - acc: 0.9334 - val_loss: 0.3193 - val_acc: 0.9175

Epoch 00018: val_loss improved from 0.32694 to 0.31927, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 54s - loss: 0.2820 - acc: 0.9386 - val_loss: 0.3442 - val_acc: 0.9103

Epoch 00019: val_loss did not improve from 0.31927
Epoch 20/25
 - 54s - loss: 0.2795 - acc: 0.9393 - val_loss: 0.3255 - val_acc: 0.9152

Epoch 00020: val_loss did not improve from 0.31927
Epoch 21/25
 - 54s - loss: 0.2692 - acc: 0.9436 - val_loss: 0.3443 - val_acc: 0.9104

Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.0000000819040434e-05.

Epoch 00021: val_loss did not improve from 0.31927
Epoch 22/25
 - 54s - loss: 0.2485 - acc: 0.9553 - val_loss: 0.2935 - val_acc: 0.9295

Epoch 00022: val_loss improved from 0.31927 to 0.29351, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 54s - loss: 0.2444 - acc: 0.9565 - val_loss: 0.3027 - val_acc: 0.9238

Epoch 00023: val_loss did not improve from 0.29351
Epoch 24/25
 - 54s - loss: 0.2407 - acc: 0.9577 - val_loss: 0.2933 - val_acc: 0.9310

Epoch 00024: val_loss improved from 0.29351 to 0.29325, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 54s - loss: 0.2391 - acc: 0.9580 - val_loss: 0.2957 - val_acc: 0.9280

Epoch 00025: val_loss did not improve from 0.29325

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 3s
 512/7968 [>.............................] - ETA: 3s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1024/7968 [==>...........................] - ETA: 3s
1120/7968 [===>..........................] - ETA: 3s
1216/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1408/7968 [====>.........................] - ETA: 3s
1504/7968 [====>.........................] - ETA: 3s
1600/7968 [=====>........................] - ETA: 3s
1728/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1920/7968 [======>.......................] - ETA: 3s
2016/7968 [======>.......................] - ETA: 3s
2112/7968 [======>.......................] - ETA: 3s
2208/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 2s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3328/7968 [===========>..................] - ETA: 2s
3424/7968 [===========>..................] - ETA: 2s
3520/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3712/7968 [============>.................] - ETA: 2s
3808/7968 [=============>................] - ETA: 2s
3904/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4096/7968 [==============>...............] - ETA: 2s
4192/7968 [==============>...............] - ETA: 1s
4288/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4480/7968 [===============>..............] - ETA: 1s
4576/7968 [================>.............] - ETA: 1s
4672/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4864/7968 [=================>............] - ETA: 1s
4992/7968 [=================>............] - ETA: 1s
5088/7968 [==================>...........] - ETA: 1s
5184/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5376/7968 [===================>..........] - ETA: 1s
5472/7968 [===================>..........] - ETA: 1s
5568/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5856/7968 [=====================>........] - ETA: 1s
5952/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 0s
6240/7968 [======================>.......] - ETA: 0s
6336/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6528/7968 [=======================>......] - ETA: 0s
6624/7968 [=======================>......] - ETA: 0s
6720/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7008/7968 [=========================>....] - ETA: 0s
7104/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7392/7968 [==========================>...] - ETA: 0s
7488/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7776/7968 [============================>.] - ETA: 0s
7872/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 528us/step
current val accuracy:0.928
current auc_score ------------------> 0.981

  32/7968 [..............................] - ETA: 15:08
 128/7968 [..............................] - ETA: 3:47 
 224/7968 [..............................] - ETA: 2:10
 320/7968 [>.............................] - ETA: 1:31
 416/7968 [>.............................] - ETA: 1:10
 512/7968 [>.............................] - ETA: 57s 
 608/7968 [=>............................] - ETA: 48s
 704/7968 [=>............................] - ETA: 41s
 800/7968 [==>...........................] - ETA: 36s
 896/7968 [==>...........................] - ETA: 32s
 992/7968 [==>...........................] - ETA: 29s
1088/7968 [===>..........................] - ETA: 26s
1184/7968 [===>..........................] - ETA: 24s
1280/7968 [===>..........................] - ETA: 22s
1376/7968 [====>.........................] - ETA: 21s
1472/7968 [====>.........................] - ETA: 19s
1568/7968 [====>.........................] - ETA: 18s
1664/7968 [=====>........................] - ETA: 17s
1760/7968 [=====>........................] - ETA: 16s
1856/7968 [=====>........................] - ETA: 15s
1952/7968 [======>.......................] - ETA: 14s
2048/7968 [======>.......................] - ETA: 13s
2144/7968 [=======>......................] - ETA: 13s
2240/7968 [=======>......................] - ETA: 12s
2336/7968 [=======>......................] - ETA: 11s
2432/7968 [========>.....................] - ETA: 11s
2528/7968 [========>.....................] - ETA: 10s
2624/7968 [========>.....................] - ETA: 10s
2720/7968 [=========>....................] - ETA: 9s 
2816/7968 [=========>....................] - ETA: 9s
2912/7968 [=========>....................] - ETA: 9s
3008/7968 [==========>...................] - ETA: 8s
3104/7968 [==========>...................] - ETA: 8s
3200/7968 [===========>..................] - ETA: 7s
3296/7968 [===========>..................] - ETA: 7s
3392/7968 [===========>..................] - ETA: 7s
3488/7968 [============>.................] - ETA: 7s
3584/7968 [============>.................] - ETA: 6s
3680/7968 [============>.................] - ETA: 6s
3776/7968 [=============>................] - ETA: 6s
3872/7968 [=============>................] - ETA: 6s
3968/7968 [=============>................] - ETA: 5s
4064/7968 [==============>...............] - ETA: 5s
4160/7968 [==============>...............] - ETA: 5s
4256/7968 [===============>..............] - ETA: 5s
4352/7968 [===============>..............] - ETA: 4s
4448/7968 [===============>..............] - ETA: 4s
4544/7968 [================>.............] - ETA: 4s
4640/7968 [================>.............] - ETA: 4s
4736/7968 [================>.............] - ETA: 4s
4832/7968 [=================>............] - ETA: 4s
4928/7968 [=================>............] - ETA: 3s
5024/7968 [=================>............] - ETA: 3s
5120/7968 [==================>...........] - ETA: 3s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 3s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 2s
5600/7968 [====================>.........] - ETA: 2s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 8s 988us/step
Best saved model val accuracy:0.931
best saved model auc_score ------------------> 0.981
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_190 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_190[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_191 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_191[0][0]             
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_82[0][0]             
__________________________________________________________________________________________________
activation_192 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_192[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_193[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 52, 96, 96)   0           concatenate_82[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_83[0][0]             
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_194[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_195[0][0]             
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 70, 96, 96)   0           concatenate_83[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_84[0][0]             
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_196[0][0]             
__________________________________________________________________________________________________
average_pooling2d_19 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_19[0][0]       
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_197[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_198[0][0]             
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_19[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_85[0][0]             
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_199[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_200[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 71, 48, 48)   0           concatenate_85[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_86[0][0]             
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_201[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_202 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_202[0][0]             
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 89, 48, 48)   0           concatenate_86[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_87[0][0]             
__________________________________________________________________________________________________
activation_203 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_203[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_204 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_204[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_205 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_205[0][0]             
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_20[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_88[0][0]             
__________________________________________________________________________________________________
activation_206 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_206[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_207 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_207[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 80, 24, 24)   0           concatenate_88[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_89[0][0]             
__________________________________________________________________________________________________
activation_208 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_208[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_209 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_209[0][0]             
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 98, 24, 24)   0           concatenate_89[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_90[0][0]             
__________________________________________________________________________________________________
activation_210 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 98)           0           activation_210[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            99          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 65s - loss: 0.5440 - acc: 0.7557 - val_loss: 0.8584 - val_acc: 0.6687

Epoch 00001: val_loss improved from inf to 0.85839, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.4719 - acc: 0.7773 - val_loss: 0.6338 - val_acc: 0.6455

Epoch 00002: val_loss improved from 0.85839 to 0.63382, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 54s - loss: 0.4443 - acc: 0.7946 - val_loss: 0.5440 - val_acc: 0.7290

Epoch 00003: val_loss improved from 0.63382 to 0.54401, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 54s - loss: 0.4244 - acc: 0.8053 - val_loss: 1.6712 - val_acc: 0.5198

Epoch 00004: val_loss did not improve from 0.54401
Epoch 5/25
 - 54s - loss: 0.4079 - acc: 0.8150 - val_loss: 0.5845 - val_acc: 0.7401

Epoch 00005: val_loss did not improve from 0.54401
Epoch 6/25
 - 54s - loss: 0.3963 - acc: 0.8252 - val_loss: 0.5219 - val_acc: 0.7474

Epoch 00006: val_loss improved from 0.54401 to 0.52193, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 54s - loss: 0.3838 - acc: 0.8329 - val_loss: 0.4785 - val_acc: 0.7944

Epoch 00007: val_loss improved from 0.52193 to 0.47846, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 54s - loss: 0.3710 - acc: 0.8428 - val_loss: 0.5203 - val_acc: 0.7686

Epoch 00008: val_loss did not improve from 0.47846
Epoch 9/25
 - 54s - loss: 0.3591 - acc: 0.8488 - val_loss: 0.7446 - val_acc: 0.6291

Epoch 00009: val_loss did not improve from 0.47846
Epoch 10/25
 - 54s - loss: 0.3508 - acc: 0.8524 - val_loss: 0.4272 - val_acc: 0.8251

Epoch 00010: val_loss improved from 0.47846 to 0.42721, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 54s - loss: 0.3391 - acc: 0.8592 - val_loss: 0.6600 - val_acc: 0.7203

Epoch 00011: val_loss did not improve from 0.42721
Epoch 12/25
 - 54s - loss: 0.3348 - acc: 0.8636 - val_loss: 0.5599 - val_acc: 0.7572

Epoch 00012: val_loss did not improve from 0.42721
Epoch 13/25
 - 54s - loss: 0.3245 - acc: 0.8679 - val_loss: 0.7921 - val_acc: 0.6638

Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0031622775894859655.

Epoch 00013: val_loss did not improve from 0.42721
Epoch 14/25
 - 54s - loss: 0.2825 - acc: 0.8886 - val_loss: 0.3696 - val_acc: 0.8444

Epoch 00014: val_loss improved from 0.42721 to 0.36958, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.2707 - acc: 0.8934 - val_loss: 0.3300 - val_acc: 0.8643

Epoch 00015: val_loss improved from 0.36958 to 0.32995, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 54s - loss: 0.2677 - acc: 0.8964 - val_loss: 0.2892 - val_acc: 0.8815

Epoch 00016: val_loss improved from 0.32995 to 0.28917, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 54s - loss: 0.2566 - acc: 0.8994 - val_loss: 0.2727 - val_acc: 0.8941

Epoch 00017: val_loss improved from 0.28917 to 0.27273, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 54s - loss: 0.2528 - acc: 0.9030 - val_loss: 0.3162 - val_acc: 0.8744

Epoch 00018: val_loss did not improve from 0.27273
Epoch 19/25
 - 54s - loss: 0.2471 - acc: 0.9038 - val_loss: 0.3225 - val_acc: 0.8685

Epoch 00019: val_loss did not improve from 0.27273
Epoch 20/25
 - 54s - loss: 0.2421 - acc: 0.9057 - val_loss: 0.3670 - val_acc: 0.8493

Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0009999999903331056.

Epoch 00020: val_loss did not improve from 0.27273
Epoch 21/25
 - 54s - loss: 0.2212 - acc: 0.9168 - val_loss: 0.2319 - val_acc: 0.9118

Epoch 00021: val_loss improved from 0.27273 to 0.23191, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 54s - loss: 0.2181 - acc: 0.9180 - val_loss: 0.2435 - val_acc: 0.9094

Epoch 00022: val_loss did not improve from 0.23191
Epoch 23/25
 - 54s - loss: 0.2148 - acc: 0.9187 - val_loss: 0.2209 - val_acc: 0.9155

Epoch 00023: val_loss improved from 0.23191 to 0.22093, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 54s - loss: 0.2151 - acc: 0.9173 - val_loss: 0.2560 - val_acc: 0.9016

Epoch 00024: val_loss did not improve from 0.22093
Epoch 25/25
 - 54s - loss: 0.2086 - acc: 0.9215 - val_loss: 0.2167 - val_acc: 0.9178

Epoch 00025: val_loss improved from 0.22093 to 0.21666, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 536us/step
current val accuracy:0.918
current auc_score ------------------> 0.975

  32/7968 [..............................] - ETA: 17:30
 128/7968 [..............................] - ETA: 4:23 
 224/7968 [..............................] - ETA: 2:30
 320/7968 [>.............................] - ETA: 1:45
 416/7968 [>.............................] - ETA: 1:20
 512/7968 [>.............................] - ETA: 1:05
 608/7968 [=>............................] - ETA: 55s 
 704/7968 [=>............................] - ETA: 47s
 800/7968 [==>...........................] - ETA: 41s
 896/7968 [==>...........................] - ETA: 37s
 992/7968 [==>...........................] - ETA: 33s
1088/7968 [===>..........................] - ETA: 30s
1184/7968 [===>..........................] - ETA: 27s
1280/7968 [===>..........................] - ETA: 25s
1376/7968 [====>.........................] - ETA: 23s
1472/7968 [====>.........................] - ETA: 22s
1568/7968 [====>.........................] - ETA: 20s
1664/7968 [=====>........................] - ETA: 19s
1760/7968 [=====>........................] - ETA: 18s
1856/7968 [=====>........................] - ETA: 17s
1952/7968 [======>.......................] - ETA: 16s
2048/7968 [======>.......................] - ETA: 15s
2144/7968 [=======>......................] - ETA: 14s
2240/7968 [=======>......................] - ETA: 13s
2336/7968 [=======>......................] - ETA: 13s
2432/7968 [========>.....................] - ETA: 12s
2528/7968 [========>.....................] - ETA: 12s
2624/7968 [========>.....................] - ETA: 11s
2720/7968 [=========>....................] - ETA: 11s
2816/7968 [=========>....................] - ETA: 10s
2912/7968 [=========>....................] - ETA: 10s
3008/7968 [==========>...................] - ETA: 9s 
3104/7968 [==========>...................] - ETA: 9s
3200/7968 [===========>..................] - ETA: 8s
3296/7968 [===========>..................] - ETA: 8s
3392/7968 [===========>..................] - ETA: 8s
3488/7968 [============>.................] - ETA: 7s
3584/7968 [============>.................] - ETA: 7s
3680/7968 [============>.................] - ETA: 7s
3776/7968 [=============>................] - ETA: 6s
3872/7968 [=============>................] - ETA: 6s
3968/7968 [=============>................] - ETA: 6s
4064/7968 [==============>...............] - ETA: 6s
4160/7968 [==============>...............] - ETA: 5s
4256/7968 [===============>..............] - ETA: 5s
4352/7968 [===============>..............] - ETA: 5s
4448/7968 [===============>..............] - ETA: 5s
4544/7968 [================>.............] - ETA: 5s
4640/7968 [================>.............] - ETA: 4s
4736/7968 [================>.............] - ETA: 4s
4832/7968 [=================>............] - ETA: 4s
4928/7968 [=================>............] - ETA: 4s
5024/7968 [=================>............] - ETA: 4s
5120/7968 [==================>...........] - ETA: 3s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 3s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 3s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 9s 1ms/step
Best saved model val accuracy:0.918
best saved model auc_score ------------------> 0.975
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_11[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_211 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_211[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_212 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_212[0][0]             
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_91[0][0]             
__________________________________________________________________________________________________
activation_213 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_213[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_214 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_214[0][0]             
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 52, 96, 96)   0           concatenate_91[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_92[0][0]             
__________________________________________________________________________________________________
activation_215 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_215[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_216 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_216[0][0]             
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 70, 96, 96)   0           concatenate_92[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_93[0][0]             
__________________________________________________________________________________________________
activation_217 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_217[0][0]             
__________________________________________________________________________________________________
average_pooling2d_21 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_21[0][0]       
__________________________________________________________________________________________________
activation_218 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_218[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_219 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_219[0][0]             
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_21[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_94[0][0]             
__________________________________________________________________________________________________
activation_220 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_220[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_221 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_221[0][0]             
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 71, 48, 48)   0           concatenate_94[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_95[0][0]             
__________________________________________________________________________________________________
activation_222 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_222[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_223 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_223[0][0]             
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 89, 48, 48)   0           concatenate_95[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_96[0][0]             
__________________________________________________________________________________________________
activation_224 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_224[0][0]             
__________________________________________________________________________________________________
average_pooling2d_22 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_22[0][0]       
__________________________________________________________________________________________________
activation_225 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_225[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_226 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_226[0][0]             
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_22[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_97[0][0]             
__________________________________________________________________________________________________
activation_227 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_227[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_228 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_228[0][0]             
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 80, 24, 24)   0           concatenate_97[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_98[0][0]             
__________________________________________________________________________________________________
activation_229 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_229[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_230 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_230[0][0]             
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 98, 24, 24)   0           concatenate_98[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_99[0][0]             
__________________________________________________________________________________________________
activation_231 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_11 (Gl (None, 98)           0           activation_231[0][0]             
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            99          global_average_pooling2d_11[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 66s - loss: 0.6788 - acc: 0.7438 - val_loss: 0.6253 - val_acc: 0.7615

Epoch 00001: val_loss improved from inf to 0.62528, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 53s - loss: 0.6020 - acc: 0.7795 - val_loss: 0.5736 - val_acc: 0.7907

Epoch 00002: val_loss improved from 0.62528 to 0.57360, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 53s - loss: 0.5701 - acc: 0.7946 - val_loss: 0.5515 - val_acc: 0.7994

Epoch 00003: val_loss improved from 0.57360 to 0.55155, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 53s - loss: 0.5507 - acc: 0.8086 - val_loss: 0.5481 - val_acc: 0.8036

Epoch 00004: val_loss improved from 0.55155 to 0.54809, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 53s - loss: 0.5346 - acc: 0.8166 - val_loss: 0.5551 - val_acc: 0.8040

Epoch 00005: val_loss did not improve from 0.54809
Epoch 6/25
 - 53s - loss: 0.5206 - acc: 0.8259 - val_loss: 0.5293 - val_acc: 0.8230

Epoch 00006: val_loss improved from 0.54809 to 0.52928, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 53s - loss: 0.5080 - acc: 0.8325 - val_loss: 0.5034 - val_acc: 0.8318

Epoch 00007: val_loss improved from 0.52928 to 0.50336, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 53s - loss: 0.4972 - acc: 0.8385 - val_loss: 0.5188 - val_acc: 0.8213

Epoch 00008: val_loss did not improve from 0.50336
Epoch 9/25
 - 53s - loss: 0.4856 - acc: 0.8446 - val_loss: 0.4845 - val_acc: 0.8426

Epoch 00009: val_loss improved from 0.50336 to 0.48453, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 53s - loss: 0.4784 - acc: 0.8479 - val_loss: 0.4748 - val_acc: 0.8493

Epoch 00010: val_loss improved from 0.48453 to 0.47484, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 53s - loss: 0.4677 - acc: 0.8549 - val_loss: 0.4736 - val_acc: 0.8488

Epoch 00011: val_loss improved from 0.47484 to 0.47358, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 53s - loss: 0.4573 - acc: 0.8602 - val_loss: 0.4810 - val_acc: 0.8483

Epoch 00012: val_loss did not improve from 0.47358
Epoch 13/25
 - 53s - loss: 0.4494 - acc: 0.8616 - val_loss: 0.4818 - val_acc: 0.8409

Epoch 00013: val_loss did not improve from 0.47358
Epoch 14/25
 - 53s - loss: 0.4442 - acc: 0.8652 - val_loss: 0.4489 - val_acc: 0.8643

Epoch 00014: val_loss improved from 0.47358 to 0.44892, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 53s - loss: 0.4318 - acc: 0.8716 - val_loss: 0.4481 - val_acc: 0.8657

Epoch 00015: val_loss improved from 0.44892 to 0.44814, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 53s - loss: 0.4269 - acc: 0.8744 - val_loss: 0.4347 - val_acc: 0.8683

Epoch 00016: val_loss improved from 0.44814 to 0.43471, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 53s - loss: 0.4201 - acc: 0.8803 - val_loss: 0.4234 - val_acc: 0.8760

Epoch 00017: val_loss improved from 0.43471 to 0.42341, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 53s - loss: 0.4145 - acc: 0.8831 - val_loss: 0.4597 - val_acc: 0.8660

Epoch 00018: val_loss did not improve from 0.42341
Epoch 19/25
 - 53s - loss: 0.4093 - acc: 0.8829 - val_loss: 0.4713 - val_acc: 0.8431

Epoch 00019: val_loss did not improve from 0.42341
Epoch 20/25
 - 53s - loss: 0.4036 - acc: 0.8863 - val_loss: 0.4851 - val_acc: 0.8577

Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.

Epoch 00020: val_loss did not improve from 0.42341
Epoch 21/25
 - 53s - loss: 0.3869 - acc: 0.8967 - val_loss: 0.4009 - val_acc: 0.8860

Epoch 00021: val_loss improved from 0.42341 to 0.40087, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 53s - loss: 0.3827 - acc: 0.8985 - val_loss: 0.3990 - val_acc: 0.8873

Epoch 00022: val_loss improved from 0.40087 to 0.39899, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 53s - loss: 0.3803 - acc: 0.8989 - val_loss: 0.4119 - val_acc: 0.8779

Epoch 00023: val_loss did not improve from 0.39899
Epoch 24/25
 - 53s - loss: 0.3778 - acc: 0.8997 - val_loss: 0.3981 - val_acc: 0.8847

Epoch 00024: val_loss improved from 0.39899 to 0.39811, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 53s - loss: 0.3762 - acc: 0.9019 - val_loss: 0.4046 - val_acc: 0.8844

Epoch 00025: val_loss did not improve from 0.39811

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 547us/step
current val accuracy:0.884
current auc_score ------------------> 0.957

  32/7968 [..............................] - ETA: 19:29
 128/7968 [..............................] - ETA: 4:52 
 224/7968 [..............................] - ETA: 2:46
 320/7968 [>.............................] - ETA: 1:56
 416/7968 [>.............................] - ETA: 1:29
 512/7968 [>.............................] - ETA: 1:12
 608/7968 [=>............................] - ETA: 1:00
 704/7968 [=>............................] - ETA: 52s 
 800/7968 [==>...........................] - ETA: 46s
 896/7968 [==>...........................] - ETA: 40s
 992/7968 [==>...........................] - ETA: 36s
1088/7968 [===>..........................] - ETA: 33s
1184/7968 [===>..........................] - ETA: 30s
1280/7968 [===>..........................] - ETA: 28s
1376/7968 [====>.........................] - ETA: 26s
1472/7968 [====>.........................] - ETA: 24s
1568/7968 [====>.........................] - ETA: 22s
1664/7968 [=====>........................] - ETA: 21s
1760/7968 [=====>........................] - ETA: 19s
1856/7968 [=====>........................] - ETA: 18s
1952/7968 [======>.......................] - ETA: 17s
2048/7968 [======>.......................] - ETA: 16s
2144/7968 [=======>......................] - ETA: 15s
2240/7968 [=======>......................] - ETA: 15s
2336/7968 [=======>......................] - ETA: 14s
2432/7968 [========>.....................] - ETA: 13s
2528/7968 [========>.....................] - ETA: 13s
2624/7968 [========>.....................] - ETA: 12s
2720/7968 [=========>....................] - ETA: 11s
2816/7968 [=========>....................] - ETA: 11s
2912/7968 [=========>....................] - ETA: 10s
3008/7968 [==========>...................] - ETA: 10s
3104/7968 [==========>...................] - ETA: 10s
3200/7968 [===========>..................] - ETA: 9s 
3296/7968 [===========>..................] - ETA: 9s
3392/7968 [===========>..................] - ETA: 8s
3488/7968 [============>.................] - ETA: 8s
3584/7968 [============>.................] - ETA: 8s
3680/7968 [============>.................] - ETA: 7s
3776/7968 [=============>................] - ETA: 7s
3872/7968 [=============>................] - ETA: 7s
3968/7968 [=============>................] - ETA: 6s
4064/7968 [==============>...............] - ETA: 6s
4160/7968 [==============>...............] - ETA: 6s
4256/7968 [===============>..............] - ETA: 6s
4352/7968 [===============>..............] - ETA: 5s
4448/7968 [===============>..............] - ETA: 5s
4544/7968 [================>.............] - ETA: 5s
4640/7968 [================>.............] - ETA: 5s
4736/7968 [================>.............] - ETA: 4s
4832/7968 [=================>............] - ETA: 4s
4928/7968 [=================>............] - ETA: 4s
5024/7968 [=================>............] - ETA: 4s
5120/7968 [==================>...........] - ETA: 4s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 3s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 3s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 9s 1ms/step
Best saved model val accuracy:0.885
best saved model auc_score ------------------> 0.959
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_12[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_232 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_232[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_233 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_233[0][0]             
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_100[0][0]            
__________________________________________________________________________________________________
activation_234 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_234[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_235 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_235[0][0]             
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 52, 96, 96)   0           concatenate_100[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_101[0][0]            
__________________________________________________________________________________________________
activation_236 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_236[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_237 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_237[0][0]             
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 70, 96, 96)   0           concatenate_101[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_102[0][0]            
__________________________________________________________________________________________________
activation_238 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_238[0][0]             
__________________________________________________________________________________________________
average_pooling2d_23 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_23[0][0]       
__________________________________________________________________________________________________
activation_239 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_239[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_240 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_240[0][0]             
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_23[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_103[0][0]            
__________________________________________________________________________________________________
activation_241 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_241[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_242 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_242[0][0]             
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 71, 48, 48)   0           concatenate_103[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_104[0][0]            
__________________________________________________________________________________________________
activation_243 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_243[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_244 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_244[0][0]             
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 89, 48, 48)   0           concatenate_104[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_105[0][0]            
__________________________________________________________________________________________________
activation_245 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_245[0][0]             
__________________________________________________________________________________________________
average_pooling2d_24 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_24[0][0]       
__________________________________________________________________________________________________
activation_246 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_246[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_247 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_247[0][0]             
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_24[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_106[0][0]            
__________________________________________________________________________________________________
activation_248 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_248[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_249 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_249[0][0]             
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 80, 24, 24)   0           concatenate_106[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_107[0][0]            
__________________________________________________________________________________________________
activation_250 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_250[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_251 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_251[0][0]             
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 98, 24, 24)   0           concatenate_107[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_108[0][0]            
__________________________________________________________________________________________________
activation_252 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_12 (Gl (None, 98)           0           activation_252[0][0]             
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 1)            99          global_average_pooling2d_12[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 66s - loss: 0.7367 - acc: 0.7198 - val_loss: 0.6815 - val_acc: 0.7482

Epoch 00001: val_loss improved from inf to 0.68151, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 53s - loss: 0.6588 - acc: 0.7600 - val_loss: 0.6337 - val_acc: 0.7653

Epoch 00002: val_loss improved from 0.68151 to 0.63368, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 53s - loss: 0.6247 - acc: 0.7735 - val_loss: 0.6068 - val_acc: 0.7787

Epoch 00003: val_loss improved from 0.63368 to 0.60677, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 53s - loss: 0.6078 - acc: 0.7819 - val_loss: 0.5941 - val_acc: 0.7839

Epoch 00004: val_loss improved from 0.60677 to 0.59412, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 53s - loss: 0.5950 - acc: 0.7866 - val_loss: 0.5892 - val_acc: 0.7863

Epoch 00005: val_loss improved from 0.59412 to 0.58916, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 53s - loss: 0.5842 - acc: 0.7945 - val_loss: 0.5711 - val_acc: 0.7988

Epoch 00006: val_loss improved from 0.58916 to 0.57113, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 53s - loss: 0.5744 - acc: 0.7999 - val_loss: 0.5634 - val_acc: 0.8011

Epoch 00007: val_loss improved from 0.57113 to 0.56343, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 53s - loss: 0.5675 - acc: 0.8026 - val_loss: 0.5552 - val_acc: 0.8077

Epoch 00008: val_loss improved from 0.56343 to 0.55517, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 53s - loss: 0.5590 - acc: 0.8064 - val_loss: 0.5482 - val_acc: 0.8055

Epoch 00009: val_loss improved from 0.55517 to 0.54821, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 53s - loss: 0.5541 - acc: 0.8101 - val_loss: 0.5514 - val_acc: 0.8046

Epoch 00010: val_loss did not improve from 0.54821
Epoch 11/25
 - 53s - loss: 0.5477 - acc: 0.8123 - val_loss: 0.5389 - val_acc: 0.8178

Epoch 00011: val_loss improved from 0.54821 to 0.53894, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 53s - loss: 0.5405 - acc: 0.8155 - val_loss: 0.5305 - val_acc: 0.8176

Epoch 00012: val_loss improved from 0.53894 to 0.53051, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 53s - loss: 0.5364 - acc: 0.8189 - val_loss: 0.5252 - val_acc: 0.8271

Epoch 00013: val_loss improved from 0.53051 to 0.52517, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 52s - loss: 0.5288 - acc: 0.8253 - val_loss: 0.5169 - val_acc: 0.8258

Epoch 00014: val_loss improved from 0.52517 to 0.51688, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 52s - loss: 0.5246 - acc: 0.8243 - val_loss: 0.5291 - val_acc: 0.8229

Epoch 00015: val_loss did not improve from 0.51688
Epoch 16/25
 - 52s - loss: 0.5181 - acc: 0.8303 - val_loss: 0.5271 - val_acc: 0.8186

Epoch 00016: val_loss did not improve from 0.51688
Epoch 17/25
 - 52s - loss: 0.5145 - acc: 0.8327 - val_loss: 0.5042 - val_acc: 0.8396

Epoch 00017: val_loss improved from 0.51688 to 0.50425, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 52s - loss: 0.5082 - acc: 0.8358 - val_loss: 0.4997 - val_acc: 0.8382

Epoch 00018: val_loss improved from 0.50425 to 0.49967, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 52s - loss: 0.5053 - acc: 0.8356 - val_loss: 0.4950 - val_acc: 0.8410

Epoch 00019: val_loss improved from 0.49967 to 0.49500, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 52s - loss: 0.5002 - acc: 0.8399 - val_loss: 0.4890 - val_acc: 0.8439

Epoch 00020: val_loss improved from 0.49500 to 0.48900, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 52s - loss: 0.4972 - acc: 0.8391 - val_loss: 0.4964 - val_acc: 0.8446

Epoch 00021: val_loss did not improve from 0.48900
Epoch 22/25
 - 52s - loss: 0.4937 - acc: 0.8415 - val_loss: 0.4877 - val_acc: 0.8522

Epoch 00022: val_loss improved from 0.48900 to 0.48775, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 52s - loss: 0.4914 - acc: 0.8430 - val_loss: 0.4823 - val_acc: 0.8478

Epoch 00023: val_loss improved from 0.48775 to 0.48233, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 53s - loss: 0.4852 - acc: 0.8477 - val_loss: 0.4806 - val_acc: 0.8470

Epoch 00024: val_loss improved from 0.48233 to 0.48057, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 52s - loss: 0.4812 - acc: 0.8491 - val_loss: 0.4752 - val_acc: 0.8514

Epoch 00025: val_loss improved from 0.48057 to 0.47515, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 538us/step
current val accuracy:0.851
current auc_score ------------------> 0.935

  32/7968 [..............................] - ETA: 21:53
 128/7968 [..............................] - ETA: 5:27 
 224/7968 [..............................] - ETA: 3:06
 320/7968 [>.............................] - ETA: 2:10
 416/7968 [>.............................] - ETA: 1:40
 512/7968 [>.............................] - ETA: 1:21
 608/7968 [=>............................] - ETA: 1:08
 704/7968 [=>............................] - ETA: 58s 
 800/7968 [==>...........................] - ETA: 51s
 896/7968 [==>...........................] - ETA: 45s
 992/7968 [==>...........................] - ETA: 40s
1088/7968 [===>..........................] - ETA: 37s
1184/7968 [===>..........................] - ETA: 33s
1280/7968 [===>..........................] - ETA: 31s
1376/7968 [====>.........................] - ETA: 28s
1472/7968 [====>.........................] - ETA: 26s
1568/7968 [====>.........................] - ETA: 25s
1664/7968 [=====>........................] - ETA: 23s
1760/7968 [=====>........................] - ETA: 22s
1856/7968 [=====>........................] - ETA: 20s
1952/7968 [======>.......................] - ETA: 19s
2048/7968 [======>.......................] - ETA: 18s
2144/7968 [=======>......................] - ETA: 17s
2240/7968 [=======>......................] - ETA: 16s
2336/7968 [=======>......................] - ETA: 15s
2432/7968 [========>.....................] - ETA: 15s
2528/7968 [========>.....................] - ETA: 14s
2624/7968 [========>.....................] - ETA: 13s
2720/7968 [=========>....................] - ETA: 13s
2816/7968 [=========>....................] - ETA: 12s
2912/7968 [=========>....................] - ETA: 11s
3008/7968 [==========>...................] - ETA: 11s
3104/7968 [==========>...................] - ETA: 10s
3200/7968 [===========>..................] - ETA: 10s
3296/7968 [===========>..................] - ETA: 10s
3392/7968 [===========>..................] - ETA: 9s 
3488/7968 [============>.................] - ETA: 9s
3584/7968 [============>.................] - ETA: 8s
3680/7968 [============>.................] - ETA: 8s
3776/7968 [=============>................] - ETA: 8s
3872/7968 [=============>................] - ETA: 7s
3968/7968 [=============>................] - ETA: 7s
4064/7968 [==============>...............] - ETA: 7s
4160/7968 [==============>...............] - ETA: 6s
4256/7968 [===============>..............] - ETA: 6s
4352/7968 [===============>..............] - ETA: 6s
4448/7968 [===============>..............] - ETA: 6s
4544/7968 [================>.............] - ETA: 5s
4640/7968 [================>.............] - ETA: 5s
4736/7968 [================>.............] - ETA: 5s
4832/7968 [=================>............] - ETA: 5s
4928/7968 [=================>............] - ETA: 4s
5024/7968 [=================>............] - ETA: 4s
5120/7968 [==================>...........] - ETA: 4s
5216/7968 [==================>...........] - ETA: 4s
5312/7968 [===================>..........] - ETA: 4s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 3s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 3s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 10s 1ms/step
Best saved model val accuracy:0.851
best saved model auc_score ------------------> 0.935
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_13[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_253 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_253[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_254 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_254[0][0]             
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_109[0][0]            
__________________________________________________________________________________________________
activation_255 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_255[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_256 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_256[0][0]             
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 52, 96, 96)   0           concatenate_109[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_110[0][0]            
__________________________________________________________________________________________________
activation_257 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_257[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_258 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_258[0][0]             
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 70, 96, 96)   0           concatenate_110[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_111[0][0]            
__________________________________________________________________________________________________
activation_259 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_259[0][0]             
__________________________________________________________________________________________________
average_pooling2d_25 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_25[0][0]       
__________________________________________________________________________________________________
activation_260 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_260[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_261 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_261[0][0]             
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_25[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_112[0][0]            
__________________________________________________________________________________________________
activation_262 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_262[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_263 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_263[0][0]             
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 71, 48, 48)   0           concatenate_112[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_113[0][0]            
__________________________________________________________________________________________________
activation_264 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_264[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_265 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_265[0][0]             
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 89, 48, 48)   0           concatenate_113[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_114[0][0]            
__________________________________________________________________________________________________
activation_266 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_266[0][0]             
__________________________________________________________________________________________________
average_pooling2d_26 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_26[0][0]       
__________________________________________________________________________________________________
activation_267 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_267[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_268 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_268[0][0]             
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_26[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_115[0][0]            
__________________________________________________________________________________________________
activation_269 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_269[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_270 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_270[0][0]             
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 80, 24, 24)   0           concatenate_115[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_116[0][0]            
__________________________________________________________________________________________________
activation_271 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_271[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_272 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_272[0][0]             
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 98, 24, 24)   0           concatenate_116[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_117[0][0]            
__________________________________________________________________________________________________
activation_273 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_13 (Gl (None, 98)           0           activation_273[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            99          global_average_pooling2d_13[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 69s - loss: 0.7415 - acc: 0.7027 - val_loss: 0.6954 - val_acc: 0.7377

Epoch 00001: val_loss improved from inf to 0.69536, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.6688 - acc: 0.7491 - val_loss: 0.6437 - val_acc: 0.7556

Epoch 00002: val_loss improved from 0.69536 to 0.64370, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 53s - loss: 0.6339 - acc: 0.7633 - val_loss: 0.6171 - val_acc: 0.7700

Epoch 00003: val_loss improved from 0.64370 to 0.61709, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 53s - loss: 0.6117 - acc: 0.7767 - val_loss: 0.5973 - val_acc: 0.7824

Epoch 00004: val_loss improved from 0.61709 to 0.59734, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 54s - loss: 0.5973 - acc: 0.7830 - val_loss: 0.5921 - val_acc: 0.7800

Epoch 00005: val_loss improved from 0.59734 to 0.59205, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 54s - loss: 0.5839 - acc: 0.7933 - val_loss: 0.5773 - val_acc: 0.7902

Epoch 00006: val_loss improved from 0.59205 to 0.57727, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 53s - loss: 0.5732 - acc: 0.7981 - val_loss: 0.5631 - val_acc: 0.7989

Epoch 00007: val_loss improved from 0.57727 to 0.56310, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 53s - loss: 0.5639 - acc: 0.8043 - val_loss: 0.5531 - val_acc: 0.8072

Epoch 00008: val_loss improved from 0.56310 to 0.55313, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 54s - loss: 0.5545 - acc: 0.8085 - val_loss: 0.5427 - val_acc: 0.8148

Epoch 00009: val_loss improved from 0.55313 to 0.54274, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 54s - loss: 0.5498 - acc: 0.8114 - val_loss: 0.5391 - val_acc: 0.8166

Epoch 00010: val_loss improved from 0.54274 to 0.53907, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 54s - loss: 0.5403 - acc: 0.8187 - val_loss: 0.5373 - val_acc: 0.8173

Epoch 00011: val_loss improved from 0.53907 to 0.53729, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 54s - loss: 0.5337 - acc: 0.8238 - val_loss: 0.5383 - val_acc: 0.8110

Epoch 00012: val_loss did not improve from 0.53729
Epoch 13/25
 - 54s - loss: 0.5285 - acc: 0.8252 - val_loss: 0.5238 - val_acc: 0.8253

Epoch 00013: val_loss improved from 0.53729 to 0.52383, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 53s - loss: 0.5220 - acc: 0.8285 - val_loss: 0.5169 - val_acc: 0.8284

Epoch 00014: val_loss improved from 0.52383 to 0.51688, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.5164 - acc: 0.8315 - val_loss: 0.5071 - val_acc: 0.8303

Epoch 00015: val_loss improved from 0.51688 to 0.50714, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 54s - loss: 0.5119 - acc: 0.8320 - val_loss: 0.5186 - val_acc: 0.8287

Epoch 00016: val_loss did not improve from 0.50714
Epoch 17/25
 - 53s - loss: 0.5049 - acc: 0.8379 - val_loss: 0.5094 - val_acc: 0.8353

Epoch 00017: val_loss did not improve from 0.50714
Epoch 18/25
 - 54s - loss: 0.5018 - acc: 0.8385 - val_loss: 0.4985 - val_acc: 0.8381

Epoch 00018: val_loss improved from 0.50714 to 0.49851, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 54s - loss: 0.4961 - acc: 0.8447 - val_loss: 0.5042 - val_acc: 0.8363

Epoch 00019: val_loss did not improve from 0.49851
Epoch 20/25
 - 54s - loss: 0.4929 - acc: 0.8464 - val_loss: 0.4970 - val_acc: 0.8425

Epoch 00020: val_loss improved from 0.49851 to 0.49703, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 54s - loss: 0.4905 - acc: 0.8443 - val_loss: 0.4806 - val_acc: 0.8476

Epoch 00021: val_loss improved from 0.49703 to 0.48058, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 54s - loss: 0.4856 - acc: 0.8475 - val_loss: 0.4810 - val_acc: 0.8507

Epoch 00022: val_loss did not improve from 0.48058
Epoch 23/25
 - 54s - loss: 0.4806 - acc: 0.8516 - val_loss: 0.4838 - val_acc: 0.8460

Epoch 00023: val_loss did not improve from 0.48058
Epoch 24/25
 - 54s - loss: 0.4778 - acc: 0.8526 - val_loss: 0.4764 - val_acc: 0.8527

Epoch 00024: val_loss improved from 0.48058 to 0.47640, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 53s - loss: 0.4738 - acc: 0.8542 - val_loss: 0.4661 - val_acc: 0.8560

Epoch 00025: val_loss improved from 0.47640 to 0.46611, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 555us/step
current val accuracy:0.856
current auc_score ------------------> 0.938

  32/7968 [..............................] - ETA: 24:37
 128/7968 [..............................] - ETA: 6:08 
 224/7968 [..............................] - ETA: 3:30
 320/7968 [>.............................] - ETA: 2:26
 416/7968 [>.............................] - ETA: 1:52
 512/7968 [>.............................] - ETA: 1:30
 608/7968 [=>............................] - ETA: 1:16
 704/7968 [=>............................] - ETA: 1:05
 800/7968 [==>...........................] - ETA: 57s 
 896/7968 [==>...........................] - ETA: 50s
 992/7968 [==>...........................] - ETA: 45s
1088/7968 [===>..........................] - ETA: 41s
1184/7968 [===>..........................] - ETA: 37s
1280/7968 [===>..........................] - ETA: 34s
1376/7968 [====>.........................] - ETA: 32s
1472/7968 [====>.........................] - ETA: 29s
1568/7968 [====>.........................] - ETA: 27s
1664/7968 [=====>........................] - ETA: 26s
1760/7968 [=====>........................] - ETA: 24s
1856/7968 [=====>........................] - ETA: 23s
1952/7968 [======>.......................] - ETA: 21s
2048/7968 [======>.......................] - ETA: 20s
2144/7968 [=======>......................] - ETA: 19s
2240/7968 [=======>......................] - ETA: 18s
2336/7968 [=======>......................] - ETA: 17s
2432/7968 [========>.....................] - ETA: 16s
2528/7968 [========>.....................] - ETA: 15s
2624/7968 [========>.....................] - ETA: 15s
2720/7968 [=========>....................] - ETA: 14s
2816/7968 [=========>....................] - ETA: 13s
2912/7968 [=========>....................] - ETA: 13s
3008/7968 [==========>...................] - ETA: 12s
3104/7968 [==========>...................] - ETA: 12s
3200/7968 [===========>..................] - ETA: 11s
3296/7968 [===========>..................] - ETA: 11s
3392/7968 [===========>..................] - ETA: 10s
3488/7968 [============>.................] - ETA: 10s
3584/7968 [============>.................] - ETA: 9s 
3680/7968 [============>.................] - ETA: 9s
3776/7968 [=============>................] - ETA: 8s
3872/7968 [=============>................] - ETA: 8s
3968/7968 [=============>................] - ETA: 8s
4064/7968 [==============>...............] - ETA: 7s
4160/7968 [==============>...............] - ETA: 7s
4256/7968 [===============>..............] - ETA: 7s
4352/7968 [===============>..............] - ETA: 6s
4448/7968 [===============>..............] - ETA: 6s
4544/7968 [================>.............] - ETA: 6s
4640/7968 [================>.............] - ETA: 6s
4736/7968 [================>.............] - ETA: 5s
4832/7968 [=================>............] - ETA: 5s
4928/7968 [=================>............] - ETA: 5s
5024/7968 [=================>............] - ETA: 5s
5120/7968 [==================>...........] - ETA: 4s
5216/7968 [==================>...........] - ETA: 4s
5312/7968 [===================>..........] - ETA: 4s
5408/7968 [===================>..........] - ETA: 4s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 3s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 10s 1ms/step
Best saved model val accuracy:0.856
best saved model auc_score ------------------> 0.938
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_14[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_274 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_274[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_275 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_275[0][0]             
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_118[0][0]            
__________________________________________________________________________________________________
activation_276 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_276[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_277 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_277[0][0]             
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 52, 96, 96)   0           concatenate_118[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_119[0][0]            
__________________________________________________________________________________________________
activation_278 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_278[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_279 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_279[0][0]             
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 70, 96, 96)   0           concatenate_119[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_120[0][0]            
__________________________________________________________________________________________________
activation_280 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_280[0][0]             
__________________________________________________________________________________________________
average_pooling2d_27 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_27[0][0]       
__________________________________________________________________________________________________
activation_281 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_281[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_282 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_282[0][0]             
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_27[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_121[0][0]            
__________________________________________________________________________________________________
activation_283 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_283[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_284 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_284[0][0]             
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 71, 48, 48)   0           concatenate_121[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_122[0][0]            
__________________________________________________________________________________________________
activation_285 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_285[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_286 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_286[0][0]             
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 89, 48, 48)   0           concatenate_122[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_123[0][0]            
__________________________________________________________________________________________________
activation_287 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_287[0][0]             
__________________________________________________________________________________________________
average_pooling2d_28 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_28[0][0]       
__________________________________________________________________________________________________
activation_288 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_288[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_289 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_289[0][0]             
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_28[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_124[0][0]            
__________________________________________________________________________________________________
activation_290 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_290[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_291 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_291[0][0]             
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 80, 24, 24)   0           concatenate_124[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_125[0][0]            
__________________________________________________________________________________________________
activation_292 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_292[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_293 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_293[0][0]             
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 98, 24, 24)   0           concatenate_125[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_126[0][0]            
__________________________________________________________________________________________________
activation_294 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_14 (Gl (None, 98)           0           activation_294[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            99          global_average_pooling2d_14[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 72s - loss: 0.5391 - acc: 0.7679 - val_loss: 0.4788 - val_acc: 0.7944

Epoch 00001: val_loss improved from inf to 0.47877, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 55s - loss: 0.4384 - acc: 0.8082 - val_loss: 0.4749 - val_acc: 0.7802

Epoch 00002: val_loss improved from 0.47877 to 0.47490, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 55s - loss: 0.3946 - acc: 0.8305 - val_loss: 0.8560 - val_acc: 0.7354

Epoch 00003: val_loss did not improve from 0.47490
Epoch 4/25
 - 55s - loss: 0.3597 - acc: 0.8506 - val_loss: 0.4626 - val_acc: 0.7907

Epoch 00004: val_loss improved from 0.47490 to 0.46258, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 55s - loss: 0.3318 - acc: 0.8678 - val_loss: 0.5008 - val_acc: 0.8353

Epoch 00005: val_loss did not improve from 0.46258
Epoch 6/25
 - 55s - loss: 0.3115 - acc: 0.8779 - val_loss: 0.4775 - val_acc: 0.7818

Epoch 00006: val_loss did not improve from 0.46258
Epoch 7/25
 - 55s - loss: 0.2928 - acc: 0.8872 - val_loss: 0.4906 - val_acc: 0.7855

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.

Epoch 00007: val_loss did not improve from 0.46258
Epoch 8/25
 - 55s - loss: 0.2333 - acc: 0.9185 - val_loss: 0.2396 - val_acc: 0.9130

Epoch 00008: val_loss improved from 0.46258 to 0.23964, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 55s - loss: 0.2148 - acc: 0.9255 - val_loss: 0.2847 - val_acc: 0.8947

Epoch 00009: val_loss did not improve from 0.23964
Epoch 10/25
 - 55s - loss: 0.2051 - acc: 0.9306 - val_loss: 0.4882 - val_acc: 0.8496

Epoch 00010: val_loss did not improve from 0.23964
Epoch 11/25
 - 55s - loss: 0.1966 - acc: 0.9339 - val_loss: 0.2074 - val_acc: 0.9291

Epoch 00011: val_loss improved from 0.23964 to 0.20738, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 55s - loss: 0.1844 - acc: 0.9399 - val_loss: 0.2101 - val_acc: 0.9287

Epoch 00012: val_loss did not improve from 0.20738
Epoch 13/25
 - 55s - loss: 0.1705 - acc: 0.9445 - val_loss: 0.2512 - val_acc: 0.9083

Epoch 00013: val_loss did not improve from 0.20738
Epoch 14/25
 - 55s - loss: 0.1649 - acc: 0.9478 - val_loss: 0.2064 - val_acc: 0.9292

Epoch 00014: val_loss improved from 0.20738 to 0.20636, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 55s - loss: 0.1619 - acc: 0.9495 - val_loss: 0.2326 - val_acc: 0.9147

Epoch 00015: val_loss did not improve from 0.20636
Epoch 16/25
 - 55s - loss: 0.1487 - acc: 0.9550 - val_loss: 0.2542 - val_acc: 0.9133

Epoch 00016: val_loss did not improve from 0.20636
Epoch 17/25
 - 55s - loss: 0.1496 - acc: 0.9530 - val_loss: 0.2638 - val_acc: 0.9157

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00017: val_loss did not improve from 0.20636
Epoch 18/25
 - 55s - loss: 0.1188 - acc: 0.9668 - val_loss: 0.1583 - val_acc: 0.9532

Epoch 00018: val_loss improved from 0.20636 to 0.15827, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 55s - loss: 0.1144 - acc: 0.9692 - val_loss: 0.1651 - val_acc: 0.9474

Epoch 00019: val_loss did not improve from 0.15827
Epoch 20/25
 - 55s - loss: 0.1121 - acc: 0.9699 - val_loss: 0.1645 - val_acc: 0.9488

Epoch 00020: val_loss did not improve from 0.15827
Epoch 21/25
 - 55s - loss: 0.1058 - acc: 0.9732 - val_loss: 0.1626 - val_acc: 0.9483

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.

Epoch 00021: val_loss did not improve from 0.15827
Epoch 22/25
 - 55s - loss: 0.0954 - acc: 0.9781 - val_loss: 0.1471 - val_acc: 0.9547

Epoch 00022: val_loss improved from 0.15827 to 0.14705, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 55s - loss: 0.0961 - acc: 0.9770 - val_loss: 0.1461 - val_acc: 0.9552

Epoch 00023: val_loss improved from 0.14705 to 0.14605, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 55s - loss: 0.0927 - acc: 0.9785 - val_loss: 0.1452 - val_acc: 0.9549

Epoch 00024: val_loss improved from 0.14605 to 0.14522, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 55s - loss: 0.0930 - acc: 0.9788 - val_loss: 0.1470 - val_acc: 0.9546

Epoch 00025: val_loss did not improve from 0.14522

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 570us/step
current val accuracy:0.955
current auc_score ------------------> 0.991

  32/7968 [..............................] - ETA: 27:18
 128/7968 [..............................] - ETA: 6:48 
 224/7968 [..............................] - ETA: 3:52
 320/7968 [>.............................] - ETA: 2:42
 416/7968 [>.............................] - ETA: 2:04
 512/7968 [>.............................] - ETA: 1:40
 608/7968 [=>............................] - ETA: 1:24
 704/7968 [=>............................] - ETA: 1:12
 800/7968 [==>...........................] - ETA: 1:03
 896/7968 [==>...........................] - ETA: 56s 
 992/7968 [==>...........................] - ETA: 50s
1088/7968 [===>..........................] - ETA: 45s
1184/7968 [===>..........................] - ETA: 41s
1280/7968 [===>..........................] - ETA: 38s
1376/7968 [====>.........................] - ETA: 35s
1472/7968 [====>.........................] - ETA: 32s
1568/7968 [====>.........................] - ETA: 30s
1664/7968 [=====>........................] - ETA: 28s
1760/7968 [=====>........................] - ETA: 26s
1856/7968 [=====>........................] - ETA: 25s
1952/7968 [======>.......................] - ETA: 23s
2048/7968 [======>.......................] - ETA: 22s
2144/7968 [=======>......................] - ETA: 21s
2240/7968 [=======>......................] - ETA: 20s
2336/7968 [=======>......................] - ETA: 19s
2432/7968 [========>.....................] - ETA: 18s
2528/7968 [========>.....................] - ETA: 17s
2624/7968 [========>.....................] - ETA: 16s
2720/7968 [=========>....................] - ETA: 15s
2816/7968 [=========>....................] - ETA: 15s
2912/7968 [=========>....................] - ETA: 14s
3008/7968 [==========>...................] - ETA: 13s
3104/7968 [==========>...................] - ETA: 13s
3200/7968 [===========>..................] - ETA: 12s
3296/7968 [===========>..................] - ETA: 12s
3392/7968 [===========>..................] - ETA: 11s
3488/7968 [============>.................] - ETA: 11s
3584/7968 [============>.................] - ETA: 10s
3680/7968 [============>.................] - ETA: 10s
3776/7968 [=============>................] - ETA: 9s 
3872/7968 [=============>................] - ETA: 9s
3968/7968 [=============>................] - ETA: 8s
4064/7968 [==============>...............] - ETA: 8s
4160/7968 [==============>...............] - ETA: 8s
4256/7968 [===============>..............] - ETA: 7s
4352/7968 [===============>..............] - ETA: 7s
4448/7968 [===============>..............] - ETA: 7s
4544/7968 [================>.............] - ETA: 6s
4640/7968 [================>.............] - ETA: 6s
4736/7968 [================>.............] - ETA: 6s
4832/7968 [=================>............] - ETA: 6s
4928/7968 [=================>............] - ETA: 5s
5024/7968 [=================>............] - ETA: 5s
5120/7968 [==================>...........] - ETA: 5s
5216/7968 [==================>...........] - ETA: 5s
5312/7968 [===================>..........] - ETA: 4s
5408/7968 [===================>..........] - ETA: 4s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 3s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 11s 1ms/step
Best saved model val accuracy:0.955
best saved model auc_score ------------------> 0.991
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_15[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_295 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_295[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_296 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_296[0][0]             
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_127[0][0]            
__________________________________________________________________________________________________
activation_297 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_297[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_298 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_298[0][0]             
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 52, 96, 96)   0           concatenate_127[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_128[0][0]            
__________________________________________________________________________________________________
activation_299 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_299[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_300 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_300[0][0]             
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 70, 96, 96)   0           concatenate_128[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_129[0][0]            
__________________________________________________________________________________________________
activation_301 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_301[0][0]             
__________________________________________________________________________________________________
average_pooling2d_29 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_29[0][0]       
__________________________________________________________________________________________________
activation_302 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_302[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_303 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_303[0][0]             
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_29[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_130[0][0]            
__________________________________________________________________________________________________
activation_304 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_304[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_305 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_305[0][0]             
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 71, 48, 48)   0           concatenate_130[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_131[0][0]            
__________________________________________________________________________________________________
activation_306 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_306[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_307 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_307[0][0]             
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 89, 48, 48)   0           concatenate_131[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_132[0][0]            
__________________________________________________________________________________________________
activation_308 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_308[0][0]             
__________________________________________________________________________________________________
average_pooling2d_30 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_30[0][0]       
__________________________________________________________________________________________________
activation_309 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_309[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_310 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_310[0][0]             
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_30[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_133[0][0]            
__________________________________________________________________________________________________
activation_311 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_311[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_312 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_312[0][0]             
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 80, 24, 24)   0           concatenate_133[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_134[0][0]            
__________________________________________________________________________________________________
activation_313 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_313[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_314 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_314[0][0]             
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 98, 24, 24)   0           concatenate_134[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_135[0][0]            
__________________________________________________________________________________________________
activation_315 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_15 (Gl (None, 98)           0           activation_315[0][0]             
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 1)            99          global_average_pooling2d_15[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 73s - loss: 0.7412 - acc: 0.6808 - val_loss: 0.6803 - val_acc: 0.7444

Epoch 00001: val_loss improved from inf to 0.68031, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.6597 - acc: 0.7537 - val_loss: 0.6378 - val_acc: 0.7619

Epoch 00002: val_loss improved from 0.68031 to 0.63784, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 54s - loss: 0.6288 - acc: 0.7687 - val_loss: 0.6125 - val_acc: 0.7770

Epoch 00003: val_loss improved from 0.63784 to 0.61247, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 54s - loss: 0.6120 - acc: 0.7765 - val_loss: 0.5992 - val_acc: 0.7845

Epoch 00004: val_loss improved from 0.61247 to 0.59917, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 54s - loss: 0.5990 - acc: 0.7837 - val_loss: 0.5883 - val_acc: 0.7904

Epoch 00005: val_loss improved from 0.59917 to 0.58835, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 54s - loss: 0.5844 - acc: 0.7932 - val_loss: 0.5740 - val_acc: 0.7976

Epoch 00006: val_loss improved from 0.58835 to 0.57402, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 54s - loss: 0.5763 - acc: 0.7944 - val_loss: 0.5708 - val_acc: 0.7964

Epoch 00007: val_loss improved from 0.57402 to 0.57078, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 54s - loss: 0.5691 - acc: 0.7985 - val_loss: 0.5624 - val_acc: 0.7948

Epoch 00008: val_loss improved from 0.57078 to 0.56238, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 54s - loss: 0.5628 - acc: 0.8014 - val_loss: 0.5569 - val_acc: 0.8022

Epoch 00009: val_loss improved from 0.56238 to 0.55691, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 54s - loss: 0.5571 - acc: 0.8046 - val_loss: 0.5482 - val_acc: 0.8077

Epoch 00010: val_loss improved from 0.55691 to 0.54821, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 54s - loss: 0.5513 - acc: 0.8072 - val_loss: 0.5439 - val_acc: 0.8072

Epoch 00011: val_loss improved from 0.54821 to 0.54388, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 54s - loss: 0.5470 - acc: 0.8109 - val_loss: 0.5340 - val_acc: 0.8139

Epoch 00012: val_loss improved from 0.54388 to 0.53397, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 54s - loss: 0.5403 - acc: 0.8144 - val_loss: 0.5286 - val_acc: 0.8133

Epoch 00013: val_loss improved from 0.53397 to 0.52855, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 54s - loss: 0.5364 - acc: 0.8150 - val_loss: 0.5305 - val_acc: 0.8148

Epoch 00014: val_loss did not improve from 0.52855
Epoch 15/25
 - 54s - loss: 0.5298 - acc: 0.8195 - val_loss: 0.5173 - val_acc: 0.8213

Epoch 00015: val_loss improved from 0.52855 to 0.51729, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 54s - loss: 0.5262 - acc: 0.8215 - val_loss: 0.5141 - val_acc: 0.8232

Epoch 00016: val_loss improved from 0.51729 to 0.51414, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 54s - loss: 0.5194 - acc: 0.8258 - val_loss: 0.5202 - val_acc: 0.8168

Epoch 00017: val_loss did not improve from 0.51414
Epoch 18/25
 - 54s - loss: 0.5168 - acc: 0.8274 - val_loss: 0.5110 - val_acc: 0.8268

Epoch 00018: val_loss improved from 0.51414 to 0.51098, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 54s - loss: 0.5108 - acc: 0.8294 - val_loss: 0.5028 - val_acc: 0.8313

Epoch 00019: val_loss improved from 0.51098 to 0.50280, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 54s - loss: 0.5075 - acc: 0.8312 - val_loss: 0.5047 - val_acc: 0.8331

Epoch 00020: val_loss did not improve from 0.50280
Epoch 21/25
 - 54s - loss: 0.5025 - acc: 0.8338 - val_loss: 0.4917 - val_acc: 0.8384

Epoch 00021: val_loss improved from 0.50280 to 0.49168, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 54s - loss: 0.4971 - acc: 0.8367 - val_loss: 0.4860 - val_acc: 0.8375

Epoch 00022: val_loss improved from 0.49168 to 0.48602, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 54s - loss: 0.4970 - acc: 0.8372 - val_loss: 0.4951 - val_acc: 0.8299

Epoch 00023: val_loss did not improve from 0.48602
Epoch 24/25
 - 54s - loss: 0.4911 - acc: 0.8404 - val_loss: 0.4840 - val_acc: 0.8419

Epoch 00024: val_loss improved from 0.48602 to 0.48399, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 54s - loss: 0.4882 - acc: 0.8430 - val_loss: 0.4766 - val_acc: 0.8444

Epoch 00025: val_loss improved from 0.48399 to 0.47661, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 570us/step
current val accuracy:0.844
current auc_score ------------------> 0.930

  32/7968 [..............................] - ETA: 30:42
 128/7968 [..............................] - ETA: 7:39 
 224/7968 [..............................] - ETA: 4:21
 320/7968 [>.............................] - ETA: 3:01
 416/7968 [>.............................] - ETA: 2:19
 512/7968 [>.............................] - ETA: 1:52
 608/7968 [=>............................] - ETA: 1:34
 704/7968 [=>............................] - ETA: 1:20
 800/7968 [==>...........................] - ETA: 1:10
 896/7968 [==>...........................] - ETA: 1:02
 992/7968 [==>...........................] - ETA: 56s 
1088/7968 [===>..........................] - ETA: 50s
1184/7968 [===>..........................] - ETA: 46s
1280/7968 [===>..........................] - ETA: 42s
1376/7968 [====>.........................] - ETA: 39s
1472/7968 [====>.........................] - ETA: 36s
1568/7968 [====>.........................] - ETA: 33s
1664/7968 [=====>........................] - ETA: 31s
1760/7968 [=====>........................] - ETA: 29s
1856/7968 [=====>........................] - ETA: 27s
1952/7968 [======>.......................] - ETA: 26s
2048/7968 [======>.......................] - ETA: 24s
2144/7968 [=======>......................] - ETA: 23s
2240/7968 [=======>......................] - ETA: 22s
2336/7968 [=======>......................] - ETA: 21s
2432/7968 [========>.....................] - ETA: 20s
2528/7968 [========>.....................] - ETA: 19s
2624/7968 [========>.....................] - ETA: 18s
2720/7968 [=========>....................] - ETA: 17s
2816/7968 [=========>....................] - ETA: 16s
2912/7968 [=========>....................] - ETA: 15s
3008/7968 [==========>...................] - ETA: 15s
3104/7968 [==========>...................] - ETA: 14s
3200/7968 [===========>..................] - ETA: 13s
3296/7968 [===========>..................] - ETA: 13s
3392/7968 [===========>..................] - ETA: 12s
3488/7968 [============>.................] - ETA: 12s
3584/7968 [============>.................] - ETA: 11s
3680/7968 [============>.................] - ETA: 11s
3776/7968 [=============>................] - ETA: 10s
3872/7968 [=============>................] - ETA: 10s
3968/7968 [=============>................] - ETA: 9s 
4064/7968 [==============>...............] - ETA: 9s
4160/7968 [==============>...............] - ETA: 8s
4256/7968 [===============>..............] - ETA: 8s
4352/7968 [===============>..............] - ETA: 8s
4448/7968 [===============>..............] - ETA: 7s
4544/7968 [================>.............] - ETA: 7s
4640/7968 [================>.............] - ETA: 7s
4736/7968 [================>.............] - ETA: 6s
4832/7968 [=================>............] - ETA: 6s
4928/7968 [=================>............] - ETA: 6s
5024/7968 [=================>............] - ETA: 6s
5120/7968 [==================>...........] - ETA: 5s
5216/7968 [==================>...........] - ETA: 5s
5312/7968 [===================>..........] - ETA: 5s
5408/7968 [===================>..........] - ETA: 4s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 4s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 12s 2ms/step
Best saved model val accuracy:0.844
best saved model auc_score ------------------> 0.930
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_16 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_16[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_316 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_316[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_317 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_317[0][0]             
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_136[0][0]            
__________________________________________________________________________________________________
activation_318 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_318[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_319 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_319[0][0]             
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 52, 96, 96)   0           concatenate_136[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_137[0][0]            
__________________________________________________________________________________________________
activation_320 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_320[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_321 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_321[0][0]             
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 70, 96, 96)   0           concatenate_137[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_138[0][0]            
__________________________________________________________________________________________________
activation_322 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_322[0][0]             
__________________________________________________________________________________________________
average_pooling2d_31 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_31[0][0]       
__________________________________________________________________________________________________
activation_323 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_323[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_324 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_324[0][0]             
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_31[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_139[0][0]            
__________________________________________________________________________________________________
activation_325 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_325[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_326 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_326[0][0]             
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 71, 48, 48)   0           concatenate_139[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_140[0][0]            
__________________________________________________________________________________________________
activation_327 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_327[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_328 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_328[0][0]             
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 89, 48, 48)   0           concatenate_140[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_141[0][0]            
__________________________________________________________________________________________________
activation_329 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_329[0][0]             
__________________________________________________________________________________________________
average_pooling2d_32 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_32[0][0]       
__________________________________________________________________________________________________
activation_330 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_330[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_331 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_331[0][0]             
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_32[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_142[0][0]            
__________________________________________________________________________________________________
activation_332 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_332[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_333 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_333[0][0]             
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 80, 24, 24)   0           concatenate_142[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_143[0][0]            
__________________________________________________________________________________________________
activation_334 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_334[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_335 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_335[0][0]             
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 98, 24, 24)   0           concatenate_143[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_144[0][0]            
__________________________________________________________________________________________________
activation_336 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_16 (Gl (None, 98)           0           activation_336[0][0]             
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 1)            99          global_average_pooling2d_16[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 74s - loss: 0.7354 - acc: 0.7006 - val_loss: 0.6815 - val_acc: 0.7408

Epoch 00001: val_loss improved from inf to 0.68147, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.6710 - acc: 0.7473 - val_loss: 0.6511 - val_acc: 0.7536

Epoch 00002: val_loss improved from 0.68147 to 0.65106, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 54s - loss: 0.6424 - acc: 0.7615 - val_loss: 0.6293 - val_acc: 0.7626

Epoch 00003: val_loss improved from 0.65106 to 0.62926, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 54s - loss: 0.6231 - acc: 0.7732 - val_loss: 0.6112 - val_acc: 0.7771

Epoch 00004: val_loss improved from 0.62926 to 0.61121, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 54s - loss: 0.6095 - acc: 0.7799 - val_loss: 0.6003 - val_acc: 0.7792

Epoch 00005: val_loss improved from 0.61121 to 0.60030, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 54s - loss: 0.5962 - acc: 0.7884 - val_loss: 0.5840 - val_acc: 0.7934

Epoch 00006: val_loss improved from 0.60030 to 0.58403, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 54s - loss: 0.5883 - acc: 0.7907 - val_loss: 0.5859 - val_acc: 0.7865

Epoch 00007: val_loss did not improve from 0.58403
Epoch 8/25
 - 54s - loss: 0.5791 - acc: 0.7921 - val_loss: 0.5683 - val_acc: 0.7966

Epoch 00008: val_loss improved from 0.58403 to 0.56827, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 54s - loss: 0.5716 - acc: 0.7996 - val_loss: 0.5600 - val_acc: 0.8077

Epoch 00009: val_loss improved from 0.56827 to 0.55997, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 54s - loss: 0.5659 - acc: 0.8026 - val_loss: 0.5559 - val_acc: 0.8016

Epoch 00010: val_loss improved from 0.55997 to 0.55588, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 54s - loss: 0.5575 - acc: 0.8069 - val_loss: 0.5524 - val_acc: 0.8033

Epoch 00011: val_loss improved from 0.55588 to 0.55243, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 54s - loss: 0.5503 - acc: 0.8097 - val_loss: 0.5378 - val_acc: 0.8145

Epoch 00012: val_loss improved from 0.55243 to 0.53780, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 54s - loss: 0.5476 - acc: 0.8118 - val_loss: 0.5344 - val_acc: 0.8148

Epoch 00013: val_loss improved from 0.53780 to 0.53436, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 54s - loss: 0.5410 - acc: 0.8156 - val_loss: 0.5310 - val_acc: 0.8219

Epoch 00014: val_loss improved from 0.53436 to 0.53097, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.5343 - acc: 0.8172 - val_loss: 0.5234 - val_acc: 0.8257

Epoch 00015: val_loss improved from 0.53097 to 0.52344, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 54s - loss: 0.5302 - acc: 0.8225 - val_loss: 0.5239 - val_acc: 0.8263

Epoch 00016: val_loss did not improve from 0.52344
Epoch 17/25
 - 54s - loss: 0.5248 - acc: 0.8238 - val_loss: 0.5127 - val_acc: 0.8316

Epoch 00017: val_loss improved from 0.52344 to 0.51272, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 54s - loss: 0.5201 - acc: 0.8294 - val_loss: 0.5219 - val_acc: 0.8240

Epoch 00018: val_loss did not improve from 0.51272
Epoch 19/25
 - 54s - loss: 0.5151 - acc: 0.8305 - val_loss: 0.5169 - val_acc: 0.8273

Epoch 00019: val_loss did not improve from 0.51272
Epoch 20/25
 - 54s - loss: 0.5116 - acc: 0.8318 - val_loss: 0.5009 - val_acc: 0.8352

Epoch 00020: val_loss improved from 0.51272 to 0.50089, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 54s - loss: 0.5047 - acc: 0.8346 - val_loss: 0.5139 - val_acc: 0.8313

Epoch 00021: val_loss did not improve from 0.50089
Epoch 22/25
 - 54s - loss: 0.4999 - acc: 0.8379 - val_loss: 0.4925 - val_acc: 0.8424

Epoch 00022: val_loss improved from 0.50089 to 0.49255, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 54s - loss: 0.4985 - acc: 0.8376 - val_loss: 0.4858 - val_acc: 0.8449

Epoch 00023: val_loss improved from 0.49255 to 0.48576, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 54s - loss: 0.4953 - acc: 0.8421 - val_loss: 0.4917 - val_acc: 0.8410

Epoch 00024: val_loss did not improve from 0.48576
Epoch 25/25
 - 54s - loss: 0.4910 - acc: 0.8442 - val_loss: 0.4873 - val_acc: 0.8368

Epoch 00025: val_loss did not improve from 0.48576

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 579us/step
current val accuracy:0.837
current auc_score ------------------> 0.929

  32/7968 [..............................] - ETA: 32:42
 128/7968 [..............................] - ETA: 8:08 
 224/7968 [..............................] - ETA: 4:37
 320/7968 [>.............................] - ETA: 3:13
 416/7968 [>.............................] - ETA: 2:27
 512/7968 [>.............................] - ETA: 1:59
 608/7968 [=>............................] - ETA: 1:40
 704/7968 [=>............................] - ETA: 1:25
 800/7968 [==>...........................] - ETA: 1:15
 896/7968 [==>...........................] - ETA: 1:06
 992/7968 [==>...........................] - ETA: 59s 
1088/7968 [===>..........................] - ETA: 53s
1184/7968 [===>..........................] - ETA: 49s
1280/7968 [===>..........................] - ETA: 45s
1376/7968 [====>.........................] - ETA: 41s
1472/7968 [====>.........................] - ETA: 38s
1568/7968 [====>.........................] - ETA: 35s
1664/7968 [=====>........................] - ETA: 33s
1760/7968 [=====>........................] - ETA: 31s
1856/7968 [=====>........................] - ETA: 29s
1952/7968 [======>.......................] - ETA: 27s
2048/7968 [======>.......................] - ETA: 26s
2144/7968 [=======>......................] - ETA: 24s
2240/7968 [=======>......................] - ETA: 23s
2336/7968 [=======>......................] - ETA: 22s
2432/7968 [========>.....................] - ETA: 21s
2528/7968 [========>.....................] - ETA: 20s
2624/7968 [========>.....................] - ETA: 19s
2720/7968 [=========>....................] - ETA: 18s
2816/7968 [=========>....................] - ETA: 17s
2912/7968 [=========>....................] - ETA: 16s
3008/7968 [==========>...................] - ETA: 15s
3104/7968 [==========>...................] - ETA: 15s
3200/7968 [===========>..................] - ETA: 14s
3296/7968 [===========>..................] - ETA: 13s
3392/7968 [===========>..................] - ETA: 13s
3488/7968 [============>.................] - ETA: 12s
3584/7968 [============>.................] - ETA: 12s
3680/7968 [============>.................] - ETA: 11s
3776/7968 [=============>................] - ETA: 11s
3872/7968 [=============>................] - ETA: 10s
3968/7968 [=============>................] - ETA: 10s
4064/7968 [==============>...............] - ETA: 9s 
4160/7968 [==============>...............] - ETA: 9s
4256/7968 [===============>..............] - ETA: 9s
4352/7968 [===============>..............] - ETA: 8s
4448/7968 [===============>..............] - ETA: 8s
4544/7968 [================>.............] - ETA: 7s
4640/7968 [================>.............] - ETA: 7s
4736/7968 [================>.............] - ETA: 7s
4832/7968 [=================>............] - ETA: 6s
4928/7968 [=================>............] - ETA: 6s
5024/7968 [=================>............] - ETA: 6s
5120/7968 [==================>...........] - ETA: 6s
5216/7968 [==================>...........] - ETA: 5s
5312/7968 [===================>..........] - ETA: 5s
5408/7968 [===================>..........] - ETA: 5s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 4s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 13s 2ms/step
Best saved model val accuracy:0.845
best saved model auc_score ------------------> 0.928
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_17[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_337 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_337[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_338 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_338[0][0]             
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_145[0][0]            
__________________________________________________________________________________________________
activation_339 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_339[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_340 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_340[0][0]             
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 52, 96, 96)   0           concatenate_145[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_146[0][0]            
__________________________________________________________________________________________________
activation_341 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_341[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_342 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_342[0][0]             
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 70, 96, 96)   0           concatenate_146[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_147[0][0]            
__________________________________________________________________________________________________
activation_343 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_343[0][0]             
__________________________________________________________________________________________________
average_pooling2d_33 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_33[0][0]       
__________________________________________________________________________________________________
activation_344 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_344[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_345 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_345[0][0]             
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_33[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_148[0][0]            
__________________________________________________________________________________________________
activation_346 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_346[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_347 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_347[0][0]             
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 71, 48, 48)   0           concatenate_148[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_149[0][0]            
__________________________________________________________________________________________________
activation_348 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_348[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_349 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_349[0][0]             
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 89, 48, 48)   0           concatenate_149[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_150[0][0]            
__________________________________________________________________________________________________
activation_350 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_350[0][0]             
__________________________________________________________________________________________________
average_pooling2d_34 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_34[0][0]       
__________________________________________________________________________________________________
activation_351 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_351[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_352 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_352[0][0]             
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_34[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_151[0][0]            
__________________________________________________________________________________________________
activation_353 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_353[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_354 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_354[0][0]             
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 80, 24, 24)   0           concatenate_151[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_152[0][0]            
__________________________________________________________________________________________________
activation_355 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_355[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_356 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_356[0][0]             
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 98, 24, 24)   0           concatenate_152[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_153[0][0]            
__________________________________________________________________________________________________
activation_357 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_17 (Gl (None, 98)           0           activation_357[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            99          global_average_pooling2d_17[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 76s - loss: 0.7568 - acc: 0.6551 - val_loss: 0.6922 - val_acc: 0.7351

Epoch 00001: val_loss improved from inf to 0.69224, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 54s - loss: 0.6705 - acc: 0.7519 - val_loss: 0.6485 - val_acc: 0.7589

Epoch 00002: val_loss improved from 0.69224 to 0.64847, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 55s - loss: 0.6407 - acc: 0.7652 - val_loss: 0.6264 - val_acc: 0.7705

Epoch 00003: val_loss improved from 0.64847 to 0.62639, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 54s - loss: 0.6223 - acc: 0.7761 - val_loss: 0.6107 - val_acc: 0.7807

Epoch 00004: val_loss improved from 0.62639 to 0.61070, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 55s - loss: 0.6078 - acc: 0.7838 - val_loss: 0.5975 - val_acc: 0.7869

Epoch 00005: val_loss improved from 0.61070 to 0.59754, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 55s - loss: 0.5975 - acc: 0.7883 - val_loss: 0.5830 - val_acc: 0.7909

Epoch 00006: val_loss improved from 0.59754 to 0.58304, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 55s - loss: 0.5855 - acc: 0.7961 - val_loss: 0.5781 - val_acc: 0.7963

Epoch 00007: val_loss improved from 0.58304 to 0.57808, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 55s - loss: 0.5766 - acc: 0.7999 - val_loss: 0.5634 - val_acc: 0.8023

Epoch 00008: val_loss improved from 0.57808 to 0.56336, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 55s - loss: 0.5678 - acc: 0.8053 - val_loss: 0.5568 - val_acc: 0.8089

Epoch 00009: val_loss improved from 0.56336 to 0.55680, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 54s - loss: 0.5602 - acc: 0.8088 - val_loss: 0.5483 - val_acc: 0.8109

Epoch 00010: val_loss improved from 0.55680 to 0.54827, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 55s - loss: 0.5534 - acc: 0.8134 - val_loss: 0.5486 - val_acc: 0.8104

Epoch 00011: val_loss did not improve from 0.54827
Epoch 12/25
 - 54s - loss: 0.5444 - acc: 0.8150 - val_loss: 0.5347 - val_acc: 0.8192

Epoch 00012: val_loss improved from 0.54827 to 0.53474, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 55s - loss: 0.5405 - acc: 0.8205 - val_loss: 0.5292 - val_acc: 0.8239

Epoch 00013: val_loss improved from 0.53474 to 0.52922, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 55s - loss: 0.5334 - acc: 0.8234 - val_loss: 0.5219 - val_acc: 0.8234

Epoch 00014: val_loss improved from 0.52922 to 0.52188, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 54s - loss: 0.5285 - acc: 0.8246 - val_loss: 0.5248 - val_acc: 0.8234

Epoch 00015: val_loss did not improve from 0.52188
Epoch 16/25
 - 55s - loss: 0.5261 - acc: 0.8250 - val_loss: 0.5206 - val_acc: 0.8234

Epoch 00016: val_loss improved from 0.52188 to 0.52061, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 55s - loss: 0.5175 - acc: 0.8312 - val_loss: 0.5085 - val_acc: 0.8345

Epoch 00017: val_loss improved from 0.52061 to 0.50851, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 54s - loss: 0.5144 - acc: 0.8316 - val_loss: 0.5111 - val_acc: 0.8294

Epoch 00018: val_loss did not improve from 0.50851
Epoch 19/25
 - 54s - loss: 0.5112 - acc: 0.8330 - val_loss: 0.5053 - val_acc: 0.8315

Epoch 00019: val_loss improved from 0.50851 to 0.50534, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 55s - loss: 0.5047 - acc: 0.8355 - val_loss: 0.4991 - val_acc: 0.8351

Epoch 00020: val_loss improved from 0.50534 to 0.49912, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 54s - loss: 0.5000 - acc: 0.8379 - val_loss: 0.4970 - val_acc: 0.8381

Epoch 00021: val_loss improved from 0.49912 to 0.49697, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 55s - loss: 0.4981 - acc: 0.8397 - val_loss: 0.4895 - val_acc: 0.8405

Epoch 00022: val_loss improved from 0.49697 to 0.48946, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 54s - loss: 0.4944 - acc: 0.8425 - val_loss: 0.4899 - val_acc: 0.8436

Epoch 00023: val_loss did not improve from 0.48946
Epoch 24/25
 - 55s - loss: 0.4902 - acc: 0.8442 - val_loss: 0.4931 - val_acc: 0.8422

Epoch 00024: val_loss did not improve from 0.48946
Epoch 25/25
 - 54s - loss: 0.4875 - acc: 0.8445 - val_loss: 0.4810 - val_acc: 0.8476

Epoch 00025: val_loss improved from 0.48946 to 0.48102, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 580us/step
current val accuracy:0.848
current auc_score ------------------> 0.930

  32/7968 [..............................] - ETA: 35:01
 128/7968 [..............................] - ETA: 8:43 
 224/7968 [..............................] - ETA: 4:57
 320/7968 [>.............................] - ETA: 3:26
 416/7968 [>.............................] - ETA: 2:38
 512/7968 [>.............................] - ETA: 2:07
 608/7968 [=>............................] - ETA: 1:46
 704/7968 [=>............................] - ETA: 1:31
 800/7968 [==>...........................] - ETA: 1:20
 896/7968 [==>...........................] - ETA: 1:10
 992/7968 [==>...........................] - ETA: 1:03
1088/7968 [===>..........................] - ETA: 57s 
1184/7968 [===>..........................] - ETA: 52s
1280/7968 [===>..........................] - ETA: 48s
1376/7968 [====>.........................] - ETA: 44s
1472/7968 [====>.........................] - ETA: 41s
1568/7968 [====>.........................] - ETA: 38s
1664/7968 [=====>........................] - ETA: 35s
1760/7968 [=====>........................] - ETA: 33s
1856/7968 [=====>........................] - ETA: 31s
1952/7968 [======>.......................] - ETA: 29s
2048/7968 [======>.......................] - ETA: 27s
2144/7968 [=======>......................] - ETA: 26s
2240/7968 [=======>......................] - ETA: 25s
2336/7968 [=======>......................] - ETA: 23s
2432/7968 [========>.....................] - ETA: 22s
2528/7968 [========>.....................] - ETA: 21s
2624/7968 [========>.....................] - ETA: 20s
2720/7968 [=========>....................] - ETA: 19s
2816/7968 [=========>....................] - ETA: 18s
2912/7968 [=========>....................] - ETA: 17s
3008/7968 [==========>...................] - ETA: 16s
3104/7968 [==========>...................] - ETA: 16s
3200/7968 [===========>..................] - ETA: 15s
3296/7968 [===========>..................] - ETA: 14s
3392/7968 [===========>..................] - ETA: 14s
3488/7968 [============>.................] - ETA: 13s
3584/7968 [============>.................] - ETA: 12s
3680/7968 [============>.................] - ETA: 12s
3776/7968 [=============>................] - ETA: 11s
3872/7968 [=============>................] - ETA: 11s
3968/7968 [=============>................] - ETA: 10s
4064/7968 [==============>...............] - ETA: 10s
4160/7968 [==============>...............] - ETA: 9s 
4256/7968 [===============>..............] - ETA: 9s
4352/7968 [===============>..............] - ETA: 9s
4448/7968 [===============>..............] - ETA: 8s
4544/7968 [================>.............] - ETA: 8s
4640/7968 [================>.............] - ETA: 8s
4736/7968 [================>.............] - ETA: 7s
4832/7968 [=================>............] - ETA: 7s
4928/7968 [=================>............] - ETA: 7s
5024/7968 [=================>............] - ETA: 6s
5120/7968 [==================>...........] - ETA: 6s
5216/7968 [==================>...........] - ETA: 6s
5312/7968 [===================>..........] - ETA: 5s
5408/7968 [===================>..........] - ETA: 5s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 4s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 13s 2ms/step
Best saved model val accuracy:0.848
best saved model auc_score ------------------> 0.930
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_18 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_18[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_358 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_358[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_359 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_359[0][0]             
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_154[0][0]            
__________________________________________________________________________________________________
activation_360 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_360[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_361 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_361[0][0]             
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 52, 96, 96)   0           concatenate_154[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_155[0][0]            
__________________________________________________________________________________________________
activation_362 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_362[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_363 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_363[0][0]             
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 70, 96, 96)   0           concatenate_155[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_156[0][0]            
__________________________________________________________________________________________________
activation_364 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_364[0][0]             
__________________________________________________________________________________________________
average_pooling2d_35 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_35[0][0]       
__________________________________________________________________________________________________
activation_365 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_365[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_366 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_366[0][0]             
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_35[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_157[0][0]            
__________________________________________________________________________________________________
activation_367 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_367[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_368 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_368[0][0]             
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 71, 48, 48)   0           concatenate_157[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_158[0][0]            
__________________________________________________________________________________________________
activation_369 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_369[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_370 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_370[0][0]             
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 89, 48, 48)   0           concatenate_158[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_159[0][0]            
__________________________________________________________________________________________________
activation_371 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_371[0][0]             
__________________________________________________________________________________________________
average_pooling2d_36 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_36[0][0]       
__________________________________________________________________________________________________
activation_372 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_372[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_373 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_373[0][0]             
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_36[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_160[0][0]            
__________________________________________________________________________________________________
activation_374 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_374[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_375 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_375[0][0]             
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 80, 24, 24)   0           concatenate_160[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_161[0][0]            
__________________________________________________________________________________________________
activation_376 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_376[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_377 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_377[0][0]             
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 98, 24, 24)   0           concatenate_161[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_162[0][0]            
__________________________________________________________________________________________________
activation_378 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_18 (Gl (None, 98)           0           activation_378[0][0]             
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 1)            99          global_average_pooling2d_18[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 78s - loss: 0.7319 - acc: 0.6968 - val_loss: 0.6772 - val_acc: 0.7440

Epoch 00001: val_loss improved from inf to 0.67725, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 55s - loss: 0.6584 - acc: 0.7555 - val_loss: 0.6381 - val_acc: 0.7598

Epoch 00002: val_loss improved from 0.67725 to 0.63807, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 55s - loss: 0.6281 - acc: 0.7690 - val_loss: 0.6133 - val_acc: 0.7682

Epoch 00003: val_loss improved from 0.63807 to 0.61332, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 55s - loss: 0.6101 - acc: 0.7801 - val_loss: 0.5939 - val_acc: 0.7816

Epoch 00004: val_loss improved from 0.61332 to 0.59391, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 55s - loss: 0.5948 - acc: 0.7892 - val_loss: 0.5839 - val_acc: 0.7880

Epoch 00005: val_loss improved from 0.59391 to 0.58390, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 55s - loss: 0.5832 - acc: 0.7943 - val_loss: 0.5731 - val_acc: 0.7952

Epoch 00006: val_loss improved from 0.58390 to 0.57311, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 55s - loss: 0.5719 - acc: 0.8008 - val_loss: 0.5608 - val_acc: 0.8033

Epoch 00007: val_loss improved from 0.57311 to 0.56083, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 55s - loss: 0.5627 - acc: 0.8046 - val_loss: 0.5512 - val_acc: 0.8091

Epoch 00008: val_loss improved from 0.56083 to 0.55124, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 55s - loss: 0.5539 - acc: 0.8106 - val_loss: 0.5441 - val_acc: 0.8125

Epoch 00009: val_loss improved from 0.55124 to 0.54408, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 55s - loss: 0.5468 - acc: 0.8140 - val_loss: 0.5325 - val_acc: 0.8175

Epoch 00010: val_loss improved from 0.54408 to 0.53248, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 55s - loss: 0.5398 - acc: 0.8189 - val_loss: 0.5260 - val_acc: 0.8218

Epoch 00011: val_loss improved from 0.53248 to 0.52596, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 55s - loss: 0.5326 - acc: 0.8234 - val_loss: 0.5210 - val_acc: 0.8263

Epoch 00012: val_loss improved from 0.52596 to 0.52098, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 55s - loss: 0.5282 - acc: 0.8249 - val_loss: 0.5436 - val_acc: 0.8089

Epoch 00013: val_loss did not improve from 0.52098
Epoch 14/25
 - 55s - loss: 0.5235 - acc: 0.8265 - val_loss: 0.5112 - val_acc: 0.8326

Epoch 00014: val_loss improved from 0.52098 to 0.51121, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 55s - loss: 0.5160 - acc: 0.8311 - val_loss: 0.5170 - val_acc: 0.8276

Epoch 00015: val_loss did not improve from 0.51121
Epoch 16/25
 - 55s - loss: 0.5134 - acc: 0.8317 - val_loss: 0.5010 - val_acc: 0.8385

Epoch 00016: val_loss improved from 0.51121 to 0.50096, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 55s - loss: 0.5065 - acc: 0.8352 - val_loss: 0.4985 - val_acc: 0.8405

Epoch 00017: val_loss improved from 0.50096 to 0.49846, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 55s - loss: 0.5021 - acc: 0.8371 - val_loss: 0.4950 - val_acc: 0.8422

Epoch 00018: val_loss improved from 0.49846 to 0.49504, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 55s - loss: 0.4991 - acc: 0.8393 - val_loss: 0.4882 - val_acc: 0.8446

Epoch 00019: val_loss improved from 0.49504 to 0.48824, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 54s - loss: 0.4954 - acc: 0.8409 - val_loss: 0.4996 - val_acc: 0.8376

Epoch 00020: val_loss did not improve from 0.48824
Epoch 21/25
 - 55s - loss: 0.4937 - acc: 0.8419 - val_loss: 0.4873 - val_acc: 0.8509

Epoch 00021: val_loss improved from 0.48824 to 0.48728, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 55s - loss: 0.4868 - acc: 0.8454 - val_loss: 0.4996 - val_acc: 0.8414

Epoch 00022: val_loss did not improve from 0.48728
Epoch 23/25
 - 55s - loss: 0.4854 - acc: 0.8462 - val_loss: 0.4816 - val_acc: 0.8469

Epoch 00023: val_loss improved from 0.48728 to 0.48163, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 54s - loss: 0.4807 - acc: 0.8503 - val_loss: 0.4791 - val_acc: 0.8502

Epoch 00024: val_loss improved from 0.48163 to 0.47910, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 55s - loss: 0.4788 - acc: 0.8496 - val_loss: 0.4706 - val_acc: 0.8568

Epoch 00025: val_loss improved from 0.47910 to 0.47059, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 4s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 582us/step
current val accuracy:0.857
current auc_score ------------------> 0.934

  32/7968 [..............................] - ETA: 39:40
 128/7968 [..............................] - ETA: 9:52 
 224/7968 [..............................] - ETA: 5:36
 320/7968 [>.............................] - ETA: 3:53
 416/7968 [>.............................] - ETA: 2:58
 512/7968 [>.............................] - ETA: 2:24
 608/7968 [=>............................] - ETA: 2:00
 704/7968 [=>............................] - ETA: 1:43
 800/7968 [==>...........................] - ETA: 1:30
 896/7968 [==>...........................] - ETA: 1:19
 992/7968 [==>...........................] - ETA: 1:11
1088/7968 [===>..........................] - ETA: 1:04
1184/7968 [===>..........................] - ETA: 58s 
1280/7968 [===>..........................] - ETA: 54s
1376/7968 [====>.........................] - ETA: 49s
1472/7968 [====>.........................] - ETA: 46s
1568/7968 [====>.........................] - ETA: 42s
1664/7968 [=====>........................] - ETA: 40s
1760/7968 [=====>........................] - ETA: 37s
1856/7968 [=====>........................] - ETA: 35s
1952/7968 [======>.......................] - ETA: 33s
2048/7968 [======>.......................] - ETA: 31s
2144/7968 [=======>......................] - ETA: 29s
2240/7968 [=======>......................] - ETA: 27s
2336/7968 [=======>......................] - ETA: 26s
2432/7968 [========>.....................] - ETA: 25s
2528/7968 [========>.....................] - ETA: 23s
2624/7968 [========>.....................] - ETA: 22s
2720/7968 [=========>....................] - ETA: 21s
2816/7968 [=========>....................] - ETA: 20s
2912/7968 [=========>....................] - ETA: 19s
3008/7968 [==========>...................] - ETA: 18s
3104/7968 [==========>...................] - ETA: 17s
3200/7968 [===========>..................] - ETA: 17s
3296/7968 [===========>..................] - ETA: 16s
3392/7968 [===========>..................] - ETA: 15s
3488/7968 [============>.................] - ETA: 14s
3584/7968 [============>.................] - ETA: 14s
3680/7968 [============>.................] - ETA: 13s
3776/7968 [=============>................] - ETA: 13s
3872/7968 [=============>................] - ETA: 12s
3968/7968 [=============>................] - ETA: 11s
4064/7968 [==============>...............] - ETA: 11s
4160/7968 [==============>...............] - ETA: 10s
4256/7968 [===============>..............] - ETA: 10s
4352/7968 [===============>..............] - ETA: 10s
4448/7968 [===============>..............] - ETA: 9s 
4544/7968 [================>.............] - ETA: 9s
4640/7968 [================>.............] - ETA: 8s
4736/7968 [================>.............] - ETA: 8s
4832/7968 [=================>............] - ETA: 8s
4928/7968 [=================>............] - ETA: 7s
5024/7968 [=================>............] - ETA: 7s
5120/7968 [==================>...........] - ETA: 6s
5216/7968 [==================>...........] - ETA: 6s
5312/7968 [===================>..........] - ETA: 6s
5408/7968 [===================>..........] - ETA: 6s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 5s
5696/7968 [====================>.........] - ETA: 5s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 4s
6080/7968 [=====================>........] - ETA: 4s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 3s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 2s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 1s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 14s 2ms/step
Best saved model val accuracy:0.857
best saved model auc_score ------------------> 0.934
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_19[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_379 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_379[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_380 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_380[0][0]             
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_163[0][0]            
__________________________________________________________________________________________________
activation_381 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_381[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_382 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_382[0][0]             
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 52, 96, 96)   0           concatenate_163[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_164[0][0]            
__________________________________________________________________________________________________
activation_383 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_383[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_384 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_384[0][0]             
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 70, 96, 96)   0           concatenate_164[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_165[0][0]            
__________________________________________________________________________________________________
activation_385 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_385[0][0]             
__________________________________________________________________________________________________
average_pooling2d_37 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_37[0][0]       
__________________________________________________________________________________________________
activation_386 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_386[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_387 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_387[0][0]             
__________________________________________________________________________________________________
concatenate_166 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_37[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_166[0][0]            
__________________________________________________________________________________________________
activation_388 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_388[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_389 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_389[0][0]             
__________________________________________________________________________________________________
concatenate_167 (Concatenate)   (None, 71, 48, 48)   0           concatenate_166[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_167[0][0]            
__________________________________________________________________________________________________
activation_390 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_390[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_391 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_391[0][0]             
__________________________________________________________________________________________________
concatenate_168 (Concatenate)   (None, 89, 48, 48)   0           concatenate_167[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_168[0][0]            
__________________________________________________________________________________________________
activation_392 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_392[0][0]             
__________________________________________________________________________________________________
average_pooling2d_38 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_38[0][0]       
__________________________________________________________________________________________________
activation_393 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_393[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_394 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_394[0][0]             
__________________________________________________________________________________________________
concatenate_169 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_38[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_169[0][0]            
__________________________________________________________________________________________________
activation_395 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_395[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_396 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_396[0][0]             
__________________________________________________________________________________________________
concatenate_170 (Concatenate)   (None, 80, 24, 24)   0           concatenate_169[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_170[0][0]            
__________________________________________________________________________________________________
activation_397 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_397[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_398 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_398[0][0]             
__________________________________________________________________________________________________
concatenate_171 (Concatenate)   (None, 98, 24, 24)   0           concatenate_170[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_171[0][0]            
__________________________________________________________________________________________________
activation_399 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_19 (Gl (None, 98)           0           activation_399[0][0]             
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 1)            99          global_average_pooling2d_19[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 82s - loss: 0.6668 - acc: 0.7507 - val_loss: 0.6175 - val_acc: 0.7716

Epoch 00001: val_loss improved from inf to 0.61747, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 57s - loss: 0.6065 - acc: 0.7807 - val_loss: 0.6078 - val_acc: 0.7801

Epoch 00002: val_loss improved from 0.61747 to 0.60782, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 57s - loss: 0.5807 - acc: 0.7944 - val_loss: 0.5638 - val_acc: 0.8001

Epoch 00003: val_loss improved from 0.60782 to 0.56376, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 57s - loss: 0.5614 - acc: 0.8040 - val_loss: 0.5495 - val_acc: 0.8091

Epoch 00004: val_loss improved from 0.56376 to 0.54954, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 57s - loss: 0.5475 - acc: 0.8113 - val_loss: 0.5334 - val_acc: 0.8180

Epoch 00005: val_loss improved from 0.54954 to 0.53337, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 57s - loss: 0.5354 - acc: 0.8174 - val_loss: 0.5212 - val_acc: 0.8219

Epoch 00006: val_loss improved from 0.53337 to 0.52125, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 57s - loss: 0.5234 - acc: 0.8247 - val_loss: 0.5164 - val_acc: 0.8238

Epoch 00007: val_loss improved from 0.52125 to 0.51639, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 57s - loss: 0.5152 - acc: 0.8273 - val_loss: 0.5037 - val_acc: 0.8326

Epoch 00008: val_loss improved from 0.51639 to 0.50367, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 57s - loss: 0.5054 - acc: 0.8328 - val_loss: 0.4947 - val_acc: 0.8351

Epoch 00009: val_loss improved from 0.50367 to 0.49468, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 10/25
 - 57s - loss: 0.4992 - acc: 0.8364 - val_loss: 0.4867 - val_acc: 0.8440

Epoch 00010: val_loss improved from 0.49468 to 0.48673, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 57s - loss: 0.4907 - acc: 0.8425 - val_loss: 0.4931 - val_acc: 0.8375

Epoch 00011: val_loss did not improve from 0.48673
Epoch 12/25
 - 57s - loss: 0.4833 - acc: 0.8468 - val_loss: 0.5001 - val_acc: 0.8348

Epoch 00012: val_loss did not improve from 0.48673
Epoch 13/25
 - 57s - loss: 0.4774 - acc: 0.8505 - val_loss: 0.4767 - val_acc: 0.8568

Epoch 00013: val_loss improved from 0.48673 to 0.47665, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 57s - loss: 0.4702 - acc: 0.8534 - val_loss: 0.4616 - val_acc: 0.8608

Epoch 00014: val_loss improved from 0.47665 to 0.46159, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 15/25
 - 57s - loss: 0.4625 - acc: 0.8562 - val_loss: 0.4623 - val_acc: 0.8578

Epoch 00015: val_loss did not improve from 0.46159
Epoch 16/25
 - 57s - loss: 0.4584 - acc: 0.8600 - val_loss: 0.4520 - val_acc: 0.8658

Epoch 00016: val_loss improved from 0.46159 to 0.45195, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 57s - loss: 0.4523 - acc: 0.8618 - val_loss: 0.4464 - val_acc: 0.8635

Epoch 00017: val_loss improved from 0.45195 to 0.44642, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 57s - loss: 0.4480 - acc: 0.8649 - val_loss: 0.4447 - val_acc: 0.8709

Epoch 00018: val_loss improved from 0.44642 to 0.44466, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 57s - loss: 0.4422 - acc: 0.8689 - val_loss: 0.4348 - val_acc: 0.8725

Epoch 00019: val_loss improved from 0.44466 to 0.43477, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 20/25
 - 57s - loss: 0.4375 - acc: 0.8717 - val_loss: 0.4443 - val_acc: 0.8686

Epoch 00020: val_loss did not improve from 0.43477
Epoch 21/25
 - 57s - loss: 0.4328 - acc: 0.8737 - val_loss: 0.4293 - val_acc: 0.8774

Epoch 00021: val_loss improved from 0.43477 to 0.42933, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 57s - loss: 0.4306 - acc: 0.8748 - val_loss: 0.4299 - val_acc: 0.8768

Epoch 00022: val_loss did not improve from 0.42933
Epoch 23/25
 - 57s - loss: 0.4210 - acc: 0.8799 - val_loss: 0.4172 - val_acc: 0.8805

Epoch 00023: val_loss improved from 0.42933 to 0.41716, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 57s - loss: 0.4199 - acc: 0.8788 - val_loss: 0.4258 - val_acc: 0.8775

Epoch 00024: val_loss did not improve from 0.41716
Epoch 25/25
 - 57s - loss: 0.4132 - acc: 0.8830 - val_loss: 0.4153 - val_acc: 0.8869

Epoch 00025: val_loss improved from 0.41716 to 0.41529, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 4s
1184/7968 [===>..........................] - ETA: 4s
1280/7968 [===>..........................] - ETA: 4s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 3s
2912/7968 [=========>....................] - ETA: 3s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 2s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 597us/step
current val accuracy:0.887
current auc_score ------------------> 0.955

  32/7968 [..............................] - ETA: 41:26
 128/7968 [..............................] - ETA: 10:18
 224/7968 [..............................] - ETA: 5:50 
 320/7968 [>.............................] - ETA: 4:04
 416/7968 [>.............................] - ETA: 3:06
 512/7968 [>.............................] - ETA: 2:30
 608/7968 [=>............................] - ETA: 2:05
 704/7968 [=>............................] - ETA: 1:47
 800/7968 [==>...........................] - ETA: 1:34
 896/7968 [==>...........................] - ETA: 1:23
 992/7968 [==>...........................] - ETA: 1:14
1088/7968 [===>..........................] - ETA: 1:07
1184/7968 [===>..........................] - ETA: 1:01
1280/7968 [===>..........................] - ETA: 56s 
1376/7968 [====>.........................] - ETA: 51s
1472/7968 [====>.........................] - ETA: 48s
1568/7968 [====>.........................] - ETA: 44s
1664/7968 [=====>........................] - ETA: 41s
1760/7968 [=====>........................] - ETA: 39s
1856/7968 [=====>........................] - ETA: 36s
1952/7968 [======>.......................] - ETA: 34s
2048/7968 [======>.......................] - ETA: 32s
2144/7968 [=======>......................] - ETA: 30s
2240/7968 [=======>......................] - ETA: 29s
2336/7968 [=======>......................] - ETA: 27s
2432/7968 [========>.....................] - ETA: 26s
2528/7968 [========>.....................] - ETA: 24s
2624/7968 [========>.....................] - ETA: 23s
2720/7968 [=========>....................] - ETA: 22s
2816/7968 [=========>....................] - ETA: 21s
2912/7968 [=========>....................] - ETA: 20s
3008/7968 [==========>...................] - ETA: 19s
3104/7968 [==========>...................] - ETA: 18s
3200/7968 [===========>..................] - ETA: 17s
3296/7968 [===========>..................] - ETA: 16s
3392/7968 [===========>..................] - ETA: 16s
3488/7968 [============>.................] - ETA: 15s
3584/7968 [============>.................] - ETA: 14s
3680/7968 [============>.................] - ETA: 14s
3776/7968 [=============>................] - ETA: 13s
3872/7968 [=============>................] - ETA: 13s
3968/7968 [=============>................] - ETA: 12s
4064/7968 [==============>...............] - ETA: 11s
4160/7968 [==============>...............] - ETA: 11s
4256/7968 [===============>..............] - ETA: 10s
4352/7968 [===============>..............] - ETA: 10s
4448/7968 [===============>..............] - ETA: 10s
4544/7968 [================>.............] - ETA: 9s 
4640/7968 [================>.............] - ETA: 9s
4736/7968 [================>.............] - ETA: 8s
4832/7968 [=================>............] - ETA: 8s
4928/7968 [=================>............] - ETA: 7s
5024/7968 [=================>............] - ETA: 7s
5120/7968 [==================>...........] - ETA: 7s
5216/7968 [==================>...........] - ETA: 6s
5312/7968 [===================>..........] - ETA: 6s
5408/7968 [===================>..........] - ETA: 6s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 5s
5696/7968 [====================>.........] - ETA: 5s
5792/7968 [====================>.........] - ETA: 5s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 4s
6080/7968 [=====================>........] - ETA: 4s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 3s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 2s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 1s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 15s 2ms/step
Best saved model val accuracy:0.887
best saved model auc_score ------------------> 0.955
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_20 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_20[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_400 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_400[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_401 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_401[0][0]             
__________________________________________________________________________________________________
concatenate_172 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_172[0][0]            
__________________________________________________________________________________________________
activation_402 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_402[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_403 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_403[0][0]             
__________________________________________________________________________________________________
concatenate_173 (Concatenate)   (None, 52, 96, 96)   0           concatenate_172[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_173[0][0]            
__________________________________________________________________________________________________
activation_404 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_404[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_405 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_405[0][0]             
__________________________________________________________________________________________________
concatenate_174 (Concatenate)   (None, 70, 96, 96)   0           concatenate_173[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_174[0][0]            
__________________________________________________________________________________________________
activation_406 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_406[0][0]             
__________________________________________________________________________________________________
average_pooling2d_39 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_39[0][0]       
__________________________________________________________________________________________________
activation_407 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_407[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_408 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_408[0][0]             
__________________________________________________________________________________________________
concatenate_175 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_39[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_175[0][0]            
__________________________________________________________________________________________________
activation_409 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_409[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_410 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_410[0][0]             
__________________________________________________________________________________________________
concatenate_176 (Concatenate)   (None, 71, 48, 48)   0           concatenate_175[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_176[0][0]            
__________________________________________________________________________________________________
activation_411 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_411[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_412 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_412[0][0]             
__________________________________________________________________________________________________
concatenate_177 (Concatenate)   (None, 89, 48, 48)   0           concatenate_176[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_177[0][0]            
__________________________________________________________________________________________________
activation_413 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_413[0][0]             
__________________________________________________________________________________________________
average_pooling2d_40 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_40[0][0]       
__________________________________________________________________________________________________
activation_414 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_414[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_415 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_415[0][0]             
__________________________________________________________________________________________________
concatenate_178 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_40[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_178[0][0]            
__________________________________________________________________________________________________
activation_416 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_416[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_417 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_417[0][0]             
__________________________________________________________________________________________________
concatenate_179 (Concatenate)   (None, 80, 24, 24)   0           concatenate_178[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_179[0][0]            
__________________________________________________________________________________________________
activation_418 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_418[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_419 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_419[0][0]             
__________________________________________________________________________________________________
concatenate_180 (Concatenate)   (None, 98, 24, 24)   0           concatenate_179[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_180[0][0]            
__________________________________________________________________________________________________
activation_420 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_20 (Gl (None, 98)           0           activation_420[0][0]             
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 1)            99          global_average_pooling2d_20[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/25
 - 81s - loss: 0.7211 - acc: 0.7331 - val_loss: 0.6728 - val_acc: 0.7531

Epoch 00001: val_loss improved from inf to 0.67276, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 2/25
 - 55s - loss: 0.6602 - acc: 0.7579 - val_loss: 0.6420 - val_acc: 0.7636

Epoch 00002: val_loss improved from 0.67276 to 0.64199, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 3/25
 - 55s - loss: 0.6345 - acc: 0.7658 - val_loss: 0.6166 - val_acc: 0.7765

Epoch 00003: val_loss improved from 0.64199 to 0.61663, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 4/25
 - 55s - loss: 0.6143 - acc: 0.7783 - val_loss: 0.5992 - val_acc: 0.7806

Epoch 00004: val_loss improved from 0.61663 to 0.59924, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 5/25
 - 55s - loss: 0.5979 - acc: 0.7849 - val_loss: 0.5856 - val_acc: 0.7938

Epoch 00005: val_loss improved from 0.59924 to 0.58559, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 6/25
 - 55s - loss: 0.5833 - acc: 0.7941 - val_loss: 0.5689 - val_acc: 0.7981

Epoch 00006: val_loss improved from 0.58559 to 0.56890, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 7/25
 - 55s - loss: 0.5745 - acc: 0.7991 - val_loss: 0.5584 - val_acc: 0.8036

Epoch 00007: val_loss improved from 0.56890 to 0.55839, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 8/25
 - 55s - loss: 0.5626 - acc: 0.8049 - val_loss: 0.5538 - val_acc: 0.8052

Epoch 00008: val_loss improved from 0.55839 to 0.55379, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 9/25
 - 55s - loss: 0.5545 - acc: 0.8095 - val_loss: 0.5553 - val_acc: 0.8075

Epoch 00009: val_loss did not improve from 0.55379
Epoch 10/25
 - 55s - loss: 0.5483 - acc: 0.8143 - val_loss: 0.5405 - val_acc: 0.8130

Epoch 00010: val_loss improved from 0.55379 to 0.54054, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 11/25
 - 55s - loss: 0.5394 - acc: 0.8168 - val_loss: 0.5386 - val_acc: 0.8170

Epoch 00011: val_loss improved from 0.54054 to 0.53855, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 12/25
 - 55s - loss: 0.5330 - acc: 0.8205 - val_loss: 0.5256 - val_acc: 0.8228

Epoch 00012: val_loss improved from 0.53855 to 0.52560, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 13/25
 - 55s - loss: 0.5285 - acc: 0.8234 - val_loss: 0.5187 - val_acc: 0.8252

Epoch 00013: val_loss improved from 0.52560 to 0.51866, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 14/25
 - 55s - loss: 0.5236 - acc: 0.8270 - val_loss: 0.5487 - val_acc: 0.8087

Epoch 00014: val_loss did not improve from 0.51866
Epoch 15/25
 - 55s - loss: 0.5183 - acc: 0.8293 - val_loss: 0.5010 - val_acc: 0.8384

Epoch 00015: val_loss improved from 0.51866 to 0.50095, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 16/25
 - 55s - loss: 0.5125 - acc: 0.8317 - val_loss: 0.4998 - val_acc: 0.8410

Epoch 00016: val_loss improved from 0.50095 to 0.49975, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 17/25
 - 55s - loss: 0.5070 - acc: 0.8375 - val_loss: 0.4942 - val_acc: 0.8411

Epoch 00017: val_loss improved from 0.49975 to 0.49420, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 18/25
 - 55s - loss: 0.5042 - acc: 0.8393 - val_loss: 0.4917 - val_acc: 0.8419

Epoch 00018: val_loss improved from 0.49420 to 0.49172, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 19/25
 - 55s - loss: 0.4983 - acc: 0.8416 - val_loss: 0.4948 - val_acc: 0.8464

Epoch 00019: val_loss did not improve from 0.49172
Epoch 20/25
 - 55s - loss: 0.4927 - acc: 0.8434 - val_loss: 0.4872 - val_acc: 0.8458

Epoch 00020: val_loss improved from 0.49172 to 0.48723, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 21/25
 - 55s - loss: 0.4897 - acc: 0.8445 - val_loss: 0.4802 - val_acc: 0.8503

Epoch 00021: val_loss improved from 0.48723 to 0.48019, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 22/25
 - 55s - loss: 0.4861 - acc: 0.8473 - val_loss: 0.4764 - val_acc: 0.8490

Epoch 00022: val_loss improved from 0.48019 to 0.47636, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 23/25
 - 55s - loss: 0.4814 - acc: 0.8508 - val_loss: 0.4706 - val_acc: 0.8548

Epoch 00023: val_loss improved from 0.47636 to 0.47063, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 24/25
 - 55s - loss: 0.4786 - acc: 0.8511 - val_loss: 0.4703 - val_acc: 0.8574

Epoch 00024: val_loss improved from 0.47063 to 0.47028, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5
Epoch 25/25
 - 55s - loss: 0.4756 - acc: 0.8528 - val_loss: 0.4677 - val_acc: 0.8557

Epoch 00025: val_loss improved from 0.47028 to 0.46771, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1425.h5

  32/7968 [..............................] - ETA: 5s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 4s
1184/7968 [===>..........................] - ETA: 4s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 3s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 2s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 588us/step
current val accuracy:0.856
current auc_score ------------------> 0.938

  32/7968 [..............................] - ETA: 45:44
  96/7968 [..............................] - ETA: 15:11
 192/7968 [..............................] - ETA: 7:32 
 288/7968 [>.............................] - ETA: 4:59
 384/7968 [>.............................] - ETA: 3:43
 480/7968 [>.............................] - ETA: 2:57
 576/7968 [=>............................] - ETA: 2:26
 672/7968 [=>............................] - ETA: 2:04
 768/7968 [=>............................] - ETA: 1:48
 864/7968 [==>...........................] - ETA: 1:35
 960/7968 [==>...........................] - ETA: 1:24
1056/7968 [==>...........................] - ETA: 1:16
1152/7968 [===>..........................] - ETA: 1:09
1248/7968 [===>..........................] - ETA: 1:03
1344/7968 [====>.........................] - ETA: 58s 
1440/7968 [====>.........................] - ETA: 54s
1536/7968 [====>.........................] - ETA: 50s
1632/7968 [=====>........................] - ETA: 46s
1728/7968 [=====>........................] - ETA: 43s
1824/7968 [=====>........................] - ETA: 40s
1920/7968 [======>.......................] - ETA: 38s
2016/7968 [======>.......................] - ETA: 36s
2112/7968 [======>.......................] - ETA: 34s
2208/7968 [=======>......................] - ETA: 32s
2304/7968 [=======>......................] - ETA: 30s
2400/7968 [========>.....................] - ETA: 29s
2496/7968 [========>.....................] - ETA: 27s
2592/7968 [========>.....................] - ETA: 26s
2688/7968 [=========>....................] - ETA: 24s
2784/7968 [=========>....................] - ETA: 23s
2880/7968 [=========>....................] - ETA: 22s
2976/7968 [==========>...................] - ETA: 21s
3072/7968 [==========>...................] - ETA: 20s
3168/7968 [==========>...................] - ETA: 19s
3264/7968 [===========>..................] - ETA: 18s
3360/7968 [===========>..................] - ETA: 17s
3456/7968 [============>.................] - ETA: 17s
3552/7968 [============>.................] - ETA: 16s
3648/7968 [============>.................] - ETA: 15s
3744/7968 [=============>................] - ETA: 15s
3840/7968 [=============>................] - ETA: 14s
3936/7968 [=============>................] - ETA: 13s
4032/7968 [==============>...............] - ETA: 13s
4128/7968 [==============>...............] - ETA: 12s
4224/7968 [==============>...............] - ETA: 12s
4320/7968 [===============>..............] - ETA: 11s
4416/7968 [===============>..............] - ETA: 11s
4512/7968 [===============>..............] - ETA: 10s
4608/7968 [================>.............] - ETA: 10s
4704/7968 [================>.............] - ETA: 9s 
4800/7968 [=================>............] - ETA: 9s
4896/7968 [=================>............] - ETA: 8s
4992/7968 [=================>............] - ETA: 8s
5088/7968 [==================>...........] - ETA: 7s
5184/7968 [==================>...........] - ETA: 7s
5280/7968 [==================>...........] - ETA: 7s
5376/7968 [===================>..........] - ETA: 6s
5472/7968 [===================>..........] - ETA: 6s
5568/7968 [===================>..........] - ETA: 6s
5664/7968 [====================>.........] - ETA: 5s
5760/7968 [====================>.........] - ETA: 5s
5856/7968 [=====================>........] - ETA: 5s
5952/7968 [=====================>........] - ETA: 4s
6048/7968 [=====================>........] - ETA: 4s
6144/7968 [======================>.......] - ETA: 4s
6240/7968 [======================>.......] - ETA: 4s
6336/7968 [======================>.......] - ETA: 3s
6432/7968 [=======================>......] - ETA: 3s
6528/7968 [=======================>......] - ETA: 3s
6624/7968 [=======================>......] - ETA: 3s
6720/7968 [========================>.....] - ETA: 2s
6816/7968 [========================>.....] - ETA: 2s
6912/7968 [=========================>....] - ETA: 2s
7008/7968 [=========================>....] - ETA: 2s
7104/7968 [=========================>....] - ETA: 1s
7200/7968 [==========================>...] - ETA: 1s
7296/7968 [==========================>...] - ETA: 1s
7392/7968 [==========================>...] - ETA: 1s
7488/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7776/7968 [============================>.] - ETA: 0s
7872/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 16s 2ms/step
Best saved model val accuracy:0.856
best saved model auc_score ------------------> 0.938
best model <keras.engine.training.Model object at 0x7fd78e11d8d0>
best run {'opt': 4}
Evalutation of best performing model:
[0.6689823958181565, 0.796236559139785]
TEST roc_auc_score 0.875
----------trials-------------
{'opt': [1]} -0.9687294284991335
{'opt': [4]} -0.996233556660655
{'opt': [2]} -0.9953784374883439
{'opt': [1]} -0.9945419051238599
{'opt': [3]} -0.9953882664443474
{'opt': [2]} -0.9924180315474206
{'opt': [3]} -0.9828145114706438
{'opt': [2]} -0.960499883060626
{'opt': [4]} -0.9810295793610071
{'opt': [0]} -0.9750655137721316
{'opt': [1]} -0.9587484387081425
{'opt': [1]} -0.934975092417389
{'opt': [3]} -0.9378175823893335
{'opt': [0]} -0.9910747093905342
{'opt': [1]} -0.9303659106099293
{'opt': [1]} -0.9282371855616164
{'opt': [1]} -0.9297937519594907
{'opt': [3]} -0.9341690550189672
{'opt': [4]} -0.9549919503370576
{'opt': [1]} -0.9380501695116873
no of  evals 20
>>> Imports:
#coding=utf-8

try:
    import numpy as np
except:
    pass

try:
    import random
except:
    pass

try:
    import h5py
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    import matplotlib.pyplot as plt
except:
    pass

try:
    import keras.backend as K
except:
    pass

try:
    from keras.initializers import RandomNormal
except:
    pass

try:
    from keras.layers.core import Dense, Dropout, Activation
except:
    pass

try:
    from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D
except:
    pass

try:
    from keras.layers import Input, concatenate, Concatenate
except:
    pass

try:
    from keras.layers import normalization, BatchNormalization, Lambda
except:
    pass

try:
    from keras.layers import Flatten, Conv2D, MaxPooling2D
except:
    pass

try:
    from keras.regularizers import l2
except:
    pass

try:
    from keras.optimizers import RMSprop, Adam, Adadelta, Adamax, Nadam
except:
    pass

try:
    from keras.utils import np_utils
except:
    pass

try:
    from keras.models import Model, Sequential
except:
    pass

try:
    from keras.models import model_from_json
except:
    pass

try:
    from keras.models import load_model
except:
    pass

try:
    import keras.initializers
except:
    pass

try:
    from keras.losses import binary_crossentropy
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
except:
    pass

try:
    from keras.utils import multi_gpu_model
except:
    pass

try:
    from sklearn.metrics import accuracy_score
except:
    pass

try:
    from sklearn.metrics import roc_auc_score
except:
    pass

try:
    from sklearn.model_selection import train_test_split
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    from hyperas.distributions import choice, uniform
except:
    pass

try:
    from keras_contrib.applications import DenseNet
except:
    pass

try:
    import argparse
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
        'opt': hp.choice('opt', [Adam(lr=1E-3), RMSprop(lr=1E-3),Adadelta(),Adamax(lr=1E-3),Nadam()]),
    }

>>> Functions
  1: def process_data():
  2:     random_seed = 7
  3: 
  4:     f = h5py.File('/home/amalli2s/thesis/keras/matchedImagesSplitClasses-2017-02-24-17-39-44-96-96-split-val0.15-tr0.7-tst0.15.hdf5','r')
  5:     X_train = f['X_train'].value
  6:     y_train = f['y_train'].value
  7:     X_test = f['X_val'].value
  8:     y_test = f['y_val'].value
  9:     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed)
 10:  
 11:     return X_train,y_train,X_val,y_val,X_test,y_test
 12: 
 13: 
>>> Data
 1: 
 2: X_train,y_train,X_val,y_val,X_test,y_test = process_data()
 3: 
 4: 
 5: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     epochs = 30
   4:     es_patience = 5
   5:     lr_patience = 3
   6:     dropout = None
   7:     depth = 25
   8:     nb_dense_block = 3
   9:     nb_filter = 16
  10:     growth_rate = 18
  11:     bn = True
  12:     reduction_ = 0.5
  13:     bs = 32
  14:     lr = 1E-3
  15:     opt = space['opt']
  16:     weight_file = 'hyperas_dn_lr_optimizer_wt_3Oct_1543.h5'
  17:     nb_classes = 1
  18:     img_dim = (2,96,96) 
  19:     n_channels = 2 
  20: 
  21:     
  22:     model  = DenseNet(depth=depth, nb_dense_block=nb_dense_block,
  23:                  growth_rate=growth_rate, nb_filter=nb_filter,
  24:                  dropout_rate=dropout,activation='sigmoid',
  25:                  input_shape=img_dim,include_top=True,
  26:                  bottleneck=bn,reduction=reduction_,
  27:                  classes=nb_classes,pooling='avg',
  28:                  weights=None)
  29:     
  30:     model.summary()
  31:     model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])
  32: 
  33:     es = EarlyStopping(monitor='val_loss', patience=es_patience,verbose=1)
  34:     checkpointer = ModelCheckpoint(filepath=weight_file,verbose=1, save_best_only=True)
  35: 
  36:     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=lr_patience, min_lr=0.5e-6,verbose=1)
  37: 
  38:     model.fit(X_train,y_train,
  39:           batch_size=bs,
  40:           epochs=epochs,
  41:           callbacks=[lr_reducer,checkpointer,es],
  42:           validation_data=(X_val,y_val),
  43:           verbose=2)
  44:     
  45:     score, acc = model.evaluate(X_val,y_val)
  46:     print("current val accuracy:%0.3f"%acc)
  47:     pred = model.predict(X_val)
  48:     auc_score = roc_auc_score(y_val,pred)
  49:     print("current auc_score ------------------> %0.3f"%auc_score)
  50: 
  51:     model = load_model(weight_file) #This is the best model
  52:     score, acc = model.evaluate(X_val,y_val)
  53:     print("Best saved model val accuracy:%0.3f"% acc)
  54:     pred = model.predict(X_val)
  55:     auc_score = roc_auc_score(y_val,pred)
  56:     print("best saved model auc_score ------------------> %0.3f"%auc_score)
  57: 
  58:     
  59:     return {'loss': -auc_score, 'status': STATUS_OK, 'model': model}  
  60: 
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_1[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_1[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_3[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_4[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 52, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_5[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 70, 96, 96)   0           concatenate_2[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_7[0][0]               
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_8[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 53, 48, 48)   0           average_pooling2d_1[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_10[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 71, 48, 48)   0           concatenate_4[0][0]              
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_12[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_13[0][0]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 89, 48, 48)   0           concatenate_5[0][0]              
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_14[0][0]              
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_15[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_16[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 62, 24, 24)   0           average_pooling2d_2[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_17[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 80, 24, 24)   0           concatenate_7[0][0]              
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_19[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_20[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 98, 24, 24)   0           concatenate_8[0][0]              
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 98)           0           activation_21[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            99          global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 54s - loss: 0.5514 - acc: 0.7875 - val_loss: 0.5029 - val_acc: 0.8136

Epoch 00001: val_loss improved from inf to 0.50288, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 50s - loss: 0.4305 - acc: 0.8429 - val_loss: 0.4917 - val_acc: 0.8223

Epoch 00002: val_loss improved from 0.50288 to 0.49166, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 50s - loss: 0.3719 - acc: 0.8700 - val_loss: 0.5661 - val_acc: 0.8038

Epoch 00003: val_loss did not improve from 0.49166
Epoch 4/30
 - 50s - loss: 0.3301 - acc: 0.8880 - val_loss: 0.3347 - val_acc: 0.8968

Epoch 00004: val_loss improved from 0.49166 to 0.33467, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 50s - loss: 0.2999 - acc: 0.9011 - val_loss: 0.3756 - val_acc: 0.8729

Epoch 00005: val_loss did not improve from 0.33467
Epoch 6/30
 - 50s - loss: 0.2758 - acc: 0.9097 - val_loss: 0.4207 - val_acc: 0.8678

Epoch 00006: val_loss did not improve from 0.33467
Epoch 7/30
 - 50s - loss: 0.2543 - acc: 0.9192 - val_loss: 0.3176 - val_acc: 0.8850

Epoch 00007: val_loss improved from 0.33467 to 0.31755, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 50s - loss: 0.2398 - acc: 0.9264 - val_loss: 0.3695 - val_acc: 0.8710

Epoch 00008: val_loss did not improve from 0.31755
Epoch 9/30
 - 50s - loss: 0.2255 - acc: 0.9301 - val_loss: 0.6136 - val_acc: 0.7810

Epoch 00009: val_loss did not improve from 0.31755
Epoch 10/30
 - 50s - loss: 0.2093 - acc: 0.9368 - val_loss: 0.3453 - val_acc: 0.8789

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.

Epoch 00010: val_loss did not improve from 0.31755
Epoch 11/30
 - 50s - loss: 0.1531 - acc: 0.9629 - val_loss: 0.1593 - val_acc: 0.9606

Epoch 00011: val_loss improved from 0.31755 to 0.15928, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 50s - loss: 0.1367 - acc: 0.9684 - val_loss: 0.1771 - val_acc: 0.9502

Epoch 00012: val_loss did not improve from 0.15928
Epoch 13/30
 - 50s - loss: 0.1259 - acc: 0.9719 - val_loss: 0.1567 - val_acc: 0.9588

Epoch 00013: val_loss improved from 0.15928 to 0.15667, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 50s - loss: 0.1198 - acc: 0.9742 - val_loss: 0.1531 - val_acc: 0.9606

Epoch 00014: val_loss improved from 0.15667 to 0.15307, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 50s - loss: 0.1135 - acc: 0.9767 - val_loss: 0.1799 - val_acc: 0.9531

Epoch 00015: val_loss did not improve from 0.15307
Epoch 16/30
 - 50s - loss: 0.1057 - acc: 0.9792 - val_loss: 0.1943 - val_acc: 0.9458

Epoch 00016: val_loss did not improve from 0.15307
Epoch 17/30
 - 50s - loss: 0.1005 - acc: 0.9802 - val_loss: 0.1625 - val_acc: 0.9591

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00017: val_loss did not improve from 0.15307
Epoch 18/30
 - 50s - loss: 0.0825 - acc: 0.9885 - val_loss: 0.1291 - val_acc: 0.9698

Epoch 00018: val_loss improved from 0.15307 to 0.12915, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 50s - loss: 0.0792 - acc: 0.9885 - val_loss: 0.1349 - val_acc: 0.9690

Epoch 00019: val_loss did not improve from 0.12915
Epoch 20/30
 - 50s - loss: 0.0753 - acc: 0.9909 - val_loss: 0.1301 - val_acc: 0.9693

Epoch 00020: val_loss did not improve from 0.12915
Epoch 21/30
 - 50s - loss: 0.0722 - acc: 0.9918 - val_loss: 0.1267 - val_acc: 0.9724

Epoch 00021: val_loss improved from 0.12915 to 0.12667, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 50s - loss: 0.0696 - acc: 0.9934 - val_loss: 0.1261 - val_acc: 0.9713

Epoch 00022: val_loss improved from 0.12667 to 0.12606, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 50s - loss: 0.0698 - acc: 0.9925 - val_loss: 0.1281 - val_acc: 0.9701

Epoch 00023: val_loss did not improve from 0.12606
Epoch 24/30
 - 50s - loss: 0.0670 - acc: 0.9935 - val_loss: 0.1339 - val_acc: 0.9693

Epoch 00024: val_loss did not improve from 0.12606
Epoch 25/30
 - 50s - loss: 0.0663 - acc: 0.9936 - val_loss: 0.1261 - val_acc: 0.9693

Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.

Epoch 00025: val_loss did not improve from 0.12606
Epoch 26/30
 - 50s - loss: 0.0611 - acc: 0.9960 - val_loss: 0.1184 - val_acc: 0.9743

Epoch 00026: val_loss improved from 0.12606 to 0.11844, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 50s - loss: 0.0596 - acc: 0.9958 - val_loss: 0.1175 - val_acc: 0.9753

Epoch 00027: val_loss improved from 0.11844 to 0.11747, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 50s - loss: 0.0596 - acc: 0.9964 - val_loss: 0.1229 - val_acc: 0.9741

Epoch 00028: val_loss did not improve from 0.11747
Epoch 29/30
 - 50s - loss: 0.0581 - acc: 0.9968 - val_loss: 0.1181 - val_acc: 0.9748

Epoch 00029: val_loss did not improve from 0.11747
Epoch 30/30
 - 50s - loss: 0.0583 - acc: 0.9966 - val_loss: 0.1169 - val_acc: 0.9744

Epoch 00030: val_loss improved from 0.11747 to 0.11694, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 3s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 2s
1824/7968 [=====>........................] - ETA: 2s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 1s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 0s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 477us/step
current val accuracy:0.974
current auc_score ------------------> 0.996

  32/7968 [..............................] - ETA: 1:55
 160/7968 [..............................] - ETA: 25s 
 288/7968 [>.............................] - ETA: 15s
 416/7968 [>.............................] - ETA: 11s
 544/7968 [=>............................] - ETA: 9s 
 672/7968 [=>............................] - ETA: 8s
 800/7968 [==>...........................] - ETA: 7s
 928/7968 [==>...........................] - ETA: 6s
1056/7968 [==>...........................] - ETA: 6s
1184/7968 [===>..........................] - ETA: 5s
1312/7968 [===>..........................] - ETA: 5s
1440/7968 [====>.........................] - ETA: 5s
1568/7968 [====>.........................] - ETA: 4s
1696/7968 [=====>........................] - ETA: 4s
1824/7968 [=====>........................] - ETA: 4s
1952/7968 [======>.......................] - ETA: 4s
2080/7968 [======>.......................] - ETA: 4s
2208/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2464/7968 [========>.....................] - ETA: 3s
2592/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2848/7968 [=========>....................] - ETA: 3s
2976/7968 [==========>...................] - ETA: 3s
3104/7968 [==========>...................] - ETA: 3s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4384/7968 [===============>..............] - ETA: 2s
4512/7968 [===============>..............] - ETA: 2s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 537us/step
Best saved model val accuracy:0.974
best saved model auc_score ------------------> 0.996
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_22[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_24[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 52, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_26[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 70, 96, 96)   0           concatenate_11[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_28[0][0]              
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_29[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_3[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_31[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_32[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 71, 48, 48)   0           concatenate_13[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_33[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 89, 48, 48)   0           concatenate_14[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_35[0][0]              
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_36[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_4[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_38[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_39[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 80, 24, 24)   0           concatenate_16[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_40[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_41[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 98, 24, 24)   0           concatenate_17[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 98)           0           activation_42[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            99          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 56s - loss: 0.5296 - acc: 0.7819 - val_loss: 0.4842 - val_acc: 0.8064

Epoch 00001: val_loss improved from inf to 0.48419, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 52s - loss: 0.4092 - acc: 0.8319 - val_loss: 0.4420 - val_acc: 0.8085

Epoch 00002: val_loss improved from 0.48419 to 0.44196, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 52s - loss: 0.3598 - acc: 0.8582 - val_loss: 0.3282 - val_acc: 0.8721

Epoch 00003: val_loss improved from 0.44196 to 0.32816, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 52s - loss: 0.3246 - acc: 0.8770 - val_loss: 0.4171 - val_acc: 0.8145

Epoch 00004: val_loss did not improve from 0.32816
Epoch 5/30
 - 52s - loss: 0.2990 - acc: 0.8879 - val_loss: 0.4015 - val_acc: 0.8402

Epoch 00005: val_loss did not improve from 0.32816
Epoch 6/30
 - 52s - loss: 0.2765 - acc: 0.9000 - val_loss: 0.4586 - val_acc: 0.8293

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006324555620737017.

Epoch 00006: val_loss did not improve from 0.32816
Epoch 7/30
 - 52s - loss: 0.2146 - acc: 0.9279 - val_loss: 0.2247 - val_acc: 0.9243

Epoch 00007: val_loss improved from 0.32816 to 0.22468, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 52s - loss: 0.1888 - acc: 0.9399 - val_loss: 0.2021 - val_acc: 0.9334

Epoch 00008: val_loss improved from 0.22468 to 0.20213, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 52s - loss: 0.1732 - acc: 0.9460 - val_loss: 0.2074 - val_acc: 0.9307

Epoch 00009: val_loss did not improve from 0.20213
Epoch 10/30
 - 52s - loss: 0.1640 - acc: 0.9489 - val_loss: 0.2041 - val_acc: 0.9307

Epoch 00010: val_loss did not improve from 0.20213
Epoch 11/30
 - 52s - loss: 0.1536 - acc: 0.9541 - val_loss: 0.1931 - val_acc: 0.9357

Epoch 00011: val_loss improved from 0.20213 to 0.19305, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 52s - loss: 0.1436 - acc: 0.9572 - val_loss: 0.2313 - val_acc: 0.9271

Epoch 00012: val_loss did not improve from 0.19305
Epoch 13/30
 - 52s - loss: 0.1358 - acc: 0.9619 - val_loss: 0.1787 - val_acc: 0.9478

Epoch 00013: val_loss improved from 0.19305 to 0.17866, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 52s - loss: 0.1321 - acc: 0.9623 - val_loss: 0.1804 - val_acc: 0.9429

Epoch 00014: val_loss did not improve from 0.17866
Epoch 15/30
 - 52s - loss: 0.1239 - acc: 0.9651 - val_loss: 0.1878 - val_acc: 0.9443

Epoch 00015: val_loss did not improve from 0.17866
Epoch 16/30
 - 52s - loss: 0.1202 - acc: 0.9666 - val_loss: 0.1729 - val_acc: 0.9501

Epoch 00016: val_loss improved from 0.17866 to 0.17287, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 52s - loss: 0.1117 - acc: 0.9699 - val_loss: 0.1998 - val_acc: 0.9360

Epoch 00017: val_loss did not improve from 0.17287
Epoch 18/30
 - 52s - loss: 0.1077 - acc: 0.9715 - val_loss: 0.1596 - val_acc: 0.9485

Epoch 00018: val_loss improved from 0.17287 to 0.15960, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 52s - loss: 0.1068 - acc: 0.9717 - val_loss: 0.1610 - val_acc: 0.9501

Epoch 00019: val_loss did not improve from 0.15960
Epoch 20/30
 - 52s - loss: 0.1021 - acc: 0.9734 - val_loss: 0.1502 - val_acc: 0.9562

Epoch 00020: val_loss improved from 0.15960 to 0.15023, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 52s - loss: 0.1045 - acc: 0.9728 - val_loss: 0.1545 - val_acc: 0.9549

Epoch 00021: val_loss did not improve from 0.15023
Epoch 22/30
 - 52s - loss: 0.1017 - acc: 0.9740 - val_loss: 0.1515 - val_acc: 0.9561

Epoch 00022: val_loss did not improve from 0.15023
Epoch 23/30
 - 52s - loss: 0.0895 - acc: 0.9789 - val_loss: 0.2168 - val_acc: 0.9308

Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00020000001279212398.

Epoch 00023: val_loss did not improve from 0.15023
Epoch 24/30
 - 52s - loss: 0.0649 - acc: 0.9896 - val_loss: 0.1204 - val_acc: 0.9709

Epoch 00024: val_loss improved from 0.15023 to 0.12037, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 52s - loss: 0.0604 - acc: 0.9914 - val_loss: 0.1448 - val_acc: 0.9635

Epoch 00025: val_loss did not improve from 0.12037
Epoch 26/30
 - 52s - loss: 0.0579 - acc: 0.9917 - val_loss: 0.1171 - val_acc: 0.9704

Epoch 00026: val_loss improved from 0.12037 to 0.11710, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 52s - loss: 0.0567 - acc: 0.9917 - val_loss: 0.1221 - val_acc: 0.9711

Epoch 00027: val_loss did not improve from 0.11710
Epoch 28/30
 - 52s - loss: 0.0531 - acc: 0.9939 - val_loss: 0.1216 - val_acc: 0.9695

Epoch 00028: val_loss did not improve from 0.11710
Epoch 29/30
 - 52s - loss: 0.0528 - acc: 0.9929 - val_loss: 0.1235 - val_acc: 0.9689

Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.324555620737017e-05.

Epoch 00029: val_loss did not improve from 0.11710
Epoch 30/30
 - 52s - loss: 0.0453 - acc: 0.9961 - val_loss: 0.1093 - val_acc: 0.9743

Epoch 00030: val_loss improved from 0.11710 to 0.10932, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 2s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 1s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 0s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 479us/step
current val accuracy:0.974
current auc_score ------------------> 0.996

  32/7968 [..............................] - ETA: 3:18
 160/7968 [..............................] - ETA: 42s 
 288/7968 [>.............................] - ETA: 24s
 416/7968 [>.............................] - ETA: 17s
 544/7968 [=>............................] - ETA: 14s
 672/7968 [=>............................] - ETA: 12s
 800/7968 [==>...........................] - ETA: 10s
 928/7968 [==>...........................] - ETA: 9s 
1056/7968 [==>...........................] - ETA: 8s
1184/7968 [===>..........................] - ETA: 7s
1312/7968 [===>..........................] - ETA: 7s
1440/7968 [====>.........................] - ETA: 6s
1568/7968 [====>.........................] - ETA: 6s
1696/7968 [=====>........................] - ETA: 5s
1824/7968 [=====>........................] - ETA: 5s
1952/7968 [======>.......................] - ETA: 5s
2080/7968 [======>.......................] - ETA: 5s
2208/7968 [=======>......................] - ETA: 4s
2336/7968 [=======>......................] - ETA: 4s
2464/7968 [========>.....................] - ETA: 4s
2592/7968 [========>.....................] - ETA: 4s
2720/7968 [=========>....................] - ETA: 4s
2848/7968 [=========>....................] - ETA: 3s
2976/7968 [==========>...................] - ETA: 3s
3104/7968 [==========>...................] - ETA: 3s
3232/7968 [===========>..................] - ETA: 3s
3360/7968 [===========>..................] - ETA: 3s
3488/7968 [============>.................] - ETA: 3s
3616/7968 [============>.................] - ETA: 3s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4384/7968 [===============>..............] - ETA: 2s
4512/7968 [===============>..............] - ETA: 2s
4640/7968 [================>.............] - ETA: 2s
4768/7968 [================>.............] - ETA: 2s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6304/7968 [======================>.......] - ETA: 1s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 582us/step
Best saved model val accuracy:0.974
best saved model auc_score ------------------> 0.996
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_43[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_44[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_45[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_46[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 52, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_47[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_48[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 70, 96, 96)   0           concatenate_20[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_49[0][0]              
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_50[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_51[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_5[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_52[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_53[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 71, 48, 48)   0           concatenate_22[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_54[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_55[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 89, 48, 48)   0           concatenate_23[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_56[0][0]              
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_57[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_58[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_6[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_59[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_60[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 80, 24, 24)   0           concatenate_25[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_61[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_62[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 98, 24, 24)   0           concatenate_26[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 98)           0           activation_63[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            99          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 58s - loss: 0.5782 - acc: 0.7830 - val_loss: 0.6952 - val_acc: 0.7982

Epoch 00001: val_loss improved from inf to 0.69515, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.4722 - acc: 0.8273 - val_loss: 0.4397 - val_acc: 0.8469

Epoch 00002: val_loss improved from 0.69515 to 0.43971, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 53s - loss: 0.4108 - acc: 0.8576 - val_loss: 0.4450 - val_acc: 0.8440

Epoch 00003: val_loss did not improve from 0.43971
Epoch 4/30
 - 53s - loss: 0.3613 - acc: 0.8774 - val_loss: 0.3649 - val_acc: 0.8789

Epoch 00004: val_loss improved from 0.43971 to 0.36494, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 53s - loss: 0.3270 - acc: 0.8913 - val_loss: 0.4198 - val_acc: 0.8678

Epoch 00005: val_loss did not improve from 0.36494
Epoch 6/30
 - 53s - loss: 0.2951 - acc: 0.9052 - val_loss: 0.5839 - val_acc: 0.8074

Epoch 00006: val_loss did not improve from 0.36494
Epoch 7/30
 - 53s - loss: 0.2732 - acc: 0.9126 - val_loss: 0.3665 - val_acc: 0.8711

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.31622776601683794.

Epoch 00007: val_loss did not improve from 0.36494
Epoch 8/30
 - 53s - loss: 0.2069 - acc: 0.9427 - val_loss: 0.2055 - val_acc: 0.9431

Epoch 00008: val_loss improved from 0.36494 to 0.20553, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 53s - loss: 0.1874 - acc: 0.9504 - val_loss: 0.2678 - val_acc: 0.9088

Epoch 00009: val_loss did not improve from 0.20553
Epoch 10/30
 - 53s - loss: 0.1736 - acc: 0.9555 - val_loss: 0.1901 - val_acc: 0.9480

Epoch 00010: val_loss improved from 0.20553 to 0.19014, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 53s - loss: 0.1630 - acc: 0.9605 - val_loss: 0.1851 - val_acc: 0.9503

Epoch 00011: val_loss improved from 0.19014 to 0.18506, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 53s - loss: 0.1535 - acc: 0.9640 - val_loss: 0.1745 - val_acc: 0.9556

Epoch 00012: val_loss improved from 0.18506 to 0.17452, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 53s - loss: 0.1435 - acc: 0.9677 - val_loss: 0.2962 - val_acc: 0.9116

Epoch 00013: val_loss did not improve from 0.17452
Epoch 14/30
 - 53s - loss: 0.1382 - acc: 0.9692 - val_loss: 0.2186 - val_acc: 0.9340

Epoch 00014: val_loss did not improve from 0.17452
Epoch 15/30
 - 53s - loss: 0.1300 - acc: 0.9725 - val_loss: 0.2237 - val_acc: 0.9322

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.09999999932782062.

Epoch 00015: val_loss did not improve from 0.17452
Epoch 16/30
 - 53s - loss: 0.1053 - acc: 0.9829 - val_loss: 0.1561 - val_acc: 0.9600

Epoch 00016: val_loss improved from 0.17452 to 0.15612, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 53s - loss: 0.1029 - acc: 0.9832 - val_loss: 0.1481 - val_acc: 0.9617

Epoch 00017: val_loss improved from 0.15612 to 0.14813, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 53s - loss: 0.0992 - acc: 0.9854 - val_loss: 0.1456 - val_acc: 0.9652

Epoch 00018: val_loss improved from 0.14813 to 0.14556, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 53s - loss: 0.0957 - acc: 0.9863 - val_loss: 0.1421 - val_acc: 0.9661

Epoch 00019: val_loss improved from 0.14556 to 0.14206, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 53s - loss: 0.0939 - acc: 0.9871 - val_loss: 0.1448 - val_acc: 0.9646

Epoch 00020: val_loss did not improve from 0.14206
Epoch 21/30
 - 53s - loss: 0.0911 - acc: 0.9880 - val_loss: 0.1441 - val_acc: 0.9640

Epoch 00021: val_loss did not improve from 0.14206
Epoch 22/30
 - 53s - loss: 0.0870 - acc: 0.9890 - val_loss: 0.1445 - val_acc: 0.9657

Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.031622777072899885.

Epoch 00022: val_loss did not improve from 0.14206
Epoch 23/30
 - 53s - loss: 0.0830 - acc: 0.9915 - val_loss: 0.1389 - val_acc: 0.9674

Epoch 00023: val_loss improved from 0.14206 to 0.13889, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 53s - loss: 0.0817 - acc: 0.9915 - val_loss: 0.1385 - val_acc: 0.9655

Epoch 00024: val_loss improved from 0.13889 to 0.13847, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 53s - loss: 0.0815 - acc: 0.9919 - val_loss: 0.1402 - val_acc: 0.9665

Epoch 00025: val_loss did not improve from 0.13847
Epoch 26/30
 - 53s - loss: 0.0788 - acc: 0.9929 - val_loss: 0.1392 - val_acc: 0.9677

Epoch 00026: val_loss did not improve from 0.13847
Epoch 27/30
 - 53s - loss: 0.0801 - acc: 0.9916 - val_loss: 0.1389 - val_acc: 0.9675

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0100000006396062.

Epoch 00027: val_loss did not improve from 0.13847
Epoch 28/30
 - 53s - loss: 0.0780 - acc: 0.9929 - val_loss: 0.1366 - val_acc: 0.9677

Epoch 00028: val_loss improved from 0.13847 to 0.13656, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 53s - loss: 0.0770 - acc: 0.9933 - val_loss: 0.1369 - val_acc: 0.9676

Epoch 00029: val_loss did not improve from 0.13656
Epoch 30/30
 - 53s - loss: 0.0781 - acc: 0.9931 - val_loss: 0.1365 - val_acc: 0.9691

Epoch 00030: val_loss improved from 0.13656 to 0.13650, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 3s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 2s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6400/7968 [=======================>......] - ETA: 0s
6528/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 492us/step
current val accuracy:0.969
current auc_score ------------------> 0.995

  32/7968 [..............................] - ETA: 5:29
 128/7968 [..............................] - ETA: 1:24
 256/7968 [..............................] - ETA: 43s 
 384/7968 [>.............................] - ETA: 29s
 512/7968 [>.............................] - ETA: 22s
 640/7968 [=>............................] - ETA: 18s
 768/7968 [=>............................] - ETA: 15s
 896/7968 [==>...........................] - ETA: 13s
1024/7968 [==>...........................] - ETA: 12s
1152/7968 [===>..........................] - ETA: 11s
1280/7968 [===>..........................] - ETA: 10s
1408/7968 [====>.........................] - ETA: 9s 
1536/7968 [====>.........................] - ETA: 8s
1664/7968 [=====>........................] - ETA: 8s
1792/7968 [=====>........................] - ETA: 7s
1920/7968 [======>.......................] - ETA: 7s
2048/7968 [======>.......................] - ETA: 6s
2176/7968 [=======>......................] - ETA: 6s
2304/7968 [=======>......................] - ETA: 6s
2432/7968 [========>.....................] - ETA: 5s
2560/7968 [========>.....................] - ETA: 5s
2688/7968 [=========>....................] - ETA: 5s
2816/7968 [=========>....................] - ETA: 4s
2944/7968 [==========>...................] - ETA: 4s
3072/7968 [==========>...................] - ETA: 4s
3200/7968 [===========>..................] - ETA: 4s
3328/7968 [===========>..................] - ETA: 4s
3456/7968 [============>.................] - ETA: 3s
3584/7968 [============>.................] - ETA: 3s
3712/7968 [============>.................] - ETA: 3s
3840/7968 [=============>................] - ETA: 3s
3968/7968 [=============>................] - ETA: 3s
4096/7968 [==============>...............] - ETA: 3s
4224/7968 [==============>...............] - ETA: 3s
4352/7968 [===============>..............] - ETA: 2s
4480/7968 [===============>..............] - ETA: 2s
4608/7968 [================>.............] - ETA: 2s
4736/7968 [================>.............] - ETA: 2s
4864/7968 [=================>............] - ETA: 2s
4992/7968 [=================>............] - ETA: 2s
5120/7968 [==================>...........] - ETA: 2s
5248/7968 [==================>...........] - ETA: 2s
5376/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5632/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6400/7968 [=======================>......] - ETA: 1s
6528/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6880/7968 [========================>.....] - ETA: 0s
7008/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7264/7968 [==========================>...] - ETA: 0s
7392/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7648/7968 [===========================>..] - ETA: 0s
7776/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 667us/step
Best saved model val accuracy:0.969
best saved model auc_score ------------------> 0.995
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_64[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_65[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_66[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_67[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 52, 96, 96)   0           concatenate_28[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_68[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_69[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 70, 96, 96)   0           concatenate_29[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_70[0][0]              
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_71[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_72[0][0]              
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_7[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_31[0][0]             
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_73[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_74[0][0]              
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 71, 48, 48)   0           concatenate_31[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_32[0][0]             
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_75[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_76[0][0]              
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 89, 48, 48)   0           concatenate_32[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_33[0][0]             
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_77[0][0]              
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_78[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_79[0][0]              
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_8[0][0]        
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_34[0][0]             
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_80[0][0]              
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_81[0][0]              
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 80, 24, 24)   0           concatenate_34[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_35[0][0]             
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_82[0][0]              
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_83[0][0]              
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 98, 24, 24)   0           concatenate_35[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_36[0][0]             
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 98)           0           activation_84[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            99          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 58s - loss: 0.6836 - acc: 0.7350 - val_loss: 0.6362 - val_acc: 0.7577

Epoch 00001: val_loss improved from inf to 0.63624, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 51s - loss: 0.6107 - acc: 0.7758 - val_loss: 0.5978 - val_acc: 0.7782

Epoch 00002: val_loss improved from 0.63624 to 0.59777, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 51s - loss: 0.5808 - acc: 0.7887 - val_loss: 0.5559 - val_acc: 0.7967

Epoch 00003: val_loss improved from 0.59777 to 0.55593, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 51s - loss: 0.5580 - acc: 0.8042 - val_loss: 0.5843 - val_acc: 0.7821

Epoch 00004: val_loss did not improve from 0.55593
Epoch 5/30
 - 51s - loss: 0.5399 - acc: 0.8117 - val_loss: 0.5217 - val_acc: 0.8169

Epoch 00005: val_loss improved from 0.55593 to 0.52175, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 51s - loss: 0.5253 - acc: 0.8236 - val_loss: 0.5221 - val_acc: 0.8222

Epoch 00006: val_loss did not improve from 0.52175
Epoch 7/30
 - 51s - loss: 0.5117 - acc: 0.8289 - val_loss: 0.5183 - val_acc: 0.8284

Epoch 00007: val_loss improved from 0.52175 to 0.51833, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 51s - loss: 0.4987 - acc: 0.8382 - val_loss: 0.5122 - val_acc: 0.8267

Epoch 00008: val_loss improved from 0.51833 to 0.51221, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 51s - loss: 0.4885 - acc: 0.8416 - val_loss: 0.4931 - val_acc: 0.8425

Epoch 00009: val_loss improved from 0.51221 to 0.49313, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 51s - loss: 0.4786 - acc: 0.8483 - val_loss: 0.5486 - val_acc: 0.8174

Epoch 00010: val_loss did not improve from 0.49313
Epoch 11/30
 - 51s - loss: 0.4690 - acc: 0.8529 - val_loss: 0.4658 - val_acc: 0.8550

Epoch 00011: val_loss improved from 0.49313 to 0.46583, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 51s - loss: 0.4592 - acc: 0.8588 - val_loss: 0.4781 - val_acc: 0.8439

Epoch 00012: val_loss did not improve from 0.46583
Epoch 13/30
 - 51s - loss: 0.4481 - acc: 0.8647 - val_loss: 0.4645 - val_acc: 0.8558

Epoch 00013: val_loss improved from 0.46583 to 0.46453, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 51s - loss: 0.4427 - acc: 0.8672 - val_loss: 0.4359 - val_acc: 0.8726

Epoch 00014: val_loss improved from 0.46453 to 0.43587, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 51s - loss: 0.4323 - acc: 0.8742 - val_loss: 0.4550 - val_acc: 0.8630

Epoch 00015: val_loss did not improve from 0.43587
Epoch 16/30
 - 51s - loss: 0.4263 - acc: 0.8749 - val_loss: 0.4525 - val_acc: 0.8604

Epoch 00016: val_loss did not improve from 0.43587
Epoch 17/30
 - 51s - loss: 0.4183 - acc: 0.8796 - val_loss: 0.4211 - val_acc: 0.8783

Epoch 00017: val_loss improved from 0.43587 to 0.42109, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 51s - loss: 0.4127 - acc: 0.8813 - val_loss: 0.4214 - val_acc: 0.8705

Epoch 00018: val_loss did not improve from 0.42109
Epoch 19/30
 - 51s - loss: 0.4030 - acc: 0.8882 - val_loss: 0.4463 - val_acc: 0.8640

Epoch 00019: val_loss did not improve from 0.42109
Epoch 20/30
 - 51s - loss: 0.3978 - acc: 0.8881 - val_loss: 0.3976 - val_acc: 0.8863

Epoch 00020: val_loss improved from 0.42109 to 0.39759, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 51s - loss: 0.3935 - acc: 0.8900 - val_loss: 0.4469 - val_acc: 0.8564

Epoch 00021: val_loss did not improve from 0.39759
Epoch 22/30
 - 51s - loss: 0.3863 - acc: 0.8945 - val_loss: 0.4055 - val_acc: 0.8819

Epoch 00022: val_loss did not improve from 0.39759
Epoch 23/30
 - 51s - loss: 0.3804 - acc: 0.8975 - val_loss: 0.3946 - val_acc: 0.8843

Epoch 00023: val_loss improved from 0.39759 to 0.39458, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 51s - loss: 0.3748 - acc: 0.8988 - val_loss: 0.4109 - val_acc: 0.8801

Epoch 00024: val_loss did not improve from 0.39458
Epoch 25/30
 - 51s - loss: 0.3706 - acc: 0.9010 - val_loss: 0.3843 - val_acc: 0.8946

Epoch 00025: val_loss improved from 0.39458 to 0.38427, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 51s - loss: 0.3657 - acc: 0.9032 - val_loss: 0.3820 - val_acc: 0.8932

Epoch 00026: val_loss improved from 0.38427 to 0.38201, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 51s - loss: 0.3595 - acc: 0.9063 - val_loss: 0.4246 - val_acc: 0.8746

Epoch 00027: val_loss did not improve from 0.38201
Epoch 28/30
 - 51s - loss: 0.3536 - acc: 0.9110 - val_loss: 0.4296 - val_acc: 0.8701

Epoch 00028: val_loss did not improve from 0.38201
Epoch 29/30
 - 51s - loss: 0.3501 - acc: 0.9121 - val_loss: 0.3848 - val_acc: 0.8924

Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.

Epoch 00029: val_loss did not improve from 0.38201
Epoch 30/30
 - 51s - loss: 0.3360 - acc: 0.9197 - val_loss: 0.3603 - val_acc: 0.9026

Epoch 00030: val_loss improved from 0.38201 to 0.36034, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 3s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 1s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 503us/step
current val accuracy:0.903
current auc_score ------------------> 0.969

  32/7968 [..............................] - ETA: 6:57
 128/7968 [..............................] - ETA: 1:46
 224/7968 [..............................] - ETA: 1:01
 320/7968 [>.............................] - ETA: 43s 
 416/7968 [>.............................] - ETA: 34s
 544/7968 [=>............................] - ETA: 26s
 672/7968 [=>............................] - ETA: 21s
 800/7968 [==>...........................] - ETA: 18s
 928/7968 [==>...........................] - ETA: 16s
1056/7968 [==>...........................] - ETA: 14s
1184/7968 [===>..........................] - ETA: 13s
1312/7968 [===>..........................] - ETA: 11s
1440/7968 [====>.........................] - ETA: 10s
1568/7968 [====>.........................] - ETA: 10s
1696/7968 [=====>........................] - ETA: 9s 
1824/7968 [=====>........................] - ETA: 8s
1952/7968 [======>.......................] - ETA: 8s
2080/7968 [======>.......................] - ETA: 7s
2208/7968 [=======>......................] - ETA: 7s
2336/7968 [=======>......................] - ETA: 6s
2464/7968 [========>.....................] - ETA: 6s
2592/7968 [========>.....................] - ETA: 6s
2720/7968 [=========>....................] - ETA: 5s
2848/7968 [=========>....................] - ETA: 5s
2976/7968 [==========>...................] - ETA: 5s
3104/7968 [==========>...................] - ETA: 5s
3232/7968 [===========>..................] - ETA: 4s
3360/7968 [===========>..................] - ETA: 4s
3488/7968 [============>.................] - ETA: 4s
3616/7968 [============>.................] - ETA: 4s
3744/7968 [=============>................] - ETA: 4s
3872/7968 [=============>................] - ETA: 3s
4000/7968 [==============>...............] - ETA: 3s
4128/7968 [==============>...............] - ETA: 3s
4256/7968 [===============>..............] - ETA: 3s
4384/7968 [===============>..............] - ETA: 3s
4512/7968 [===============>..............] - ETA: 3s
4640/7968 [================>.............] - ETA: 2s
4768/7968 [================>.............] - ETA: 2s
4896/7968 [=================>............] - ETA: 2s
5024/7968 [=================>............] - ETA: 2s
5152/7968 [==================>...........] - ETA: 2s
5280/7968 [==================>...........] - ETA: 2s
5408/7968 [===================>..........] - ETA: 2s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6304/7968 [======================>.......] - ETA: 1s
6432/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 6s 719us/step
Best saved model val accuracy:0.903
best saved model auc_score ------------------> 0.969
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_85[0][0]              
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_86[0][0]              
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_37[0][0]             
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_87[0][0]              
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 52, 96, 96)   0           concatenate_37[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_38[0][0]             
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_89[0][0]              
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_90[0][0]              
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 70, 96, 96)   0           concatenate_38[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_39[0][0]             
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_91[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_92[0][0]              
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_93[0][0]              
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_9[0][0]        
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_40[0][0]             
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_94[0][0]              
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_95[0][0]              
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 71, 48, 48)   0           concatenate_40[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_41[0][0]             
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_96[0][0]              
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_97[0][0]              
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 89, 48, 48)   0           concatenate_41[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_42[0][0]             
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_98[0][0]              
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_99[0][0]              
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_100[0][0]             
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_10[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_43[0][0]             
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_101[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_102[0][0]             
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 80, 24, 24)   0           concatenate_43[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_44[0][0]             
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_103[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_104[0][0]             
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 98, 24, 24)   0           concatenate_44[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_45[0][0]             
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 98)           0           activation_105[0][0]             
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            99          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 58s - loss: 0.5879 - acc: 0.7827 - val_loss: 0.5333 - val_acc: 0.8053

Epoch 00001: val_loss improved from inf to 0.53333, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 51s - loss: 0.4926 - acc: 0.8281 - val_loss: 0.4682 - val_acc: 0.8395

Epoch 00002: val_loss improved from 0.53333 to 0.46821, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 51s - loss: 0.4413 - acc: 0.8559 - val_loss: 0.4298 - val_acc: 0.8739

Epoch 00003: val_loss improved from 0.46821 to 0.42984, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 52s - loss: 0.4004 - acc: 0.8770 - val_loss: 0.3886 - val_acc: 0.8881

Epoch 00004: val_loss improved from 0.42984 to 0.38862, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 51s - loss: 0.3677 - acc: 0.8911 - val_loss: 0.3509 - val_acc: 0.8990

Epoch 00005: val_loss improved from 0.38862 to 0.35094, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 51s - loss: 0.3388 - acc: 0.9029 - val_loss: 0.3553 - val_acc: 0.8926

Epoch 00006: val_loss did not improve from 0.35094
Epoch 7/30
 - 51s - loss: 0.3153 - acc: 0.9142 - val_loss: 0.5014 - val_acc: 0.8297

Epoch 00007: val_loss did not improve from 0.35094
Epoch 8/30
 - 51s - loss: 0.2931 - acc: 0.9228 - val_loss: 0.3645 - val_acc: 0.8845

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.

Epoch 00008: val_loss did not improve from 0.35094
Epoch 9/30
 - 52s - loss: 0.2510 - acc: 0.9415 - val_loss: 0.2485 - val_acc: 0.9381

Epoch 00009: val_loss improved from 0.35094 to 0.24848, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 52s - loss: 0.2349 - acc: 0.9490 - val_loss: 0.2478 - val_acc: 0.9400

Epoch 00010: val_loss improved from 0.24848 to 0.24784, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 52s - loss: 0.2281 - acc: 0.9506 - val_loss: 0.2353 - val_acc: 0.9468

Epoch 00011: val_loss improved from 0.24784 to 0.23532, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 52s - loss: 0.2176 - acc: 0.9545 - val_loss: 0.2389 - val_acc: 0.9449

Epoch 00012: val_loss did not improve from 0.23532
Epoch 13/30
 - 52s - loss: 0.2097 - acc: 0.9587 - val_loss: 0.2402 - val_acc: 0.9428

Epoch 00013: val_loss did not improve from 0.23532
Epoch 14/30
 - 52s - loss: 0.2032 - acc: 0.9613 - val_loss: 0.2418 - val_acc: 0.9426

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00014: val_loss did not improve from 0.23532
Epoch 15/30
 - 52s - loss: 0.1890 - acc: 0.9674 - val_loss: 0.2150 - val_acc: 0.9544

Epoch 00015: val_loss improved from 0.23532 to 0.21496, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 52s - loss: 0.1823 - acc: 0.9705 - val_loss: 0.2159 - val_acc: 0.9546

Epoch 00016: val_loss did not improve from 0.21496
Epoch 17/30
 - 52s - loss: 0.1820 - acc: 0.9703 - val_loss: 0.2135 - val_acc: 0.9552

Epoch 00017: val_loss improved from 0.21496 to 0.21347, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 52s - loss: 0.1800 - acc: 0.9713 - val_loss: 0.2133 - val_acc: 0.9536

Epoch 00018: val_loss improved from 0.21347 to 0.21333, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 52s - loss: 0.1763 - acc: 0.9731 - val_loss: 0.2163 - val_acc: 0.9524

Epoch 00019: val_loss did not improve from 0.21333
Epoch 20/30
 - 52s - loss: 0.1738 - acc: 0.9731 - val_loss: 0.2161 - val_acc: 0.9529

Epoch 00020: val_loss did not improve from 0.21333
Epoch 21/30
 - 52s - loss: 0.1732 - acc: 0.9745 - val_loss: 0.2087 - val_acc: 0.9581

Epoch 00021: val_loss improved from 0.21333 to 0.20868, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 52s - loss: 0.1707 - acc: 0.9752 - val_loss: 0.2103 - val_acc: 0.9558

Epoch 00022: val_loss did not improve from 0.20868
Epoch 23/30
 - 52s - loss: 0.1687 - acc: 0.9748 - val_loss: 0.2080 - val_acc: 0.9567

Epoch 00023: val_loss improved from 0.20868 to 0.20802, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 52s - loss: 0.1659 - acc: 0.9765 - val_loss: 0.2082 - val_acc: 0.9556

Epoch 00024: val_loss did not improve from 0.20802
Epoch 25/30
 - 52s - loss: 0.1645 - acc: 0.9780 - val_loss: 0.2067 - val_acc: 0.9559

Epoch 00025: val_loss improved from 0.20802 to 0.20669, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 52s - loss: 0.1643 - acc: 0.9773 - val_loss: 0.2155 - val_acc: 0.9519

Epoch 00026: val_loss did not improve from 0.20669
Epoch 27/30
 - 51s - loss: 0.1599 - acc: 0.9795 - val_loss: 0.2081 - val_acc: 0.9571

Epoch 00027: val_loss did not improve from 0.20669
Epoch 28/30
 - 52s - loss: 0.1599 - acc: 0.9789 - val_loss: 0.2044 - val_acc: 0.9558

Epoch 00028: val_loss improved from 0.20669 to 0.20439, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 51s - loss: 0.1570 - acc: 0.9801 - val_loss: 0.2037 - val_acc: 0.9561

Epoch 00029: val_loss improved from 0.20439 to 0.20366, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 52s - loss: 0.1542 - acc: 0.9818 - val_loss: 0.2031 - val_acc: 0.9575

Epoch 00030: val_loss improved from 0.20366 to 0.20310, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 4s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2080/7968 [======>.......................] - ETA: 3s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 512us/step
current val accuracy:0.957
current auc_score ------------------> 0.992

  32/7968 [..............................] - ETA: 7:01
 128/7968 [..............................] - ETA: 1:47
 256/7968 [..............................] - ETA: 54s 
 384/7968 [>.............................] - ETA: 37s
 512/7968 [>.............................] - ETA: 28s
 640/7968 [=>............................] - ETA: 22s
 768/7968 [=>............................] - ETA: 19s
 896/7968 [==>...........................] - ETA: 16s
1024/7968 [==>...........................] - ETA: 14s
1152/7968 [===>..........................] - ETA: 13s
1280/7968 [===>..........................] - ETA: 12s
1408/7968 [====>.........................] - ETA: 11s
1536/7968 [====>.........................] - ETA: 10s
1664/7968 [=====>........................] - ETA: 9s 
1792/7968 [=====>........................] - ETA: 8s
1920/7968 [======>.......................] - ETA: 8s
2048/7968 [======>.......................] - ETA: 7s
2176/7968 [=======>......................] - ETA: 7s
2304/7968 [=======>......................] - ETA: 6s
2432/7968 [========>.....................] - ETA: 6s
2560/7968 [========>.....................] - ETA: 6s
2688/7968 [=========>....................] - ETA: 5s
2816/7968 [=========>....................] - ETA: 5s
2944/7968 [==========>...................] - ETA: 5s
3072/7968 [==========>...................] - ETA: 5s
3200/7968 [===========>..................] - ETA: 4s
3328/7968 [===========>..................] - ETA: 4s
3456/7968 [============>.................] - ETA: 4s
3584/7968 [============>.................] - ETA: 4s
3712/7968 [============>.................] - ETA: 4s
3840/7968 [=============>................] - ETA: 3s
3968/7968 [=============>................] - ETA: 3s
4096/7968 [==============>...............] - ETA: 3s
4224/7968 [==============>...............] - ETA: 3s
4352/7968 [===============>..............] - ETA: 3s
4480/7968 [===============>..............] - ETA: 3s
4608/7968 [================>.............] - ETA: 2s
4736/7968 [================>.............] - ETA: 2s
4864/7968 [=================>............] - ETA: 2s
4992/7968 [=================>............] - ETA: 2s
5120/7968 [==================>...........] - ETA: 2s
5248/7968 [==================>...........] - ETA: 2s
5376/7968 [===================>..........] - ETA: 2s
5504/7968 [===================>..........] - ETA: 1s
5632/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6400/7968 [=======================>......] - ETA: 1s
6528/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 0s
6784/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7552/7968 [===========================>..] - ETA: 0s
7680/7968 [===========================>..] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 6s 710us/step
Best saved model val accuracy:0.957
best saved model auc_score ------------------> 0.992
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_106[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_107[0][0]             
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_46[0][0]             
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_108[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_109[0][0]             
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 52, 96, 96)   0           concatenate_46[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_47[0][0]             
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_110[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_111[0][0]             
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 70, 96, 96)   0           concatenate_47[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_48[0][0]             
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_112[0][0]             
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_113[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_114[0][0]             
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_11[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_49[0][0]             
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_115[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_116[0][0]             
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 71, 48, 48)   0           concatenate_49[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_50[0][0]             
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_117[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_118[0][0]             
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 89, 48, 48)   0           concatenate_50[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_51[0][0]             
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_119[0][0]             
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_120[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_121[0][0]             
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_12[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_52[0][0]             
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_122[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_123[0][0]             
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 80, 24, 24)   0           concatenate_52[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_53[0][0]             
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_124[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_125[0][0]             
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 98, 24, 24)   0           concatenate_53[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_54[0][0]             
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 98)           0           activation_126[0][0]             
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            99          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 61s - loss: 0.7421 - acc: 0.7162 - val_loss: 0.7028 - val_acc: 0.7341

Epoch 00001: val_loss improved from inf to 0.70280, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.6851 - acc: 0.7419 - val_loss: 0.6649 - val_acc: 0.7484

Epoch 00002: val_loss improved from 0.70280 to 0.66493, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 53s - loss: 0.6497 - acc: 0.7648 - val_loss: 0.6332 - val_acc: 0.7624

Epoch 00003: val_loss improved from 0.66493 to 0.63318, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 54s - loss: 0.6263 - acc: 0.7738 - val_loss: 0.6141 - val_acc: 0.7755

Epoch 00004: val_loss improved from 0.63318 to 0.61411, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 54s - loss: 0.6133 - acc: 0.7778 - val_loss: 0.6037 - val_acc: 0.7814

Epoch 00005: val_loss improved from 0.61411 to 0.60369, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 54s - loss: 0.6022 - acc: 0.7834 - val_loss: 0.5878 - val_acc: 0.7841

Epoch 00006: val_loss improved from 0.60369 to 0.58776, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 54s - loss: 0.5927 - acc: 0.7865 - val_loss: 0.5921 - val_acc: 0.7870

Epoch 00007: val_loss did not improve from 0.58776
Epoch 8/30
 - 53s - loss: 0.5837 - acc: 0.7896 - val_loss: 0.5723 - val_acc: 0.7909

Epoch 00008: val_loss improved from 0.58776 to 0.57228, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 54s - loss: 0.5769 - acc: 0.7928 - val_loss: 0.5656 - val_acc: 0.7951

Epoch 00009: val_loss improved from 0.57228 to 0.56561, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 53s - loss: 0.5700 - acc: 0.7973 - val_loss: 0.5619 - val_acc: 0.7978

Epoch 00010: val_loss improved from 0.56561 to 0.56188, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 54s - loss: 0.5611 - acc: 0.8019 - val_loss: 0.5571 - val_acc: 0.7988

Epoch 00011: val_loss improved from 0.56188 to 0.55713, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 54s - loss: 0.5554 - acc: 0.8056 - val_loss: 0.5514 - val_acc: 0.8020

Epoch 00012: val_loss improved from 0.55713 to 0.55138, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 54s - loss: 0.5510 - acc: 0.8078 - val_loss: 0.5437 - val_acc: 0.8124

Epoch 00013: val_loss improved from 0.55138 to 0.54374, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 53s - loss: 0.5430 - acc: 0.8133 - val_loss: 0.5360 - val_acc: 0.8115

Epoch 00014: val_loss improved from 0.54374 to 0.53604, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 54s - loss: 0.5388 - acc: 0.8150 - val_loss: 0.5272 - val_acc: 0.8192

Epoch 00015: val_loss improved from 0.53604 to 0.52723, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 54s - loss: 0.5344 - acc: 0.8172 - val_loss: 0.5275 - val_acc: 0.8190

Epoch 00016: val_loss did not improve from 0.52723
Epoch 17/30
 - 54s - loss: 0.5301 - acc: 0.8198 - val_loss: 0.5152 - val_acc: 0.8245

Epoch 00017: val_loss improved from 0.52723 to 0.51525, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 53s - loss: 0.5246 - acc: 0.8229 - val_loss: 0.5187 - val_acc: 0.8261

Epoch 00018: val_loss did not improve from 0.51525
Epoch 19/30
 - 53s - loss: 0.5185 - acc: 0.8278 - val_loss: 0.5076 - val_acc: 0.8313

Epoch 00019: val_loss improved from 0.51525 to 0.50761, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 53s - loss: 0.5143 - acc: 0.8297 - val_loss: 0.5037 - val_acc: 0.8327

Epoch 00020: val_loss improved from 0.50761 to 0.50366, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 53s - loss: 0.5111 - acc: 0.8300 - val_loss: 0.5012 - val_acc: 0.8363

Epoch 00021: val_loss improved from 0.50366 to 0.50117, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 53s - loss: 0.5084 - acc: 0.8317 - val_loss: 0.5069 - val_acc: 0.8348

Epoch 00022: val_loss did not improve from 0.50117
Epoch 23/30
 - 53s - loss: 0.5040 - acc: 0.8360 - val_loss: 0.5077 - val_acc: 0.8327

Epoch 00023: val_loss did not improve from 0.50117
Epoch 24/30
 - 53s - loss: 0.4991 - acc: 0.8399 - val_loss: 0.4962 - val_acc: 0.8384

Epoch 00024: val_loss improved from 0.50117 to 0.49622, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 53s - loss: 0.4955 - acc: 0.8396 - val_loss: 0.4874 - val_acc: 0.8405

Epoch 00025: val_loss improved from 0.49622 to 0.48739, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 53s - loss: 0.4946 - acc: 0.8406 - val_loss: 0.4907 - val_acc: 0.8412

Epoch 00026: val_loss did not improve from 0.48739
Epoch 27/30
 - 53s - loss: 0.4897 - acc: 0.8448 - val_loss: 0.4808 - val_acc: 0.8459

Epoch 00027: val_loss improved from 0.48739 to 0.48075, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 54s - loss: 0.4841 - acc: 0.8470 - val_loss: 0.4790 - val_acc: 0.8461

Epoch 00028: val_loss improved from 0.48075 to 0.47900, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 53s - loss: 0.4825 - acc: 0.8469 - val_loss: 0.4794 - val_acc: 0.8530

Epoch 00029: val_loss did not improve from 0.47900
Epoch 30/30
 - 53s - loss: 0.4791 - acc: 0.8491 - val_loss: 0.4704 - val_acc: 0.8554

Epoch 00030: val_loss improved from 0.47900 to 0.47035, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 4s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 928/7968 [==>...........................] - ETA: 3s
1056/7968 [==>...........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1312/7968 [===>..........................] - ETA: 3s
1440/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2080/7968 [======>.......................] - ETA: 2s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2464/7968 [========>.....................] - ETA: 2s
2592/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2848/7968 [=========>....................] - ETA: 2s
2976/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3232/7968 [===========>..................] - ETA: 2s
3360/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3616/7968 [============>.................] - ETA: 2s
3744/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
4000/7968 [==============>...............] - ETA: 2s
4128/7968 [==============>...............] - ETA: 1s
4256/7968 [===============>..............] - ETA: 1s
4384/7968 [===============>..............] - ETA: 1s
4512/7968 [===============>..............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4768/7968 [================>.............] - ETA: 1s
4896/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5664/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5920/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6304/7968 [======================>.......] - ETA: 0s
6432/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6688/7968 [========================>.....] - ETA: 0s
6816/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7200/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7456/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 507us/step
current val accuracy:0.855
current auc_score ------------------> 0.937

  32/7968 [..............................] - ETA: 9:02
 128/7968 [..............................] - ETA: 2:17
 224/7968 [..............................] - ETA: 1:19
 320/7968 [>.............................] - ETA: 56s 
 416/7968 [>.............................] - ETA: 43s
 512/7968 [>.............................] - ETA: 35s
 640/7968 [=>............................] - ETA: 28s
 736/7968 [=>............................] - ETA: 25s
 864/7968 [==>...........................] - ETA: 21s
 960/7968 [==>...........................] - ETA: 19s
1056/7968 [==>...........................] - ETA: 17s
1152/7968 [===>..........................] - ETA: 16s
1248/7968 [===>..........................] - ETA: 15s
1344/7968 [====>.........................] - ETA: 14s
1472/7968 [====>.........................] - ETA: 13s
1568/7968 [====>.........................] - ETA: 12s
1696/7968 [=====>........................] - ETA: 11s
1792/7968 [=====>........................] - ETA: 10s
1920/7968 [======>.......................] - ETA: 10s
2048/7968 [======>.......................] - ETA: 9s 
2144/7968 [=======>......................] - ETA: 8s
2240/7968 [=======>......................] - ETA: 8s
2368/7968 [=======>......................] - ETA: 8s
2496/7968 [========>.....................] - ETA: 7s
2592/7968 [========>.....................] - ETA: 7s
2688/7968 [=========>....................] - ETA: 7s
2784/7968 [=========>....................] - ETA: 6s
2912/7968 [=========>....................] - ETA: 6s
3040/7968 [==========>...................] - ETA: 6s
3136/7968 [==========>...................] - ETA: 5s
3264/7968 [===========>..................] - ETA: 5s
3360/7968 [===========>..................] - ETA: 5s
3488/7968 [============>.................] - ETA: 5s
3584/7968 [============>.................] - ETA: 4s
3680/7968 [============>.................] - ETA: 4s
3808/7968 [=============>................] - ETA: 4s
3904/7968 [=============>................] - ETA: 4s
4032/7968 [==============>...............] - ETA: 4s
4128/7968 [==============>...............] - ETA: 4s
4256/7968 [===============>..............] - ETA: 3s
4384/7968 [===============>..............] - ETA: 3s
4480/7968 [===============>..............] - ETA: 3s
4576/7968 [================>.............] - ETA: 3s
4672/7968 [================>.............] - ETA: 3s
4768/7968 [================>.............] - ETA: 3s
4864/7968 [=================>............] - ETA: 3s
4960/7968 [=================>............] - ETA: 2s
5088/7968 [==================>...........] - ETA: 2s
5184/7968 [==================>...........] - ETA: 2s
5280/7968 [==================>...........] - ETA: 2s
5408/7968 [===================>..........] - ETA: 2s
5536/7968 [===================>..........] - ETA: 2s
5632/7968 [====================>.........] - ETA: 2s
5760/7968 [====================>.........] - ETA: 1s
5856/7968 [=====================>........] - ETA: 1s
5952/7968 [=====================>........] - ETA: 1s
6048/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 1s
6240/7968 [======================>.......] - ETA: 1s
6336/7968 [======================>.......] - ETA: 1s
6432/7968 [=======================>......] - ETA: 1s
6528/7968 [=======================>......] - ETA: 1s
6624/7968 [=======================>......] - ETA: 1s
6720/7968 [========================>.....] - ETA: 1s
6816/7968 [========================>.....] - ETA: 0s
6912/7968 [=========================>....] - ETA: 0s
7008/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7648/7968 [===========================>..] - ETA: 0s
7744/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7936/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 6s 796us/step
Best saved model val accuracy:0.855
best saved model auc_score ------------------> 0.937
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_127[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_128[0][0]             
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_55[0][0]             
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_129[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_130[0][0]             
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 52, 96, 96)   0           concatenate_55[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_56[0][0]             
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_131[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_132[0][0]             
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 70, 96, 96)   0           concatenate_56[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_57[0][0]             
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_133[0][0]             
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_134[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_135[0][0]             
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_13[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_58[0][0]             
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_136[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_137[0][0]             
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 71, 48, 48)   0           concatenate_58[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_59[0][0]             
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_138[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_139[0][0]             
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 89, 48, 48)   0           concatenate_59[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_60[0][0]             
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_140[0][0]             
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_141[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_142[0][0]             
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_14[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_61[0][0]             
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_143[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_144[0][0]             
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 80, 24, 24)   0           concatenate_61[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_62[0][0]             
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_145[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_146[0][0]             
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 98, 24, 24)   0           concatenate_62[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_63[0][0]             
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 98)           0           activation_147[0][0]             
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            99          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 60s - loss: 0.6864 - acc: 0.7323 - val_loss: 0.6251 - val_acc: 0.7725

Epoch 00001: val_loss improved from inf to 0.62514, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 52s - loss: 0.6029 - acc: 0.7828 - val_loss: 0.5870 - val_acc: 0.7769

Epoch 00002: val_loss improved from 0.62514 to 0.58696, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 52s - loss: 0.5745 - acc: 0.7955 - val_loss: 0.5626 - val_acc: 0.7953

Epoch 00003: val_loss improved from 0.58696 to 0.56263, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 52s - loss: 0.5523 - acc: 0.8067 - val_loss: 0.5427 - val_acc: 0.8058

Epoch 00004: val_loss improved from 0.56263 to 0.54274, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 52s - loss: 0.5339 - acc: 0.8179 - val_loss: 0.5318 - val_acc: 0.8155

Epoch 00005: val_loss improved from 0.54274 to 0.53181, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 52s - loss: 0.5198 - acc: 0.8264 - val_loss: 0.5012 - val_acc: 0.8350

Epoch 00006: val_loss improved from 0.53181 to 0.50121, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 52s - loss: 0.5023 - acc: 0.8353 - val_loss: 0.5042 - val_acc: 0.8315

Epoch 00007: val_loss did not improve from 0.50121
Epoch 8/30
 - 52s - loss: 0.4895 - acc: 0.8409 - val_loss: 0.4858 - val_acc: 0.8367

Epoch 00008: val_loss improved from 0.50121 to 0.48583, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 52s - loss: 0.4779 - acc: 0.8468 - val_loss: 0.4627 - val_acc: 0.8562

Epoch 00009: val_loss improved from 0.48583 to 0.46275, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 52s - loss: 0.4664 - acc: 0.8534 - val_loss: 0.4553 - val_acc: 0.8636

Epoch 00010: val_loss improved from 0.46275 to 0.45528, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 52s - loss: 0.4559 - acc: 0.8598 - val_loss: 0.4506 - val_acc: 0.8671

Epoch 00011: val_loss improved from 0.45528 to 0.45060, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 52s - loss: 0.4468 - acc: 0.8655 - val_loss: 0.4534 - val_acc: 0.8638

Epoch 00012: val_loss did not improve from 0.45060
Epoch 13/30
 - 52s - loss: 0.4363 - acc: 0.8706 - val_loss: 0.4345 - val_acc: 0.8709

Epoch 00013: val_loss improved from 0.45060 to 0.43454, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 52s - loss: 0.4297 - acc: 0.8731 - val_loss: 0.4489 - val_acc: 0.8690

Epoch 00014: val_loss did not improve from 0.43454
Epoch 15/30
 - 52s - loss: 0.4181 - acc: 0.8788 - val_loss: 0.4144 - val_acc: 0.8835

Epoch 00015: val_loss improved from 0.43454 to 0.41445, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 52s - loss: 0.4132 - acc: 0.8824 - val_loss: 0.4138 - val_acc: 0.8901

Epoch 00016: val_loss improved from 0.41445 to 0.41383, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 52s - loss: 0.4037 - acc: 0.8872 - val_loss: 0.4008 - val_acc: 0.8832

Epoch 00017: val_loss improved from 0.41383 to 0.40081, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 52s - loss: 0.3967 - acc: 0.8908 - val_loss: 0.4199 - val_acc: 0.8823

Epoch 00018: val_loss did not improve from 0.40081
Epoch 19/30
 - 52s - loss: 0.3911 - acc: 0.8945 - val_loss: 0.3969 - val_acc: 0.8901

Epoch 00019: val_loss improved from 0.40081 to 0.39694, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 52s - loss: 0.3809 - acc: 0.8973 - val_loss: 0.3913 - val_acc: 0.8921

Epoch 00020: val_loss improved from 0.39694 to 0.39127, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 52s - loss: 0.3778 - acc: 0.8984 - val_loss: 0.3829 - val_acc: 0.8958

Epoch 00021: val_loss improved from 0.39127 to 0.38292, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 52s - loss: 0.3720 - acc: 0.9002 - val_loss: 0.4190 - val_acc: 0.8726

Epoch 00022: val_loss did not improve from 0.38292
Epoch 23/30
 - 52s - loss: 0.3650 - acc: 0.9035 - val_loss: 0.3688 - val_acc: 0.9030

Epoch 00023: val_loss improved from 0.38292 to 0.36881, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 52s - loss: 0.3588 - acc: 0.9077 - val_loss: 0.3869 - val_acc: 0.8933

Epoch 00024: val_loss did not improve from 0.36881
Epoch 25/30
 - 52s - loss: 0.3545 - acc: 0.9111 - val_loss: 0.3750 - val_acc: 0.8973

Epoch 00025: val_loss did not improve from 0.36881
Epoch 26/30
 - 52s - loss: 0.3482 - acc: 0.9142 - val_loss: 0.3776 - val_acc: 0.8952

Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.

Epoch 00026: val_loss did not improve from 0.36881
Epoch 27/30
 - 52s - loss: 0.3335 - acc: 0.9215 - val_loss: 0.3555 - val_acc: 0.9098

Epoch 00027: val_loss improved from 0.36881 to 0.35548, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 52s - loss: 0.3322 - acc: 0.9196 - val_loss: 0.3547 - val_acc: 0.9098

Epoch 00028: val_loss improved from 0.35548 to 0.35467, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 52s - loss: 0.3281 - acc: 0.9239 - val_loss: 0.3535 - val_acc: 0.9111

Epoch 00029: val_loss improved from 0.35467 to 0.35354, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 52s - loss: 0.3270 - acc: 0.9238 - val_loss: 0.3535 - val_acc: 0.9094

Epoch 00030: val_loss did not improve from 0.35354

  32/7968 [..............................] - ETA: 4s
 160/7968 [..............................] - ETA: 4s
 288/7968 [>.............................] - ETA: 3s
 416/7968 [>.............................] - ETA: 3s
 544/7968 [=>............................] - ETA: 3s
 672/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
1024/7968 [==>...........................] - ETA: 3s
1152/7968 [===>..........................] - ETA: 3s
1248/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1504/7968 [====>.........................] - ETA: 3s
1600/7968 [=====>........................] - ETA: 3s
1696/7968 [=====>........................] - ETA: 3s
1824/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2080/7968 [======>.......................] - ETA: 3s
2208/7968 [=======>......................] - ETA: 2s
2336/7968 [=======>......................] - ETA: 2s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2656/7968 [=========>....................] - ETA: 2s
2784/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3040/7968 [==========>...................] - ETA: 2s
3168/7968 [==========>...................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3424/7968 [===========>..................] - ETA: 2s
3552/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3808/7968 [=============>................] - ETA: 2s
3936/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4192/7968 [==============>...............] - ETA: 1s
4320/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4576/7968 [================>.............] - ETA: 1s
4672/7968 [================>.............] - ETA: 1s
4800/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5152/7968 [==================>...........] - ETA: 1s
5280/7968 [==================>...........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5536/7968 [===================>..........] - ETA: 1s
5632/7968 [====================>.........] - ETA: 1s
5760/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6144/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6496/7968 [=======================>......] - ETA: 0s
6592/7968 [=======================>......] - ETA: 0s
6720/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6976/7968 [=========================>....] - ETA: 0s
7104/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7360/7968 [==========================>...] - ETA: 0s
7488/7968 [===========================>..] - ETA: 0s
7584/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7840/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 518us/step
current val accuracy:0.909
current auc_score ------------------> 0.971

  32/7968 [..............................] - ETA: 10:54
 128/7968 [..............................] - ETA: 2:45 
 224/7968 [..............................] - ETA: 1:34
 320/7968 [>.............................] - ETA: 1:06
 448/7968 [>.............................] - ETA: 48s 
 576/7968 [=>............................] - ETA: 37s
 704/7968 [=>............................] - ETA: 30s
 832/7968 [==>...........................] - ETA: 26s
 960/7968 [==>...........................] - ETA: 22s
1088/7968 [===>..........................] - ETA: 20s
1216/7968 [===>..........................] - ETA: 18s
1344/7968 [====>.........................] - ETA: 16s
1472/7968 [====>.........................] - ETA: 14s
1600/7968 [=====>........................] - ETA: 13s
1728/7968 [=====>........................] - ETA: 12s
1824/7968 [=====>........................] - ETA: 12s
1952/7968 [======>.......................] - ETA: 11s
2080/7968 [======>.......................] - ETA: 10s
2208/7968 [=======>......................] - ETA: 9s 
2336/7968 [=======>......................] - ETA: 9s
2464/7968 [========>.....................] - ETA: 8s
2592/7968 [========>.....................] - ETA: 8s
2720/7968 [=========>....................] - ETA: 7s
2848/7968 [=========>....................] - ETA: 7s
2944/7968 [==========>...................] - ETA: 7s
3072/7968 [==========>...................] - ETA: 6s
3168/7968 [==========>...................] - ETA: 6s
3296/7968 [===========>..................] - ETA: 6s
3424/7968 [===========>..................] - ETA: 5s
3552/7968 [============>.................] - ETA: 5s
3648/7968 [============>.................] - ETA: 5s
3776/7968 [=============>................] - ETA: 5s
3872/7968 [=============>................] - ETA: 4s
4000/7968 [==============>...............] - ETA: 4s
4128/7968 [==============>...............] - ETA: 4s
4256/7968 [===============>..............] - ETA: 4s
4384/7968 [===============>..............] - ETA: 4s
4480/7968 [===============>..............] - ETA: 3s
4576/7968 [================>.............] - ETA: 3s
4672/7968 [================>.............] - ETA: 3s
4768/7968 [================>.............] - ETA: 3s
4864/7968 [=================>............] - ETA: 3s
4960/7968 [=================>............] - ETA: 3s
5088/7968 [==================>...........] - ETA: 2s
5184/7968 [==================>...........] - ETA: 2s
5280/7968 [==================>...........] - ETA: 2s
5376/7968 [===================>..........] - ETA: 2s
5504/7968 [===================>..........] - ETA: 2s
5600/7968 [====================>.........] - ETA: 2s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5920/7968 [=====================>........] - ETA: 1s
6016/7968 [=====================>........] - ETA: 1s
6112/7968 [======================>.......] - ETA: 1s
6208/7968 [======================>.......] - ETA: 1s
6336/7968 [======================>.......] - ETA: 1s
6432/7968 [=======================>......] - ETA: 1s
6528/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6880/7968 [========================>.....] - ETA: 0s
6976/7968 [=========================>....] - ETA: 0s
7072/7968 [=========================>....] - ETA: 0s
7168/7968 [=========================>....] - ETA: 0s
7296/7968 [==========================>...] - ETA: 0s
7392/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7648/7968 [===========================>..] - ETA: 0s
7776/7968 [============================>.] - ETA: 0s
7872/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 7s 851us/step
Best saved model val accuracy:0.911
best saved model auc_score ------------------> 0.971
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_148[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_149[0][0]             
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_64[0][0]             
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_150[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_151[0][0]             
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 52, 96, 96)   0           concatenate_64[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_65[0][0]             
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_152[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_153[0][0]             
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 70, 96, 96)   0           concatenate_65[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_66[0][0]             
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_154[0][0]             
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_155[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_156[0][0]             
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_15[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_67[0][0]             
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_157[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_158[0][0]             
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 71, 48, 48)   0           concatenate_67[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_68[0][0]             
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_159[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_160[0][0]             
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 89, 48, 48)   0           concatenate_68[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_69[0][0]             
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_161[0][0]             
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_162[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_163[0][0]             
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_16[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_70[0][0]             
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_164[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_165[0][0]             
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 80, 24, 24)   0           concatenate_70[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_71[0][0]             
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_166[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_167[0][0]             
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 98, 24, 24)   0           concatenate_71[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_72[0][0]             
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 98)           0           activation_168[0][0]             
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            99          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 64s - loss: 0.7436 - acc: 0.6916 - val_loss: 0.6880 - val_acc: 0.7455

Epoch 00001: val_loss improved from inf to 0.68804, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 54s - loss: 0.6694 - acc: 0.7581 - val_loss: 0.6461 - val_acc: 0.7633

Epoch 00002: val_loss improved from 0.68804 to 0.64613, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 54s - loss: 0.6360 - acc: 0.7752 - val_loss: 0.6201 - val_acc: 0.7764

Epoch 00003: val_loss improved from 0.64613 to 0.62013, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 54s - loss: 0.6163 - acc: 0.7815 - val_loss: 0.6084 - val_acc: 0.7791

Epoch 00004: val_loss improved from 0.62013 to 0.60841, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 54s - loss: 0.6052 - acc: 0.7852 - val_loss: 0.5943 - val_acc: 0.7859

Epoch 00005: val_loss improved from 0.60841 to 0.59428, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 54s - loss: 0.5971 - acc: 0.7887 - val_loss: 0.5872 - val_acc: 0.7914

Epoch 00006: val_loss improved from 0.59428 to 0.58725, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 54s - loss: 0.5890 - acc: 0.7912 - val_loss: 0.5752 - val_acc: 0.7939

Epoch 00007: val_loss improved from 0.58725 to 0.57524, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 54s - loss: 0.5821 - acc: 0.7927 - val_loss: 0.5700 - val_acc: 0.7959

Epoch 00008: val_loss improved from 0.57524 to 0.57000, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 54s - loss: 0.5746 - acc: 0.8003 - val_loss: 0.5609 - val_acc: 0.8046

Epoch 00009: val_loss improved from 0.57000 to 0.56091, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 54s - loss: 0.5689 - acc: 0.8022 - val_loss: 0.5661 - val_acc: 0.7991

Epoch 00010: val_loss did not improve from 0.56091
Epoch 11/30
 - 54s - loss: 0.5604 - acc: 0.8079 - val_loss: 0.5478 - val_acc: 0.8107

Epoch 00011: val_loss improved from 0.56091 to 0.54782, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 54s - loss: 0.5548 - acc: 0.8099 - val_loss: 0.5406 - val_acc: 0.8153

Epoch 00012: val_loss improved from 0.54782 to 0.54058, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 54s - loss: 0.5488 - acc: 0.8124 - val_loss: 0.5364 - val_acc: 0.8186

Epoch 00013: val_loss improved from 0.54058 to 0.53641, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 54s - loss: 0.5432 - acc: 0.8122 - val_loss: 0.5302 - val_acc: 0.8199

Epoch 00014: val_loss improved from 0.53641 to 0.53020, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 54s - loss: 0.5360 - acc: 0.8205 - val_loss: 0.5262 - val_acc: 0.8235

Epoch 00015: val_loss improved from 0.53020 to 0.52619, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 54s - loss: 0.5338 - acc: 0.8206 - val_loss: 0.5176 - val_acc: 0.8282

Epoch 00016: val_loss improved from 0.52619 to 0.51760, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 54s - loss: 0.5245 - acc: 0.8255 - val_loss: 0.5131 - val_acc: 0.8309

Epoch 00017: val_loss improved from 0.51760 to 0.51306, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 54s - loss: 0.5222 - acc: 0.8257 - val_loss: 0.5141 - val_acc: 0.8289

Epoch 00018: val_loss did not improve from 0.51306
Epoch 19/30
 - 54s - loss: 0.5182 - acc: 0.8289 - val_loss: 0.5032 - val_acc: 0.8368

Epoch 00019: val_loss improved from 0.51306 to 0.50317, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 54s - loss: 0.5122 - acc: 0.8329 - val_loss: 0.4991 - val_acc: 0.8411

Epoch 00020: val_loss improved from 0.50317 to 0.49906, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 54s - loss: 0.5084 - acc: 0.8348 - val_loss: 0.4943 - val_acc: 0.8409

Epoch 00021: val_loss improved from 0.49906 to 0.49434, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 54s - loss: 0.5031 - acc: 0.8366 - val_loss: 0.4951 - val_acc: 0.8426

Epoch 00022: val_loss did not improve from 0.49434
Epoch 23/30
 - 54s - loss: 0.4996 - acc: 0.8405 - val_loss: 0.4935 - val_acc: 0.8414

Epoch 00023: val_loss improved from 0.49434 to 0.49347, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 54s - loss: 0.4948 - acc: 0.8449 - val_loss: 0.4835 - val_acc: 0.8479

Epoch 00024: val_loss improved from 0.49347 to 0.48350, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 54s - loss: 0.4908 - acc: 0.8448 - val_loss: 0.4824 - val_acc: 0.8503

Epoch 00025: val_loss improved from 0.48350 to 0.48239, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 54s - loss: 0.4872 - acc: 0.8468 - val_loss: 0.4746 - val_acc: 0.8512

Epoch 00026: val_loss improved from 0.48239 to 0.47456, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 54s - loss: 0.4828 - acc: 0.8499 - val_loss: 0.4729 - val_acc: 0.8534

Epoch 00027: val_loss improved from 0.47456 to 0.47285, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 54s - loss: 0.4799 - acc: 0.8511 - val_loss: 0.4834 - val_acc: 0.8510

Epoch 00028: val_loss did not improve from 0.47285
Epoch 29/30
 - 54s - loss: 0.4760 - acc: 0.8519 - val_loss: 0.4755 - val_acc: 0.8517

Epoch 00029: val_loss did not improve from 0.47285
Epoch 30/30
 - 54s - loss: 0.4711 - acc: 0.8555 - val_loss: 0.4651 - val_acc: 0.8598

Epoch 00030: val_loss improved from 0.47285 to 0.46512, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 3s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 2s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 529us/step
current val accuracy:0.860
current auc_score ------------------> 0.939

  32/7968 [..............................] - ETA: 12:40
 128/7968 [..............................] - ETA: 3:11 
 224/7968 [..............................] - ETA: 1:49
 320/7968 [>.............................] - ETA: 1:17
 416/7968 [>.............................] - ETA: 59s 
 512/7968 [>.............................] - ETA: 48s
 608/7968 [=>............................] - ETA: 40s
 704/7968 [=>............................] - ETA: 35s
 800/7968 [==>...........................] - ETA: 31s
 896/7968 [==>...........................] - ETA: 27s
 992/7968 [==>...........................] - ETA: 25s
1088/7968 [===>..........................] - ETA: 22s
1184/7968 [===>..........................] - ETA: 21s
1280/7968 [===>..........................] - ETA: 19s
1376/7968 [====>.........................] - ETA: 18s
1472/7968 [====>.........................] - ETA: 16s
1568/7968 [====>.........................] - ETA: 15s
1664/7968 [=====>........................] - ETA: 14s
1760/7968 [=====>........................] - ETA: 14s
1856/7968 [=====>........................] - ETA: 13s
1952/7968 [======>.......................] - ETA: 12s
2048/7968 [======>.......................] - ETA: 11s
2144/7968 [=======>......................] - ETA: 11s
2240/7968 [=======>......................] - ETA: 10s
2336/7968 [=======>......................] - ETA: 10s
2432/7968 [========>.....................] - ETA: 9s 
2528/7968 [========>.....................] - ETA: 9s
2624/7968 [========>.....................] - ETA: 9s
2720/7968 [=========>....................] - ETA: 8s
2816/7968 [=========>....................] - ETA: 8s
2912/7968 [=========>....................] - ETA: 7s
3008/7968 [==========>...................] - ETA: 7s
3104/7968 [==========>...................] - ETA: 7s
3200/7968 [===========>..................] - ETA: 7s
3296/7968 [===========>..................] - ETA: 6s
3392/7968 [===========>..................] - ETA: 6s
3488/7968 [============>.................] - ETA: 6s
3584/7968 [============>.................] - ETA: 6s
3680/7968 [============>.................] - ETA: 5s
3776/7968 [=============>................] - ETA: 5s
3872/7968 [=============>................] - ETA: 5s
3968/7968 [=============>................] - ETA: 5s
4064/7968 [==============>...............] - ETA: 4s
4160/7968 [==============>...............] - ETA: 4s
4256/7968 [===============>..............] - ETA: 4s
4352/7968 [===============>..............] - ETA: 4s
4448/7968 [===============>..............] - ETA: 4s
4544/7968 [================>.............] - ETA: 4s
4640/7968 [================>.............] - ETA: 3s
4736/7968 [================>.............] - ETA: 3s
4832/7968 [=================>............] - ETA: 3s
4928/7968 [=================>............] - ETA: 3s
5024/7968 [=================>............] - ETA: 3s
5120/7968 [==================>...........] - ETA: 3s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 2s
5408/7968 [===================>..........] - ETA: 2s
5504/7968 [===================>..........] - ETA: 2s
5600/7968 [====================>.........] - ETA: 2s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 7s 911us/step
Best saved model val accuracy:0.860
best saved model auc_score ------------------> 0.939
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_169[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_170[0][0]             
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_73[0][0]             
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_171[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_172[0][0]             
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 52, 96, 96)   0           concatenate_73[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_74[0][0]             
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_173[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 70, 96, 96)   0           concatenate_74[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_75[0][0]             
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_175[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_176[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_177[0][0]             
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_17[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_76[0][0]             
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_178[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_179[0][0]             
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 71, 48, 48)   0           concatenate_76[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_77[0][0]             
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_180[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_181[0][0]             
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 89, 48, 48)   0           concatenate_77[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_78[0][0]             
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_182[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_183[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_184[0][0]             
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_18[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_79[0][0]             
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_185[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_186[0][0]             
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 80, 24, 24)   0           concatenate_79[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_80[0][0]             
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_187[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_188[0][0]             
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 98, 24, 24)   0           concatenate_80[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_81[0][0]             
__________________________________________________________________________________________________
activation_189 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 98)           0           activation_189[0][0]             
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            99          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 65s - loss: 0.5969 - acc: 0.7842 - val_loss: 0.5629 - val_acc: 0.8012

Epoch 00001: val_loss improved from inf to 0.56291, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 54s - loss: 0.5235 - acc: 0.8207 - val_loss: 0.5198 - val_acc: 0.8320

Epoch 00002: val_loss improved from 0.56291 to 0.51976, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 54s - loss: 0.4881 - acc: 0.8381 - val_loss: 0.4956 - val_acc: 0.8404

Epoch 00003: val_loss improved from 0.51976 to 0.49564, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 54s - loss: 0.4654 - acc: 0.8498 - val_loss: 0.4823 - val_acc: 0.8464

Epoch 00004: val_loss improved from 0.49564 to 0.48226, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 54s - loss: 0.4433 - acc: 0.8631 - val_loss: 0.4383 - val_acc: 0.8675

Epoch 00005: val_loss improved from 0.48226 to 0.43828, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 54s - loss: 0.4289 - acc: 0.8705 - val_loss: 0.4177 - val_acc: 0.8722

Epoch 00006: val_loss improved from 0.43828 to 0.41773, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 54s - loss: 0.4098 - acc: 0.8790 - val_loss: 0.4324 - val_acc: 0.8695

Epoch 00007: val_loss did not improve from 0.41773
Epoch 8/30
 - 54s - loss: 0.3944 - acc: 0.8870 - val_loss: 0.3916 - val_acc: 0.8889

Epoch 00008: val_loss improved from 0.41773 to 0.39161, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 54s - loss: 0.3837 - acc: 0.8917 - val_loss: 0.4226 - val_acc: 0.8758

Epoch 00009: val_loss did not improve from 0.39161
Epoch 10/30
 - 54s - loss: 0.3713 - acc: 0.8977 - val_loss: 0.3994 - val_acc: 0.8798

Epoch 00010: val_loss did not improve from 0.39161
Epoch 11/30
 - 54s - loss: 0.3567 - acc: 0.9056 - val_loss: 0.4538 - val_acc: 0.8559

Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.0000000819040434e-05.

Epoch 00011: val_loss did not improve from 0.39161
Epoch 12/30
 - 54s - loss: 0.3367 - acc: 0.9157 - val_loss: 0.3507 - val_acc: 0.9042

Epoch 00012: val_loss improved from 0.39161 to 0.35072, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 54s - loss: 0.3294 - acc: 0.9185 - val_loss: 0.3455 - val_acc: 0.9096

Epoch 00013: val_loss improved from 0.35072 to 0.34549, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 54s - loss: 0.3251 - acc: 0.9206 - val_loss: 0.3405 - val_acc: 0.9103

Epoch 00014: val_loss improved from 0.34549 to 0.34051, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 54s - loss: 0.3203 - acc: 0.9228 - val_loss: 0.3590 - val_acc: 0.9029

Epoch 00015: val_loss did not improve from 0.34051
Epoch 16/30
 - 54s - loss: 0.3177 - acc: 0.9245 - val_loss: 0.3416 - val_acc: 0.9120

Epoch 00016: val_loss did not improve from 0.34051
Epoch 17/30
 - 54s - loss: 0.3144 - acc: 0.9253 - val_loss: 0.3335 - val_acc: 0.9127

Epoch 00017: val_loss improved from 0.34051 to 0.33345, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 54s - loss: 0.3111 - acc: 0.9274 - val_loss: 0.3292 - val_acc: 0.9155

Epoch 00018: val_loss improved from 0.33345 to 0.32919, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 54s - loss: 0.3051 - acc: 0.9293 - val_loss: 0.3381 - val_acc: 0.9088

Epoch 00019: val_loss did not improve from 0.32919
Epoch 20/30
 - 54s - loss: 0.3036 - acc: 0.9303 - val_loss: 0.3407 - val_acc: 0.9108

Epoch 00020: val_loss did not improve from 0.32919
Epoch 21/30
 - 54s - loss: 0.2996 - acc: 0.9325 - val_loss: 0.3471 - val_acc: 0.9083

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.3245557357800085e-06.

Epoch 00021: val_loss did not improve from 0.32919
Epoch 22/30
 - 54s - loss: 0.2899 - acc: 0.9374 - val_loss: 0.3210 - val_acc: 0.9208

Epoch 00022: val_loss improved from 0.32919 to 0.32098, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 54s - loss: 0.2890 - acc: 0.9371 - val_loss: 0.3198 - val_acc: 0.9222

Epoch 00023: val_loss improved from 0.32098 to 0.31979, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 54s - loss: 0.2889 - acc: 0.9386 - val_loss: 0.3226 - val_acc: 0.9207

Epoch 00024: val_loss did not improve from 0.31979
Epoch 25/30
 - 54s - loss: 0.2867 - acc: 0.9386 - val_loss: 0.3224 - val_acc: 0.9237

Epoch 00025: val_loss did not improve from 0.31979
Epoch 26/30
 - 54s - loss: 0.2892 - acc: 0.9364 - val_loss: 0.3187 - val_acc: 0.9224

Epoch 00026: val_loss improved from 0.31979 to 0.31874, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 54s - loss: 0.2838 - acc: 0.9395 - val_loss: 0.3186 - val_acc: 0.9214

Epoch 00027: val_loss improved from 0.31874 to 0.31862, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 54s - loss: 0.2855 - acc: 0.9396 - val_loss: 0.3160 - val_acc: 0.9232

Epoch 00028: val_loss improved from 0.31862 to 0.31602, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 54s - loss: 0.2837 - acc: 0.9399 - val_loss: 0.3185 - val_acc: 0.9234

Epoch 00029: val_loss did not improve from 0.31602
Epoch 30/30
 - 54s - loss: 0.2834 - acc: 0.9386 - val_loss: 0.3156 - val_acc: 0.9237

Epoch 00030: val_loss improved from 0.31602 to 0.31560, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 3s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 2s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 0s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 529us/step
current val accuracy:0.924
current auc_score ------------------> 0.976

  32/7968 [..............................] - ETA: 15:14
 128/7968 [..............................] - ETA: 3:49 
 224/7968 [..............................] - ETA: 2:11
 320/7968 [>.............................] - ETA: 1:31
 416/7968 [>.............................] - ETA: 1:10
 512/7968 [>.............................] - ETA: 57s 
 608/7968 [=>............................] - ETA: 48s
 704/7968 [=>............................] - ETA: 41s
 800/7968 [==>...........................] - ETA: 36s
 896/7968 [==>...........................] - ETA: 32s
 992/7968 [==>...........................] - ETA: 29s
1088/7968 [===>..........................] - ETA: 26s
1184/7968 [===>..........................] - ETA: 24s
1280/7968 [===>..........................] - ETA: 22s
1376/7968 [====>.........................] - ETA: 21s
1472/7968 [====>.........................] - ETA: 19s
1568/7968 [====>.........................] - ETA: 18s
1664/7968 [=====>........................] - ETA: 17s
1760/7968 [=====>........................] - ETA: 16s
1856/7968 [=====>........................] - ETA: 15s
1952/7968 [======>.......................] - ETA: 14s
2048/7968 [======>.......................] - ETA: 13s
2144/7968 [=======>......................] - ETA: 13s
2240/7968 [=======>......................] - ETA: 12s
2336/7968 [=======>......................] - ETA: 11s
2432/7968 [========>.....................] - ETA: 11s
2528/7968 [========>.....................] - ETA: 10s
2624/7968 [========>.....................] - ETA: 10s
2720/7968 [=========>....................] - ETA: 9s 
2816/7968 [=========>....................] - ETA: 9s
2912/7968 [=========>....................] - ETA: 9s
3008/7968 [==========>...................] - ETA: 8s
3104/7968 [==========>...................] - ETA: 8s
3200/7968 [===========>..................] - ETA: 8s
3296/7968 [===========>..................] - ETA: 7s
3392/7968 [===========>..................] - ETA: 7s
3488/7968 [============>.................] - ETA: 7s
3584/7968 [============>.................] - ETA: 6s
3680/7968 [============>.................] - ETA: 6s
3776/7968 [=============>................] - ETA: 6s
3872/7968 [=============>................] - ETA: 6s
3968/7968 [=============>................] - ETA: 5s
4064/7968 [==============>...............] - ETA: 5s
4160/7968 [==============>...............] - ETA: 5s
4256/7968 [===============>..............] - ETA: 5s
4352/7968 [===============>..............] - ETA: 4s
4448/7968 [===============>..............] - ETA: 4s
4544/7968 [================>.............] - ETA: 4s
4640/7968 [================>.............] - ETA: 4s
4736/7968 [================>.............] - ETA: 4s
4832/7968 [=================>............] - ETA: 4s
4928/7968 [=================>............] - ETA: 3s
5024/7968 [=================>............] - ETA: 3s
5120/7968 [==================>...........] - ETA: 3s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 3s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 2s
5600/7968 [====================>.........] - ETA: 2s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 8s 1000us/step
Best saved model val accuracy:0.924
best saved model auc_score ------------------> 0.976
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_190 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_190[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_191 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_191[0][0]             
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_82[0][0]             
__________________________________________________________________________________________________
activation_192 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_192[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_193 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_193[0][0]             
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 52, 96, 96)   0           concatenate_82[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_83[0][0]             
__________________________________________________________________________________________________
activation_194 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_194[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_195 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_195[0][0]             
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 70, 96, 96)   0           concatenate_83[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_84[0][0]             
__________________________________________________________________________________________________
activation_196 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_196[0][0]             
__________________________________________________________________________________________________
average_pooling2d_19 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_19[0][0]       
__________________________________________________________________________________________________
activation_197 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_197[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_198 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_198[0][0]             
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_19[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_85[0][0]             
__________________________________________________________________________________________________
activation_199 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_199[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_200 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_200[0][0]             
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 71, 48, 48)   0           concatenate_85[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_86[0][0]             
__________________________________________________________________________________________________
activation_201 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_201[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_202 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_202[0][0]             
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 89, 48, 48)   0           concatenate_86[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_87[0][0]             
__________________________________________________________________________________________________
activation_203 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_203[0][0]             
__________________________________________________________________________________________________
average_pooling2d_20 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_20[0][0]       
__________________________________________________________________________________________________
activation_204 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_204[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_205 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_205[0][0]             
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_20[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_88[0][0]             
__________________________________________________________________________________________________
activation_206 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_206[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_207 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_207[0][0]             
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 80, 24, 24)   0           concatenate_88[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_89[0][0]             
__________________________________________________________________________________________________
activation_208 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_208[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_209 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_209[0][0]             
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 98, 24, 24)   0           concatenate_89[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_90[0][0]             
__________________________________________________________________________________________________
activation_210 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 98)           0           activation_210[0][0]             
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            99          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 65s - loss: 0.5676 - acc: 0.7778 - val_loss: 0.5265 - val_acc: 0.7979

Epoch 00001: val_loss improved from inf to 0.52649, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.4421 - acc: 0.8312 - val_loss: 0.4191 - val_acc: 0.8478

Epoch 00002: val_loss improved from 0.52649 to 0.41913, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 53s - loss: 0.3686 - acc: 0.8667 - val_loss: 0.3808 - val_acc: 0.8532

Epoch 00003: val_loss improved from 0.41913 to 0.38080, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 53s - loss: 0.3239 - acc: 0.8866 - val_loss: 0.3535 - val_acc: 0.8768

Epoch 00004: val_loss improved from 0.38080 to 0.35351, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 53s - loss: 0.2862 - acc: 0.9025 - val_loss: 0.2943 - val_acc: 0.8992

Epoch 00005: val_loss improved from 0.35351 to 0.29434, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 53s - loss: 0.2603 - acc: 0.9116 - val_loss: 0.2681 - val_acc: 0.9088

Epoch 00006: val_loss improved from 0.29434 to 0.26810, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 53s - loss: 0.2402 - acc: 0.9222 - val_loss: 0.2578 - val_acc: 0.9143

Epoch 00007: val_loss improved from 0.26810 to 0.25777, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 53s - loss: 0.2232 - acc: 0.9291 - val_loss: 0.2231 - val_acc: 0.9268

Epoch 00008: val_loss improved from 0.25777 to 0.22312, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 53s - loss: 0.2078 - acc: 0.9356 - val_loss: 0.2251 - val_acc: 0.9290

Epoch 00009: val_loss did not improve from 0.22312
Epoch 10/30
 - 53s - loss: 0.1954 - acc: 0.9397 - val_loss: 0.2132 - val_acc: 0.9371

Epoch 00010: val_loss improved from 0.22312 to 0.21318, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 53s - loss: 0.1825 - acc: 0.9460 - val_loss: 0.1874 - val_acc: 0.9423

Epoch 00011: val_loss improved from 0.21318 to 0.18744, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 53s - loss: 0.1750 - acc: 0.9493 - val_loss: 0.2297 - val_acc: 0.9277

Epoch 00012: val_loss did not improve from 0.18744
Epoch 13/30
 - 53s - loss: 0.1596 - acc: 0.9542 - val_loss: 0.1579 - val_acc: 0.9552

Epoch 00013: val_loss improved from 0.18744 to 0.15791, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 53s - loss: 0.1576 - acc: 0.9559 - val_loss: 0.2492 - val_acc: 0.9261

Epoch 00014: val_loss did not improve from 0.15791
Epoch 15/30
 - 53s - loss: 0.1450 - acc: 0.9612 - val_loss: 0.2111 - val_acc: 0.9282

Epoch 00015: val_loss did not improve from 0.15791
Epoch 16/30
 - 53s - loss: 0.1426 - acc: 0.9608 - val_loss: 0.2725 - val_acc: 0.9234

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.

Epoch 00016: val_loss did not improve from 0.15791
Epoch 17/30
 - 53s - loss: 0.1008 - acc: 0.9794 - val_loss: 0.1169 - val_acc: 0.9720

Epoch 00017: val_loss improved from 0.15791 to 0.11690, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 53s - loss: 0.0877 - acc: 0.9833 - val_loss: 0.1207 - val_acc: 0.9671

Epoch 00018: val_loss did not improve from 0.11690
Epoch 19/30
 - 53s - loss: 0.0851 - acc: 0.9840 - val_loss: 0.1118 - val_acc: 0.9731

Epoch 00019: val_loss improved from 0.11690 to 0.11177, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 53s - loss: 0.0801 - acc: 0.9853 - val_loss: 0.1170 - val_acc: 0.9710

Epoch 00020: val_loss did not improve from 0.11177
Epoch 21/30
 - 53s - loss: 0.0759 - acc: 0.9869 - val_loss: 0.1333 - val_acc: 0.9651

Epoch 00021: val_loss did not improve from 0.11177
Epoch 22/30
 - 53s - loss: 0.0691 - acc: 0.9893 - val_loss: 0.1171 - val_acc: 0.9710

Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00022: val_loss did not improve from 0.11177
Epoch 23/30
 - 53s - loss: 0.0607 - acc: 0.9929 - val_loss: 0.0951 - val_acc: 0.9787

Epoch 00023: val_loss improved from 0.11177 to 0.09513, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 53s - loss: 0.0559 - acc: 0.9947 - val_loss: 0.0921 - val_acc: 0.9803

Epoch 00024: val_loss improved from 0.09513 to 0.09212, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 53s - loss: 0.0545 - acc: 0.9949 - val_loss: 0.0936 - val_acc: 0.9784

Epoch 00025: val_loss did not improve from 0.09212
Epoch 26/30
 - 53s - loss: 0.0530 - acc: 0.9956 - val_loss: 0.0921 - val_acc: 0.9789

Epoch 00026: val_loss improved from 0.09212 to 0.09206, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 53s - loss: 0.0521 - acc: 0.9954 - val_loss: 0.0942 - val_acc: 0.9775

Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.1622778103685084e-05.

Epoch 00027: val_loss did not improve from 0.09206
Epoch 28/30
 - 53s - loss: 0.0489 - acc: 0.9966 - val_loss: 0.0901 - val_acc: 0.9799

Epoch 00028: val_loss improved from 0.09206 to 0.09006, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 53s - loss: 0.0480 - acc: 0.9971 - val_loss: 0.0916 - val_acc: 0.9794

Epoch 00029: val_loss did not improve from 0.09006
Epoch 30/30
 - 53s - loss: 0.0479 - acc: 0.9967 - val_loss: 0.0901 - val_acc: 0.9798

Epoch 00030: val_loss did not improve from 0.09006

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 3s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 534us/step
current val accuracy:0.980
current auc_score ------------------> 0.998

  32/7968 [..............................] - ETA: 17:36
 128/7968 [..............................] - ETA: 4:24 
 224/7968 [..............................] - ETA: 2:31
 320/7968 [>.............................] - ETA: 1:45
 416/7968 [>.............................] - ETA: 1:21
 512/7968 [>.............................] - ETA: 1:05
 608/7968 [=>............................] - ETA: 55s 
 704/7968 [=>............................] - ETA: 47s
 800/7968 [==>...........................] - ETA: 41s
 896/7968 [==>...........................] - ETA: 37s
 992/7968 [==>...........................] - ETA: 33s
1088/7968 [===>..........................] - ETA: 30s
1184/7968 [===>..........................] - ETA: 28s
1280/7968 [===>..........................] - ETA: 25s
1376/7968 [====>.........................] - ETA: 24s
1472/7968 [====>.........................] - ETA: 22s
1568/7968 [====>.........................] - ETA: 20s
1664/7968 [=====>........................] - ETA: 19s
1760/7968 [=====>........................] - ETA: 18s
1856/7968 [=====>........................] - ETA: 17s
1952/7968 [======>.......................] - ETA: 16s
2048/7968 [======>.......................] - ETA: 15s
2144/7968 [=======>......................] - ETA: 14s
2240/7968 [=======>......................] - ETA: 14s
2336/7968 [=======>......................] - ETA: 13s
2432/7968 [========>.....................] - ETA: 12s
2528/7968 [========>.....................] - ETA: 12s
2624/7968 [========>.....................] - ETA: 11s
2720/7968 [=========>....................] - ETA: 11s
2816/7968 [=========>....................] - ETA: 10s
2912/7968 [=========>....................] - ETA: 10s
3008/7968 [==========>...................] - ETA: 9s 
3104/7968 [==========>...................] - ETA: 9s
3200/7968 [===========>..................] - ETA: 8s
3296/7968 [===========>..................] - ETA: 8s
3392/7968 [===========>..................] - ETA: 8s
3488/7968 [============>.................] - ETA: 7s
3584/7968 [============>.................] - ETA: 7s
3680/7968 [============>.................] - ETA: 7s
3776/7968 [=============>................] - ETA: 7s
3872/7968 [=============>................] - ETA: 6s
3968/7968 [=============>................] - ETA: 6s
4064/7968 [==============>...............] - ETA: 6s
4160/7968 [==============>...............] - ETA: 5s
4256/7968 [===============>..............] - ETA: 5s
4352/7968 [===============>..............] - ETA: 5s
4448/7968 [===============>..............] - ETA: 5s
4544/7968 [================>.............] - ETA: 5s
4640/7968 [================>.............] - ETA: 4s
4736/7968 [================>.............] - ETA: 4s
4832/7968 [=================>............] - ETA: 4s
4928/7968 [=================>............] - ETA: 4s
5024/7968 [=================>............] - ETA: 4s
5120/7968 [==================>...........] - ETA: 3s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 3s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 3s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 2s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 1s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 9s 1ms/step
Best saved model val accuracy:0.980
best saved model auc_score ------------------> 0.998
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_11[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_211 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_211[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_212 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_212[0][0]             
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_91[0][0]             
__________________________________________________________________________________________________
activation_213 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_213[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_214 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_214[0][0]             
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 52, 96, 96)   0           concatenate_91[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_92[0][0]             
__________________________________________________________________________________________________
activation_215 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_215[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_216 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_216[0][0]             
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 70, 96, 96)   0           concatenate_92[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_93[0][0]             
__________________________________________________________________________________________________
activation_217 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_217[0][0]             
__________________________________________________________________________________________________
average_pooling2d_21 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_21[0][0]       
__________________________________________________________________________________________________
activation_218 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_218[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_219 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_219[0][0]             
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 53, 48, 48)   0           average_pooling2d_21[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_94[0][0]             
__________________________________________________________________________________________________
activation_220 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_220[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_221 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_221[0][0]             
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 71, 48, 48)   0           concatenate_94[0][0]             
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_95[0][0]             
__________________________________________________________________________________________________
activation_222 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_222[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_223 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_223[0][0]             
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 89, 48, 48)   0           concatenate_95[0][0]             
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_96[0][0]             
__________________________________________________________________________________________________
activation_224 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_224[0][0]             
__________________________________________________________________________________________________
average_pooling2d_22 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_22[0][0]       
__________________________________________________________________________________________________
activation_225 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_225[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_226 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_226[0][0]             
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 62, 24, 24)   0           average_pooling2d_22[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_97[0][0]             
__________________________________________________________________________________________________
activation_227 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_227[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_228 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_228[0][0]             
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 80, 24, 24)   0           concatenate_97[0][0]             
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_98[0][0]             
__________________________________________________________________________________________________
activation_229 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_229[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_230 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_230[0][0]             
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 98, 24, 24)   0           concatenate_98[0][0]             
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_99[0][0]             
__________________________________________________________________________________________________
activation_231 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_11 (Gl (None, 98)           0           activation_231[0][0]             
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 1)            99          global_average_pooling2d_11[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 65s - loss: 0.7601 - acc: 0.6831 - val_loss: 0.7136 - val_acc: 0.7297

Epoch 00001: val_loss improved from inf to 0.71364, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.6923 - acc: 0.7423 - val_loss: 0.6713 - val_acc: 0.7536

Epoch 00002: val_loss improved from 0.71364 to 0.67131, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 52s - loss: 0.6533 - acc: 0.7626 - val_loss: 0.6384 - val_acc: 0.7654

Epoch 00003: val_loss improved from 0.67131 to 0.63840, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 52s - loss: 0.6328 - acc: 0.7685 - val_loss: 0.6225 - val_acc: 0.7723

Epoch 00004: val_loss improved from 0.63840 to 0.62250, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 52s - loss: 0.6187 - acc: 0.7753 - val_loss: 0.6084 - val_acc: 0.7738

Epoch 00005: val_loss improved from 0.62250 to 0.60842, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 52s - loss: 0.6043 - acc: 0.7809 - val_loss: 0.6019 - val_acc: 0.7879

Epoch 00006: val_loss improved from 0.60842 to 0.60186, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 52s - loss: 0.5949 - acc: 0.7852 - val_loss: 0.5813 - val_acc: 0.7918

Epoch 00007: val_loss improved from 0.60186 to 0.58134, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 52s - loss: 0.5876 - acc: 0.7890 - val_loss: 0.5753 - val_acc: 0.7890

Epoch 00008: val_loss improved from 0.58134 to 0.57531, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 52s - loss: 0.5794 - acc: 0.7921 - val_loss: 0.5689 - val_acc: 0.7914

Epoch 00009: val_loss improved from 0.57531 to 0.56890, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 52s - loss: 0.5721 - acc: 0.7988 - val_loss: 0.5567 - val_acc: 0.8026

Epoch 00010: val_loss improved from 0.56890 to 0.55670, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 52s - loss: 0.5652 - acc: 0.8018 - val_loss: 0.5584 - val_acc: 0.8021

Epoch 00011: val_loss did not improve from 0.55670
Epoch 12/30
 - 52s - loss: 0.5583 - acc: 0.8061 - val_loss: 0.5587 - val_acc: 0.8031

Epoch 00012: val_loss did not improve from 0.55670
Epoch 13/30
 - 52s - loss: 0.5534 - acc: 0.8086 - val_loss: 0.5402 - val_acc: 0.8130

Epoch 00013: val_loss improved from 0.55670 to 0.54024, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 52s - loss: 0.5485 - acc: 0.8132 - val_loss: 0.5345 - val_acc: 0.8170

Epoch 00014: val_loss improved from 0.54024 to 0.53446, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 52s - loss: 0.5438 - acc: 0.8123 - val_loss: 0.5324 - val_acc: 0.8166

Epoch 00015: val_loss improved from 0.53446 to 0.53244, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 52s - loss: 0.5388 - acc: 0.8173 - val_loss: 0.5355 - val_acc: 0.8174

Epoch 00016: val_loss did not improve from 0.53244
Epoch 17/30
 - 52s - loss: 0.5349 - acc: 0.8181 - val_loss: 0.5186 - val_acc: 0.8239

Epoch 00017: val_loss improved from 0.53244 to 0.51862, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 52s - loss: 0.5298 - acc: 0.8209 - val_loss: 0.5162 - val_acc: 0.8282

Epoch 00018: val_loss improved from 0.51862 to 0.51619, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 52s - loss: 0.5250 - acc: 0.8233 - val_loss: 0.5136 - val_acc: 0.8274

Epoch 00019: val_loss improved from 0.51619 to 0.51360, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 52s - loss: 0.5218 - acc: 0.8249 - val_loss: 0.5074 - val_acc: 0.8321

Epoch 00020: val_loss improved from 0.51360 to 0.50736, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 52s - loss: 0.5177 - acc: 0.8273 - val_loss: 0.5066 - val_acc: 0.8303

Epoch 00021: val_loss improved from 0.50736 to 0.50664, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 53s - loss: 0.5128 - acc: 0.8300 - val_loss: 0.5050 - val_acc: 0.8350

Epoch 00022: val_loss improved from 0.50664 to 0.50501, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 52s - loss: 0.5085 - acc: 0.8315 - val_loss: 0.5118 - val_acc: 0.8316

Epoch 00023: val_loss did not improve from 0.50501
Epoch 24/30
 - 52s - loss: 0.5056 - acc: 0.8353 - val_loss: 0.4984 - val_acc: 0.8368

Epoch 00024: val_loss improved from 0.50501 to 0.49842, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 52s - loss: 0.5013 - acc: 0.8376 - val_loss: 0.4900 - val_acc: 0.8405

Epoch 00025: val_loss improved from 0.49842 to 0.49002, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 52s - loss: 0.4980 - acc: 0.8382 - val_loss: 0.4889 - val_acc: 0.8390

Epoch 00026: val_loss improved from 0.49002 to 0.48888, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 53s - loss: 0.4973 - acc: 0.8392 - val_loss: 0.4832 - val_acc: 0.8417

Epoch 00027: val_loss improved from 0.48888 to 0.48318, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 53s - loss: 0.4938 - acc: 0.8414 - val_loss: 0.4867 - val_acc: 0.8419

Epoch 00028: val_loss did not improve from 0.48318
Epoch 29/30
 - 52s - loss: 0.4898 - acc: 0.8428 - val_loss: 0.4787 - val_acc: 0.8475

Epoch 00029: val_loss improved from 0.48318 to 0.47869, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 52s - loss: 0.4855 - acc: 0.8466 - val_loss: 0.4832 - val_acc: 0.8463

Epoch 00030: val_loss did not improve from 0.47869

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 3s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 2s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 1s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 535us/step
current val accuracy:0.846
current auc_score ------------------> 0.931

  32/7968 [..............................] - ETA: 19:21
 128/7968 [..............................] - ETA: 4:50 
 224/7968 [..............................] - ETA: 2:45
 320/7968 [>.............................] - ETA: 1:55
 416/7968 [>.............................] - ETA: 1:28
 512/7968 [>.............................] - ETA: 1:12
 608/7968 [=>............................] - ETA: 1:00
 704/7968 [=>............................] - ETA: 52s 
 800/7968 [==>...........................] - ETA: 45s
 896/7968 [==>...........................] - ETA: 40s
 992/7968 [==>...........................] - ETA: 36s
1088/7968 [===>..........................] - ETA: 33s
1184/7968 [===>..........................] - ETA: 30s
1280/7968 [===>..........................] - ETA: 28s
1376/7968 [====>.........................] - ETA: 26s
1472/7968 [====>.........................] - ETA: 24s
1568/7968 [====>.........................] - ETA: 22s
1664/7968 [=====>........................] - ETA: 21s
1760/7968 [=====>........................] - ETA: 19s
1856/7968 [=====>........................] - ETA: 18s
1952/7968 [======>.......................] - ETA: 17s
2048/7968 [======>.......................] - ETA: 16s
2144/7968 [=======>......................] - ETA: 15s
2240/7968 [=======>......................] - ETA: 15s
2336/7968 [=======>......................] - ETA: 14s
2432/7968 [========>.....................] - ETA: 13s
2528/7968 [========>.....................] - ETA: 13s
2624/7968 [========>.....................] - ETA: 12s
2720/7968 [=========>....................] - ETA: 11s
2816/7968 [=========>....................] - ETA: 11s
2912/7968 [=========>....................] - ETA: 10s
3008/7968 [==========>...................] - ETA: 10s
3104/7968 [==========>...................] - ETA: 9s 
3200/7968 [===========>..................] - ETA: 9s
3296/7968 [===========>..................] - ETA: 9s
3392/7968 [===========>..................] - ETA: 8s
3488/7968 [============>.................] - ETA: 8s
3584/7968 [============>.................] - ETA: 8s
3680/7968 [============>.................] - ETA: 7s
3776/7968 [=============>................] - ETA: 7s
3872/7968 [=============>................] - ETA: 7s
3968/7968 [=============>................] - ETA: 6s
4064/7968 [==============>...............] - ETA: 6s
4160/7968 [==============>...............] - ETA: 6s
4256/7968 [===============>..............] - ETA: 6s
4352/7968 [===============>..............] - ETA: 5s
4448/7968 [===============>..............] - ETA: 5s
4544/7968 [================>.............] - ETA: 5s
4640/7968 [================>.............] - ETA: 5s
4736/7968 [================>.............] - ETA: 4s
4832/7968 [=================>............] - ETA: 4s
4928/7968 [=================>............] - ETA: 4s
5024/7968 [=================>............] - ETA: 4s
5120/7968 [==================>...........] - ETA: 4s
5216/7968 [==================>...........] - ETA: 3s
5312/7968 [===================>..........] - ETA: 3s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 3s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 2s
5888/7968 [=====================>........] - ETA: 2s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 1s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 9s 1ms/step
Best saved model val accuracy:0.848
best saved model auc_score ------------------> 0.931
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_12[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_232 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_232[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_233 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_233[0][0]             
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_100[0][0]            
__________________________________________________________________________________________________
activation_234 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_234[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_235 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_235[0][0]             
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 52, 96, 96)   0           concatenate_100[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_101[0][0]            
__________________________________________________________________________________________________
activation_236 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_236[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_237 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_237[0][0]             
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 70, 96, 96)   0           concatenate_101[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_102[0][0]            
__________________________________________________________________________________________________
activation_238 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_238[0][0]             
__________________________________________________________________________________________________
average_pooling2d_23 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_23[0][0]       
__________________________________________________________________________________________________
activation_239 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_239[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_240 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_240[0][0]             
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_23[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_103[0][0]            
__________________________________________________________________________________________________
activation_241 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_241[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_242 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_242[0][0]             
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 71, 48, 48)   0           concatenate_103[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_104[0][0]            
__________________________________________________________________________________________________
activation_243 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_243[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_244 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_244[0][0]             
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 89, 48, 48)   0           concatenate_104[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_105[0][0]            
__________________________________________________________________________________________________
activation_245 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_245[0][0]             
__________________________________________________________________________________________________
average_pooling2d_24 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_24[0][0]       
__________________________________________________________________________________________________
activation_246 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_246[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_247 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_247[0][0]             
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_24[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_106[0][0]            
__________________________________________________________________________________________________
activation_248 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_248[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_249 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_249[0][0]             
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 80, 24, 24)   0           concatenate_106[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_107[0][0]            
__________________________________________________________________________________________________
activation_250 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_250[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_251 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_251[0][0]             
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 98, 24, 24)   0           concatenate_107[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_108[0][0]            
__________________________________________________________________________________________________
activation_252 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_12 (Gl (None, 98)           0           activation_252[0][0]             
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 1)            99          global_average_pooling2d_12[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 67s - loss: 0.7277 - acc: 0.7202 - val_loss: 0.6767 - val_acc: 0.7474

Epoch 00001: val_loss improved from inf to 0.67674, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.6622 - acc: 0.7551 - val_loss: 0.6536 - val_acc: 0.7525

Epoch 00002: val_loss improved from 0.67674 to 0.65363, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 53s - loss: 0.6399 - acc: 0.7640 - val_loss: 0.6307 - val_acc: 0.7710

Epoch 00003: val_loss improved from 0.65363 to 0.63066, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 52s - loss: 0.6237 - acc: 0.7696 - val_loss: 0.6133 - val_acc: 0.7737

Epoch 00004: val_loss improved from 0.63066 to 0.61330, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 52s - loss: 0.6111 - acc: 0.7786 - val_loss: 0.5992 - val_acc: 0.7823

Epoch 00005: val_loss improved from 0.61330 to 0.59922, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 53s - loss: 0.5989 - acc: 0.7841 - val_loss: 0.5895 - val_acc: 0.7853

Epoch 00006: val_loss improved from 0.59922 to 0.58946, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 52s - loss: 0.5896 - acc: 0.7885 - val_loss: 0.5892 - val_acc: 0.7864

Epoch 00007: val_loss improved from 0.58946 to 0.58922, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 52s - loss: 0.5807 - acc: 0.7950 - val_loss: 0.5743 - val_acc: 0.7923

Epoch 00008: val_loss improved from 0.58922 to 0.57429, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 52s - loss: 0.5714 - acc: 0.8029 - val_loss: 0.5651 - val_acc: 0.8015

Epoch 00009: val_loss improved from 0.57429 to 0.56509, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 52s - loss: 0.5641 - acc: 0.8041 - val_loss: 0.5847 - val_acc: 0.7805

Epoch 00010: val_loss did not improve from 0.56509
Epoch 11/30
 - 52s - loss: 0.5580 - acc: 0.8067 - val_loss: 0.5516 - val_acc: 0.8065

Epoch 00011: val_loss improved from 0.56509 to 0.55162, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 52s - loss: 0.5522 - acc: 0.8110 - val_loss: 0.5465 - val_acc: 0.8119

Epoch 00012: val_loss improved from 0.55162 to 0.54645, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 52s - loss: 0.5431 - acc: 0.8170 - val_loss: 0.5469 - val_acc: 0.8102

Epoch 00013: val_loss did not improve from 0.54645
Epoch 14/30
 - 52s - loss: 0.5400 - acc: 0.8178 - val_loss: 0.5290 - val_acc: 0.8215

Epoch 00014: val_loss improved from 0.54645 to 0.52900, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 52s - loss: 0.5336 - acc: 0.8204 - val_loss: 0.5248 - val_acc: 0.8194

Epoch 00015: val_loss improved from 0.52900 to 0.52481, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 52s - loss: 0.5282 - acc: 0.8249 - val_loss: 0.5177 - val_acc: 0.8302

Epoch 00016: val_loss improved from 0.52481 to 0.51772, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 52s - loss: 0.5224 - acc: 0.8280 - val_loss: 0.5144 - val_acc: 0.8283

Epoch 00017: val_loss improved from 0.51772 to 0.51440, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 52s - loss: 0.5172 - acc: 0.8303 - val_loss: 0.5228 - val_acc: 0.8213

Epoch 00018: val_loss did not improve from 0.51440
Epoch 19/30
 - 52s - loss: 0.5114 - acc: 0.8343 - val_loss: 0.5048 - val_acc: 0.8358

Epoch 00019: val_loss improved from 0.51440 to 0.50483, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 52s - loss: 0.5074 - acc: 0.8355 - val_loss: 0.5028 - val_acc: 0.8346

Epoch 00020: val_loss improved from 0.50483 to 0.50282, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 52s - loss: 0.5027 - acc: 0.8386 - val_loss: 0.5049 - val_acc: 0.8338

Epoch 00021: val_loss did not improve from 0.50282
Epoch 22/30
 - 52s - loss: 0.5007 - acc: 0.8383 - val_loss: 0.4935 - val_acc: 0.8387

Epoch 00022: val_loss improved from 0.50282 to 0.49349, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 52s - loss: 0.4970 - acc: 0.8430 - val_loss: 0.4996 - val_acc: 0.8431

Epoch 00023: val_loss did not improve from 0.49349
Epoch 24/30
 - 52s - loss: 0.4926 - acc: 0.8446 - val_loss: 0.4899 - val_acc: 0.8430

Epoch 00024: val_loss improved from 0.49349 to 0.48987, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 52s - loss: 0.4905 - acc: 0.8453 - val_loss: 0.4826 - val_acc: 0.8450

Epoch 00025: val_loss improved from 0.48987 to 0.48264, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 52s - loss: 0.4859 - acc: 0.8496 - val_loss: 0.5008 - val_acc: 0.8394

Epoch 00026: val_loss did not improve from 0.48264
Epoch 27/30
 - 52s - loss: 0.4818 - acc: 0.8496 - val_loss: 0.4853 - val_acc: 0.8468

Epoch 00027: val_loss did not improve from 0.48264
Epoch 28/30
 - 52s - loss: 0.4788 - acc: 0.8532 - val_loss: 0.4723 - val_acc: 0.8493

Epoch 00028: val_loss improved from 0.48264 to 0.47234, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 52s - loss: 0.4759 - acc: 0.8540 - val_loss: 0.4756 - val_acc: 0.8513

Epoch 00029: val_loss did not improve from 0.47234
Epoch 30/30
 - 52s - loss: 0.4747 - acc: 0.8533 - val_loss: 0.4773 - val_acc: 0.8569

Epoch 00030: val_loss did not improve from 0.47234

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 3s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 2s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 1s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 545us/step
current val accuracy:0.857
current auc_score ------------------> 0.935

  32/7968 [..............................] - ETA: 21:43
 128/7968 [..............................] - ETA: 5:25 
 224/7968 [..............................] - ETA: 3:05
 320/7968 [>.............................] - ETA: 2:09
 416/7968 [>.............................] - ETA: 1:39
 512/7968 [>.............................] - ETA: 1:20
 608/7968 [=>............................] - ETA: 1:07
 704/7968 [=>............................] - ETA: 58s 
 800/7968 [==>...........................] - ETA: 50s
 896/7968 [==>...........................] - ETA: 45s
 992/7968 [==>...........................] - ETA: 40s
1088/7968 [===>..........................] - ETA: 37s
1184/7968 [===>..........................] - ETA: 33s
1280/7968 [===>..........................] - ETA: 31s
1376/7968 [====>.........................] - ETA: 28s
1472/7968 [====>.........................] - ETA: 26s
1568/7968 [====>.........................] - ETA: 24s
1664/7968 [=====>........................] - ETA: 23s
1760/7968 [=====>........................] - ETA: 21s
1856/7968 [=====>........................] - ETA: 20s
1952/7968 [======>.......................] - ETA: 19s
2048/7968 [======>.......................] - ETA: 18s
2144/7968 [=======>......................] - ETA: 17s
2240/7968 [=======>......................] - ETA: 16s
2336/7968 [=======>......................] - ETA: 15s
2432/7968 [========>.....................] - ETA: 15s
2528/7968 [========>.....................] - ETA: 14s
2624/7968 [========>.....................] - ETA: 13s
2720/7968 [=========>....................] - ETA: 13s
2816/7968 [=========>....................] - ETA: 12s
2912/7968 [=========>....................] - ETA: 11s
3008/7968 [==========>...................] - ETA: 11s
3104/7968 [==========>...................] - ETA: 10s
3200/7968 [===========>..................] - ETA: 10s
3296/7968 [===========>..................] - ETA: 10s
3392/7968 [===========>..................] - ETA: 9s 
3488/7968 [============>.................] - ETA: 9s
3584/7968 [============>.................] - ETA: 8s
3680/7968 [============>.................] - ETA: 8s
3776/7968 [=============>................] - ETA: 8s
3872/7968 [=============>................] - ETA: 7s
3968/7968 [=============>................] - ETA: 7s
4064/7968 [==============>...............] - ETA: 7s
4160/7968 [==============>...............] - ETA: 6s
4256/7968 [===============>..............] - ETA: 6s
4352/7968 [===============>..............] - ETA: 6s
4448/7968 [===============>..............] - ETA: 6s
4544/7968 [================>.............] - ETA: 5s
4640/7968 [================>.............] - ETA: 5s
4736/7968 [================>.............] - ETA: 5s
4832/7968 [=================>............] - ETA: 5s
4928/7968 [=================>............] - ETA: 4s
5024/7968 [=================>............] - ETA: 4s
5120/7968 [==================>...........] - ETA: 4s
5216/7968 [==================>...........] - ETA: 4s
5312/7968 [===================>..........] - ETA: 4s
5408/7968 [===================>..........] - ETA: 3s
5504/7968 [===================>..........] - ETA: 3s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 3s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 2s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 1s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 10s 1ms/step
Best saved model val accuracy:0.849
best saved model auc_score ------------------> 0.932
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_13[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_253 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_253[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_254 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_254[0][0]             
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_109[0][0]            
__________________________________________________________________________________________________
activation_255 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_255[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_256 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_256[0][0]             
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 52, 96, 96)   0           concatenate_109[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_110[0][0]            
__________________________________________________________________________________________________
activation_257 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_257[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_258 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_258[0][0]             
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 70, 96, 96)   0           concatenate_110[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_111[0][0]            
__________________________________________________________________________________________________
activation_259 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_259[0][0]             
__________________________________________________________________________________________________
average_pooling2d_25 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_25[0][0]       
__________________________________________________________________________________________________
activation_260 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_260[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_261 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_261[0][0]             
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_25[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_112[0][0]            
__________________________________________________________________________________________________
activation_262 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_262[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_263 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_263[0][0]             
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 71, 48, 48)   0           concatenate_112[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_113[0][0]            
__________________________________________________________________________________________________
activation_264 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_264[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_265 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_265[0][0]             
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 89, 48, 48)   0           concatenate_113[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_114[0][0]            
__________________________________________________________________________________________________
activation_266 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_266[0][0]             
__________________________________________________________________________________________________
average_pooling2d_26 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_26[0][0]       
__________________________________________________________________________________________________
activation_267 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_267[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_268 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_268[0][0]             
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_26[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_115[0][0]            
__________________________________________________________________________________________________
activation_269 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_269[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_270 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_270[0][0]             
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 80, 24, 24)   0           concatenate_115[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_116[0][0]            
__________________________________________________________________________________________________
activation_271 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_271[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_272 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_272[0][0]             
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 98, 24, 24)   0           concatenate_116[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_117[0][0]            
__________________________________________________________________________________________________
activation_273 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_13 (Gl (None, 98)           0           activation_273[0][0]             
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 1)            99          global_average_pooling2d_13[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 69s - loss: 0.7229 - acc: 0.7128 - val_loss: 0.6634 - val_acc: 0.7589

Epoch 00001: val_loss improved from inf to 0.66335, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.6466 - acc: 0.7645 - val_loss: 0.6339 - val_acc: 0.7620

Epoch 00002: val_loss improved from 0.66335 to 0.63389, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 53s - loss: 0.6169 - acc: 0.7778 - val_loss: 0.6100 - val_acc: 0.7742

Epoch 00003: val_loss improved from 0.63389 to 0.61004, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 53s - loss: 0.6005 - acc: 0.7860 - val_loss: 0.5941 - val_acc: 0.7890

Epoch 00004: val_loss improved from 0.61004 to 0.59414, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 53s - loss: 0.5848 - acc: 0.7964 - val_loss: 0.5754 - val_acc: 0.7934

Epoch 00005: val_loss improved from 0.59414 to 0.57538, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 53s - loss: 0.5724 - acc: 0.8005 - val_loss: 0.5654 - val_acc: 0.8002

Epoch 00006: val_loss improved from 0.57538 to 0.56537, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 53s - loss: 0.5628 - acc: 0.8053 - val_loss: 0.5537 - val_acc: 0.8065

Epoch 00007: val_loss improved from 0.56537 to 0.55369, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 53s - loss: 0.5541 - acc: 0.8080 - val_loss: 0.5475 - val_acc: 0.8058

Epoch 00008: val_loss improved from 0.55369 to 0.54754, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 53s - loss: 0.5491 - acc: 0.8116 - val_loss: 0.5366 - val_acc: 0.8141

Epoch 00009: val_loss improved from 0.54754 to 0.53662, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 53s - loss: 0.5417 - acc: 0.8152 - val_loss: 0.5305 - val_acc: 0.8159

Epoch 00010: val_loss improved from 0.53662 to 0.53053, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 53s - loss: 0.5357 - acc: 0.8181 - val_loss: 0.5260 - val_acc: 0.8200

Epoch 00011: val_loss improved from 0.53053 to 0.52600, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 53s - loss: 0.5304 - acc: 0.8229 - val_loss: 0.5188 - val_acc: 0.8217

Epoch 00012: val_loss improved from 0.52600 to 0.51875, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 53s - loss: 0.5253 - acc: 0.8233 - val_loss: 0.5272 - val_acc: 0.8208

Epoch 00013: val_loss did not improve from 0.51875
Epoch 14/30
 - 53s - loss: 0.5218 - acc: 0.8245 - val_loss: 0.5135 - val_acc: 0.8309

Epoch 00014: val_loss improved from 0.51875 to 0.51349, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 53s - loss: 0.5160 - acc: 0.8300 - val_loss: 0.5158 - val_acc: 0.8277

Epoch 00015: val_loss did not improve from 0.51349
Epoch 16/30
 - 53s - loss: 0.5111 - acc: 0.8320 - val_loss: 0.5073 - val_acc: 0.8337

Epoch 00016: val_loss improved from 0.51349 to 0.50729, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 53s - loss: 0.5064 - acc: 0.8328 - val_loss: 0.5114 - val_acc: 0.8323

Epoch 00017: val_loss did not improve from 0.50729
Epoch 18/30
 - 53s - loss: 0.5013 - acc: 0.8376 - val_loss: 0.5025 - val_acc: 0.8361

Epoch 00018: val_loss improved from 0.50729 to 0.50254, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 53s - loss: 0.4968 - acc: 0.8391 - val_loss: 0.5007 - val_acc: 0.8347

Epoch 00019: val_loss improved from 0.50254 to 0.50073, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 53s - loss: 0.4925 - acc: 0.8420 - val_loss: 0.4879 - val_acc: 0.8419

Epoch 00020: val_loss improved from 0.50073 to 0.48789, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 53s - loss: 0.4893 - acc: 0.8433 - val_loss: 0.4851 - val_acc: 0.8412

Epoch 00021: val_loss improved from 0.48789 to 0.48509, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 53s - loss: 0.4868 - acc: 0.8466 - val_loss: 0.4902 - val_acc: 0.8395

Epoch 00022: val_loss did not improve from 0.48509
Epoch 23/30
 - 53s - loss: 0.4830 - acc: 0.8473 - val_loss: 0.4847 - val_acc: 0.8419

Epoch 00023: val_loss improved from 0.48509 to 0.48470, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 53s - loss: 0.4796 - acc: 0.8494 - val_loss: 0.4778 - val_acc: 0.8441

Epoch 00024: val_loss improved from 0.48470 to 0.47777, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 53s - loss: 0.4745 - acc: 0.8497 - val_loss: 0.4891 - val_acc: 0.8395

Epoch 00025: val_loss did not improve from 0.47777
Epoch 26/30
 - 53s - loss: 0.4734 - acc: 0.8502 - val_loss: 0.4671 - val_acc: 0.8505

Epoch 00026: val_loss improved from 0.47777 to 0.46713, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 53s - loss: 0.4693 - acc: 0.8543 - val_loss: 0.4655 - val_acc: 0.8558

Epoch 00027: val_loss improved from 0.46713 to 0.46549, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 53s - loss: 0.4640 - acc: 0.8561 - val_loss: 0.4669 - val_acc: 0.8515

Epoch 00028: val_loss did not improve from 0.46549
Epoch 29/30
 - 53s - loss: 0.4639 - acc: 0.8561 - val_loss: 0.4668 - val_acc: 0.8517

Epoch 00029: val_loss did not improve from 0.46549
Epoch 30/30
 - 53s - loss: 0.4580 - acc: 0.8598 - val_loss: 0.4563 - val_acc: 0.8579

Epoch 00030: val_loss improved from 0.46549 to 0.45627, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 3s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 2s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 0s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 556us/step
current val accuracy:0.858
current auc_score ------------------> 0.938

  32/7968 [..............................] - ETA: 24:42
 128/7968 [..............................] - ETA: 6:09 
 224/7968 [..............................] - ETA: 3:30
 320/7968 [>.............................] - ETA: 2:27
 416/7968 [>.............................] - ETA: 1:52
 512/7968 [>.............................] - ETA: 1:31
 608/7968 [=>............................] - ETA: 1:16
 704/7968 [=>............................] - ETA: 1:05
 800/7968 [==>...........................] - ETA: 57s 
 896/7968 [==>...........................] - ETA: 51s
 992/7968 [==>...........................] - ETA: 45s
1088/7968 [===>..........................] - ETA: 41s
1184/7968 [===>..........................] - ETA: 38s
1280/7968 [===>..........................] - ETA: 35s
1376/7968 [====>.........................] - ETA: 32s
1472/7968 [====>.........................] - ETA: 30s
1568/7968 [====>.........................] - ETA: 28s
1664/7968 [=====>........................] - ETA: 26s
1760/7968 [=====>........................] - ETA: 24s
1856/7968 [=====>........................] - ETA: 23s
1952/7968 [======>.......................] - ETA: 21s
2048/7968 [======>.......................] - ETA: 20s
2144/7968 [=======>......................] - ETA: 19s
2240/7968 [=======>......................] - ETA: 18s
2336/7968 [=======>......................] - ETA: 17s
2432/7968 [========>.....................] - ETA: 16s
2528/7968 [========>.....................] - ETA: 15s
2624/7968 [========>.....................] - ETA: 15s
2720/7968 [=========>....................] - ETA: 14s
2816/7968 [=========>....................] - ETA: 13s
2912/7968 [=========>....................] - ETA: 13s
3008/7968 [==========>...................] - ETA: 12s
3104/7968 [==========>...................] - ETA: 12s
3200/7968 [===========>..................] - ETA: 11s
3296/7968 [===========>..................] - ETA: 11s
3392/7968 [===========>..................] - ETA: 10s
3488/7968 [============>.................] - ETA: 10s
3584/7968 [============>.................] - ETA: 9s 
3680/7968 [============>.................] - ETA: 9s
3776/7968 [=============>................] - ETA: 9s
3872/7968 [=============>................] - ETA: 8s
3968/7968 [=============>................] - ETA: 8s
4064/7968 [==============>...............] - ETA: 7s
4160/7968 [==============>...............] - ETA: 7s
4256/7968 [===============>..............] - ETA: 7s
4352/7968 [===============>..............] - ETA: 7s
4448/7968 [===============>..............] - ETA: 6s
4544/7968 [================>.............] - ETA: 6s
4640/7968 [================>.............] - ETA: 6s
4736/7968 [================>.............] - ETA: 5s
4832/7968 [=================>............] - ETA: 5s
4928/7968 [=================>............] - ETA: 5s
5024/7968 [=================>............] - ETA: 5s
5120/7968 [==================>...........] - ETA: 4s
5216/7968 [==================>...........] - ETA: 4s
5312/7968 [===================>..........] - ETA: 4s
5408/7968 [===================>..........] - ETA: 4s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 3s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 3s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 2s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 1s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 11s 1ms/step
Best saved model val accuracy:0.858
best saved model auc_score ------------------> 0.938
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_14[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_274 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_274[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_275 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_275[0][0]             
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_118[0][0]            
__________________________________________________________________________________________________
activation_276 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_276[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_277 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_277[0][0]             
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 52, 96, 96)   0           concatenate_118[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_119[0][0]            
__________________________________________________________________________________________________
activation_278 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_278[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_279 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_279[0][0]             
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 70, 96, 96)   0           concatenate_119[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_120[0][0]            
__________________________________________________________________________________________________
activation_280 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_280[0][0]             
__________________________________________________________________________________________________
average_pooling2d_27 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_27[0][0]       
__________________________________________________________________________________________________
activation_281 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_281[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_282 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_282[0][0]             
__________________________________________________________________________________________________
concatenate_121 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_27[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_121[0][0]            
__________________________________________________________________________________________________
activation_283 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_283[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_284 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_284[0][0]             
__________________________________________________________________________________________________
concatenate_122 (Concatenate)   (None, 71, 48, 48)   0           concatenate_121[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_122[0][0]            
__________________________________________________________________________________________________
activation_285 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_285[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_286 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_286[0][0]             
__________________________________________________________________________________________________
concatenate_123 (Concatenate)   (None, 89, 48, 48)   0           concatenate_122[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_123[0][0]            
__________________________________________________________________________________________________
activation_287 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_287[0][0]             
__________________________________________________________________________________________________
average_pooling2d_28 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_28[0][0]       
__________________________________________________________________________________________________
activation_288 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_288[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_289 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_289[0][0]             
__________________________________________________________________________________________________
concatenate_124 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_28[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_124[0][0]            
__________________________________________________________________________________________________
activation_290 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_290[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_291 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_291[0][0]             
__________________________________________________________________________________________________
concatenate_125 (Concatenate)   (None, 80, 24, 24)   0           concatenate_124[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_125[0][0]            
__________________________________________________________________________________________________
activation_292 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_292[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_293 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_293[0][0]             
__________________________________________________________________________________________________
concatenate_126 (Concatenate)   (None, 98, 24, 24)   0           concatenate_125[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_126[0][0]            
__________________________________________________________________________________________________
activation_294 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_14 (Gl (None, 98)           0           activation_294[0][0]             
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 1)            99          global_average_pooling2d_14[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 73s - loss: 0.6316 - acc: 0.7676 - val_loss: 0.5843 - val_acc: 0.7910

Epoch 00001: val_loss improved from inf to 0.58427, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 55s - loss: 0.5723 - acc: 0.7969 - val_loss: 0.5841 - val_acc: 0.7790

Epoch 00002: val_loss improved from 0.58427 to 0.58409, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 55s - loss: 0.5471 - acc: 0.8085 - val_loss: 0.5277 - val_acc: 0.8085

Epoch 00003: val_loss improved from 0.58409 to 0.52772, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 55s - loss: 0.5293 - acc: 0.8169 - val_loss: 0.5315 - val_acc: 0.8207

Epoch 00004: val_loss did not improve from 0.52772
Epoch 5/30
 - 55s - loss: 0.5148 - acc: 0.8232 - val_loss: 0.5006 - val_acc: 0.8312

Epoch 00005: val_loss improved from 0.52772 to 0.50056, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 55s - loss: 0.4979 - acc: 0.8342 - val_loss: 0.4920 - val_acc: 0.8336

Epoch 00006: val_loss improved from 0.50056 to 0.49201, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 55s - loss: 0.4875 - acc: 0.8405 - val_loss: 0.4810 - val_acc: 0.8434

Epoch 00007: val_loss improved from 0.49201 to 0.48099, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 55s - loss: 0.4792 - acc: 0.8462 - val_loss: 0.4588 - val_acc: 0.8529

Epoch 00008: val_loss improved from 0.48099 to 0.45880, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 55s - loss: 0.4691 - acc: 0.8499 - val_loss: 0.4730 - val_acc: 0.8470

Epoch 00009: val_loss did not improve from 0.45880
Epoch 10/30
 - 55s - loss: 0.4575 - acc: 0.8540 - val_loss: 0.4962 - val_acc: 0.8299

Epoch 00010: val_loss did not improve from 0.45880
Epoch 11/30
 - 55s - loss: 0.4495 - acc: 0.8604 - val_loss: 0.4485 - val_acc: 0.8616

Epoch 00011: val_loss improved from 0.45880 to 0.44855, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 55s - loss: 0.4404 - acc: 0.8669 - val_loss: 0.4399 - val_acc: 0.8667

Epoch 00012: val_loss improved from 0.44855 to 0.43993, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 55s - loss: 0.4332 - acc: 0.8689 - val_loss: 0.4315 - val_acc: 0.8755

Epoch 00013: val_loss improved from 0.43993 to 0.43154, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 55s - loss: 0.4253 - acc: 0.8733 - val_loss: 0.4209 - val_acc: 0.8758

Epoch 00014: val_loss improved from 0.43154 to 0.42092, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 55s - loss: 0.4150 - acc: 0.8780 - val_loss: 0.4080 - val_acc: 0.8798

Epoch 00015: val_loss improved from 0.42092 to 0.40804, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 55s - loss: 0.4090 - acc: 0.8823 - val_loss: 0.4329 - val_acc: 0.8676

Epoch 00016: val_loss did not improve from 0.40804
Epoch 17/30
 - 55s - loss: 0.4017 - acc: 0.8860 - val_loss: 0.4179 - val_acc: 0.8778

Epoch 00017: val_loss did not improve from 0.40804
Epoch 18/30
 - 55s - loss: 0.3948 - acc: 0.8888 - val_loss: 0.3978 - val_acc: 0.8883

Epoch 00018: val_loss improved from 0.40804 to 0.39780, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 55s - loss: 0.3870 - acc: 0.8927 - val_loss: 0.4197 - val_acc: 0.8724

Epoch 00019: val_loss did not improve from 0.39780
Epoch 20/30
 - 54s - loss: 0.3812 - acc: 0.8966 - val_loss: 0.4137 - val_acc: 0.8746

Epoch 00020: val_loss did not improve from 0.39780
Epoch 21/30
 - 54s - loss: 0.3755 - acc: 0.8978 - val_loss: 0.4042 - val_acc: 0.8879

Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000409520217e-05.

Epoch 00021: val_loss did not improve from 0.39780
Epoch 22/30
 - 54s - loss: 0.3647 - acc: 0.9043 - val_loss: 0.3934 - val_acc: 0.8864

Epoch 00022: val_loss improved from 0.39780 to 0.39340, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 54s - loss: 0.3567 - acc: 0.9088 - val_loss: 0.3708 - val_acc: 0.9010

Epoch 00023: val_loss improved from 0.39340 to 0.37076, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 54s - loss: 0.3567 - acc: 0.9076 - val_loss: 0.3643 - val_acc: 0.9059

Epoch 00024: val_loss improved from 0.37076 to 0.36430, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 54s - loss: 0.3554 - acc: 0.9081 - val_loss: 0.3631 - val_acc: 0.9021

Epoch 00025: val_loss improved from 0.36430 to 0.36306, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 54s - loss: 0.3520 - acc: 0.9110 - val_loss: 0.3710 - val_acc: 0.9026

Epoch 00026: val_loss did not improve from 0.36306
Epoch 27/30
 - 54s - loss: 0.3499 - acc: 0.9116 - val_loss: 0.3626 - val_acc: 0.9016

Epoch 00027: val_loss improved from 0.36306 to 0.36257, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 54s - loss: 0.3455 - acc: 0.9141 - val_loss: 0.3607 - val_acc: 0.9045

Epoch 00028: val_loss improved from 0.36257 to 0.36071, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 54s - loss: 0.3477 - acc: 0.9131 - val_loss: 0.3585 - val_acc: 0.9061

Epoch 00029: val_loss improved from 0.36071 to 0.35853, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 54s - loss: 0.3430 - acc: 0.9142 - val_loss: 0.3652 - val_acc: 0.9044

Epoch 00030: val_loss did not improve from 0.35853

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 564us/step
current val accuracy:0.904
current auc_score ------------------> 0.969

  32/7968 [..............................] - ETA: 27:04
 128/7968 [..............................] - ETA: 6:44 
 224/7968 [..............................] - ETA: 3:50
 320/7968 [>.............................] - ETA: 2:40
 416/7968 [>.............................] - ETA: 2:03
 512/7968 [>.............................] - ETA: 1:39
 608/7968 [=>............................] - ETA: 1:23
 704/7968 [=>............................] - ETA: 1:11
 800/7968 [==>...........................] - ETA: 1:02
 896/7968 [==>...........................] - ETA: 55s 
 992/7968 [==>...........................] - ETA: 49s
1088/7968 [===>..........................] - ETA: 45s
1184/7968 [===>..........................] - ETA: 41s
1280/7968 [===>..........................] - ETA: 37s
1376/7968 [====>.........................] - ETA: 35s
1472/7968 [====>.........................] - ETA: 32s
1568/7968 [====>.........................] - ETA: 30s
1664/7968 [=====>........................] - ETA: 28s
1760/7968 [=====>........................] - ETA: 26s
1856/7968 [=====>........................] - ETA: 25s
1952/7968 [======>.......................] - ETA: 23s
2048/7968 [======>.......................] - ETA: 22s
2144/7968 [=======>......................] - ETA: 21s
2240/7968 [=======>......................] - ETA: 19s
2336/7968 [=======>......................] - ETA: 18s
2432/7968 [========>.....................] - ETA: 18s
2528/7968 [========>.....................] - ETA: 17s
2624/7968 [========>.....................] - ETA: 16s
2720/7968 [=========>....................] - ETA: 15s
2816/7968 [=========>....................] - ETA: 14s
2912/7968 [=========>....................] - ETA: 14s
3008/7968 [==========>...................] - ETA: 13s
3104/7968 [==========>...................] - ETA: 13s
3200/7968 [===========>..................] - ETA: 12s
3296/7968 [===========>..................] - ETA: 11s
3392/7968 [===========>..................] - ETA: 11s
3488/7968 [============>.................] - ETA: 10s
3584/7968 [============>.................] - ETA: 10s
3680/7968 [============>.................] - ETA: 10s
3776/7968 [=============>................] - ETA: 9s 
3872/7968 [=============>................] - ETA: 9s
3968/7968 [=============>................] - ETA: 8s
4064/7968 [==============>...............] - ETA: 8s
4160/7968 [==============>...............] - ETA: 8s
4256/7968 [===============>..............] - ETA: 7s
4352/7968 [===============>..............] - ETA: 7s
4448/7968 [===============>..............] - ETA: 7s
4544/7968 [================>.............] - ETA: 6s
4640/7968 [================>.............] - ETA: 6s
4736/7968 [================>.............] - ETA: 6s
4832/7968 [=================>............] - ETA: 6s
4928/7968 [=================>............] - ETA: 5s
5024/7968 [=================>............] - ETA: 5s
5120/7968 [==================>...........] - ETA: 5s
5216/7968 [==================>...........] - ETA: 5s
5312/7968 [===================>..........] - ETA: 4s
5408/7968 [===================>..........] - ETA: 4s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 3s
5792/7968 [====================>.........] - ETA: 3s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 2s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 1s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 11s 1ms/step
Best saved model val accuracy:0.906
best saved model auc_score ------------------> 0.969
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_15[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_295 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_295[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_296 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_296[0][0]             
__________________________________________________________________________________________________
concatenate_127 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_127[0][0]            
__________________________________________________________________________________________________
activation_297 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_297[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_298 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_298[0][0]             
__________________________________________________________________________________________________
concatenate_128 (Concatenate)   (None, 52, 96, 96)   0           concatenate_127[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_128[0][0]            
__________________________________________________________________________________________________
activation_299 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_299[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_300 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_300[0][0]             
__________________________________________________________________________________________________
concatenate_129 (Concatenate)   (None, 70, 96, 96)   0           concatenate_128[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_129[0][0]            
__________________________________________________________________________________________________
activation_301 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_301[0][0]             
__________________________________________________________________________________________________
average_pooling2d_29 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_29[0][0]       
__________________________________________________________________________________________________
activation_302 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_302[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_303 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_303[0][0]             
__________________________________________________________________________________________________
concatenate_130 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_29[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_130[0][0]            
__________________________________________________________________________________________________
activation_304 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_304[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_305 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_305[0][0]             
__________________________________________________________________________________________________
concatenate_131 (Concatenate)   (None, 71, 48, 48)   0           concatenate_130[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_131[0][0]            
__________________________________________________________________________________________________
activation_306 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_306[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_307 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_307[0][0]             
__________________________________________________________________________________________________
concatenate_132 (Concatenate)   (None, 89, 48, 48)   0           concatenate_131[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_132[0][0]            
__________________________________________________________________________________________________
activation_308 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_308[0][0]             
__________________________________________________________________________________________________
average_pooling2d_30 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_30[0][0]       
__________________________________________________________________________________________________
activation_309 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_309[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_310 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_310[0][0]             
__________________________________________________________________________________________________
concatenate_133 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_30[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_133[0][0]            
__________________________________________________________________________________________________
activation_311 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_311[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_312 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_312[0][0]             
__________________________________________________________________________________________________
concatenate_134 (Concatenate)   (None, 80, 24, 24)   0           concatenate_133[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_134[0][0]            
__________________________________________________________________________________________________
activation_313 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_313[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_314 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_314[0][0]             
__________________________________________________________________________________________________
concatenate_135 (Concatenate)   (None, 98, 24, 24)   0           concatenate_134[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_135[0][0]            
__________________________________________________________________________________________________
activation_315 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_15 (Gl (None, 98)           0           activation_315[0][0]             
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 1)            99          global_average_pooling2d_15[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 71s - loss: 0.8072 - acc: 0.6320 - val_loss: 0.7085 - val_acc: 0.7329

Epoch 00001: val_loss improved from inf to 0.70854, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 53s - loss: 0.6859 - acc: 0.7484 - val_loss: 0.6685 - val_acc: 0.7462

Epoch 00002: val_loss improved from 0.70854 to 0.66848, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 53s - loss: 0.6617 - acc: 0.7576 - val_loss: 0.6486 - val_acc: 0.7563

Epoch 00003: val_loss improved from 0.66848 to 0.64858, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 53s - loss: 0.6421 - acc: 0.7655 - val_loss: 0.6298 - val_acc: 0.7622

Epoch 00004: val_loss improved from 0.64858 to 0.62981, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 53s - loss: 0.6227 - acc: 0.7735 - val_loss: 0.6086 - val_acc: 0.7738

Epoch 00005: val_loss improved from 0.62981 to 0.60862, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 53s - loss: 0.6092 - acc: 0.7779 - val_loss: 0.6041 - val_acc: 0.7791

Epoch 00006: val_loss improved from 0.60862 to 0.60408, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 53s - loss: 0.5978 - acc: 0.7850 - val_loss: 0.5879 - val_acc: 0.7846

Epoch 00007: val_loss improved from 0.60408 to 0.58792, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 53s - loss: 0.5896 - acc: 0.7874 - val_loss: 0.5771 - val_acc: 0.7877

Epoch 00008: val_loss improved from 0.58792 to 0.57713, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 53s - loss: 0.5796 - acc: 0.7917 - val_loss: 0.5687 - val_acc: 0.7948

Epoch 00009: val_loss improved from 0.57713 to 0.56873, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 53s - loss: 0.5722 - acc: 0.7973 - val_loss: 0.5605 - val_acc: 0.7982

Epoch 00010: val_loss improved from 0.56873 to 0.56052, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 53s - loss: 0.5630 - acc: 0.8018 - val_loss: 0.5489 - val_acc: 0.8079

Epoch 00011: val_loss improved from 0.56052 to 0.54894, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 53s - loss: 0.5568 - acc: 0.8057 - val_loss: 0.5556 - val_acc: 0.7992

Epoch 00012: val_loss did not improve from 0.54894
Epoch 13/30
 - 53s - loss: 0.5516 - acc: 0.8086 - val_loss: 0.5480 - val_acc: 0.8020

Epoch 00013: val_loss improved from 0.54894 to 0.54796, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 53s - loss: 0.5453 - acc: 0.8103 - val_loss: 0.5329 - val_acc: 0.8170

Epoch 00014: val_loss improved from 0.54796 to 0.53288, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 53s - loss: 0.5389 - acc: 0.8141 - val_loss: 0.5247 - val_acc: 0.8166

Epoch 00015: val_loss improved from 0.53288 to 0.52472, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 53s - loss: 0.5342 - acc: 0.8154 - val_loss: 0.5402 - val_acc: 0.8131

Epoch 00016: val_loss did not improve from 0.52472
Epoch 17/30
 - 53s - loss: 0.5293 - acc: 0.8190 - val_loss: 0.5240 - val_acc: 0.8193

Epoch 00017: val_loss improved from 0.52472 to 0.52398, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 53s - loss: 0.5255 - acc: 0.8175 - val_loss: 0.5352 - val_acc: 0.8076

Epoch 00018: val_loss did not improve from 0.52398
Epoch 19/30
 - 53s - loss: 0.5229 - acc: 0.8222 - val_loss: 0.5091 - val_acc: 0.8272

Epoch 00019: val_loss improved from 0.52398 to 0.50914, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 53s - loss: 0.5190 - acc: 0.8236 - val_loss: 0.5010 - val_acc: 0.8333

Epoch 00020: val_loss improved from 0.50914 to 0.50099, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 53s - loss: 0.5136 - acc: 0.8279 - val_loss: 0.5005 - val_acc: 0.8366

Epoch 00021: val_loss improved from 0.50099 to 0.50054, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 53s - loss: 0.5098 - acc: 0.8311 - val_loss: 0.5018 - val_acc: 0.8306

Epoch 00022: val_loss did not improve from 0.50054
Epoch 23/30
 - 53s - loss: 0.5058 - acc: 0.8303 - val_loss: 0.4931 - val_acc: 0.8370

Epoch 00023: val_loss improved from 0.50054 to 0.49306, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 53s - loss: 0.5050 - acc: 0.8296 - val_loss: 0.4946 - val_acc: 0.8370

Epoch 00024: val_loss did not improve from 0.49306
Epoch 25/30
 - 53s - loss: 0.4996 - acc: 0.8333 - val_loss: 0.4899 - val_acc: 0.8394

Epoch 00025: val_loss improved from 0.49306 to 0.48992, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 53s - loss: 0.4965 - acc: 0.8345 - val_loss: 0.5063 - val_acc: 0.8335

Epoch 00026: val_loss did not improve from 0.48992
Epoch 27/30
 - 53s - loss: 0.4938 - acc: 0.8380 - val_loss: 0.5055 - val_acc: 0.8267

Epoch 00027: val_loss did not improve from 0.48992
Epoch 28/30
 - 53s - loss: 0.4903 - acc: 0.8386 - val_loss: 0.4771 - val_acc: 0.8441

Epoch 00028: val_loss improved from 0.48992 to 0.47708, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 53s - loss: 0.4870 - acc: 0.8400 - val_loss: 0.4866 - val_acc: 0.8450

Epoch 00029: val_loss did not improve from 0.47708
Epoch 30/30
 - 53s - loss: 0.4829 - acc: 0.8438 - val_loss: 0.4712 - val_acc: 0.8510

Epoch 00030: val_loss improved from 0.47708 to 0.47118, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 3s
 992/7968 [==>...........................] - ETA: 3s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 2s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 1s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 4s 563us/step
current val accuracy:0.851
current auc_score ------------------> 0.931

  32/7968 [..............................] - ETA: 30:19
 128/7968 [..............................] - ETA: 7:33 
 224/7968 [..............................] - ETA: 4:17
 320/7968 [>.............................] - ETA: 2:59
 416/7968 [>.............................] - ETA: 2:17
 512/7968 [>.............................] - ETA: 1:51
 608/7968 [=>............................] - ETA: 1:32
 704/7968 [=>............................] - ETA: 1:19
 800/7968 [==>...........................] - ETA: 1:09
 896/7968 [==>...........................] - ETA: 1:01
 992/7968 [==>...........................] - ETA: 55s 
1088/7968 [===>..........................] - ETA: 50s
1184/7968 [===>..........................] - ETA: 45s
1280/7968 [===>..........................] - ETA: 42s
1376/7968 [====>.........................] - ETA: 38s
1472/7968 [====>.........................] - ETA: 36s
1568/7968 [====>.........................] - ETA: 33s
1664/7968 [=====>........................] - ETA: 31s
1760/7968 [=====>........................] - ETA: 29s
1856/7968 [=====>........................] - ETA: 27s
1952/7968 [======>.......................] - ETA: 26s
2048/7968 [======>.......................] - ETA: 24s
2144/7968 [=======>......................] - ETA: 23s
2240/7968 [=======>......................] - ETA: 22s
2336/7968 [=======>......................] - ETA: 20s
2432/7968 [========>.....................] - ETA: 19s
2528/7968 [========>.....................] - ETA: 18s
2624/7968 [========>.....................] - ETA: 18s
2720/7968 [=========>....................] - ETA: 17s
2816/7968 [=========>....................] - ETA: 16s
2912/7968 [=========>....................] - ETA: 15s
3008/7968 [==========>...................] - ETA: 14s
3104/7968 [==========>...................] - ETA: 14s
3200/7968 [===========>..................] - ETA: 13s
3296/7968 [===========>..................] - ETA: 13s
3392/7968 [===========>..................] - ETA: 12s
3488/7968 [============>.................] - ETA: 11s
3584/7968 [============>.................] - ETA: 11s
3680/7968 [============>.................] - ETA: 11s
3776/7968 [=============>................] - ETA: 10s
3872/7968 [=============>................] - ETA: 10s
3968/7968 [=============>................] - ETA: 9s 
4064/7968 [==============>...............] - ETA: 9s
4160/7968 [==============>...............] - ETA: 8s
4256/7968 [===============>..............] - ETA: 8s
4352/7968 [===============>..............] - ETA: 8s
4448/7968 [===============>..............] - ETA: 7s
4544/7968 [================>.............] - ETA: 7s
4640/7968 [================>.............] - ETA: 7s
4736/7968 [================>.............] - ETA: 6s
4832/7968 [=================>............] - ETA: 6s
4928/7968 [=================>............] - ETA: 6s
5024/7968 [=================>............] - ETA: 5s
5120/7968 [==================>...........] - ETA: 5s
5216/7968 [==================>...........] - ETA: 5s
5312/7968 [===================>..........] - ETA: 5s
5408/7968 [===================>..........] - ETA: 4s
5504/7968 [===================>..........] - ETA: 4s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 4s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 3s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 2s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 12s 1ms/step
Best saved model val accuracy:0.851
best saved model auc_score ------------------> 0.931
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_16 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_16[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_316 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_316[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_317 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_317[0][0]             
__________________________________________________________________________________________________
concatenate_136 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_136[0][0]            
__________________________________________________________________________________________________
activation_318 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_318[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_319 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_319[0][0]             
__________________________________________________________________________________________________
concatenate_137 (Concatenate)   (None, 52, 96, 96)   0           concatenate_136[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_137[0][0]            
__________________________________________________________________________________________________
activation_320 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_320[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_321 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_321[0][0]             
__________________________________________________________________________________________________
concatenate_138 (Concatenate)   (None, 70, 96, 96)   0           concatenate_137[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_138[0][0]            
__________________________________________________________________________________________________
activation_322 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_322[0][0]             
__________________________________________________________________________________________________
average_pooling2d_31 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_31[0][0]       
__________________________________________________________________________________________________
activation_323 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_323[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_324 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_324[0][0]             
__________________________________________________________________________________________________
concatenate_139 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_31[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_139[0][0]            
__________________________________________________________________________________________________
activation_325 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_325[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_326 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_326[0][0]             
__________________________________________________________________________________________________
concatenate_140 (Concatenate)   (None, 71, 48, 48)   0           concatenate_139[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_140[0][0]            
__________________________________________________________________________________________________
activation_327 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_327[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_328 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_328[0][0]             
__________________________________________________________________________________________________
concatenate_141 (Concatenate)   (None, 89, 48, 48)   0           concatenate_140[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_141[0][0]            
__________________________________________________________________________________________________
activation_329 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_329[0][0]             
__________________________________________________________________________________________________
average_pooling2d_32 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_32[0][0]       
__________________________________________________________________________________________________
activation_330 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_330[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_331 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_331[0][0]             
__________________________________________________________________________________________________
concatenate_142 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_32[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_142[0][0]            
__________________________________________________________________________________________________
activation_332 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_332[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_333 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_333[0][0]             
__________________________________________________________________________________________________
concatenate_143 (Concatenate)   (None, 80, 24, 24)   0           concatenate_142[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_143[0][0]            
__________________________________________________________________________________________________
activation_334 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_334[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_335 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_335[0][0]             
__________________________________________________________________________________________________
concatenate_144 (Concatenate)   (None, 98, 24, 24)   0           concatenate_143[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_144[0][0]            
__________________________________________________________________________________________________
activation_336 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_16 (Gl (None, 98)           0           activation_336[0][0]             
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 1)            99          global_average_pooling2d_16[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 74s - loss: 0.7456 - acc: 0.7040 - val_loss: 0.6988 - val_acc: 0.7377

Epoch 00001: val_loss improved from inf to 0.69884, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 54s - loss: 0.6742 - acc: 0.7527 - val_loss: 0.6519 - val_acc: 0.7622

Epoch 00002: val_loss improved from 0.69884 to 0.65193, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 54s - loss: 0.6423 - acc: 0.7644 - val_loss: 0.6319 - val_acc: 0.7659

Epoch 00003: val_loss improved from 0.65193 to 0.63190, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 54s - loss: 0.6230 - acc: 0.7753 - val_loss: 0.6090 - val_acc: 0.7784

Epoch 00004: val_loss improved from 0.63190 to 0.60897, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 54s - loss: 0.6077 - acc: 0.7822 - val_loss: 0.5922 - val_acc: 0.7870

Epoch 00005: val_loss improved from 0.60897 to 0.59218, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 54s - loss: 0.5930 - acc: 0.7883 - val_loss: 0.5779 - val_acc: 0.7947

Epoch 00006: val_loss improved from 0.59218 to 0.57791, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 54s - loss: 0.5820 - acc: 0.7959 - val_loss: 0.5761 - val_acc: 0.7939

Epoch 00007: val_loss improved from 0.57791 to 0.57615, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 54s - loss: 0.5735 - acc: 0.7983 - val_loss: 0.5578 - val_acc: 0.8070

Epoch 00008: val_loss improved from 0.57615 to 0.55782, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 54s - loss: 0.5635 - acc: 0.8048 - val_loss: 0.5561 - val_acc: 0.8028

Epoch 00009: val_loss improved from 0.55782 to 0.55614, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 54s - loss: 0.5566 - acc: 0.8100 - val_loss: 0.5472 - val_acc: 0.8090

Epoch 00010: val_loss improved from 0.55614 to 0.54719, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 54s - loss: 0.5515 - acc: 0.8086 - val_loss: 0.5443 - val_acc: 0.8094

Epoch 00011: val_loss improved from 0.54719 to 0.54428, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 54s - loss: 0.5432 - acc: 0.8160 - val_loss: 0.5284 - val_acc: 0.8257

Epoch 00012: val_loss improved from 0.54428 to 0.52844, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 54s - loss: 0.5388 - acc: 0.8164 - val_loss: 0.5312 - val_acc: 0.8204

Epoch 00013: val_loss did not improve from 0.52844
Epoch 14/30
 - 54s - loss: 0.5324 - acc: 0.8198 - val_loss: 0.5226 - val_acc: 0.8251

Epoch 00014: val_loss improved from 0.52844 to 0.52258, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 54s - loss: 0.5284 - acc: 0.8227 - val_loss: 0.5160 - val_acc: 0.8293

Epoch 00015: val_loss improved from 0.52258 to 0.51595, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 54s - loss: 0.5220 - acc: 0.8246 - val_loss: 0.5128 - val_acc: 0.8284

Epoch 00016: val_loss improved from 0.51595 to 0.51276, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 54s - loss: 0.5165 - acc: 0.8288 - val_loss: 0.5133 - val_acc: 0.8306

Epoch 00017: val_loss did not improve from 0.51276
Epoch 18/30
 - 54s - loss: 0.5127 - acc: 0.8295 - val_loss: 0.5021 - val_acc: 0.8355

Epoch 00018: val_loss improved from 0.51276 to 0.50206, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 54s - loss: 0.5074 - acc: 0.8347 - val_loss: 0.4945 - val_acc: 0.8372

Epoch 00019: val_loss improved from 0.50206 to 0.49450, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 56s - loss: 0.5045 - acc: 0.8340 - val_loss: 0.5006 - val_acc: 0.8401

Epoch 00020: val_loss did not improve from 0.49450
Epoch 21/30
 - 54s - loss: 0.5022 - acc: 0.8377 - val_loss: 0.4907 - val_acc: 0.8407

Epoch 00021: val_loss improved from 0.49450 to 0.49070, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 54s - loss: 0.4942 - acc: 0.8422 - val_loss: 0.5017 - val_acc: 0.8371

Epoch 00022: val_loss did not improve from 0.49070
Epoch 23/30
 - 54s - loss: 0.4931 - acc: 0.8403 - val_loss: 0.4832 - val_acc: 0.8464

Epoch 00023: val_loss improved from 0.49070 to 0.48324, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 54s - loss: 0.4901 - acc: 0.8419 - val_loss: 0.4900 - val_acc: 0.8438

Epoch 00024: val_loss did not improve from 0.48324
Epoch 25/30
 - 54s - loss: 0.4851 - acc: 0.8453 - val_loss: 0.4839 - val_acc: 0.8469

Epoch 00025: val_loss did not improve from 0.48324
Epoch 26/30
 - 54s - loss: 0.4850 - acc: 0.8457 - val_loss: 0.4827 - val_acc: 0.8439

Epoch 00026: val_loss improved from 0.48324 to 0.48267, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 54s - loss: 0.4773 - acc: 0.8483 - val_loss: 0.4740 - val_acc: 0.8527

Epoch 00027: val_loss improved from 0.48267 to 0.47405, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 54s - loss: 0.4767 - acc: 0.8486 - val_loss: 0.4829 - val_acc: 0.8496

Epoch 00028: val_loss did not improve from 0.47405
Epoch 29/30
 - 54s - loss: 0.4732 - acc: 0.8519 - val_loss: 0.4626 - val_acc: 0.8560

Epoch 00029: val_loss improved from 0.47405 to 0.46259, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 54s - loss: 0.4710 - acc: 0.8535 - val_loss: 0.4732 - val_acc: 0.8493

Epoch 00030: val_loss did not improve from 0.46259

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 581us/step
current val accuracy:0.849
current auc_score ------------------> 0.937

  32/7968 [..............................] - ETA: 33:07
 128/7968 [..............................] - ETA: 8:14 
 224/7968 [..............................] - ETA: 4:41
 320/7968 [>.............................] - ETA: 3:15
 416/7968 [>.............................] - ETA: 2:29
 512/7968 [>.............................] - ETA: 2:00
 608/7968 [=>............................] - ETA: 1:41
 704/7968 [=>............................] - ETA: 1:26
 800/7968 [==>...........................] - ETA: 1:15
 896/7968 [==>...........................] - ETA: 1:07
 992/7968 [==>...........................] - ETA: 1:00
1088/7968 [===>..........................] - ETA: 54s 
1184/7968 [===>..........................] - ETA: 49s
1280/7968 [===>..........................] - ETA: 45s
1376/7968 [====>.........................] - ETA: 42s
1472/7968 [====>.........................] - ETA: 39s
1568/7968 [====>.........................] - ETA: 36s
1664/7968 [=====>........................] - ETA: 34s
1760/7968 [=====>........................] - ETA: 31s
1856/7968 [=====>........................] - ETA: 29s
1952/7968 [======>.......................] - ETA: 28s
2048/7968 [======>.......................] - ETA: 26s
2144/7968 [=======>......................] - ETA: 25s
2240/7968 [=======>......................] - ETA: 23s
2336/7968 [=======>......................] - ETA: 22s
2432/7968 [========>.....................] - ETA: 21s
2528/7968 [========>.....................] - ETA: 20s
2624/7968 [========>.....................] - ETA: 19s
2720/7968 [=========>....................] - ETA: 18s
2816/7968 [=========>....................] - ETA: 17s
2912/7968 [=========>....................] - ETA: 16s
3008/7968 [==========>...................] - ETA: 16s
3104/7968 [==========>...................] - ETA: 15s
3200/7968 [===========>..................] - ETA: 14s
3296/7968 [===========>..................] - ETA: 14s
3392/7968 [===========>..................] - ETA: 13s
3488/7968 [============>.................] - ETA: 12s
3584/7968 [============>.................] - ETA: 12s
3680/7968 [============>.................] - ETA: 11s
3776/7968 [=============>................] - ETA: 11s
3872/7968 [=============>................] - ETA: 10s
3968/7968 [=============>................] - ETA: 10s
4064/7968 [==============>...............] - ETA: 9s 
4160/7968 [==============>...............] - ETA: 9s
4256/7968 [===============>..............] - ETA: 9s
4352/7968 [===============>..............] - ETA: 8s
4448/7968 [===============>..............] - ETA: 8s
4544/7968 [================>.............] - ETA: 8s
4640/7968 [================>.............] - ETA: 7s
4736/7968 [================>.............] - ETA: 7s
4832/7968 [=================>............] - ETA: 7s
4928/7968 [=================>............] - ETA: 6s
5024/7968 [=================>............] - ETA: 6s
5120/7968 [==================>...........] - ETA: 6s
5216/7968 [==================>...........] - ETA: 5s
5312/7968 [===================>..........] - ETA: 5s
5408/7968 [===================>..........] - ETA: 5s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 4s
5696/7968 [====================>.........] - ETA: 4s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 3s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 2s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 1s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 13s 2ms/step
Best saved model val accuracy:0.856
best saved model auc_score ------------------> 0.937
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_17[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_337 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_337[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_338 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_338[0][0]             
__________________________________________________________________________________________________
concatenate_145 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_145[0][0]            
__________________________________________________________________________________________________
activation_339 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_339[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_340 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_340[0][0]             
__________________________________________________________________________________________________
concatenate_146 (Concatenate)   (None, 52, 96, 96)   0           concatenate_145[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_146[0][0]            
__________________________________________________________________________________________________
activation_341 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_341[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_342 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_342[0][0]             
__________________________________________________________________________________________________
concatenate_147 (Concatenate)   (None, 70, 96, 96)   0           concatenate_146[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_147[0][0]            
__________________________________________________________________________________________________
activation_343 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_343[0][0]             
__________________________________________________________________________________________________
average_pooling2d_33 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_33[0][0]       
__________________________________________________________________________________________________
activation_344 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_344[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_345 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_345[0][0]             
__________________________________________________________________________________________________
concatenate_148 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_33[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_148[0][0]            
__________________________________________________________________________________________________
activation_346 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_346[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_347 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_347[0][0]             
__________________________________________________________________________________________________
concatenate_149 (Concatenate)   (None, 71, 48, 48)   0           concatenate_148[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_149[0][0]            
__________________________________________________________________________________________________
activation_348 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_348[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_349 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_349[0][0]             
__________________________________________________________________________________________________
concatenate_150 (Concatenate)   (None, 89, 48, 48)   0           concatenate_149[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_150[0][0]            
__________________________________________________________________________________________________
activation_350 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_350[0][0]             
__________________________________________________________________________________________________
average_pooling2d_34 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_34[0][0]       
__________________________________________________________________________________________________
activation_351 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_351[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_352 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_352[0][0]             
__________________________________________________________________________________________________
concatenate_151 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_34[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_151[0][0]            
__________________________________________________________________________________________________
activation_353 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_353[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_354 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_354[0][0]             
__________________________________________________________________________________________________
concatenate_152 (Concatenate)   (None, 80, 24, 24)   0           concatenate_151[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_152[0][0]            
__________________________________________________________________________________________________
activation_355 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_355[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_356 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_356[0][0]             
__________________________________________________________________________________________________
concatenate_153 (Concatenate)   (None, 98, 24, 24)   0           concatenate_152[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_153[0][0]            
__________________________________________________________________________________________________
activation_357 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_17 (Gl (None, 98)           0           activation_357[0][0]             
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 1)            99          global_average_pooling2d_17[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 76s - loss: 0.7441 - acc: 0.7040 - val_loss: 0.6832 - val_acc: 0.7516

Epoch 00001: val_loss improved from inf to 0.68319, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 54s - loss: 0.6626 - acc: 0.7593 - val_loss: 0.6443 - val_acc: 0.7615

Epoch 00002: val_loss improved from 0.68319 to 0.64431, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 54s - loss: 0.6354 - acc: 0.7660 - val_loss: 0.6217 - val_acc: 0.7674

Epoch 00003: val_loss improved from 0.64431 to 0.62166, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 54s - loss: 0.6173 - acc: 0.7714 - val_loss: 0.6029 - val_acc: 0.7769

Epoch 00004: val_loss improved from 0.62166 to 0.60291, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 54s - loss: 0.6024 - acc: 0.7814 - val_loss: 0.5890 - val_acc: 0.7819

Epoch 00005: val_loss improved from 0.60291 to 0.58903, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 54s - loss: 0.5898 - acc: 0.7887 - val_loss: 0.5789 - val_acc: 0.7920

Epoch 00006: val_loss improved from 0.58903 to 0.57893, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 55s - loss: 0.5785 - acc: 0.7938 - val_loss: 0.5656 - val_acc: 0.8016

Epoch 00007: val_loss improved from 0.57893 to 0.56560, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 54s - loss: 0.5682 - acc: 0.7983 - val_loss: 0.5590 - val_acc: 0.7982

Epoch 00008: val_loss improved from 0.56560 to 0.55903, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 54s - loss: 0.5611 - acc: 0.8028 - val_loss: 0.5541 - val_acc: 0.8010

Epoch 00009: val_loss improved from 0.55903 to 0.55413, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 54s - loss: 0.5544 - acc: 0.8067 - val_loss: 0.5427 - val_acc: 0.8085

Epoch 00010: val_loss improved from 0.55413 to 0.54271, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 54s - loss: 0.5481 - acc: 0.8092 - val_loss: 0.5389 - val_acc: 0.8099

Epoch 00011: val_loss improved from 0.54271 to 0.53891, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 54s - loss: 0.5404 - acc: 0.8133 - val_loss: 0.5352 - val_acc: 0.8139

Epoch 00012: val_loss improved from 0.53891 to 0.53520, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 54s - loss: 0.5360 - acc: 0.8190 - val_loss: 0.5246 - val_acc: 0.8215

Epoch 00013: val_loss improved from 0.53520 to 0.52460, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 54s - loss: 0.5299 - acc: 0.8201 - val_loss: 0.5177 - val_acc: 0.8269

Epoch 00014: val_loss improved from 0.52460 to 0.51774, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 54s - loss: 0.5265 - acc: 0.8241 - val_loss: 0.5152 - val_acc: 0.8269

Epoch 00015: val_loss improved from 0.51774 to 0.51524, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 54s - loss: 0.5225 - acc: 0.8259 - val_loss: 0.5125 - val_acc: 0.8315

Epoch 00016: val_loss improved from 0.51524 to 0.51246, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 55s - loss: 0.5192 - acc: 0.8261 - val_loss: 0.5036 - val_acc: 0.8356

Epoch 00017: val_loss improved from 0.51246 to 0.50364, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 54s - loss: 0.5122 - acc: 0.8312 - val_loss: 0.5009 - val_acc: 0.8386

Epoch 00018: val_loss improved from 0.50364 to 0.50088, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 54s - loss: 0.5074 - acc: 0.8343 - val_loss: 0.5002 - val_acc: 0.8381

Epoch 00019: val_loss improved from 0.50088 to 0.50016, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 54s - loss: 0.5046 - acc: 0.8353 - val_loss: 0.4940 - val_acc: 0.8415

Epoch 00020: val_loss improved from 0.50016 to 0.49399, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 54s - loss: 0.5008 - acc: 0.8367 - val_loss: 0.4892 - val_acc: 0.8465

Epoch 00021: val_loss improved from 0.49399 to 0.48921, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 54s - loss: 0.4961 - acc: 0.8410 - val_loss: 0.4911 - val_acc: 0.8404

Epoch 00022: val_loss did not improve from 0.48921
Epoch 23/30
 - 54s - loss: 0.4936 - acc: 0.8414 - val_loss: 0.4846 - val_acc: 0.8559

Epoch 00023: val_loss improved from 0.48921 to 0.48465, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 54s - loss: 0.4915 - acc: 0.8414 - val_loss: 0.4863 - val_acc: 0.8451

Epoch 00024: val_loss did not improve from 0.48465
Epoch 25/30
 - 54s - loss: 0.4870 - acc: 0.8442 - val_loss: 0.4768 - val_acc: 0.8527

Epoch 00025: val_loss improved from 0.48465 to 0.47676, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 54s - loss: 0.4837 - acc: 0.8468 - val_loss: 0.4820 - val_acc: 0.8519

Epoch 00026: val_loss did not improve from 0.47676
Epoch 27/30
 - 54s - loss: 0.4769 - acc: 0.8513 - val_loss: 0.4744 - val_acc: 0.8517

Epoch 00027: val_loss improved from 0.47676 to 0.47442, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 54s - loss: 0.4785 - acc: 0.8493 - val_loss: 0.4817 - val_acc: 0.8514

Epoch 00028: val_loss did not improve from 0.47442
Epoch 29/30
 - 54s - loss: 0.4748 - acc: 0.8513 - val_loss: 0.4672 - val_acc: 0.8582

Epoch 00029: val_loss improved from 0.47442 to 0.46722, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 54s - loss: 0.4727 - acc: 0.8512 - val_loss: 0.4668 - val_acc: 0.8569

Epoch 00030: val_loss improved from 0.46722 to 0.46683, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 3s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 2s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 581us/step
current val accuracy:0.857
current auc_score ------------------> 0.936

  32/7968 [..............................] - ETA: 36:34
 128/7968 [..............................] - ETA: 9:05 
 224/7968 [..............................] - ETA: 5:10
 320/7968 [>.............................] - ETA: 3:35
 416/7968 [>.............................] - ETA: 2:44
 512/7968 [>.............................] - ETA: 2:13
 608/7968 [=>............................] - ETA: 1:51
 704/7968 [=>............................] - ETA: 1:35
 800/7968 [==>...........................] - ETA: 1:23
 896/7968 [==>...........................] - ETA: 1:13
 992/7968 [==>...........................] - ETA: 1:06
1088/7968 [===>..........................] - ETA: 59s 
1184/7968 [===>..........................] - ETA: 54s
1280/7968 [===>..........................] - ETA: 50s
1376/7968 [====>.........................] - ETA: 46s
1472/7968 [====>.........................] - ETA: 42s
1568/7968 [====>.........................] - ETA: 39s
1664/7968 [=====>........................] - ETA: 37s
1760/7968 [=====>........................] - ETA: 34s
1856/7968 [=====>........................] - ETA: 32s
1952/7968 [======>.......................] - ETA: 30s
2048/7968 [======>.......................] - ETA: 29s
2144/7968 [=======>......................] - ETA: 27s
2240/7968 [=======>......................] - ETA: 25s
2336/7968 [=======>......................] - ETA: 24s
2432/7968 [========>.....................] - ETA: 23s
2528/7968 [========>.....................] - ETA: 22s
2624/7968 [========>.....................] - ETA: 21s
2720/7968 [=========>....................] - ETA: 20s
2816/7968 [=========>....................] - ETA: 19s
2912/7968 [=========>....................] - ETA: 18s
3008/7968 [==========>...................] - ETA: 17s
3104/7968 [==========>...................] - ETA: 16s
3200/7968 [===========>..................] - ETA: 15s
3296/7968 [===========>..................] - ETA: 15s
3392/7968 [===========>..................] - ETA: 14s
3488/7968 [============>.................] - ETA: 13s
3584/7968 [============>.................] - ETA: 13s
3680/7968 [============>.................] - ETA: 12s
3776/7968 [=============>................] - ETA: 12s
3872/7968 [=============>................] - ETA: 11s
3968/7968 [=============>................] - ETA: 11s
4064/7968 [==============>...............] - ETA: 10s
4160/7968 [==============>...............] - ETA: 10s
4256/7968 [===============>..............] - ETA: 9s 
4352/7968 [===============>..............] - ETA: 9s
4448/7968 [===============>..............] - ETA: 9s
4544/7968 [================>.............] - ETA: 8s
4640/7968 [================>.............] - ETA: 8s
4736/7968 [================>.............] - ETA: 7s
4832/7968 [=================>............] - ETA: 7s
4928/7968 [=================>............] - ETA: 7s
5024/7968 [=================>............] - ETA: 6s
5120/7968 [==================>...........] - ETA: 6s
5216/7968 [==================>...........] - ETA: 6s
5312/7968 [===================>..........] - ETA: 5s
5408/7968 [===================>..........] - ETA: 5s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 5s
5696/7968 [====================>.........] - ETA: 4s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 4s
6080/7968 [=====================>........] - ETA: 3s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 2s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 1s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 14s 2ms/step
Best saved model val accuracy:0.857
best saved model auc_score ------------------> 0.936
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_18 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_18[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_358 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_358[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_359 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_359[0][0]             
__________________________________________________________________________________________________
concatenate_154 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_154[0][0]            
__________________________________________________________________________________________________
activation_360 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_360[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_361 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_361[0][0]             
__________________________________________________________________________________________________
concatenate_155 (Concatenate)   (None, 52, 96, 96)   0           concatenate_154[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_155[0][0]            
__________________________________________________________________________________________________
activation_362 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_362[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_363 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_363[0][0]             
__________________________________________________________________________________________________
concatenate_156 (Concatenate)   (None, 70, 96, 96)   0           concatenate_155[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_156[0][0]            
__________________________________________________________________________________________________
activation_364 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_364[0][0]             
__________________________________________________________________________________________________
average_pooling2d_35 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_35[0][0]       
__________________________________________________________________________________________________
activation_365 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_365[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_366 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_366[0][0]             
__________________________________________________________________________________________________
concatenate_157 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_35[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_157[0][0]            
__________________________________________________________________________________________________
activation_367 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_367[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_368 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_368[0][0]             
__________________________________________________________________________________________________
concatenate_158 (Concatenate)   (None, 71, 48, 48)   0           concatenate_157[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_158[0][0]            
__________________________________________________________________________________________________
activation_369 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_369[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_370 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_370[0][0]             
__________________________________________________________________________________________________
concatenate_159 (Concatenate)   (None, 89, 48, 48)   0           concatenate_158[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_159[0][0]            
__________________________________________________________________________________________________
activation_371 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_371[0][0]             
__________________________________________________________________________________________________
average_pooling2d_36 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_36[0][0]       
__________________________________________________________________________________________________
activation_372 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_372[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_373 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_373[0][0]             
__________________________________________________________________________________________________
concatenate_160 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_36[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_160[0][0]            
__________________________________________________________________________________________________
activation_374 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_374[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_375 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_375[0][0]             
__________________________________________________________________________________________________
concatenate_161 (Concatenate)   (None, 80, 24, 24)   0           concatenate_160[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_161[0][0]            
__________________________________________________________________________________________________
activation_376 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_376[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_377 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_377[0][0]             
__________________________________________________________________________________________________
concatenate_162 (Concatenate)   (None, 98, 24, 24)   0           concatenate_161[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_162[0][0]            
__________________________________________________________________________________________________
activation_378 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_18 (Gl (None, 98)           0           activation_378[0][0]             
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 1)            99          global_average_pooling2d_18[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 79s - loss: 0.7267 - acc: 0.7009 - val_loss: 0.6761 - val_acc: 0.7477

Epoch 00001: val_loss improved from inf to 0.67614, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 55s - loss: 0.6566 - acc: 0.7607 - val_loss: 0.6351 - val_acc: 0.7682

Epoch 00002: val_loss improved from 0.67614 to 0.63514, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 54s - loss: 0.6279 - acc: 0.7706 - val_loss: 0.6139 - val_acc: 0.7738

Epoch 00003: val_loss improved from 0.63514 to 0.61391, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 54s - loss: 0.6107 - acc: 0.7797 - val_loss: 0.5953 - val_acc: 0.7870

Epoch 00004: val_loss improved from 0.61391 to 0.59526, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 57s - loss: 0.5975 - acc: 0.7874 - val_loss: 0.5953 - val_acc: 0.7871

Epoch 00005: val_loss did not improve from 0.59526
Epoch 6/30
 - 55s - loss: 0.5881 - acc: 0.7917 - val_loss: 0.5803 - val_acc: 0.7919

Epoch 00006: val_loss improved from 0.59526 to 0.58034, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 54s - loss: 0.5790 - acc: 0.7938 - val_loss: 0.5699 - val_acc: 0.7968

Epoch 00007: val_loss improved from 0.58034 to 0.56986, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 54s - loss: 0.5694 - acc: 0.8008 - val_loss: 0.5570 - val_acc: 0.8064

Epoch 00008: val_loss improved from 0.56986 to 0.55696, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 55s - loss: 0.5614 - acc: 0.8055 - val_loss: 0.5660 - val_acc: 0.7988

Epoch 00009: val_loss did not improve from 0.55696
Epoch 10/30
 - 54s - loss: 0.5559 - acc: 0.8074 - val_loss: 0.5439 - val_acc: 0.8139

Epoch 00010: val_loss improved from 0.55696 to 0.54389, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 54s - loss: 0.5494 - acc: 0.8129 - val_loss: 0.5364 - val_acc: 0.8171

Epoch 00011: val_loss improved from 0.54389 to 0.53644, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 54s - loss: 0.5423 - acc: 0.8161 - val_loss: 0.5292 - val_acc: 0.8209

Epoch 00012: val_loss improved from 0.53644 to 0.52917, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 54s - loss: 0.5351 - acc: 0.8221 - val_loss: 0.5264 - val_acc: 0.8220

Epoch 00013: val_loss improved from 0.52917 to 0.52638, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 55s - loss: 0.5316 - acc: 0.8197 - val_loss: 0.5271 - val_acc: 0.8234

Epoch 00014: val_loss did not improve from 0.52638
Epoch 15/30
 - 54s - loss: 0.5266 - acc: 0.8241 - val_loss: 0.5195 - val_acc: 0.8273

Epoch 00015: val_loss improved from 0.52638 to 0.51948, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 16/30
 - 54s - loss: 0.5214 - acc: 0.8272 - val_loss: 0.5343 - val_acc: 0.8185

Epoch 00016: val_loss did not improve from 0.51948
Epoch 17/30
 - 54s - loss: 0.5141 - acc: 0.8289 - val_loss: 0.5099 - val_acc: 0.8320

Epoch 00017: val_loss improved from 0.51948 to 0.50995, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 54s - loss: 0.5114 - acc: 0.8329 - val_loss: 0.5033 - val_acc: 0.8335

Epoch 00018: val_loss improved from 0.50995 to 0.50330, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 54s - loss: 0.5057 - acc: 0.8327 - val_loss: 0.4956 - val_acc: 0.8384

Epoch 00019: val_loss improved from 0.50330 to 0.49560, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 20/30
 - 54s - loss: 0.5020 - acc: 0.8379 - val_loss: 0.4953 - val_acc: 0.8424

Epoch 00020: val_loss improved from 0.49560 to 0.49529, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 55s - loss: 0.4944 - acc: 0.8420 - val_loss: 0.4978 - val_acc: 0.8360

Epoch 00021: val_loss did not improve from 0.49529
Epoch 22/30
 - 54s - loss: 0.4931 - acc: 0.8418 - val_loss: 0.4820 - val_acc: 0.8494

Epoch 00022: val_loss improved from 0.49529 to 0.48202, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 55s - loss: 0.4878 - acc: 0.8435 - val_loss: 0.4811 - val_acc: 0.8460

Epoch 00023: val_loss improved from 0.48202 to 0.48115, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 24/30
 - 55s - loss: 0.4830 - acc: 0.8477 - val_loss: 0.4741 - val_acc: 0.8514

Epoch 00024: val_loss improved from 0.48115 to 0.47412, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 54s - loss: 0.4806 - acc: 0.8507 - val_loss: 0.4710 - val_acc: 0.8578

Epoch 00025: val_loss improved from 0.47412 to 0.47096, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 54s - loss: 0.4762 - acc: 0.8518 - val_loss: 0.4699 - val_acc: 0.8567

Epoch 00026: val_loss improved from 0.47096 to 0.46991, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 54s - loss: 0.4729 - acc: 0.8538 - val_loss: 0.4627 - val_acc: 0.8567

Epoch 00027: val_loss improved from 0.46991 to 0.46274, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 55s - loss: 0.4693 - acc: 0.8544 - val_loss: 0.4589 - val_acc: 0.8622

Epoch 00028: val_loss improved from 0.46274 to 0.45889, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 54s - loss: 0.4661 - acc: 0.8572 - val_loss: 0.4607 - val_acc: 0.8622

Epoch 00029: val_loss did not improve from 0.45889
Epoch 30/30
 - 55s - loss: 0.4606 - acc: 0.8591 - val_loss: 0.4645 - val_acc: 0.8495

Epoch 00030: val_loss did not improve from 0.45889

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 4s
1184/7968 [===>..........................] - ETA: 3s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 3s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 1s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 0s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 581us/step
current val accuracy:0.850
current auc_score ------------------> 0.942

  32/7968 [..............................] - ETA: 39:58
 128/7968 [..............................] - ETA: 9:56 
 224/7968 [..............................] - ETA: 5:38
 320/7968 [>.............................] - ETA: 3:55
 416/7968 [>.............................] - ETA: 2:59
 512/7968 [>.............................] - ETA: 2:25
 608/7968 [=>............................] - ETA: 2:01
 704/7968 [=>............................] - ETA: 1:44
 800/7968 [==>...........................] - ETA: 1:30
 896/7968 [==>...........................] - ETA: 1:20
 992/7968 [==>...........................] - ETA: 1:12
1088/7968 [===>..........................] - ETA: 1:05
1184/7968 [===>..........................] - ETA: 59s 
1280/7968 [===>..........................] - ETA: 54s
1376/7968 [====>.........................] - ETA: 50s
1472/7968 [====>.........................] - ETA: 46s
1568/7968 [====>.........................] - ETA: 43s
1664/7968 [=====>........................] - ETA: 40s
1760/7968 [=====>........................] - ETA: 37s
1856/7968 [=====>........................] - ETA: 35s
1952/7968 [======>.......................] - ETA: 33s
2048/7968 [======>.......................] - ETA: 31s
2144/7968 [=======>......................] - ETA: 29s
2240/7968 [=======>......................] - ETA: 28s
2336/7968 [=======>......................] - ETA: 26s
2432/7968 [========>.....................] - ETA: 25s
2528/7968 [========>.....................] - ETA: 24s
2624/7968 [========>.....................] - ETA: 22s
2720/7968 [=========>....................] - ETA: 21s
2816/7968 [=========>....................] - ETA: 20s
2912/7968 [=========>....................] - ETA: 19s
3008/7968 [==========>...................] - ETA: 18s
3104/7968 [==========>...................] - ETA: 18s
3200/7968 [===========>..................] - ETA: 17s
3296/7968 [===========>..................] - ETA: 16s
3392/7968 [===========>..................] - ETA: 15s
3488/7968 [============>.................] - ETA: 15s
3584/7968 [============>.................] - ETA: 14s
3680/7968 [============>.................] - ETA: 13s
3776/7968 [=============>................] - ETA: 13s
3872/7968 [=============>................] - ETA: 12s
3968/7968 [=============>................] - ETA: 12s
4064/7968 [==============>...............] - ETA: 11s
4160/7968 [==============>...............] - ETA: 11s
4256/7968 [===============>..............] - ETA: 10s
4352/7968 [===============>..............] - ETA: 10s
4448/7968 [===============>..............] - ETA: 9s 
4544/7968 [================>.............] - ETA: 9s
4640/7968 [================>.............] - ETA: 8s
4736/7968 [================>.............] - ETA: 8s
4832/7968 [=================>............] - ETA: 8s
4928/7968 [=================>............] - ETA: 7s
5024/7968 [=================>............] - ETA: 7s
5120/7968 [==================>...........] - ETA: 7s
5216/7968 [==================>...........] - ETA: 6s
5312/7968 [===================>..........] - ETA: 6s
5408/7968 [===================>..........] - ETA: 6s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 5s
5696/7968 [====================>.........] - ETA: 5s
5792/7968 [====================>.........] - ETA: 4s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 4s
6080/7968 [=====================>........] - ETA: 4s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 3s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 2s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 1s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 14s 2ms/step
Best saved model val accuracy:0.862
best saved model auc_score ------------------> 0.940
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_19[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_379 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_379[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_380 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_380[0][0]             
__________________________________________________________________________________________________
concatenate_163 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_163[0][0]            
__________________________________________________________________________________________________
activation_381 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_381[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_382 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_382[0][0]             
__________________________________________________________________________________________________
concatenate_164 (Concatenate)   (None, 52, 96, 96)   0           concatenate_163[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_164[0][0]            
__________________________________________________________________________________________________
activation_383 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_383[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_384 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_384[0][0]             
__________________________________________________________________________________________________
concatenate_165 (Concatenate)   (None, 70, 96, 96)   0           concatenate_164[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_165[0][0]            
__________________________________________________________________________________________________
activation_385 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_385[0][0]             
__________________________________________________________________________________________________
average_pooling2d_37 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_37[0][0]       
__________________________________________________________________________________________________
activation_386 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_386[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_387 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_387[0][0]             
__________________________________________________________________________________________________
concatenate_166 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_37[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_166[0][0]            
__________________________________________________________________________________________________
activation_388 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_388[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_389 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_389[0][0]             
__________________________________________________________________________________________________
concatenate_167 (Concatenate)   (None, 71, 48, 48)   0           concatenate_166[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_167[0][0]            
__________________________________________________________________________________________________
activation_390 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_390[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_391 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_391[0][0]             
__________________________________________________________________________________________________
concatenate_168 (Concatenate)   (None, 89, 48, 48)   0           concatenate_167[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_168[0][0]            
__________________________________________________________________________________________________
activation_392 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_392[0][0]             
__________________________________________________________________________________________________
average_pooling2d_38 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_38[0][0]       
__________________________________________________________________________________________________
activation_393 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_393[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_394 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_394[0][0]             
__________________________________________________________________________________________________
concatenate_169 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_38[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_169[0][0]            
__________________________________________________________________________________________________
activation_395 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_395[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_396 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_396[0][0]             
__________________________________________________________________________________________________
concatenate_170 (Concatenate)   (None, 80, 24, 24)   0           concatenate_169[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_170[0][0]            
__________________________________________________________________________________________________
activation_397 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_397[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_398 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_398[0][0]             
__________________________________________________________________________________________________
concatenate_171 (Concatenate)   (None, 98, 24, 24)   0           concatenate_170[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_171[0][0]            
__________________________________________________________________________________________________
activation_399 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_19 (Gl (None, 98)           0           activation_399[0][0]             
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 1)            99          global_average_pooling2d_19[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 82s - loss: 0.7141 - acc: 0.7359 - val_loss: 0.6813 - val_acc: 0.7472

Epoch 00001: val_loss improved from inf to 0.68129, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 57s - loss: 0.6661 - acc: 0.7564 - val_loss: 0.6509 - val_acc: 0.7585

Epoch 00002: val_loss improved from 0.68129 to 0.65090, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 57s - loss: 0.6439 - acc: 0.7631 - val_loss: 0.6365 - val_acc: 0.7624

Epoch 00003: val_loss improved from 0.65090 to 0.63655, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 57s - loss: 0.6325 - acc: 0.7690 - val_loss: 0.6261 - val_acc: 0.7688

Epoch 00004: val_loss improved from 0.63655 to 0.62610, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 57s - loss: 0.6222 - acc: 0.7740 - val_loss: 0.6163 - val_acc: 0.7741

Epoch 00005: val_loss improved from 0.62610 to 0.61634, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 57s - loss: 0.6112 - acc: 0.7800 - val_loss: 0.6096 - val_acc: 0.7691

Epoch 00006: val_loss improved from 0.61634 to 0.60957, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 57s - loss: 0.6039 - acc: 0.7823 - val_loss: 0.5937 - val_acc: 0.7856

Epoch 00007: val_loss improved from 0.60957 to 0.59369, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 57s - loss: 0.5937 - acc: 0.7866 - val_loss: 0.5835 - val_acc: 0.7884

Epoch 00008: val_loss improved from 0.59369 to 0.58355, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 57s - loss: 0.5868 - acc: 0.7904 - val_loss: 0.5768 - val_acc: 0.7941

Epoch 00009: val_loss improved from 0.58355 to 0.57685, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 57s - loss: 0.5805 - acc: 0.7930 - val_loss: 0.5736 - val_acc: 0.7907

Epoch 00010: val_loss improved from 0.57685 to 0.57361, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 57s - loss: 0.5720 - acc: 0.7985 - val_loss: 0.5627 - val_acc: 0.8005

Epoch 00011: val_loss improved from 0.57361 to 0.56270, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 57s - loss: 0.5673 - acc: 0.8026 - val_loss: 0.5583 - val_acc: 0.8030

Epoch 00012: val_loss improved from 0.56270 to 0.55830, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 57s - loss: 0.5609 - acc: 0.8059 - val_loss: 0.5518 - val_acc: 0.8070

Epoch 00013: val_loss improved from 0.55830 to 0.55178, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 57s - loss: 0.5531 - acc: 0.8080 - val_loss: 0.5461 - val_acc: 0.8064

Epoch 00014: val_loss improved from 0.55178 to 0.54608, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 57s - loss: 0.5518 - acc: 0.8088 - val_loss: 0.5489 - val_acc: 0.8091

Epoch 00015: val_loss did not improve from 0.54608
Epoch 16/30
 - 57s - loss: 0.5465 - acc: 0.8132 - val_loss: 0.5351 - val_acc: 0.8129

Epoch 00016: val_loss improved from 0.54608 to 0.53513, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 57s - loss: 0.5442 - acc: 0.8134 - val_loss: 0.5325 - val_acc: 0.8150

Epoch 00017: val_loss improved from 0.53513 to 0.53248, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 57s - loss: 0.5374 - acc: 0.8180 - val_loss: 0.5275 - val_acc: 0.8180

Epoch 00018: val_loss improved from 0.53248 to 0.52752, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 19/30
 - 57s - loss: 0.5345 - acc: 0.8165 - val_loss: 0.5302 - val_acc: 0.8181

Epoch 00019: val_loss did not improve from 0.52752
Epoch 20/30
 - 57s - loss: 0.5307 - acc: 0.8213 - val_loss: 0.5219 - val_acc: 0.8204

Epoch 00020: val_loss improved from 0.52752 to 0.52187, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 57s - loss: 0.5281 - acc: 0.8228 - val_loss: 0.5187 - val_acc: 0.8229

Epoch 00021: val_loss improved from 0.52187 to 0.51866, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 57s - loss: 0.5241 - acc: 0.8242 - val_loss: 0.5134 - val_acc: 0.8242

Epoch 00022: val_loss improved from 0.51866 to 0.51341, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 57s - loss: 0.5187 - acc: 0.8262 - val_loss: 0.5143 - val_acc: 0.8268

Epoch 00023: val_loss did not improve from 0.51341
Epoch 24/30
 - 57s - loss: 0.5203 - acc: 0.8260 - val_loss: 0.5129 - val_acc: 0.8268

Epoch 00024: val_loss improved from 0.51341 to 0.51291, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 25/30
 - 57s - loss: 0.5147 - acc: 0.8301 - val_loss: 0.5042 - val_acc: 0.8333

Epoch 00025: val_loss improved from 0.51291 to 0.50417, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 57s - loss: 0.5107 - acc: 0.8304 - val_loss: 0.5002 - val_acc: 0.8320

Epoch 00026: val_loss improved from 0.50417 to 0.50018, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 57s - loss: 0.5060 - acc: 0.8348 - val_loss: 0.4990 - val_acc: 0.8345

Epoch 00027: val_loss improved from 0.50018 to 0.49904, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 57s - loss: 0.5040 - acc: 0.8350 - val_loss: 0.5022 - val_acc: 0.8336

Epoch 00028: val_loss did not improve from 0.49904
Epoch 29/30
 - 57s - loss: 0.5010 - acc: 0.8379 - val_loss: 0.4927 - val_acc: 0.8370

Epoch 00029: val_loss improved from 0.49904 to 0.49272, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 57s - loss: 0.5006 - acc: 0.8363 - val_loss: 0.4992 - val_acc: 0.8386

Epoch 00030: val_loss did not improve from 0.49272

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 4s
1184/7968 [===>..........................] - ETA: 4s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 3s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 2s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 591us/step
current val accuracy:0.839
current auc_score ------------------> 0.925

  32/7968 [..............................] - ETA: 41:25
 128/7968 [..............................] - ETA: 10:17
 224/7968 [..............................] - ETA: 5:50 
 320/7968 [>.............................] - ETA: 4:03
 416/7968 [>.............................] - ETA: 3:06
 512/7968 [>.............................] - ETA: 2:30
 608/7968 [=>............................] - ETA: 2:05
 704/7968 [=>............................] - ETA: 1:47
 800/7968 [==>...........................] - ETA: 1:34
 896/7968 [==>...........................] - ETA: 1:23
 992/7968 [==>...........................] - ETA: 1:14
1088/7968 [===>..........................] - ETA: 1:07
1184/7968 [===>..........................] - ETA: 1:01
1280/7968 [===>..........................] - ETA: 56s 
1376/7968 [====>.........................] - ETA: 51s
1472/7968 [====>.........................] - ETA: 48s
1568/7968 [====>.........................] - ETA: 44s
1664/7968 [=====>........................] - ETA: 41s
1760/7968 [=====>........................] - ETA: 39s
1856/7968 [=====>........................] - ETA: 36s
1952/7968 [======>.......................] - ETA: 34s
2048/7968 [======>.......................] - ETA: 32s
2144/7968 [=======>......................] - ETA: 30s
2240/7968 [=======>......................] - ETA: 29s
2336/7968 [=======>......................] - ETA: 27s
2432/7968 [========>.....................] - ETA: 26s
2528/7968 [========>.....................] - ETA: 24s
2624/7968 [========>.....................] - ETA: 23s
2720/7968 [=========>....................] - ETA: 22s
2816/7968 [=========>....................] - ETA: 21s
2912/7968 [=========>....................] - ETA: 20s
3008/7968 [==========>...................] - ETA: 19s
3104/7968 [==========>...................] - ETA: 18s
3200/7968 [===========>..................] - ETA: 17s
3296/7968 [===========>..................] - ETA: 17s
3392/7968 [===========>..................] - ETA: 16s
3488/7968 [============>.................] - ETA: 15s
3584/7968 [============>.................] - ETA: 14s
3680/7968 [============>.................] - ETA: 14s
3776/7968 [=============>................] - ETA: 13s
3872/7968 [=============>................] - ETA: 13s
3968/7968 [=============>................] - ETA: 12s
4064/7968 [==============>...............] - ETA: 11s
4160/7968 [==============>...............] - ETA: 11s
4256/7968 [===============>..............] - ETA: 10s
4352/7968 [===============>..............] - ETA: 10s
4448/7968 [===============>..............] - ETA: 10s
4544/7968 [================>.............] - ETA: 9s 
4640/7968 [================>.............] - ETA: 9s
4736/7968 [================>.............] - ETA: 8s
4832/7968 [=================>............] - ETA: 8s
4928/7968 [=================>............] - ETA: 8s
5024/7968 [=================>............] - ETA: 7s
5120/7968 [==================>...........] - ETA: 7s
5216/7968 [==================>...........] - ETA: 6s
5312/7968 [===================>..........] - ETA: 6s
5408/7968 [===================>..........] - ETA: 6s
5504/7968 [===================>..........] - ETA: 5s
5600/7968 [====================>.........] - ETA: 5s
5696/7968 [====================>.........] - ETA: 5s
5792/7968 [====================>.........] - ETA: 5s
5888/7968 [=====================>........] - ETA: 4s
5984/7968 [=====================>........] - ETA: 4s
6080/7968 [=====================>........] - ETA: 4s
6176/7968 [======================>.......] - ETA: 3s
6272/7968 [======================>.......] - ETA: 3s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 3s
6560/7968 [=======================>......] - ETA: 2s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 2s
7040/7968 [=========================>....] - ETA: 1s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 1s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 15s 2ms/step
Best saved model val accuracy:0.837
best saved model auc_score ------------------> 0.925
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_20 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 16, 96, 96)   288         input_20[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 16, 96, 96)   64          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_400 (Activation)     (None, 16, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_bottleneck_conv2D (Co (None, 72, 96, 96)   1152        activation_400[0][0]             
__________________________________________________________________________________________________
dense_0_0_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_401 (Activation)     (None, 72, 96, 96)   0           dense_0_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_401[0][0]             
__________________________________________________________________________________________________
concatenate_172 (Concatenate)   (None, 34, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 34, 96, 96)   136         concatenate_172[0][0]            
__________________________________________________________________________________________________
activation_402 (Activation)     (None, 34, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_bottleneck_conv2D (Co (None, 72, 96, 96)   2448        activation_402[0][0]             
__________________________________________________________________________________________________
dense_0_1_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_403 (Activation)     (None, 72, 96, 96)   0           dense_0_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_403[0][0]             
__________________________________________________________________________________________________
concatenate_173 (Concatenate)   (None, 52, 96, 96)   0           concatenate_172[0][0]            
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 52, 96, 96)   208         concatenate_173[0][0]            
__________________________________________________________________________________________________
activation_404 (Activation)     (None, 52, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_bottleneck_conv2D (Co (None, 72, 96, 96)   3744        activation_404[0][0]             
__________________________________________________________________________________________________
dense_0_2_bottleneck_bn (BatchN (None, 72, 96, 96)   288         dense_0_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_405 (Activation)     (None, 72, 96, 96)   0           dense_0_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 18, 96, 96)   11664       activation_405[0][0]             
__________________________________________________________________________________________________
concatenate_174 (Concatenate)   (None, 70, 96, 96)   0           concatenate_173[0][0]            
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_0_bn (BatchNormalization)    (None, 70, 96, 96)   280         concatenate_174[0][0]            
__________________________________________________________________________________________________
activation_406 (Activation)     (None, 70, 96, 96)   0           tr_0_bn[0][0]                    
__________________________________________________________________________________________________
tr_0_conv2D (Conv2D)            (None, 35, 96, 96)   2450        activation_406[0][0]             
__________________________________________________________________________________________________
average_pooling2d_39 (AveragePo (None, 35, 48, 48)   0           tr_0_conv2D[0][0]                
__________________________________________________________________________________________________
dense_1_0_bn (BatchNormalizatio (None, 35, 48, 48)   140         average_pooling2d_39[0][0]       
__________________________________________________________________________________________________
activation_407 (Activation)     (None, 35, 48, 48)   0           dense_1_0_bn[0][0]               
__________________________________________________________________________________________________
dense_1_0_bottleneck_conv2D (Co (None, 72, 48, 48)   2520        activation_407[0][0]             
__________________________________________________________________________________________________
dense_1_0_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_408 (Activation)     (None, 72, 48, 48)   0           dense_1_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_0_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_408[0][0]             
__________________________________________________________________________________________________
concatenate_175 (Concatenate)   (None, 53, 48, 48)   0           average_pooling2d_39[0][0]       
                                                                 dense_1_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_1_bn (BatchNormalizatio (None, 53, 48, 48)   212         concatenate_175[0][0]            
__________________________________________________________________________________________________
activation_409 (Activation)     (None, 53, 48, 48)   0           dense_1_1_bn[0][0]               
__________________________________________________________________________________________________
dense_1_1_bottleneck_conv2D (Co (None, 72, 48, 48)   3816        activation_409[0][0]             
__________________________________________________________________________________________________
dense_1_1_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_410 (Activation)     (None, 72, 48, 48)   0           dense_1_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_1_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_410[0][0]             
__________________________________________________________________________________________________
concatenate_176 (Concatenate)   (None, 71, 48, 48)   0           concatenate_175[0][0]            
                                                                 dense_1_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_1_2_bn (BatchNormalizatio (None, 71, 48, 48)   284         concatenate_176[0][0]            
__________________________________________________________________________________________________
activation_411 (Activation)     (None, 71, 48, 48)   0           dense_1_2_bn[0][0]               
__________________________________________________________________________________________________
dense_1_2_bottleneck_conv2D (Co (None, 72, 48, 48)   5112        activation_411[0][0]             
__________________________________________________________________________________________________
dense_1_2_bottleneck_bn (BatchN (None, 72, 48, 48)   288         dense_1_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_412 (Activation)     (None, 72, 48, 48)   0           dense_1_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_1_2_conv2D (Conv2D)       (None, 18, 48, 48)   11664       activation_412[0][0]             
__________________________________________________________________________________________________
concatenate_177 (Concatenate)   (None, 89, 48, 48)   0           concatenate_176[0][0]            
                                                                 dense_1_2_conv2D[0][0]           
__________________________________________________________________________________________________
tr_1_bn (BatchNormalization)    (None, 89, 48, 48)   356         concatenate_177[0][0]            
__________________________________________________________________________________________________
activation_413 (Activation)     (None, 89, 48, 48)   0           tr_1_bn[0][0]                    
__________________________________________________________________________________________________
tr_1_conv2D (Conv2D)            (None, 44, 48, 48)   3916        activation_413[0][0]             
__________________________________________________________________________________________________
average_pooling2d_40 (AveragePo (None, 44, 24, 24)   0           tr_1_conv2D[0][0]                
__________________________________________________________________________________________________
dense_2_0_bn (BatchNormalizatio (None, 44, 24, 24)   176         average_pooling2d_40[0][0]       
__________________________________________________________________________________________________
activation_414 (Activation)     (None, 44, 24, 24)   0           dense_2_0_bn[0][0]               
__________________________________________________________________________________________________
dense_2_0_bottleneck_conv2D (Co (None, 72, 24, 24)   3168        activation_414[0][0]             
__________________________________________________________________________________________________
dense_2_0_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_0_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_415 (Activation)     (None, 72, 24, 24)   0           dense_2_0_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_0_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_415[0][0]             
__________________________________________________________________________________________________
concatenate_178 (Concatenate)   (None, 62, 24, 24)   0           average_pooling2d_40[0][0]       
                                                                 dense_2_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_1_bn (BatchNormalizatio (None, 62, 24, 24)   248         concatenate_178[0][0]            
__________________________________________________________________________________________________
activation_416 (Activation)     (None, 62, 24, 24)   0           dense_2_1_bn[0][0]               
__________________________________________________________________________________________________
dense_2_1_bottleneck_conv2D (Co (None, 72, 24, 24)   4464        activation_416[0][0]             
__________________________________________________________________________________________________
dense_2_1_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_1_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_417 (Activation)     (None, 72, 24, 24)   0           dense_2_1_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_1_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_417[0][0]             
__________________________________________________________________________________________________
concatenate_179 (Concatenate)   (None, 80, 24, 24)   0           concatenate_178[0][0]            
                                                                 dense_2_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_2_2_bn (BatchNormalizatio (None, 80, 24, 24)   320         concatenate_179[0][0]            
__________________________________________________________________________________________________
activation_418 (Activation)     (None, 80, 24, 24)   0           dense_2_2_bn[0][0]               
__________________________________________________________________________________________________
dense_2_2_bottleneck_conv2D (Co (None, 72, 24, 24)   5760        activation_418[0][0]             
__________________________________________________________________________________________________
dense_2_2_bottleneck_bn (BatchN (None, 72, 24, 24)   288         dense_2_2_bottleneck_conv2D[0][0]
__________________________________________________________________________________________________
activation_419 (Activation)     (None, 72, 24, 24)   0           dense_2_2_bottleneck_bn[0][0]    
__________________________________________________________________________________________________
dense_2_2_conv2D (Conv2D)       (None, 18, 24, 24)   11664       activation_419[0][0]             
__________________________________________________________________________________________________
concatenate_180 (Concatenate)   (None, 98, 24, 24)   0           concatenate_179[0][0]            
                                                                 dense_2_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 98, 24, 24)   392         concatenate_180[0][0]            
__________________________________________________________________________________________________
activation_420 (Activation)     (None, 98, 24, 24)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_20 (Gl (None, 98)           0           activation_420[0][0]             
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 1)            99          global_average_pooling2d_20[0][0]
==================================================================================================
Total params: 149,321
Trainable params: 146,617
Non-trainable params: 2,704
__________________________________________________________________________________________________
Train on 31872 samples, validate on 7968 samples
Epoch 1/30
 - 81s - loss: 0.7626 - acc: 0.6682 - val_loss: 0.6995 - val_acc: 0.7408

Epoch 00001: val_loss improved from inf to 0.69947, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 2/30
 - 55s - loss: 0.6743 - acc: 0.7542 - val_loss: 0.6492 - val_acc: 0.7578

Epoch 00002: val_loss improved from 0.69947 to 0.64920, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 3/30
 - 55s - loss: 0.6379 - acc: 0.7639 - val_loss: 0.6202 - val_acc: 0.7684

Epoch 00003: val_loss improved from 0.64920 to 0.62020, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 4/30
 - 55s - loss: 0.6199 - acc: 0.7703 - val_loss: 0.6045 - val_acc: 0.7735

Epoch 00004: val_loss improved from 0.62020 to 0.60451, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 5/30
 - 55s - loss: 0.6056 - acc: 0.7800 - val_loss: 0.5935 - val_acc: 0.7844

Epoch 00005: val_loss improved from 0.60451 to 0.59347, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 6/30
 - 55s - loss: 0.5926 - acc: 0.7866 - val_loss: 0.5816 - val_acc: 0.7912

Epoch 00006: val_loss improved from 0.59347 to 0.58160, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 7/30
 - 55s - loss: 0.5805 - acc: 0.7926 - val_loss: 0.5666 - val_acc: 0.7974

Epoch 00007: val_loss improved from 0.58160 to 0.56662, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 8/30
 - 55s - loss: 0.5715 - acc: 0.7987 - val_loss: 0.5603 - val_acc: 0.8030

Epoch 00008: val_loss improved from 0.56662 to 0.56028, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 9/30
 - 55s - loss: 0.5631 - acc: 0.8029 - val_loss: 0.5549 - val_acc: 0.7991

Epoch 00009: val_loss improved from 0.56028 to 0.55493, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 10/30
 - 55s - loss: 0.5563 - acc: 0.8049 - val_loss: 0.5458 - val_acc: 0.8099

Epoch 00010: val_loss improved from 0.55493 to 0.54580, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 11/30
 - 55s - loss: 0.5490 - acc: 0.8073 - val_loss: 0.5420 - val_acc: 0.8139

Epoch 00011: val_loss improved from 0.54580 to 0.54203, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 12/30
 - 55s - loss: 0.5432 - acc: 0.8117 - val_loss: 0.5345 - val_acc: 0.8161

Epoch 00012: val_loss improved from 0.54203 to 0.53454, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 13/30
 - 55s - loss: 0.5373 - acc: 0.8117 - val_loss: 0.5344 - val_acc: 0.8161

Epoch 00013: val_loss improved from 0.53454 to 0.53442, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 14/30
 - 55s - loss: 0.5324 - acc: 0.8179 - val_loss: 0.5246 - val_acc: 0.8230

Epoch 00014: val_loss improved from 0.53442 to 0.52462, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 15/30
 - 55s - loss: 0.5278 - acc: 0.8224 - val_loss: 0.5429 - val_acc: 0.8040

Epoch 00015: val_loss did not improve from 0.52462
Epoch 16/30
 - 54s - loss: 0.5219 - acc: 0.8253 - val_loss: 0.5223 - val_acc: 0.8237

Epoch 00016: val_loss improved from 0.52462 to 0.52231, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 17/30
 - 55s - loss: 0.5182 - acc: 0.8247 - val_loss: 0.5074 - val_acc: 0.8279

Epoch 00017: val_loss improved from 0.52231 to 0.50739, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 18/30
 - 55s - loss: 0.5120 - acc: 0.8290 - val_loss: 0.5103 - val_acc: 0.8256

Epoch 00018: val_loss did not improve from 0.50739
Epoch 19/30
 - 55s - loss: 0.5097 - acc: 0.8282 - val_loss: 0.5121 - val_acc: 0.8304

Epoch 00019: val_loss did not improve from 0.50739
Epoch 20/30
 - 55s - loss: 0.5045 - acc: 0.8331 - val_loss: 0.4981 - val_acc: 0.8298

Epoch 00020: val_loss improved from 0.50739 to 0.49813, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 21/30
 - 55s - loss: 0.5028 - acc: 0.8345 - val_loss: 0.4969 - val_acc: 0.8348

Epoch 00021: val_loss improved from 0.49813 to 0.49695, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 22/30
 - 55s - loss: 0.4979 - acc: 0.8367 - val_loss: 0.4885 - val_acc: 0.8401

Epoch 00022: val_loss improved from 0.49695 to 0.48846, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 23/30
 - 55s - loss: 0.4956 - acc: 0.8388 - val_loss: 0.4937 - val_acc: 0.8422

Epoch 00023: val_loss did not improve from 0.48846
Epoch 24/30
 - 55s - loss: 0.4906 - acc: 0.8419 - val_loss: 0.4942 - val_acc: 0.8379

Epoch 00024: val_loss did not improve from 0.48846
Epoch 25/30
 - 55s - loss: 0.4869 - acc: 0.8425 - val_loss: 0.4848 - val_acc: 0.8441

Epoch 00025: val_loss improved from 0.48846 to 0.48477, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 26/30
 - 55s - loss: 0.4827 - acc: 0.8462 - val_loss: 0.4793 - val_acc: 0.8490

Epoch 00026: val_loss improved from 0.48477 to 0.47929, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 27/30
 - 55s - loss: 0.4799 - acc: 0.8483 - val_loss: 0.4761 - val_acc: 0.8522

Epoch 00027: val_loss improved from 0.47929 to 0.47611, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 28/30
 - 55s - loss: 0.4766 - acc: 0.8499 - val_loss: 0.4730 - val_acc: 0.8524

Epoch 00028: val_loss improved from 0.47611 to 0.47300, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 29/30
 - 55s - loss: 0.4738 - acc: 0.8520 - val_loss: 0.4702 - val_acc: 0.8507

Epoch 00029: val_loss improved from 0.47300 to 0.47018, saving model to hyperas_dn_lr_optimizer_wt_3Oct_1543.h5
Epoch 30/30
 - 55s - loss: 0.4714 - acc: 0.8525 - val_loss: 0.4804 - val_acc: 0.8486

Epoch 00030: val_loss did not improve from 0.47018

  32/7968 [..............................] - ETA: 4s
 128/7968 [..............................] - ETA: 4s
 224/7968 [..............................] - ETA: 4s
 320/7968 [>.............................] - ETA: 4s
 416/7968 [>.............................] - ETA: 4s
 512/7968 [>.............................] - ETA: 4s
 608/7968 [=>............................] - ETA: 4s
 704/7968 [=>............................] - ETA: 4s
 800/7968 [==>...........................] - ETA: 4s
 896/7968 [==>...........................] - ETA: 4s
 992/7968 [==>...........................] - ETA: 4s
1088/7968 [===>..........................] - ETA: 4s
1184/7968 [===>..........................] - ETA: 4s
1280/7968 [===>..........................] - ETA: 3s
1376/7968 [====>.........................] - ETA: 3s
1472/7968 [====>.........................] - ETA: 3s
1568/7968 [====>.........................] - ETA: 3s
1664/7968 [=====>........................] - ETA: 3s
1760/7968 [=====>........................] - ETA: 3s
1856/7968 [=====>........................] - ETA: 3s
1952/7968 [======>.......................] - ETA: 3s
2048/7968 [======>.......................] - ETA: 3s
2144/7968 [=======>......................] - ETA: 3s
2240/7968 [=======>......................] - ETA: 3s
2336/7968 [=======>......................] - ETA: 3s
2432/7968 [========>.....................] - ETA: 3s
2528/7968 [========>.....................] - ETA: 3s
2624/7968 [========>.....................] - ETA: 3s
2720/7968 [=========>....................] - ETA: 3s
2816/7968 [=========>....................] - ETA: 3s
2912/7968 [=========>....................] - ETA: 2s
3008/7968 [==========>...................] - ETA: 2s
3104/7968 [==========>...................] - ETA: 2s
3200/7968 [===========>..................] - ETA: 2s
3296/7968 [===========>..................] - ETA: 2s
3392/7968 [===========>..................] - ETA: 2s
3488/7968 [============>.................] - ETA: 2s
3584/7968 [============>.................] - ETA: 2s
3680/7968 [============>.................] - ETA: 2s
3776/7968 [=============>................] - ETA: 2s
3872/7968 [=============>................] - ETA: 2s
3968/7968 [=============>................] - ETA: 2s
4064/7968 [==============>...............] - ETA: 2s
4160/7968 [==============>...............] - ETA: 2s
4256/7968 [===============>..............] - ETA: 2s
4352/7968 [===============>..............] - ETA: 2s
4448/7968 [===============>..............] - ETA: 2s
4544/7968 [================>.............] - ETA: 2s
4640/7968 [================>.............] - ETA: 1s
4736/7968 [================>.............] - ETA: 1s
4832/7968 [=================>............] - ETA: 1s
4928/7968 [=================>............] - ETA: 1s
5024/7968 [=================>............] - ETA: 1s
5120/7968 [==================>...........] - ETA: 1s
5216/7968 [==================>...........] - ETA: 1s
5312/7968 [===================>..........] - ETA: 1s
5408/7968 [===================>..........] - ETA: 1s
5504/7968 [===================>..........] - ETA: 1s
5600/7968 [====================>.........] - ETA: 1s
5696/7968 [====================>.........] - ETA: 1s
5792/7968 [====================>.........] - ETA: 1s
5888/7968 [=====================>........] - ETA: 1s
5984/7968 [=====================>........] - ETA: 1s
6080/7968 [=====================>........] - ETA: 1s
6176/7968 [======================>.......] - ETA: 1s
6272/7968 [======================>.......] - ETA: 1s
6368/7968 [======================>.......] - ETA: 0s
6464/7968 [=======================>......] - ETA: 0s
6560/7968 [=======================>......] - ETA: 0s
6656/7968 [========================>.....] - ETA: 0s
6752/7968 [========================>.....] - ETA: 0s
6848/7968 [========================>.....] - ETA: 0s
6944/7968 [=========================>....] - ETA: 0s
7040/7968 [=========================>....] - ETA: 0s
7136/7968 [=========================>....] - ETA: 0s
7232/7968 [==========================>...] - ETA: 0s
7328/7968 [==========================>...] - ETA: 0s
7424/7968 [==========================>...] - ETA: 0s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 5s 597us/step
current val accuracy:0.849
current auc_score ------------------> 0.934

  32/7968 [..............................] - ETA: 46:18
 128/7968 [..............................] - ETA: 11:30
 224/7968 [..............................] - ETA: 6:31 
 320/7968 [>.............................] - ETA: 4:32
 416/7968 [>.............................] - ETA: 3:27
 512/7968 [>.............................] - ETA: 2:47
 608/7968 [=>............................] - ETA: 2:19
 704/7968 [=>............................] - ETA: 1:59
 800/7968 [==>...........................] - ETA: 1:44
 896/7968 [==>...........................] - ETA: 1:32
 992/7968 [==>...........................] - ETA: 1:22
1088/7968 [===>..........................] - ETA: 1:14
1184/7968 [===>..........................] - ETA: 1:08
1280/7968 [===>..........................] - ETA: 1:02
1376/7968 [====>.........................] - ETA: 57s 
1472/7968 [====>.........................] - ETA: 53s
1568/7968 [====>.........................] - ETA: 49s
1664/7968 [=====>........................] - ETA: 46s
1760/7968 [=====>........................] - ETA: 43s
1856/7968 [=====>........................] - ETA: 40s
1952/7968 [======>.......................] - ETA: 38s
2048/7968 [======>.......................] - ETA: 35s
2144/7968 [=======>......................] - ETA: 33s
2240/7968 [=======>......................] - ETA: 32s
2336/7968 [=======>......................] - ETA: 30s
2432/7968 [========>.....................] - ETA: 28s
2528/7968 [========>.....................] - ETA: 27s
2624/7968 [========>.....................] - ETA: 26s
2720/7968 [=========>....................] - ETA: 24s
2816/7968 [=========>....................] - ETA: 23s
2912/7968 [=========>....................] - ETA: 22s
3008/7968 [==========>...................] - ETA: 21s
3104/7968 [==========>...................] - ETA: 20s
3200/7968 [===========>..................] - ETA: 19s
3296/7968 [===========>..................] - ETA: 18s
3392/7968 [===========>..................] - ETA: 17s
3488/7968 [============>.................] - ETA: 17s
3584/7968 [============>.................] - ETA: 16s
3680/7968 [============>.................] - ETA: 15s
3776/7968 [=============>................] - ETA: 14s
3872/7968 [=============>................] - ETA: 14s
3968/7968 [=============>................] - ETA: 13s
4064/7968 [==============>...............] - ETA: 13s
4160/7968 [==============>...............] - ETA: 12s
4256/7968 [===============>..............] - ETA: 11s
4352/7968 [===============>..............] - ETA: 11s
4448/7968 [===============>..............] - ETA: 10s
4544/7968 [================>.............] - ETA: 10s
4640/7968 [================>.............] - ETA: 10s
4736/7968 [================>.............] - ETA: 9s 
4832/7968 [=================>............] - ETA: 9s
4928/7968 [=================>............] - ETA: 8s
5024/7968 [=================>............] - ETA: 8s
5120/7968 [==================>...........] - ETA: 7s
5216/7968 [==================>...........] - ETA: 7s
5312/7968 [===================>..........] - ETA: 7s
5408/7968 [===================>..........] - ETA: 6s
5504/7968 [===================>..........] - ETA: 6s
5600/7968 [====================>.........] - ETA: 6s
5696/7968 [====================>.........] - ETA: 5s
5792/7968 [====================>.........] - ETA: 5s
5888/7968 [=====================>........] - ETA: 5s
5984/7968 [=====================>........] - ETA: 4s
6080/7968 [=====================>........] - ETA: 4s
6176/7968 [======================>.......] - ETA: 4s
6272/7968 [======================>.......] - ETA: 4s
6368/7968 [======================>.......] - ETA: 3s
6464/7968 [=======================>......] - ETA: 3s
6560/7968 [=======================>......] - ETA: 3s
6656/7968 [========================>.....] - ETA: 2s
6752/7968 [========================>.....] - ETA: 2s
6848/7968 [========================>.....] - ETA: 2s
6944/7968 [=========================>....] - ETA: 2s
7040/7968 [=========================>....] - ETA: 2s
7136/7968 [=========================>....] - ETA: 1s
7232/7968 [==========================>...] - ETA: 1s
7328/7968 [==========================>...] - ETA: 1s
7424/7968 [==========================>...] - ETA: 1s
7520/7968 [===========================>..] - ETA: 0s
7616/7968 [===========================>..] - ETA: 0s
7712/7968 [============================>.] - ETA: 0s
7808/7968 [============================>.] - ETA: 0s
7904/7968 [============================>.] - ETA: 0s
7968/7968 [==============================] - 16s 2ms/step
Best saved model val accuracy:0.851
best saved model auc_score ------------------> 0.934
best model <keras.engine.training.Model object at 0x7f52cc186c18>
best run {'opt': 0}
Evalutation of best performing model:
[0.5996364669133258, 0.8182795698924731]
TEST roc_auc_score 0.887
----------trials-------------
{'opt': [1]} -0.9962919948445866
{'opt': [4]} -0.9958631121297381
{'opt': [2]} -0.9952783837567176
{'opt': [1]} -0.9685371337957814
{'opt': [3]} -0.9920452872928233
{'opt': [2]} -0.9367471397738029
{'opt': [3]} -0.970648847189473
{'opt': [2]} -0.9387796544643119
{'opt': [4]} -0.9763205958464345
{'opt': [0]} -0.9978438672859077
{'opt': [1]} -0.931226322297012
{'opt': [1]} -0.9320230032853916
{'opt': [3]} -0.9375840501750563
{'opt': [0]} -0.968503299504923
{'opt': [1]} -0.9311012866355433
{'opt': [1]} -0.9370321164917786
{'opt': [1]} -0.9363842874813375
{'opt': [3]} -0.9398688414029902
{'opt': [4]} -0.9249923069517433
{'opt': [1]} -0.9341197842266932
python hyperas_dn_lr_optimizer_test3.py -n 20
python hyperas_dn_lr_optimizer_test4.py -n 20
python hyperas_dn_lr_optimizer_test5.py -n 15
python hyperas_dn_lr_optimizer_test6.py -n 15
