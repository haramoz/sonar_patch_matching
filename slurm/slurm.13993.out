python hello-world.py
python hyperas_simple.py
python hyperas_contrastive_loss.py
python densenet_siamese_best_run.py
python hyperas_densenet.py
python hyperas_densenet_siamese.py
python densenet_simple.py
python keras_densenet_siamese.py
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_1[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_1[0][0]               
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_2[0][0]               
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 42, 96, 96)   0           concatenate_1[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_2[0][0]              
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_3[0][0]               
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 54, 96, 96)   0           concatenate_2[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 54)           0           activation_4[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            55          global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 21s - loss: 0.5536 - acc: 0.7326 - val_loss: 2.7022 - val_acc: 0.6489
Epoch 2/20
 - 18s - loss: 0.5038 - acc: 0.7518 - val_loss: 0.6825 - val_acc: 0.7190
Epoch 3/20
 - 18s - loss: 0.4759 - acc: 0.7671 - val_loss: 0.4801 - val_acc: 0.7960
Epoch 4/20
 - 18s - loss: 0.4615 - acc: 0.7778 - val_loss: 0.8383 - val_acc: 0.8251
Epoch 5/20
 - 18s - loss: 0.4508 - acc: 0.7863 - val_loss: 0.9109 - val_acc: 0.5535
Epoch 6/20
 - 18s - loss: 0.4410 - acc: 0.7915 - val_loss: 1.3285 - val_acc: 0.6226

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 7/20
 - 17s - loss: 0.4105 - acc: 0.8084 - val_loss: 1.1290 - val_acc: 0.7372
Epoch 8/20
 - 17s - loss: 0.4012 - acc: 0.8158 - val_loss: 0.4622 - val_acc: 0.7848
Epoch 9/20
 - 17s - loss: 0.3951 - acc: 0.8171 - val_loss: 0.4573 - val_acc: 0.8175
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3552/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4128/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4704/7440 [=================>............] - ETA: 0s
4992/7440 [===================>..........] - ETA: 0s
5280/7440 [====================>.........] - ETA: 0s
5568/7440 [=====================>........] - ETA: 0s
5856/7440 [======================>.......] - ETA: 0s
6144/7440 [=======================>......] - ETA: 0s
6432/7440 [========================>.....] - ETA: 0s
6720/7440 [==========================>...] - ETA: 0s
7008/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 178us/step
Test accuracy: 0.8174731182795699
Test accuracy 0.6: 0.8362903225806452
auc_score ------------------>  0.905165228639149
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_2[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_5[0][0]               
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_6[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 42, 96, 96)   0           concatenate_4[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_5[0][0]              
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_7[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 54, 96, 96)   0           concatenate_5[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_6[0][0]              
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 54)           0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            55          global_average_pooling2d_2[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5366 - acc: 0.7439 - val_loss: 0.4956 - val_acc: 0.7637
Epoch 2/20
 - 17s - loss: 0.4877 - acc: 0.7658 - val_loss: 1.3996 - val_acc: 0.5210
Epoch 3/20
 - 17s - loss: 0.4662 - acc: 0.7757 - val_loss: 0.5234 - val_acc: 0.7495
Epoch 4/20
 - 17s - loss: 0.4534 - acc: 0.7835 - val_loss: 0.4637 - val_acc: 0.8103
Epoch 5/20
 - 17s - loss: 0.4495 - acc: 0.7869 - val_loss: 0.5149 - val_acc: 0.7444
Epoch 6/20
 - 17s - loss: 0.4390 - acc: 0.7898 - val_loss: 0.5374 - val_acc: 0.7515
Epoch 7/20
 - 17s - loss: 0.4372 - acc: 0.7916 - val_loss: 0.5295 - val_acc: 0.7833

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 8/20
 - 17s - loss: 0.4108 - acc: 0.8039 - val_loss: 0.7578 - val_acc: 0.6999
Epoch 9/20
 - 18s - loss: 0.4072 - acc: 0.8082 - val_loss: 0.4792 - val_acc: 0.7625
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 1s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 191us/step
Test accuracy: 0.7625
Test accuracy 0.6: 0.7771505376344086
auc_score ------------------>  0.8762500361313447
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_3[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_9[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_7[0][0]              
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_10[0][0]              
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 42, 96, 96)   0           concatenate_7[0][0]              
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_8[0][0]              
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_11[0][0]              
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 54, 96, 96)   0           concatenate_8[0][0]              
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_9[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_3 (Glo (None, 54)           0           activation_12[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            55          global_average_pooling2d_3[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5498 - acc: 0.7365 - val_loss: 0.5540 - val_acc: 0.7073
Epoch 2/20
 - 17s - loss: 0.4919 - acc: 0.7653 - val_loss: 0.5341 - val_acc: 0.7931
Epoch 3/20
 - 17s - loss: 0.4707 - acc: 0.7746 - val_loss: 1.1332 - val_acc: 0.7091
Epoch 4/20
 - 17s - loss: 0.4565 - acc: 0.7854 - val_loss: 0.8842 - val_acc: 0.7382
Epoch 5/20
 - 17s - loss: 0.4541 - acc: 0.7868 - val_loss: 0.4916 - val_acc: 0.7966
Epoch 6/20
 - 17s - loss: 0.4490 - acc: 0.7862 - val_loss: 3.9135 - val_acc: 0.5099
Epoch 7/20
 - 17s - loss: 0.4442 - acc: 0.7914 - val_loss: 1.4789 - val_acc: 0.5644
Epoch 8/20
 - 17s - loss: 0.4430 - acc: 0.7923 - val_loss: 0.5112 - val_acc: 0.7902

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 9/20
 - 17s - loss: 0.4214 - acc: 0.8017 - val_loss: 0.3590 - val_acc: 0.8509
Epoch 10/20
 - 17s - loss: 0.4150 - acc: 0.8057 - val_loss: 0.6780 - val_acc: 0.7458
Epoch 11/20
 - 17s - loss: 0.4091 - acc: 0.8081 - val_loss: 0.6702 - val_acc: 0.8324
Epoch 12/20
 - 17s - loss: 0.4065 - acc: 0.8094 - val_loss: 0.3925 - val_acc: 0.8349

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0029999998973718025.
Epoch 13/20
 - 17s - loss: 0.3947 - acc: 0.8165 - val_loss: 0.3824 - val_acc: 0.8534
Epoch 14/20
 - 17s - loss: 0.3907 - acc: 0.8178 - val_loss: 0.4284 - val_acc: 0.8030
Epoch 15/20
 - 17s - loss: 0.3889 - acc: 0.8197 - val_loss: 0.3907 - val_acc: 0.8276

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009486832326692812.
Epoch 16/20
 - 17s - loss: 0.3831 - acc: 0.8221 - val_loss: 0.3395 - val_acc: 0.8554
Epoch 17/20
 - 17s - loss: 0.3820 - acc: 0.8230 - val_loss: 0.3530 - val_acc: 0.8539
Epoch 18/20
 - 17s - loss: 0.3825 - acc: 0.8250 - val_loss: 0.3347 - val_acc: 0.8579
Epoch 19/20
 - 17s - loss: 0.3826 - acc: 0.8244 - val_loss: 0.3511 - val_acc: 0.8605
Epoch 20/20
 - 17s - loss: 0.3817 - acc: 0.8248 - val_loss: 0.3451 - val_acc: 0.8501

  32/7440 [..............................] - ETA: 1s
 352/7440 [>.............................] - ETA: 1s
 672/7440 [=>............................] - ETA: 1s
 992/7440 [===>..........................] - ETA: 1s
1312/7440 [====>.........................] - ETA: 1s
1632/7440 [=====>........................] - ETA: 0s
1952/7440 [======>.......................] - ETA: 0s
2272/7440 [========>.....................] - ETA: 0s
2592/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3232/7440 [============>.................] - ETA: 0s
3520/7440 [=============>................] - ETA: 0s
3840/7440 [==============>...............] - ETA: 0s
4160/7440 [===============>..............] - ETA: 0s
4480/7440 [=================>............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5120/7440 [===================>..........] - ETA: 0s
5440/7440 [====================>.........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7296/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 170us/step
Test accuracy: 0.8501344086021505
Test accuracy 0.6: 0.8293010752688172
auc_score ------------------>  0.9349427679500519
Saved model to disk
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_4[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_13[0][0]              
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_14[0][0]              
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 42, 96, 96)   0           concatenate_10[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_15[0][0]              
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 54, 96, 96)   0           concatenate_11[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_12[0][0]             
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_4 (Glo (None, 54)           0           activation_16[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            55          global_average_pooling2d_4[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5359 - acc: 0.7410 - val_loss: 0.4793 - val_acc: 0.8094
Epoch 2/20
 - 17s - loss: 0.4835 - acc: 0.7680 - val_loss: 3.0202 - val_acc: 0.5364
Epoch 3/20
 - 17s - loss: 0.4700 - acc: 0.7735 - val_loss: 0.7180 - val_acc: 0.7122
Epoch 4/20
 - 17s - loss: 0.4590 - acc: 0.7783 - val_loss: 0.5018 - val_acc: 0.7347

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 5/20
 - 17s - loss: 0.4281 - acc: 0.7949 - val_loss: 0.5267 - val_acc: 0.7382
Epoch 6/20
 - 17s - loss: 0.4235 - acc: 0.8005 - val_loss: 0.7659 - val_acc: 0.6832
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1216/7440 [===>..........................] - ETA: 1s
1536/7440 [=====>........................] - ETA: 1s
1856/7440 [======>.......................] - ETA: 0s
2176/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2816/7440 [==========>...................] - ETA: 0s
3104/7440 [===========>..................] - ETA: 0s
3424/7440 [============>.................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4032/7440 [===============>..............] - ETA: 0s
4320/7440 [================>.............] - ETA: 0s
4608/7440 [=================>............] - ETA: 0s
4896/7440 [==================>...........] - ETA: 0s
5184/7440 [===================>..........] - ETA: 0s
5472/7440 [=====================>........] - ETA: 0s
5760/7440 [======================>.......] - ETA: 0s
6048/7440 [=======================>......] - ETA: 0s
6336/7440 [========================>.....] - ETA: 0s
6624/7440 [=========================>....] - ETA: 0s
6912/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 176us/step
Test accuracy: 0.6831989247311828
Test accuracy 0.6: 0.7198924731182795
auc_score ------------------>  0.8516359911550468
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_5[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_17[0][0]              
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_18[0][0]              
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 42, 96, 96)   0           concatenate_13[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_19[0][0]              
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 54, 96, 96)   0           concatenate_14[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_15[0][0]             
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_5 (Glo (None, 54)           0           activation_20[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            55          global_average_pooling2d_5[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5448 - acc: 0.7351 - val_loss: 0.5615 - val_acc: 0.7958
Epoch 2/20
 - 17s - loss: 0.4891 - acc: 0.7612 - val_loss: 0.5497 - val_acc: 0.7410
Epoch 3/20
 - 17s - loss: 0.4635 - acc: 0.7768 - val_loss: 4.4225 - val_acc: 0.6187
Epoch 4/20
 - 17s - loss: 0.4539 - acc: 0.7855 - val_loss: 1.1982 - val_acc: 0.7078
Epoch 5/20
 - 17s - loss: 0.4402 - acc: 0.7908 - val_loss: 0.6247 - val_acc: 0.7874

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 6/20
 - 17s - loss: 0.4154 - acc: 0.8062 - val_loss: 0.7691 - val_acc: 0.7083
Epoch 00006: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 640/7440 [=>............................] - ETA: 1s
 960/7440 [==>...........................] - ETA: 1s
1280/7440 [====>.........................] - ETA: 1s
1600/7440 [=====>........................] - ETA: 0s
1920/7440 [======>.......................] - ETA: 0s
2208/7440 [=======>......................] - ETA: 0s
2496/7440 [=========>....................] - ETA: 0s
2816/7440 [==========>...................] - ETA: 0s
3136/7440 [===========>..................] - ETA: 0s
3424/7440 [============>.................] - ETA: 0s
3744/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5568/7440 [=====================>........] - ETA: 0s
5888/7440 [======================>.......] - ETA: 0s
6208/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6848/7440 [==========================>...] - ETA: 0s
7168/7440 [===========================>..] - ETA: 0s
7440/7440 [==============================] - 1s 171us/step
Test accuracy: 0.7083333333333334
Test accuracy 0.6: 0.6600806451612903
auc_score ------------------>  0.7129315527806683
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_6[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_21[0][0]              
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_16[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_22[0][0]              
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 42, 96, 96)   0           concatenate_16[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_17[0][0]             
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_23[0][0]              
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 54, 96, 96)   0           concatenate_17[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_18[0][0]             
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_6 (Glo (None, 54)           0           activation_24[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 1)            55          global_average_pooling2d_6[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5366 - acc: 0.7414 - val_loss: 1.0009 - val_acc: 0.7465
Epoch 2/20
 - 17s - loss: 0.4856 - acc: 0.7665 - val_loss: 0.6701 - val_acc: 0.6657
Epoch 3/20
 - 17s - loss: 0.4681 - acc: 0.7732 - val_loss: 1.3259 - val_acc: 0.5103
Epoch 4/20
 - 17s - loss: 0.4583 - acc: 0.7796 - val_loss: 0.6269 - val_acc: 0.6538
Epoch 5/20
 - 17s - loss: 0.4450 - acc: 0.7869 - val_loss: 0.5475 - val_acc: 0.7526
Epoch 6/20
 - 17s - loss: 0.4335 - acc: 0.7973 - val_loss: 0.7219 - val_acc: 0.7110
Epoch 7/20
 - 17s - loss: 0.4286 - acc: 0.7997 - val_loss: 0.6505 - val_acc: 0.6860
Epoch 8/20
 - 17s - loss: 0.4209 - acc: 0.8041 - val_loss: 0.4785 - val_acc: 0.7698
Epoch 9/20
 - 17s - loss: 0.4138 - acc: 0.8102 - val_loss: 0.6276 - val_acc: 0.7042
Epoch 10/20
 - 17s - loss: 0.4043 - acc: 0.8157 - val_loss: 0.5934 - val_acc: 0.7507
Epoch 11/20
 - 16s - loss: 0.3997 - acc: 0.8179 - val_loss: 0.6743 - val_acc: 0.7304

Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 12/20
 - 17s - loss: 0.3755 - acc: 0.8274 - val_loss: 0.5583 - val_acc: 0.7606
Epoch 13/20
 - 17s - loss: 0.3681 - acc: 0.8331 - val_loss: 0.5153 - val_acc: 0.7349
Epoch 00013: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 179us/step
Test accuracy: 0.7349462365591398
Test accuracy 0.6: 0.7401881720430108
auc_score ------------------>  0.8444973046016881
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_7[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_25[0][0]              
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_19[0][0]             
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_26[0][0]              
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 42, 96, 96)   0           concatenate_19[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_20[0][0]             
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_27[0][0]              
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 54, 96, 96)   0           concatenate_20[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_21[0][0]             
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_7 (Glo (None, 54)           0           activation_28[0][0]              
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 1)            55          global_average_pooling2d_7[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 19s - loss: 0.5424 - acc: 0.7374 - val_loss: 1.2492 - val_acc: 0.6512
Epoch 2/20
 - 17s - loss: 0.4978 - acc: 0.7614 - val_loss: 0.6177 - val_acc: 0.6816
Epoch 3/20
 - 17s - loss: 0.4808 - acc: 0.7690 - val_loss: 0.5412 - val_acc: 0.7734
Epoch 4/20
 - 18s - loss: 0.4637 - acc: 0.7787 - val_loss: 0.6006 - val_acc: 0.7331
Epoch 5/20
 - 18s - loss: 0.4538 - acc: 0.7845 - val_loss: 0.8920 - val_acc: 0.6759
Epoch 6/20
 - 18s - loss: 0.4425 - acc: 0.7931 - val_loss: 1.0109 - val_acc: 0.7376

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 7/20
 - 17s - loss: 0.4186 - acc: 0.8069 - val_loss: 0.4514 - val_acc: 0.8151
Epoch 8/20
 - 17s - loss: 0.4081 - acc: 0.8114 - val_loss: 0.4596 - val_acc: 0.7653
Epoch 9/20
 - 17s - loss: 0.4015 - acc: 0.8156 - val_loss: 0.4837 - val_acc: 0.7655
Epoch 10/20
 - 17s - loss: 0.3938 - acc: 0.8193 - val_loss: 0.5762 - val_acc: 0.7280

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0029999998973718025.
Epoch 11/20
 - 17s - loss: 0.3810 - acc: 0.8246 - val_loss: 0.5118 - val_acc: 0.7434
Epoch 12/20
 - 17s - loss: 0.3795 - acc: 0.8294 - val_loss: 0.4336 - val_acc: 0.7879
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4096/7440 [===============>..............] - ETA: 0s
4416/7440 [================>.............] - ETA: 0s
4736/7440 [==================>...........] - ETA: 0s
5024/7440 [===================>..........] - ETA: 0s
5344/7440 [====================>.........] - ETA: 0s
5632/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6272/7440 [========================>.....] - ETA: 0s
6560/7440 [=========================>....] - ETA: 0s
6880/7440 [==========================>...] - ETA: 0s
7200/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 175us/step
Test accuracy: 0.7879032258064517
Test accuracy 0.6: 0.7932795698924732
auc_score ------------------>  0.8762057029714417
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_8[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_29[0][0]              
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_22[0][0]             
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_30[0][0]              
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 42, 96, 96)   0           concatenate_22[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_23[0][0]             
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_31[0][0]              
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 54, 96, 96)   0           concatenate_23[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_24[0][0]             
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_8 (Glo (None, 54)           0           activation_32[0][0]              
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 1)            55          global_average_pooling2d_8[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5513 - acc: 0.7349 - val_loss: 0.5239 - val_acc: 0.7473
Epoch 2/20
 - 17s - loss: 0.4938 - acc: 0.7631 - val_loss: 2.2839 - val_acc: 0.6562
Epoch 3/20
 - 17s - loss: 0.4753 - acc: 0.7707 - val_loss: 0.4766 - val_acc: 0.8065
Epoch 4/20
 - 17s - loss: 0.4570 - acc: 0.7794 - val_loss: 0.5379 - val_acc: 0.8372
Epoch 5/20
 - 17s - loss: 0.4476 - acc: 0.7858 - val_loss: 0.6151 - val_acc: 0.7442
Epoch 6/20
 - 17s - loss: 0.4383 - acc: 0.7920 - val_loss: 0.9701 - val_acc: 0.6169

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 7/20
 - 17s - loss: 0.4116 - acc: 0.8049 - val_loss: 0.4399 - val_acc: 0.8043
Epoch 8/20
 - 17s - loss: 0.4014 - acc: 0.8118 - val_loss: 0.6293 - val_acc: 0.7052
Epoch 9/20
 - 17s - loss: 0.3980 - acc: 0.8146 - val_loss: 0.7855 - val_acc: 0.6833
Epoch 00009: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 640/7440 [=>............................] - ETA: 1s
 960/7440 [==>...........................] - ETA: 1s
1280/7440 [====>.........................] - ETA: 1s
1568/7440 [=====>........................] - ETA: 1s
1856/7440 [======>.......................] - ETA: 0s
2144/7440 [=======>......................] - ETA: 0s
2432/7440 [========>.....................] - ETA: 0s
2752/7440 [==========>...................] - ETA: 0s
3072/7440 [===========>..................] - ETA: 0s
3360/7440 [============>.................] - ETA: 0s
3648/7440 [=============>................] - ETA: 0s
3936/7440 [==============>...............] - ETA: 0s
4224/7440 [================>.............] - ETA: 0s
4512/7440 [=================>............] - ETA: 0s
4800/7440 [==================>...........] - ETA: 0s
5088/7440 [===================>..........] - ETA: 0s
5376/7440 [====================>.........] - ETA: 0s
5664/7440 [=====================>........] - ETA: 0s
5952/7440 [=======================>......] - ETA: 0s
6240/7440 [========================>.....] - ETA: 0s
6528/7440 [=========================>....] - ETA: 0s
6816/7440 [==========================>...] - ETA: 0s
7104/7440 [===========================>..] - ETA: 0s
7392/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 176us/step
Test accuracy: 0.6833333333333333
Test accuracy 0.6: 0.709274193548387
auc_score ------------------>  0.8354573144294138
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_9[0][0]                    
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_33[0][0]              
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_25[0][0]             
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_34[0][0]              
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 42, 96, 96)   0           concatenate_25[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_26[0][0]             
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_35[0][0]              
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 54, 96, 96)   0           concatenate_26[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_27[0][0]             
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_9 (Glo (None, 54)           0           activation_36[0][0]              
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 1)            55          global_average_pooling2d_9[0][0] 
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 18s - loss: 0.5408 - acc: 0.7365 - val_loss: 0.9683 - val_acc: 0.6984
Epoch 2/20
 - 17s - loss: 0.4914 - acc: 0.7592 - val_loss: 0.7329 - val_acc: 0.6700
Epoch 3/20
 - 17s - loss: 0.4700 - acc: 0.7699 - val_loss: 0.8966 - val_acc: 0.7735
Epoch 4/20
 - 17s - loss: 0.4562 - acc: 0.7784 - val_loss: 0.6940 - val_acc: 0.8060
Epoch 5/20
 - 17s - loss: 0.4493 - acc: 0.7859 - val_loss: 1.0709 - val_acc: 0.7751
Epoch 6/20
 - 17s - loss: 0.4430 - acc: 0.7876 - val_loss: 0.6946 - val_acc: 0.7589
Epoch 7/20
 - 17s - loss: 0.4346 - acc: 0.7944 - val_loss: 0.6005 - val_acc: 0.8453
Epoch 8/20
 - 17s - loss: 0.4330 - acc: 0.7949 - val_loss: 0.6575 - val_acc: 0.7343
Epoch 9/20
 - 17s - loss: 0.4251 - acc: 0.8013 - val_loss: 0.5431 - val_acc: 0.7534
Epoch 10/20
 - 17s - loss: 0.4238 - acc: 0.8018 - val_loss: 0.6517 - val_acc: 0.6919
Epoch 11/20
 - 17s - loss: 0.4167 - acc: 0.8025 - val_loss: 0.6898 - val_acc: 0.7492
Epoch 12/20
 - 17s - loss: 0.4104 - acc: 0.8082 - val_loss: 0.6154 - val_acc: 0.8075

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 00012: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4640/7440 [=================>............] - ETA: 0s
4928/7440 [==================>...........] - ETA: 0s
5216/7440 [====================>.........] - ETA: 0s
5504/7440 [=====================>........] - ETA: 0s
5792/7440 [======================>.......] - ETA: 0s
6080/7440 [=======================>......] - ETA: 0s
6368/7440 [========================>.....] - ETA: 0s
6656/7440 [=========================>....] - ETA: 0s
6944/7440 [===========================>..] - ETA: 0s
7232/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 181us/step
Test accuracy: 0.8075268817204301
Test accuracy 0.6: 0.8225806451612904
auc_score ------------------>  0.9033405957336109
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_10 (InputLayer)           (None, 2, 96, 96)    0                                            
__________________________________________________________________________________________________
initial_conv2D (Conv2D)         (None, 18, 96, 96)   324         input_10[0][0]                   
__________________________________________________________________________________________________
dense_0_0_bn (BatchNormalizatio (None, 18, 96, 96)   72          initial_conv2D[0][0]             
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 18, 96, 96)   0           dense_0_0_bn[0][0]               
__________________________________________________________________________________________________
dense_0_0_conv2D (Conv2D)       (None, 12, 96, 96)   1944        activation_37[0][0]              
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 30, 96, 96)   0           initial_conv2D[0][0]             
                                                                 dense_0_0_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_1_bn (BatchNormalizatio (None, 30, 96, 96)   120         concatenate_28[0][0]             
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 30, 96, 96)   0           dense_0_1_bn[0][0]               
__________________________________________________________________________________________________
dense_0_1_conv2D (Conv2D)       (None, 12, 96, 96)   3240        activation_38[0][0]              
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 42, 96, 96)   0           concatenate_28[0][0]             
                                                                 dense_0_1_conv2D[0][0]           
__________________________________________________________________________________________________
dense_0_2_bn (BatchNormalizatio (None, 42, 96, 96)   168         concatenate_29[0][0]             
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 42, 96, 96)   0           dense_0_2_bn[0][0]               
__________________________________________________________________________________________________
dense_0_2_conv2D (Conv2D)       (None, 12, 96, 96)   4536        activation_39[0][0]              
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 54, 96, 96)   0           concatenate_29[0][0]             
                                                                 dense_0_2_conv2D[0][0]           
__________________________________________________________________________________________________
final_bn (BatchNormalization)   (None, 54, 96, 96)   216         concatenate_30[0][0]             
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 54, 96, 96)   0           final_bn[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_10 (Gl (None, 54)           0           activation_40[0][0]              
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            55          global_average_pooling2d_10[0][0]
==================================================================================================
Total params: 10,675
Trainable params: 10,387
Non-trainable params: 288
__________________________________________________________________________________________________
Train on 39840 samples, validate on 7440 samples
Epoch 1/20
 - 19s - loss: 0.5607 - acc: 0.7287 - val_loss: 0.8632 - val_acc: 0.7140
Epoch 2/20
 - 17s - loss: 0.5046 - acc: 0.7548 - val_loss: 0.7690 - val_acc: 0.6172
Epoch 3/20
 - 17s - loss: 0.4745 - acc: 0.7700 - val_loss: 3.2543 - val_acc: 0.5000
Epoch 4/20
 - 17s - loss: 0.4591 - acc: 0.7780 - val_loss: 1.2836 - val_acc: 0.7293
Epoch 5/20
 - 17s - loss: 0.4474 - acc: 0.7871 - val_loss: 0.4754 - val_acc: 0.8078
Epoch 6/20
 - 17s - loss: 0.4364 - acc: 0.7927 - val_loss: 0.4563 - val_acc: 0.8226
Epoch 7/20
 - 17s - loss: 0.4271 - acc: 0.7982 - val_loss: 0.6959 - val_acc: 0.7036
Epoch 8/20
 - 17s - loss: 0.4194 - acc: 0.8024 - val_loss: 1.0000 - val_acc: 0.6648
Epoch 9/20
 - 17s - loss: 0.4155 - acc: 0.8070 - val_loss: 1.0250 - val_acc: 0.5860

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.009486832768457897.
Epoch 10/20
 - 17s - loss: 0.3888 - acc: 0.8218 - val_loss: 0.5759 - val_acc: 0.7563
Epoch 11/20
 - 17s - loss: 0.3801 - acc: 0.8268 - val_loss: 0.5488 - val_acc: 0.7577
Epoch 00011: early stopping

  32/7440 [..............................] - ETA: 1s
 320/7440 [>.............................] - ETA: 1s
 608/7440 [=>............................] - ETA: 1s
 896/7440 [==>...........................] - ETA: 1s
1184/7440 [===>..........................] - ETA: 1s
1472/7440 [====>.........................] - ETA: 1s
1760/7440 [======>.......................] - ETA: 1s
2048/7440 [=======>......................] - ETA: 0s
2336/7440 [========>.....................] - ETA: 0s
2624/7440 [=========>....................] - ETA: 0s
2912/7440 [==========>...................] - ETA: 0s
3200/7440 [===========>..................] - ETA: 0s
3488/7440 [=============>................] - ETA: 0s
3776/7440 [==============>...............] - ETA: 0s
4064/7440 [===============>..............] - ETA: 0s
4352/7440 [================>.............] - ETA: 0s
4672/7440 [=================>............] - ETA: 0s
4960/7440 [===================>..........] - ETA: 0s
5248/7440 [====================>.........] - ETA: 0s
5536/7440 [=====================>........] - ETA: 0s
5824/7440 [======================>.......] - ETA: 0s
6112/7440 [=======================>......] - ETA: 0s
6400/7440 [========================>.....] - ETA: 0s
6688/7440 [=========================>....] - ETA: 0s
6976/7440 [===========================>..] - ETA: 0s
7264/7440 [============================>.] - ETA: 0s
7440/7440 [==============================] - 1s 177us/step
Test accuracy: 0.7576612903225807
Test accuracy 0.6: 0.7782258064516129
auc_score ------------------>  0.908210414498786
[0.905, 0.876, 0.935, 0.852, 0.713, 0.844, 0.876, 0.835, 0.903, 0.908]
0.865 ± 0.059
